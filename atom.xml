<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>watson&#39;blogs</title>
  
  
  <link href="https://watsonlu6.github.io/atom.xml" rel="self"/>
  
  <link href="https://watsonlu6.github.io/"/>
  <updated>2024-09-08T09:47:49.358Z</updated>
  <id>https://watsonlu6.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Cpeh PG常见故障处理</title>
    <link href="https://watsonlu6.github.io/Ceph-PG%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"/>
    <id>https://watsonlu6.github.io/Ceph-PG%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</id>
    <published>2022-10-08T06:53:31.000Z</published>
    <updated>2024-09-08T09:47:49.358Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PG故障排除"><a href="#PG故障排除" class="headerlink" title="PG故障排除"></a>PG故障排除</h2><h4 id="PG-永远无法变干净"><a href="#PG-永远无法变干净" class="headerlink" title="PG 永远无法变干净"></a>PG 永远无法变干净</h4><p>如果在创建集群后，任何 PG 保持在 active 状态、active+remapped 状态或 active+degraded 状态，并且永远无法达到 active+clean 状态，那么可能是配置出现了问题。在这种情况下，可能需要查看 Pool、PG 和 CRUSH 配置参考的设置，并进行适当调整。一般来说，运行集群时应使用多个 OSD，并且池的大小应大于两个对象副本。</p><ol><li><p><strong>单节点集群</strong><br>  Ceph 不再提供单节点操作的文档。按定义，分布式计算系统不应在单节点上运行。在包含 Ceph 守护进程的单节点上安装客户端内核模块可能会由于 Linux 内核本身的问题而导致死锁（除非使用虚拟机作为客户端）。尽管有上述限制，您仍可以在单节点配置中实验 Ceph。<br>  要在单节点上创建集群，必须在创建监视器和 OSD 之前，将 Ceph 配置文件中的 <code>osd_crush_chooseleaf_type</code> 设置从默认的 1（表示主机或节点）更改为 0（表示 OSD）。这告诉 Ceph 允许在同一主机上放置另一个 OSD。如果您尝试设置单节点集群，并且 <code>osd_crush_chooseleaf_type</code> 大于 0，Ceph 将尝试将一个 OSD 的 PG 与另一个 OSD 的 PG 放置在另一个节点、机箱、机架、行或数据中心中，具体取决于设置。<br>  <strong>提示</strong><br>  不要将内核客户端直接安装在与 Ceph 存储集群相同的节点上。这样可能会产生内核冲突。然而，您可以在单节点上通过虚拟机 (VMs) 安装内核客户端。<br>  如果您使用单个磁盘创建 OSD，则必须先手动创建数据目录。</p></li><li><p><strong>OSDs 少于副本</strong><br>  如果两个 OSD 处于 up 和 in 状态，但 PG 并未处于 active + clean 状态，可能是 <code>osd_pool_default_size</code> 设置大于 2。<br>  解决这种情况有几种方法。如果您想以 active + degraded 状态运行集群并保持两个副本，您可以将 <code>osd_pool_default_min_size</code> 设置为 2，这样可以在 active + degraded 状态下写入对象。您也可以将 <code>osd_pool_default_size</code> 设置为 2，这样只会有两个存储副本（原始副本和一个副本）。在这种情况下，集群应达到 active + clean 状态。<br>  <strong>注意</strong><br>  您可以在集群运行时进行更改。如果您在 Ceph 配置文件中进行更改，可能需要重新启动集群。</p></li><li><p><strong>POOL SIZE &#x3D; 1</strong><br>  如果将 <code>osd_pool_default_size</code> 设置为 1，则对象只有一个副本。OSDs 依赖其他 OSDs 来告诉它们应该拥有哪些对象。如果一个 OSD 有一个对象副本而没有第二个副本，那么就没有第二个 OSD 来告诉第一个 OSD 它应该拥有这个副本。对于映射到第一个 OSD 的每个 PG（请参见 <code>ceph pg dump</code>），您可以通过运行以下命令来强制第一个 OSD 注意它需要的 PG：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd force-create-pg &lt;pgid&gt;</span><br></pre></td></tr></table></figure></li><li><p><strong>CRUSH MAP 错误</strong><br>  如果集群中的任何 PG 不干净，则可能是 CRUSH 映射中存在错误。</p></li></ol><h4 id="PG卡住"><a href="#PG卡住" class="headerlink" title="PG卡住"></a>PG卡住</h4><p>PG 进入“degraded”或“peering”状态在组件故障后是正常的。这些状态通常反映了故障恢复过程中的预期进展。然而，若一个 PG 长时间停留在这些状态中，可能是更大问题的迹象。因此，Ceph 监视器会在 PG “卡住”在非最佳状态时发出警告。具体检查的状态包括：</p><ul><li><p><strong>inactive</strong> - PG 长时间未处于活跃状态（即无法处理读写请求）。</p></li><li><p><strong>unclean</strong> - PG 长时间未处于干净状态（即无法完全从先前的故障中恢复）。</p></li><li><p><strong>stale</strong> - PG 状态未被 ceph-osd 更新。这表明存储该 PG 的所有节点可能都已宕机。<br>通过运行以下命令来列出卡住的PG：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump_stuck stale</span><br><span class="line">ceph pg dump_stuck inactive</span><br><span class="line">ceph pg dump_stuck unclean</span><br></pre></td></tr></table></figure></li><li><p><strong>卡住的 stale PG</strong> 通常表明关键的 ceph-osd 守护进程未运行。</p></li><li><p><strong>卡住的 inactive PG</strong> 通常表明存在 peering 问题（见 PG Down - Peering Failure）。</p></li><li><p><strong>卡住的 unclean PG</strong> 通常表明某些因素阻止了恢复完成，可能是未找到的对象（见 Unfound Objects）。</p></li></ul><h4 id="PG-下线-对等连接失败"><a href="#PG-下线-对等连接失败" class="headerlink" title="PG 下线 - 对等连接失败"></a>PG 下线 - 对等连接失败</h4><p>在某些情况下，ceph-osd peering 过程可能会遇到问题，导致 PG 无法变为活跃和可用。在这种情况下，运行 <code>ceph health detail</code> 命令会报告类似以下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">HEALTH_ERR 7 pgs degraded; 12 pgs down; 12 pgs peering; 1 pgs recovering; 6 pgs stuck unclean; 114/3300 degraded (3.455%); 1/3 <span class="keyword">in</span> osds are down</span><br><span class="line">...</span><br><span class="line">pg 0.5 is down+peering</span><br><span class="line">pg 1.4 is down+peering</span><br><span class="line">...</span><br><span class="line">osd.1 is down since epoch 69, last address 192.168.106.220:6801/8651</span><br></pre></td></tr></table></figure><p>查询集群以确定为什么 PG 被标记为 down，可以运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">ceph pg 0.5 query</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;state&quot;</span>: <span class="string">&quot;down+peering&quot;</span>,</span><br><span class="line">  ...</span><br><span class="line">  <span class="string">&quot;recovery_state&quot;</span>: [</span><br><span class="line">       &#123; <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Started\/Primary\/Peering\/GetInfo&quot;</span>,</span><br><span class="line">         <span class="string">&quot;enter_time&quot;</span>: <span class="string">&quot;2012-03-06 14:40:16.169679&quot;</span>,</span><br><span class="line">         <span class="string">&quot;requested_info_from&quot;</span>: []&#125;,</span><br><span class="line">       &#123; <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Started\/Primary\/Peering&quot;</span>,</span><br><span class="line">         <span class="string">&quot;enter_time&quot;</span>: <span class="string">&quot;2012-03-06 14:40:16.169659&quot;</span>,</span><br><span class="line">         <span class="string">&quot;probing_osds&quot;</span>: [</span><br><span class="line">               0,</span><br><span class="line">               1],</span><br><span class="line">         <span class="string">&quot;blocked&quot;</span>: <span class="string">&quot;peering is blocked due to down osds&quot;</span>,</span><br><span class="line">         <span class="string">&quot;down_osds_we_would_probe&quot;</span>: [</span><br><span class="line">               1],</span><br><span class="line">         <span class="string">&quot;peering_blocked_by&quot;</span>: [</span><br><span class="line">               &#123; <span class="string">&quot;osd&quot;</span>: 1,</span><br><span class="line">                 <span class="string">&quot;current_lost_at&quot;</span>: 0,</span><br><span class="line">                 <span class="string">&quot;comment&quot;</span>: <span class="string">&quot;starting or marking this osd lost may let us proceed&quot;</span>&#125;]&#125;,</span><br><span class="line">       &#123; <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Started&quot;</span>,</span><br><span class="line">         <span class="string">&quot;enter_time&quot;</span>: <span class="string">&quot;2012-03-06 14:40:16.169513&quot;</span>&#125;</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>recovery_state</code> 部分告诉我们 peering 被阻止是由于 ceph-osd 守护进程宕机，具体是 osd.1。在这种情况下，我们可以启动该 ceph-osd，恢复将继续进行。或者，如果 osd.1 发生灾难性故障（例如磁盘故障），可以告知集群该 OSD 已丢失，并指示集群尽可能应对。</p><p><strong>重要</strong><br>告知集群一个 OSD 已丢失是危险的，因为集群不能保证其他副本的数据是一致和最新的。<br>要报告 OSD 丢失并指示 Ceph 继续尝试恢复，可以运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd lost 1</span><br></pre></td></tr></table></figure><p>恢复将继续进行。</p><h4 id="未找到的对象"><a href="#未找到的对象" class="headerlink" title="未找到的对象"></a>未找到的对象</h4><p>在某些故障组合下，Ceph 可能会报告未找到的对象，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">HEALTH_WARN 1 pgs degraded; 78/3778 unfound (2.065%)</span><br><span class="line">pg 2.4 is active+degraded, 78 unfound</span><br></pre></td></tr></table></figure><p>这意味着存储集群知道一些对象（或现有对象的新副本）存在，但没有找到它们的副本。以下是这种情况可能发生的示例：一个 PG 的数据在两个 OSD 上，我们称它们为“1”和“2”：</p><ol><li>OSD 1 发生故障。</li><li>OSD 2 单独处理一些写操作。</li><li>OSD 1 重新上线。</li><li>OSD 1 和 OSD 2 重新进行 peering，O<strong>SD 1 上缺失的对象被排队等待恢复。</strong></li><li>在新的对象被复制之前，OSD 2 发生故障。<br>此时，OSD 1 知道这些对象存在，但没有活跃的 ceph-osd 拥有这些对象的副本。在这种情况下，对这些对象的 IO 请求将被阻塞，集群希望故障节点尽快恢复。这被认为比直接返回 IO 错误给用户更可取。</li></ol><p><strong>注意</strong><br>上述情况是将 <code>size=2</code> 设置在复制池和 <code>m=1</code> 设置在纠删码池时可能导致数据丢失的原因之一。</p><p>通过运行以下命令来识别哪些对象未找到：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">ceph pg 2.4 list_unfound [starting offset, <span class="keyword">in</span> json]</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;num_missing&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;num_unfound&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;objects&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="string">&quot;oid&quot;</span>: &#123;</span><br><span class="line">              <span class="string">&quot;oid&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">              <span class="string">&quot;key&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">              <span class="string">&quot;snapid&quot;</span>: -2,</span><br><span class="line">              <span class="string">&quot;hash&quot;</span>: 2249616407,</span><br><span class="line">              <span class="string">&quot;max&quot;</span>: 0,</span><br><span class="line">              <span class="string">&quot;pool&quot;</span>: 2,</span><br><span class="line">              <span class="string">&quot;namespace&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="string">&quot;need&quot;</span>: <span class="string">&quot;43&#x27;251&quot;</span>,</span><br><span class="line">          <span class="string">&quot;have&quot;</span>: <span class="string">&quot;0&#x27;0&quot;</span>,</span><br><span class="line">          <span class="string">&quot;flags&quot;</span>: <span class="string">&quot;none&quot;</span>,</span><br><span class="line">          <span class="string">&quot;clean_regions&quot;</span>: <span class="string">&quot;clean_offsets: [], clean_omap: 0, new_object: 1&quot;</span>,</span><br><span class="line">          <span class="string">&quot;locations&quot;</span>: [</span><br><span class="line">              <span class="string">&quot;0(3)&quot;</span>,</span><br><span class="line">              <span class="string">&quot;4(2)&quot;</span></span><br><span class="line">          ]</span><br><span class="line">      &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;state&quot;</span>: <span class="string">&quot;NotRecovering&quot;</span>,</span><br><span class="line">  <span class="string">&quot;available_might_have_unfound&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">&quot;might_have_unfound&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">          <span class="string">&quot;osd&quot;</span>: <span class="string">&quot;2(4)&quot;</span>,</span><br><span class="line">          <span class="string">&quot;status&quot;</span>: <span class="string">&quot;osd is down&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;more&quot;</span>: <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果单次结果中列出的对象过多，<code>more</code> 字段将为 <code>true</code>，你可以查询更多信息。（最终命令行工具将会隐藏这一点，但现在还没有。）<br>接下来，你可以识别哪些 OSD 已被探测或可能包含数据。<br>在列表的末尾（在 <code>more: false</code> 之前），<code>might_have_unfound</code> 是在 <code>available_might_have_unfound</code> 为 <code>true</code> 时提供的。这相当于 <code>ceph pg #.# query</code> 的输出。它可以避免直接使用 <code>query</code>。提供的 <code>might_have_unfound</code> 信息与 <code>query</code> 的输出方式相同，仅不同的是，状态为“已探测”的 OSD 会被忽略。<br><strong>查询的使用：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph pg 2.4 query</span><br><span class="line"><span class="string">&quot;recovery_state&quot;</span>: [</span><br><span class="line">     &#123; <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Started\/Primary\/Active&quot;</span>,</span><br><span class="line">       <span class="string">&quot;enter_time&quot;</span>: <span class="string">&quot;2012-03-06 15:15:46.713212&quot;</span>,</span><br><span class="line">       <span class="string">&quot;might_have_unfound&quot;</span>: [</span><br><span class="line">             &#123; <span class="string">&quot;osd&quot;</span>: 1,</span><br><span class="line">               <span class="string">&quot;status&quot;</span>: <span class="string">&quot;osd is down&quot;</span>&#125;]&#125;</span><br></pre></td></tr></table></figure><p>在这种情况下，集群知道 osd.1 可能有数据，但它已宕机。以下是可能的状态范围：</p><ul><li>已探测</li><li>正在查询</li><li>OSD 已宕机</li><li>尚未查询</li></ul><p>有时集群需要一些时间来查询可能的位置。可能还有其他未列出的对象可能存在的位置。例如：如果一个 OSD 停止并从集群中移除，然后集群完全恢复，然后通过随后的故障集群最终出现未找到的对象，集群将忽略已移除的 OSD。（然而，这种情况不太可能发生。）</p><p>如果所有可能的位置都已被查询，且对象仍然丢失，你可能需要放弃这些丢失的对象。这仅在发生了不寻常的故障组合，导致集群在写入之前了解了写入操作的情况时才可能发生。要将“未找到”的对象标记为“丢失”，运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg 2.5 mark_unfound_lost revert|delete</span><br></pre></td></tr></table></figure><p>这里的最后一个参数（<code>revert|delete</code>）指定了集群应如何处理丢失的对象。</p><ul><li><code>delete</code> 选项将使集群完全忘记这些对象。</li><li><code>revert</code> 选项（对于纠删码池不可用）将回滚到对象的先前版本，或（如果它是新对象）完全忘记该对象。使用 <code>revert</code> 时请小心，因为它可能会混淆期望对象存在的应用程序。</li></ul><h4 id="无家-PG"><a href="#无家-PG" class="headerlink" title="无家 PG"></a>无家 PG</h4><p>如果所有具有特定 PG 副本的 OSD 都发生故障，那么包含这些 PG 的对象存储子集将变得不可用，监视器将无法接收到这些 PG 的状态更新。监视器会将主 OSD 已故障的任何 PG 标记为 stale。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph health</span><br><span class="line">HEALTH_WARN 24 pgs stale; 3/300 <span class="keyword">in</span> osds are down</span><br></pre></td></tr></table></figure><p>通过运行以下命令来识别哪些 PG 是 stale 的，以及最后一个存储这些 stale PG 的 OSD 是哪些：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">HEALTH_WARN 24 pgs stale; 3/300 <span class="keyword">in</span> osds are down</span><br><span class="line">...</span><br><span class="line">pg 2.5 is stuck stale+active+remapped, last acting [2,0]</span><br><span class="line">...</span><br><span class="line">osd.10 is down since epoch 23, last address 192.168.106.220:6800/11080</span><br><span class="line">osd.11 is down since epoch 13, last address 192.168.106.220:6803/11539</span><br><span class="line">osd.12 is down since epoch 24, last address 192.168.106.220:6806/11861</span><br></pre></td></tr></table></figure><p>此输出表明 PG 2.5（pg 2.5）最后由 osd.0 和 osd.2 管理。重启这些 OSD 以允许集群恢复该 PG。</p><h4 id="只有少数-OSD-接收数据"><a href="#只有少数-OSD-接收数据" class="headerlink" title="只有少数 OSD 接收数据"></a>只有少数 OSD 接收数据</h4><p>如果集群中的只有少数节点在接收数据，请检查池中的 PG 数量，如 PG 文档中所述。由于 PG 会在涉及将集群中的 PG 数量除以集群中 OSD 数量的操作中映射到 OSD，因此在这种操作中，少量的 PG（余数）有时不会在集群中分布。在这种情况下，创建一个 PG 数量是 OSD 数量倍数的池。有关详细信息，请参见 PG。有关如何更改用于确定每个池分配多少 PG 的默认值的说明，请参见 Pool、PG 和 CRUSH 配置参考。</p><h4 id="无法写入数据"><a href="#无法写入数据" class="headerlink" title="无法写入数据"></a>无法写入数据</h4><p>如果集群正常运行，但一些 OSD 已经关闭且无法写入数据，请确保在池中运行了最小数量的 OSD。如果池中没有运行最小数量的 OSD，Ceph 不会允许你向其写入数据，因为无法保证 Ceph 可以复制你的数据。有关详细信息，请参见 Pool、PG 和 CRUSH 配置参考中的 <code>osd_pool_default_min_size</code>。</p><h3 id="PG-状态不一致"><a href="#PG-状态不一致" class="headerlink" title="PG 状态不一致"></a>PG 状态不一致</h3><p>如果命令 <code>ceph health detail</code> 返回 active + clean + inconsistent 状态，这可能表示在清理过程中发生了错误。通过运行以下命令来识别不一致的 PG：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph health detail</span><br><span class="line">HEALTH_ERR 1 pgs inconsistent; 2 scrub errors</span><br><span class="line">pg 0.6 is active+clean+inconsistent, acting [0,1,2]</span><br><span class="line">2 scrub errors</span><br></pre></td></tr></table></figure><p>或者，如果你希望以编程方式检查输出，可以运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rados list-inconsistent-pg rbd</span><br><span class="line">[<span class="string">&quot;0.6&quot;</span>]</span><br></pre></td></tr></table></figure><p>在最坏的情况下，我们可能会在多个对象中发现不同的不一致。如果 PG 0.6 中名为 foo 的对象被截断，<code>rados list-inconsistent-pg rbd</code> 的输出可能类似于：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">rados list-inconsistent-obj <span class="number">0.6</span> --format=json-pretty</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;epoch&quot;</span><span class="punctuation">:</span> <span class="number">14</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;inconsistents&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;foo&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;nspace&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;locator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;snap&quot;</span><span class="punctuation">:</span> <span class="string">&quot;head&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">1</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;errors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;data_digest_mismatch&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;size_mismatch&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;union_shard_errors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;data_digest_mismatch_info&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;size_mismatch_info&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;selected_object_info&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0:602f83fe:::foo:head(16&#x27;1 client.4110.0:1 dirty|data_digest|omap_digest s 968 uv 1 dd e978e67f od ffffffff alloc_hint [0 0 0])&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;shards&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;osd&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;errors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">968</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;omap_digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0xffffffff&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;data_digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0xe978e67f&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;osd&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;errors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">968</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;omap_digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0xffffffff&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;data_digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0xe978e67f&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;osd&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;errors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                        <span class="string">&quot;data_digest_mismatch_info&quot;</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="string">&quot;size_mismatch_info&quot;</span></span><br><span class="line">                    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;omap_digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0xffffffff&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;data_digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0xffffffff&quot;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>在这种情况下，输出表示以下内容：</p><ul><li>唯一的不一致对象名为 foo，其头部存在不一致。</li><li>不一致分为两类：<ul><li><code>errors</code>：这些错误表示分片之间的不一致，没有指示哪些分片有问题。检查 <code>shards</code> 数组中的错误，以确定问题所在。<ul><li><code>data_digest_mismatch</code>：从 OSD.2 读取的副本摘要与从 OSD.0 和 OSD.1 读取的副本摘要不同。</li><li><code>size_mismatch</code>：从 OSD.2 读取的副本大小为 0，但 OSD.0 和 OSD.1 报告的大小为 968。</li></ul></li><li><code>union_shard_errors</code>：分片数组中所有特定分片错误的联合。这些错误包括 <code>read_error</code> 和其他类似错误。以 <code>oi</code> 结尾的错误表示与 <code>selected_object_info</code> 进行比较。检查 <code>shards</code> 数组以确定哪些分片存在哪些错误。<ul><li><code>data_digest_mismatch_info</code>：对象信息中存储的摘要不是 <code>0xffffffff</code>，这是从 OSD.2 读取的分片计算出的。</li><li><code>size_mismatch_info</code>：对象信息中存储的大小与从 OSD.2 读取的大小不同，后者为 0。</li></ul></li></ul></li></ul><p><strong>警告</strong>：如果 <code>read_error</code> 列在分片的错误属性中，可能是由于物理存储错误导致的不一致。在这种情况下，请检查该 OSD 使用的存储。</p><p>在尝试修复驱动器之前，请检查 dmesg 和 smartctl 的输出。</p><p>要修复不一致的 PG，运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg repair &#123;placement-group-ID&#125;</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg repair 1.4</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：PG ID 形式为 N.xxxxx，其中 N 是包含 PG 的池的编号。命令 <code>ceph osd listpools</code> 和 <code>ceph osd dump | grep pool</code> 返回池编号的列表。</p><p>如果由于时钟偏差而定期收到 active + clean + inconsistent 状态，请考虑配置监视主机上的 NTP 守护进程以充当对等体。有关更多信息，请参见《网络时间协议和 Ceph 时钟设置》。</p><h4 id="关于-PG-修复的更多信息"><a href="#关于-PG-修复的更多信息" class="headerlink" title="关于 PG 修复的更多信息"></a>关于 PG 修复的更多信息</h4><p>Ceph 存储并更新集群中存储对象的校验和。当在 PG 上执行清理时，主 OSD 尝试从其副本中选择一个权威副本。只有一个可能的情况是一致的。执行深度清理后，Ceph 计算从磁盘读取的每个对象的校验和，并将其与先前记录的校验和进行比较。如果当前校验和与先前记录的校验和不匹配，则该不匹配被视为不一致。在复制池的情况下，任何副本的校验和与权威副本的校验和之间的任何不匹配都意味着存在不一致。发现这些不一致会导致 PG 状态被设置为不一致。</p><p><code>pg repair</code> 命令尝试修复各种类型的不一致。当 <code>pg repair</code> 发现不一致的 PG 时，它尝试用权威副本的摘要覆盖不一致副本的摘要。当 <code>pg repair</code> 在复制池中发现不一致副本时，它将不一致副本标记为丢失。在复制池的情况下，恢复超出了 <code>pg repair</code> 的范围。</p><p>对于编码和 BlueStore 池，Ceph 会自动执行修复，如果 <code>osd_scrub_auto_repair</code>（默认值为 false）设置为 true 并且发现的错误不超过 <code>osd_scrub_auto_repair_num_errors</code>（默认值为 5）。</p><p><code>pg repair</code> 命令不会解决所有问题。Ceph 不会自动修复发现有不一致的 PG。</p><p>RADOS 对象或 omap 的校验和并不总是可用。校验和是逐步计算的。如果一个复制对象非顺序更新，涉及的写入操作会更改对象并使其校验和无效。在重新计算校验和时不会读取整个对象。即使校验和不可用，<code>pg repair</code> 命令也能够进行修复，如 Filestore 中的情况。使用复制 Filestore 池的用户可能会更倾向于手动修复，而不是使用 <code>ceph pg repair</code>。</p><p>该材料适用于 Filestore，但不适用于 BlueStore，后者具有自己的内部校验和。匹配记录的校验和和计算的校验和不能证明任何特定副本实际上是权威的。如果没有校验和可用，<code>pg repair</code> 倾向于主数据，但这可能不是未损坏的副本。因此，在发现不一致时需要人工干预。这种干预有时涉及使用 <code>ceph-objectstore-tool</code>。</p><h4 id="编码池-PG-不是-active-clean"><a href="#编码池-PG-不是-active-clean" class="headerlink" title="编码池 PG 不是 active+clean"></a>编码池 PG 不是 active+clean</h4><p>如果 CRUSH 无法找到足够的 OSD 映射到 PG，它将显示为 2147483647，这是 ITEM_NONE 或未找到 OSD。例如：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2,1,6,0,5,8,2147483647,7,4]</span><br></pre></td></tr></table></figure><p>OSD 数量不足</p><p>如果 Ceph 集群只有八个 OSD，而一个编码池需要九个 OSD，则集群会显示“OSD 不足”。在这种情况下，你可以创建</p><p>另一个需要更少 OSD 的编码池，运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd erasure-code-profile <span class="built_in">set</span> myprofile k=5 m=3</span><br><span class="line">ceph osd pool create erasurepool erasure myprofile</span><br></pre></td></tr></table></figure><p>或者添加新的 OSD，PG 将自动使用它们。</p><h3 id="CRUSH-约束无法满足"><a href="#CRUSH-约束无法满足" class="headerlink" title="CRUSH 约束无法满足"></a>CRUSH 约束无法满足</h3><p>如果集群中有足够的 OSD，可能是 CRUSH 规则施加了无法满足的约束。如果两个主机上有十个 OSD，而 CRUSH 规则要求同一主机上的两个 OSD 不得在同一 PG 中使用，则映射可能失败，因为只会找到两个 OSD。通过显示（“转储”）规则来检查约束，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush rule <span class="built_in">ls</span></span><br><span class="line">[</span><br><span class="line">    <span class="string">&quot;replicated_rule&quot;</span>,</span><br><span class="line">    <span class="string">&quot;erasurepool&quot;</span>]</span><br><span class="line">$ ceph osd crush rule dump erasurepool</span><br><span class="line">&#123; <span class="string">&quot;rule_id&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;rule_name&quot;</span>: <span class="string">&quot;erasurepool&quot;</span>,</span><br><span class="line">  <span class="string">&quot;type&quot;</span>: 3,</span><br><span class="line">  <span class="string">&quot;steps&quot;</span>: [</span><br><span class="line">        &#123; <span class="string">&quot;op&quot;</span>: <span class="string">&quot;take&quot;</span>,</span><br><span class="line">          <span class="string">&quot;item&quot;</span>: -1,</span><br><span class="line">          <span class="string">&quot;item_name&quot;</span>: <span class="string">&quot;default&quot;</span>&#125;,</span><br><span class="line">        &#123; <span class="string">&quot;op&quot;</span>: <span class="string">&quot;chooseleaf_indep&quot;</span>,</span><br><span class="line">          <span class="string">&quot;num&quot;</span>: 0,</span><br><span class="line">          <span class="string">&quot;type&quot;</span>: <span class="string">&quot;host&quot;</span>&#125;,</span><br><span class="line">        &#123; <span class="string">&quot;op&quot;</span>: <span class="string">&quot;emit&quot;</span>&#125;]&#125;</span><br></pre></td></tr></table></figure><p>通过运行以下命令解决此问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd erasure-code-profile <span class="built_in">set</span> myprofile crush-failure-domain=osd</span><br><span class="line">ceph osd pool create erasurepool erasure myprofile</span><br></pre></td></tr></table></figure><h3 id="CRUSH-过早放弃"><a href="#CRUSH-过早放弃" class="headerlink" title="CRUSH 过早放弃"></a>CRUSH 过早放弃</h3><p>如果 Ceph 集群只有足够的 OSD 来映射 PG（例如，总共九个 OSD 的集群和每个 PG 需要九个 OSD 的编码池），则可能 CRUSH 在找到映射之前就放弃了。可以通过以下方式解决此问题：</p><ul><li>降低编码池的要求，以使用更少的 OSD（这需要创建另一个池，因为编码配置文件不能动态修改）。</li><li>向集群中添加更多 OSD（这不需要修改编码池，因为它会自动变干净）。</li><li>使用手动创建的 CRUSH 规则，尝试更多次找到合适的映射。可以通过设置 <code>set_choose_tries</code> 为大于默认值的值来修改现有的 CRUSH 规则。</li></ul><p>首先，通过在从集群中提取 crushmap 后使用 <code>crushtool</code> 验证问题。这可以确保你的实验不会修改 Ceph 集群，只在本地文件上操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush rule dump erasurepool</span><br><span class="line">&#123; <span class="string">&quot;rule_id&quot;</span>: 1,</span><br><span class="line">  <span class="string">&quot;rule_name&quot;</span>: <span class="string">&quot;erasurepool&quot;</span>,</span><br><span class="line">  <span class="string">&quot;type&quot;</span>: 3,</span><br><span class="line">  <span class="string">&quot;steps&quot;</span>: [</span><br><span class="line">        &#123; <span class="string">&quot;op&quot;</span>: <span class="string">&quot;take&quot;</span>,</span><br><span class="line">          <span class="string">&quot;item&quot;</span>: -1,</span><br><span class="line">          <span class="string">&quot;item_name&quot;</span>: <span class="string">&quot;default&quot;</span>&#125;,</span><br><span class="line">        &#123; <span class="string">&quot;op&quot;</span>: <span class="string">&quot;chooseleaf_indep&quot;</span>,</span><br><span class="line">          <span class="string">&quot;num&quot;</span>: 0,</span><br><span class="line">          <span class="string">&quot;type&quot;</span>: <span class="string">&quot;host&quot;</span>&#125;,</span><br><span class="line">        &#123; <span class="string">&quot;op&quot;</span>: <span class="string">&quot;emit&quot;</span>&#125;]&#125;</span><br><span class="line">$ ceph osd getcrushmap &gt; crush.map</span><br><span class="line">got crush map from osdmap epoch 13</span><br><span class="line">$ crushtool -i crush.map --<span class="built_in">test</span> --show-bad-mappings \</span><br><span class="line">   --rule 1 \</span><br><span class="line">   --num-rep 9 \</span><br><span class="line">   --min-x 1 --max-x $((<span class="number">1024</span> * <span class="number">1024</span>))</span><br><span class="line">bad mapping rule 8 x 43 num_rep 9 result [3,2,7,1,2147483647,8,5,6,0]</span><br><span class="line">bad mapping rule 8 x 79 num_rep 9 result [6,0,2,1,4,7,2147483647,5,8]</span><br><span class="line">bad mapping rule 8 x 173 num_rep 9 result [0,4,6,8,2,1,3,7,2147483647]</span><br></pre></td></tr></table></figure><p>在这里，<code>--num-rep</code> 是编码规则需要的 OSD 数量，<code>--rule</code> 是 <code>ceph osd crush rule dump</code> 显示的 <code>rule_id</code> 值。此测试将尝试映射一百万个值（在此示例中，范围定义为 [–min-x,–max-x]），并且必须显示至少一个错误映射。如果此测试没有输出任何内容，则表示所有映射都成功，你可以确定集群中的问题不是由于坏映射造成的。</p><h3 id="更改-set-choose-tries-的值"><a href="#更改-set-choose-tries-的值" class="headerlink" title="更改 set_choose_tries 的值"></a>更改 <code>set_choose_tries</code> 的值</h3><p>将 CRUSH 映射解压以编辑 CRUSH 规则，运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crushtool --decompile crush.map &gt; crush.txt</span><br></pre></td></tr></table></figure><p>在规则中添加以下行：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">step set_choose_tries 100</span><br></pre></td></tr></table></figure><p><code>crush.txt</code> 文件的相关部分将类似于：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rule erasurepool &#123;</span><br><span class="line">        id 1</span><br><span class="line">        type erasure</span><br><span class="line">        step set_chooseleaf_tries 5</span><br><span class="line">        step set_choose_tries 100</span><br><span class="line">        step take default</span><br><span class="line">        step chooseleaf indep 0 type host</span><br><span class="line">        step emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重新编译并重新测试 CRUSH 规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crushtool --compile crush.txt -o better-crush.map</span><br></pre></td></tr></table></figure><p>当所有映射成功时，通过使用 <code>crushtool</code> 命令的 <code>--show-choose-tries</code> 选项显示找到所有映射所需的尝试次数的直方图，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">crushtool -i better-crush.map --<span class="built_in">test</span> --show-bad-mappings \</span><br><span class="line">   --show-choose-tries \</span><br><span class="line">   --rule 1 \</span><br><span class="line">   --num-rep 9 \</span><br><span class="line">   --min-x 1 --max-x $((<span class="number">1024</span> * <span class="number">1024</span>))</span><br><span class="line">  ...</span><br><span class="line">  11:        42</span><br><span class="line">  12:        44</span><br><span class="line">  13:        54</span><br><span class="line">  14:        45</span><br><span class="line">  15:        35</span><br><span class="line">  16:        34</span><br><span class="line">  17:        30</span><br><span class="line">  18:        25</span><br><span class="line">  19:        19</span><br><span class="line">  20:        22</span><br><span class="line">  21:        20</span><br><span class="line">  22:        17</span><br><span class="line">  23:        13</span><br><span class="line">  24:        16</span><br><span class="line">  25:        13</span><br><span class="line">  26:        11</span><br><span class="line">  27:        11</span><br><span class="line">  28:        13</span><br><span class="line">  29:        11</span><br><span class="line">  30:        10</span><br><span class="line">  31:         6</span><br><span class="line">  32:         5</span><br><span class="line">  33:        10</span><br><span class="line">  34:         3</span><br><span class="line">  35:         7</span><br><span class="line">  36:         5</span><br><span class="line">  37:         2</span><br><span class="line">  38:         5</span><br><span class="line">  39:         5</span><br><span class="line">  40:         2</span><br><span class="line">  41:         5</span><br><span class="line">  42:         4</span><br><span class="line">  43:         1</span><br><span class="line">  44:         2</span><br><span class="line">  45:         2</span><br><span class="line">  46:         3</span><br><span class="line">  47:         1</span><br><span class="line">  48:         0</span><br><span class="line">  ...</span><br><span class="line">  102:         0</span><br><span class="line">  103:         1</span><br><span class="line">  104:         0</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>这个输出表示映射 42 个 PG 需要 11 次尝试，映射 44 个 PG 需要 12 次尝试等。最大尝试次数是防止坏映射的 <code>set_choose_tries</code> 的最小值（例如，上述输出中的 103，因为没有超过 103 次尝试的 PG 被映射）。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;PG故障排除&quot;&gt;&lt;a href=&quot;#PG故障排除&quot; class=&quot;headerlink&quot; title=&quot;PG故障排除&quot;&gt;&lt;/a&gt;PG故障排除&lt;/h2&gt;&lt;h4 id=&quot;PG-永远无法变干净&quot;&gt;&lt;a href=&quot;#PG-永远无法变干净&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Cpeh OSD常见故障处理</title>
    <link href="https://watsonlu6.github.io/Ceph-OSD%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"/>
    <id>https://watsonlu6.github.io/Ceph-OSD%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</id>
    <published>2022-10-07T06:53:31.000Z</published>
    <updated>2024-09-08T14:58:28.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OSD故障排除"><a href="#OSD故障排除" class="headerlink" title="OSD故障排除"></a>OSD故障排除</h2><p>在故障排除集群的 OSD 之前，请先检查监视器和网络。<br>首先，确定监视器是否有法定人数。运行 <code>ceph health</code> 命令或 <code>ceph -s</code> 命令，如果 Ceph 显示 <code>HEALTH_OK</code>，则表示有监视器法定人数。如果监视器没有法定人数或监视器状态出现错误，请在继续之前处理监视器问题。接下来，检查网络以确保其正常运行。网络对 OSD 的操作和性能有显著影响。检查主机端是否有丢包，并检查交换机端是否有 CRC 错误。</p><h4 id="获取-OSD-数据"><a href="#获取-OSD-数据" class="headerlink" title="获取 OSD 数据"></a>获取 OSD 数据</h4><p>在故障排除 OSD 时，收集关于 OSD 的不同信息非常有用。一些信息来自于对 OSD 的监控（例如，运行 <code>ceph osd tree</code> 命令）。额外的信息涉及到集群的拓扑结构，以下部分将讨论这些内容。</p><ol><li><p><strong>Ceph 日志</strong><br>  Ceph 的日志文件存储在 <code>/var/log/ceph</code> 下。除非路径已被更改（或者您在存储日志位置不同的容器化环境中），否则可以通过以下命令列出日志文件：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> /var/log/ceph</span><br></pre></td></tr></table></figure><p>  如果日志详情不够，请调整日志级别。要确保 Ceph 在高日志量下表现良好，请参阅“日志记录和调试”。</p></li><li><p><strong>管理员套接字</strong><br>使用管理员套接字工具来检索运行时信息。首先，列出 Ceph 守护进程的套接字，通过以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> /var/run/ceph</span><br></pre></td></tr></table></figure><p>接下来，运行如下命令（将 <code>&#123;daemon-name&#125;</code> 替换为特定守护进程的名称，例如 <code>osd.0</code>）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph daemon &#123;daemon-name&#125; <span class="built_in">help</span></span><br></pre></td></tr></table></figure><p>或者，指定 <code>&#123;socket-file&#125;</code>（“套接字文件”是 <code>/var/run/ceph</code> 中的特定文件）运行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph daemon &#123;socket-file&#125; <span class="built_in">help</span></span><br></pre></td></tr></table></figure><p>管理员套接字可以完成许多任务，包括：</p></li></ol><ul><li>列出 Ceph 配置运行时</li><li>转储历史操作</li><li>转储操作优先级队列状态</li><li>转储正在进行的操作</li><li>转储性能计数器</li></ul><ol start="3"><li><p><strong>显示空闲空间</strong><br>可能会出现文件系统问题。要显示文件系统的空闲空间，请运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">df</span> -h</span><br></pre></td></tr></table></figure><p>要查看此命令支持的语法和选项，请运行 <code>df --help</code>。</p></li><li><p><strong>I&#x2F;O 统计</strong><br><code>iostat</code> 工具可用于识别 I&#x2F;O 相关的问题。运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iostat -x</span><br></pre></td></tr></table></figure></li><li><p><strong>诊断信息</strong><br>要从内核中检索诊断信息，请运行 <code>dmesg</code> 命令，并使用 <code>less</code>、<code>more</code>、<code>grep</code> 或 <code>tail</code> 进行指定输出。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dmesg | grep scsi</span><br></pre></td></tr></table></figure></li></ol><h4 id="停止而不重新平衡"><a href="#停止而不重新平衡" class="headerlink" title="停止而不重新平衡"></a>停止而不重新平衡</h4><p>有时可能需要对集群的子集进行维护或解决影响故障域（例如机架）的问题。然而，当停止 OSD 进行维护时，可能希望防止 CRUSH 自动重新平衡集群。为避免这种重新平衡行为，可以通过运行以下命令将集群设置为 <code>noout</code> 状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">set</span> noout</span><br></pre></td></tr></table></figure><p><strong>警告</strong><br>这更多是一个思考练习，用于让读者理解故障域和 CRUSH 行为，而不是建议在 Luminous 版本后的环境中运行 <code>ceph osd set noout</code>。当 OSD 返回到正常状态时，重新平衡将恢复，<code>ceph osd set noout</code> 命令引入的更改将被撤销。</p><p>然而，在 Luminous 及后续版本中，更安全的做法是只标记受影响的 OSD。要添加或删除特定 OSD 的 <code>noout</code> 标志，可以运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd add-noout osd.0</span><br><span class="line">ceph osd rm-noout osd.0</span><br></pre></td></tr></table></figure><p>也可以标记整个 CRUSH 组。例如，如果计划停用 <code>prod-ceph-data1701</code> 以增加 RAM，可以运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd set-group noout prod-ceph-data1701</span><br></pre></td></tr></table></figure><p>设置标志后，停止 OSD 和需要维护的故障域内的其他 Ceph 服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop ceph\*.service ceph\*.target</span><br></pre></td></tr></table></figure><p><strong>注意</strong><br>当 OSD 被停止时，该 OSD 内的任何放置组将被标记为降级。<br>维护完成后，需重新启动 OSD 和其他停止的守护进程。然而，如果主机在维护过程中重启，则不需要重新启动，系统会自动恢复。要重新启动 OSD 或其他守护进程，使用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl start ceph.target</span><br></pre></td></tr></table></figure><p>最后，根据需要取消 <code>noout</code> 标志，可以运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">unset</span> noout</span><br><span class="line">ceph osd unset-group noout prod-ceph-data1701</span><br></pre></td></tr></table></figure><p>许多现代 Linux 发行版使用 systemd 进行服务管理。然而，对于某些操作系统（尤其是旧版本），可能需要发出等效的服务或启动&#x2F;停止命令。</p><h4 id="OSD-未运行"><a href="#OSD-未运行" class="headerlink" title="OSD 未运行"></a>OSD 未运行</h4><p>在正常情况下，重新启动 ceph-osd 守护进程将允许它重新加入集群并恢复。</p><p><strong>OSD 无法启动</strong><br>如果集群已启动，但某个 OSD 无法启动，请检查以下内容：</p><ul><li><strong>配置文件</strong>：如果您在新的安装中无法启动 OSD，请检查配置文件以确保其符合标准（例如，确保使用的是 <code>host</code> 而不是 <code>hostname</code> 等）。</li><li><strong>检查路径</strong>：确保配置中指定的路径与实际存在的数据和元数据路径对应（例如，日志、WAL 和 DB 的路径）。将 OSD 数据与元数据分开，以查看配置文件和实际挂载是否存在错误。如果有，这些错误可能解释了为何 OSD 无法启动。要将元数据存储在单独的块设备上，可以对驱动器进行分区或使用 LVM，并为每个 OSD 分配一个分区。</li><li><strong>检查最大线程数</strong>：如果集群中有一个节点的 OSD 数量特别高，可能会达到默认的最大线程数（通常是 32,000）。这在恢复过程中尤其可能发生。将最大线程数增加到允许的最大线程数（4194303）可能有助于解决问题。要将线程数增加到最大值，请运行以下命令：  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w kernel.pid_max=4194303</span><br></pre></td></tr></table></figure>如果增加线程数解决了问题，您必须通过在 <code>/etc/sysctl.d</code> 文件夹下的文件或在主 <code>/etc/sysctl.conf</code> 文件中包含 <code>kernel.pid_max</code> 设置来使更改永久生效。例如：  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel.pid_max = 4194303</span><br></pre></td></tr></table></figure></li><li>**检查 <code>nf_conntrack</code>**：这个连接跟踪和连接限制系统对许多生产 Ceph 集群造成问题。问题往往缓慢而微妙地出现。随着集群拓扑和客户端工作负载的增长，神秘和间歇性的连接失败和性能问题会越来越多，尤其是在一天中的某些时候。为开始评估问题，请检查 syslog 历史中的 “table full” 事件。解决这种问题的一种方法是：首先，使用 sysctl 实用程序将 <code>nf_conntrack_max</code> 设置为更高的值。接下来，将 <code>nf_conntrack_buckets</code> 的值提高到 <code>nf_conntrack_buckets × 8 = nf_conntrack_max</code>；这可能需要在 sysctl 之外运行命令（例如，<code>echo 131072 &gt; /sys/module/nf_conntrack/parameters/hashsize</code>）。另一种解决方法是将相关内核模块列入黑名单，从而完全禁用处理。这种方法强大但脆弱。模块及其列出顺序可能因内核版本而异。即使被列入黑名单，iptables 和 docker 有时仍会激活连接跟踪，因此我们建议对调优参数采用“设置并忘记”的策略。在现代系统中，这种方法不会消耗显著资源。</li><li><strong>内核版本</strong>：确定正在使用的内核版本和发行版。默认情况下，Ceph 使用的第三方工具可能存在缺陷或与某些发行版或内核版本冲突（例如，Google 的 gperftools 和 TCMalloc）。检查操作系统建议和每个 Ceph 版本的发行说明，以确保您已解决与内核相关的任何问题。</li><li><strong>段错误</strong>：如果出现段错误，请提高日志级别并重新启动有问题的守护进程。如果段错误重复发生，请在 Ceph bug 跟踪器（<a href="https://tracker.ceph.com/projects/ceph%EF%BC%89%E5%92%8C">https://tracker.ceph.com/projects/ceph）和</a> dev 及 ceph-users 邮件列表归档（<a href="https://ceph.io/resources%EF%BC%89%E4%B8%AD%E6%90%9C%E7%B4%A2%E6%98%AF%E5%90%A6%E6%9C%89%E5%85%B6%E4%BB%96%E4%BA%BA%E9%81%87%E5%88%B0%E5%B9%B6%E6%8A%A5%E5%91%8A%E4%BA%86%E8%BF%99%E4%BA%9B%E9%97%AE%E9%A2%98%E3%80%82%E5%A6%82%E6%9E%9C%E8%BF%99%E7%A1%AE%E5%AE%9E%E6%98%AF%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E5%92%8C%E7%8B%AC%E7%89%B9%E7%9A%84%E6%95%85%E9%9A%9C%EF%BC%8C%E8%AF%B7%E5%9C%A8">https://ceph.io/resources）中搜索是否有其他人遇到并报告了这些问题。如果这确实是一个新的和独特的故障，请在</a> dev 邮件列表中发布，并提供以下信息：正在运行的具体 Ceph 版本、ceph.conf（秘密已用 XXX 替换）、监视器状态输出和日志文件摘录。</li></ul><p><strong>OSD 失败</strong><br>当 OSD 失败时，表示 ceph-osd 进程无响应或已死亡，相应的 OSD 已被标记为下线。存活的 ceph-osd 守护进程将向监视器报告该 OSD 似乎已经下线，并且 <code>ceph health</code> 命令的输出中将显示新的状态，如下例所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph health</span><br><span class="line">HEALTH_WARN 1/3 <span class="keyword">in</span> osds are down</span><br></pre></td></tr></table></figure><p>当有一个或多个 OSD 被标记为下线时，会引发此健康警报。要查看哪些 OSD 已下线，请为命令添加详细信息，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">HEALTH_WARN 1/3 <span class="keyword">in</span> osds are down</span><br><span class="line">osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080</span><br></pre></td></tr></table></figure><p>或者，运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tree down</span><br></pre></td></tr></table></figure><p>如果由于驱动器故障或其他故障导致某个 ceph-osd 守护进程无法工作或重新启动，则其日志文件（位于 <code>/var/log/ceph</code> 下）中应存在错误消息。如果 ceph-osd 守护进程因心跳故障或自杀超时错误而停止，则底层驱动器或文件系统可能无响应。检查 dmesg 输出和 syslog 输出中的驱动器错误或内核错误。可能需要指定某些标志（例如，<code>dmesg -T</code> 以查看人类可读的时间戳）以避免将旧错误误认为新错误。</p><p>如果整个主机的 OSD 都下线，请检查是否存在网络错误或主机的硬件问题。如果 OSD 问题是软件错误（例如，断言失败或其他意外错误）的结果，请在 bug 跟踪器、dev 邮件列表归档和 ceph-users 邮件列表归档中搜索是否有问题报告。如果没有明确的修复或现有 bug，请向 ceph-devel 邮件列表报告问题。</p><p><strong>没有可用驱动器空间</strong><br>如果 OSD 已满，Ceph 会通过确保不会将新数据写入 OSD 来防止数据丢失。在正常运行的集群中，当集群的 OSD 和池接近某些“满”比率时，会引发健康检查。<code>mon_osd_full_ratio</code> 阈值默认为 0.95（即 95% 的容量）：这是防止客户端写入数据的点。<code>mon_osd_backfillfull_ratio</code> 阈值默认为 0.90（即 90% 的容量）：这是防止开始回填的点。<code>mon_osd_nearfull_ratio</code> 阈值默认为 0.85（即 85% 的容量）：这是引发 OSD_NEARFULL 健康检查的点。集群中的 OSD 在 Ceph 分配的数据量上会有所不同。要通过显示每个 OSD 的数据使用情况来检查“满”状态，请运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">df</span></span><br></pre></td></tr></table></figure><p>要通过显示集群的总体数据使用情况和池之间的数据分布来检查“满”状态，请运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph <span class="built_in">df</span></span><br></pre></td></tr></table></figure><p>在检查 <code>ceph df</code> 命令的输出时，请特别注意最满的 OSD，而不是原始空间使用的百分比。如果单个 OSD 变满，所有对该 OSD 池的写入可能会失败。当 <code>ceph df</code> 报告池的可用空间时，它会考虑相对于池中最满 OSD 的比率设置。为了平衡数据分布，可以采取两种方法：（1）使用 <code>reweight-by-utilization</code> 命令逐步将数据从过满的 OSD 移动到不足满的 OSD，或者（2）在 Luminous 的后续版本及以后的版本中，利用 ceph-mgr balancer 模块自动执行相同任务。要调整“满”比率，请运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd set-nearfull-ratio &lt;<span class="built_in">float</span>[0.0-1.0]&gt;</span><br><span class="line">ceph osd set-full-ratio &lt;<span class="built_in">float</span>[0.0-1.0]&gt;</span><br><span class="line">ceph osd set-backfillfull-ratio &lt;<span class="built_in">float</span>[0.0-1.0]&gt;</span><br></pre></td></tr></table></figure><p>有时，集群问题的原因是 OSD 失败。这可能发生在测试过程中，或者因为集群较小、非常满或不平衡。当 OSD 或节点占据集群数据的过高比例时，组件故障或自然增长可能导致接近满和满的比率被超过。在测试 Ceph 对 OSD 故障的恢复能力时，建议保留足够的空闲磁盘空间，并考虑暂时降低 OSD 的满比率、回填满比率和接近满比率。<br>OSD 的“满”状态在 <code>ceph health</code> 命令的输出中可见，如下例所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph health</span><br><span class="line">HEALTH_WARN 1 nearfull osd(s)</span><br></pre></td></tr></table></figure><p>有关详细信息，请使用详细命令，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">HEALTH_ERR 1 full osd(s);</span><br><span class="line"></span><br><span class="line"> 1 backfillfull osd(s); 1 nearfull osd(s)</span><br><span class="line">osd.3 is full at 97%</span><br><span class="line">osd.4 is backfill full at 91%</span><br><span class="line">osd.2 is near full at 87%</span><br></pre></td></tr></table></figure><p>为解决满集群问题，建议通过添加 OSD 来增加容量。添加新 OSD 允许集群将数据重新分配到新提供的存储空间。查找浪费空间的 rados bench 孤儿对象。</p><p>如果旧版 Filestore OSD 由于已满而无法启动，可以通过删除满 OSD 上的一小部分放置组目录来回收空间。</p><p><strong>重要</strong></p><p>如果您选择在满 OSD 上删除放置组目录，请勿在其他满 OSD 上删除相同的放置组目录。否则，您将丢失数据。您必须在至少一个 OSD 上保留数据的至少一个副本。删除放置组目录是一种罕见且极端的干预，不应轻易进行。</p><h4 id="OSD-运行缓慢-无响应"><a href="#OSD-运行缓慢-无响应" class="headerlink" title="OSD 运行缓慢&#x2F;无响应"></a>OSD 运行缓慢&#x2F;无响应</h4><p>OSD 有时会运行缓慢或无响应。在排查这个常见问题时，建议在调查 OSD 性能问题之前排除其他可能性。例如，确保网络正常工作，确认 OSD 正在运行，并检查 OSD 是否限制了恢复流量。<br><strong>提示</strong><br>在 Luminous 版本之前，某些 up 和 in 状态的 OSD 有时不可用或运行缓慢，因为恢复中的 OSD 消耗了系统资源。更新版本通过防止这种现象提供了更好的恢复处理。</p><ol><li><p><strong>网络问题</strong><br>  作为分布式存储系统，Ceph 依赖网络进行 OSD 对等和复制、故障恢复以及周期性心跳。网络问题可能导致 OSD 延迟和抖动的 OSD。更多信息请参见“抖动 OSD”。要确保 Ceph 进程和 Ceph 相关进程连接正常并在监听，请运行以下命令：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">netstat -a | grep ceph</span><br><span class="line">netstat -l | grep ceph</span><br><span class="line"><span class="built_in">sudo</span> netstat -p | grep ceph</span><br></pre></td></tr></table></figure><p>  要检查网络统计信息，请运行以下命令：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -s</span><br></pre></td></tr></table></figure></li><li><p><strong>驱动器配置</strong><br>  SAS 或 SATA 存储驱动器应仅容纳一个 OSD，但 NVMe 驱动器可以轻松容纳两个或更多。然而，如果其他进程共享驱动器，读写吞吐量可能会受到瓶颈。此类进程包括：日志&#x2F;元数据、操作系统、Ceph 监视器、syslog 日志、其他 OSD 和非 Ceph 进程。<br>  由于 Ceph 在日志记录后确认写入，快速 SSD 是加速响应时间的一个有吸引力的选项——特别是在使用 XFS 或 ext4 文件系统作为传统 FileStore OSD 时。相比之下，Btrfs 文件系统可以同时进行写入和日志记录。（然而，不推荐在生产环境中使用 Btrfs。）<br>  <strong>注意</strong><br>  对驱动器进行分区不会改变其总吞吐量或顺序读&#x2F;写限制。通过在单独的分区中运行日志，吞吐量可能会有所改善，但更好的做法是将日志运行在单独的物理驱动器中。<br>  <strong>警告</strong><br>  Reef 不支持 FileStore。Reef 之后的版本不再支持 FileStore。提到 FileStore 的信息仅适用于 Quincy 版本及 Quincy 之前的版本。</p></li><li><p><strong>坏扇区&#x2F;碎片化磁盘</strong><br>  检查驱动器是否存在坏块、碎片化和其他可能导致性能显著下降的错误。检查驱动器错误的有用工具包括 dmesg、syslog 日志和 smartctl（在 smartmontools 包中）。<br>  <strong>注意</strong><br>  smartmontools 7.0 及更高版本提供 NVMe 状态直通和 JSON 输出。</p></li><li><p><strong>共存的监视器&#x2F;OSD</strong><br>  尽管监视器是相对轻量的进程，但当监视器与 OSD 运行在同一主机上时，可能会出现性能问题。监视器发出许多 fsync() 调用，这可能干扰其他工作负载。当监视器与 OSD 共存在同一存储驱动器上时，性能问题尤其严重。此外，如果监视器运行的是较旧的内核（3.0 之前）或没有 syncfs(2) 系统调用的内核，那么同一主机上运行的多个 OSD 可能会进行太多提交，从而影响彼此的性能。这种问题有时会导致所谓的“突发写入”。</p></li><li><p><strong>共存进程</strong><br>  在与 OSD 运行在同一硬件上时，处理写入数据到 Ceph 的进程（例如基于云的解决方案和虚拟机）可能会导致显著的 OSD 延迟。因此，一般不推荐将这些进程与 OSD 共存在同一硬件上。推荐的做法是优化某些主机用于 Ceph，其他主机用于其他进程。这种将 Ceph 操作与其他应用程序分开的做法可能有助于提高性能，并简化故障排除和维护。在同一硬件上运行共存进程有时被称为“融合”。使用 Ceph 时，仅在具备专业知识和经过考虑后再进行融合。</p></li><li><p><strong>日志级别</strong><br>  高日志级别可能导致性能问题。操作人员有时会提高日志级别以跟踪问题，然后忘记在之后降低它们。在这种情况下，OSD 可能会消耗宝贵的系统资源，将不必要的详细日志写入磁盘。任何希望使用高日志级别的人都应考虑将驱动器挂载到日志的默认路径（例如，&#x2F;var&#x2F;log&#x2F;ceph&#x2F;$cluster-$name.log）。</p></li><li><p><strong>恢复限制</strong><br>  根据您的配置，Ceph 可能会减少恢复速率以保持客户端或 OSD 性能，或者可能会增加恢复速率到影响客户端或 OSD 性能的程度。检查客户端或 OSD 是否正在恢复。</p></li><li><p><strong>内核版本</strong><br>  检查您运行的内核版本。较旧的内核可能缺少改进 Ceph 性能的更新。</p></li><li><p><strong>内核 SyncFS 问题</strong><br>  如果您遇到 SyncFS 的内核问题，请尝试每个主机运行一个 OSD 以查看性能是否提高。旧的内核可能没有足够新的 glibc 版本来支持 syncfs(2)。</p></li><li><p><strong>文件系统问题</strong><br>  在 Luminous 版本之后，我们建议使用 BlueStore 后端部署集群。当运行 Luminous 版本之前的版本时，或者如果您有特别的理由使用旧的 Filestore 后端，我们建议使用 XFS。<br>  我们不推荐使用 Btrfs 或 ext4。Btrfs 文件系统有许多吸引人的特性，但可能会导致性能问题和虚假的 ENOSPC 错误。由于 xattr 限制破坏了对长对象名称的支持，我们不推荐在 Filestore OSD 中使用 ext4。</p></li><li><p><strong>内存不足</strong><br>  我们建议每个 OSD 守护进程至少配备 4GB 内存，并建议将其增加到 6GB 或 8GB。在正常操作期间，您可能会注意到 ceph-osd 进程只使用了其中的一小部分。您可能会被诱使将多余的内存用于共存应用程序，或减少每个节点的内存容量。然而，当 OSD 正在恢复时，其内存使用会急剧增加。如果在恢复过程中没有足够的内存，OSD 性能会显著下降，守护进程可能会崩溃或被 Linux OOM Killer 杀死。</p></li><li><p><strong>请求阻塞或请求缓慢</strong><br>  当 ceph-osd 守护进程对请求响应缓慢时，集群日志会收到报告操作处理时间过长的消息。警告阈值默认为 30 秒，可以通过 osd_op_complaint_time 设置进行配置。<br>  旧版 Ceph 报告旧请求：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osd.0 192.168.106.220:6800/18813 312 : [WRN] old request osd_op(client.5099.0:790 fatty_26485_object789 [write 0~4096] 2.5e54f643) v4 received at 2012-03-06 15:42:56.054801 currently waiting <span class="keyword">for</span> sub ops</span><br></pre></td></tr></table></figure><p>  新版 Ceph 报告慢请求：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="built_in">date</span>&#125; &#123;osd.num&#125; [WRN] 1 slow requests, 1 included below; oldest blocked <span class="keyword">for</span> &gt; 30.005692 secs</span><br><span class="line">&#123;<span class="built_in">date</span>&#125; &#123;osd.num&#125;  [WRN] slow request 30.005692 seconds old, received at &#123;date-time&#125;: osd_op(client.4240.0:8 benchmark_data_ceph-1_39426_object7 [write 0~4194304] 0.69848840) v4 currently waiting <span class="keyword">for</span> subops from [610]</span><br></pre></td></tr></table></figure><p>  可能的原因包括：</p></li></ol><ul><li>驱动器故障（检查 dmesg 输出）</li><li>内核文件系统中的错误（检查 dmesg 输出）</li><li>集群过载（检查系统负载、iostat 等）</li><li>ceph-osd 守护进程中的错误</li></ul><p>  可能的解决方案：</p><ul><li>从 Ceph 主机中移除虚拟机</li><li>升级内核</li><li>升级 Ceph</li><li>重启 OSD</li><li>更换故障或有问题的组件</li></ul><ol start="13"><li><strong>调试缓慢请求</strong><br>  如果您运行 <code>ceph daemon osd.&lt;id&gt; dump_historic_ops</code> 或 <code>ceph daemon osd.&lt;id&gt; dump_ops_in_flight</code>，您将看到一组操作和每个操作经历的事件列表。这些事件简要描述如下。<br>  来自 Messenger 层的事件：<ul><li><code>header_read</code>: Messenger 开始从网络读取消息的时间。</li><li><code>throttled</code>: Messenger 尝试获取内存节流空间以将消息读入内存的时间。</li><li><code>all_read</code>: Messenger 完成从网络读取消息的时间。</li><li><code>dispatched</code>: Messenger 将消息交给 OSD 的时间。</li><li><code>initiated</code>: 这与 <code>header_read</code> 相同。存在这两个事件是历史上的异常。</li></ul></li></ol><p>  来自 OSD 处理操作的事件:<br>    - <code>queued_for_pg</code>: 操作已被放入队列等待 PG 处理。<br>    - <code>reached_pg</code>: PG 开始执行操作。<br>    - <code>waiting for *</code>: 操作在等待其他工作完成后才能继续（例如，新 OSDMap；对象目标的检查；PG 的对等完成；这些都在消息中指定）。<br>    - <code>started</code>: 操作已被接受为 OSD 应执行的任务，并且正在执行中。<br>    - <code>waiting for subops from</code>: 操作已被发送到副本 OSD。</p><p>  来自 Filestore 的事件：<br>    - <code>commit_queued_for_journal_write</code>: 操作已交给 FileStore。<br>    - <code>write_thread_in_journal_buffer</code>: 操作在日志的缓冲区中，等待持久化（作为下一次磁盘写入）。<br>    - <code>journaled_completion_queued</code>: 操作已被写入日志，回调已排队等待调用。</p><p>  来自 OSD 在数据已交给底层存储后的事件：<br>    - <code>op_commit</code>: 操作已由主 OSD 提交（即，写入日志）。<br>    - <code>op_applied</code>: 操作已写入主 OSD 的后台文件系统（即，在内存中应用但未刷新到磁盘）。<br>    - <code>sub_op_applied</code>: 副本的 <code>op_applied</code>。<br>    - <code>sub_op_committed</code>: 副本的 <code>op_commit</code>（仅对 EC 池）。<br>    - <code>sub_op_commit_rec/sub_op_apply_rec from &lt;X&gt;</code>: 主 OSD 在听到上述消息后进行标记，但针对特定副本（即 <X>）。<br>    - <code>commit_sent</code>: 我们向客户端（或主 OSD，针对子操作）发送了回复。</p><p>虽然一些事件可能看起来冗余，但它们跨越了内部代码中的重要边界（例如，跨锁将数据传递到新线程中）。</p><h4 id="OSD抖动"><a href="#OSD抖动" class="headerlink" title="OSD抖动"></a>OSD抖动</h4><p>“抖动”是指 OSD 被快速重复标记为上线然后下线的现象。本节解释如何识别抖动以及如何减轻它。</p><p>当 OSD 进行对等和检查心跳时，它们使用集群（后端）网络。如果您的 OSD 节点有两个网络端口，将一个端口专用于公共网络，另一个端口专用于私有网络，可以避免网络维护和网络故障对集群或客户端造成的重大影响。在这种情况下，可以考虑将两个链接仅用于公共网络：使用绑定（LACP）或等成本路由（例如 FRR），可以获得更高的吞吐量容差、容错能力和减少 OSD 抖动。</p><p>当私有网络（甚至单个主机链接）故障或降级时，而公共网络正常运行，OSD 可能无法很好地处理这种情况。在这种情况下，OSD 使用公共网络向监视器报告彼此故障，同时将自己标记为上线。然后，监视器再次通过公共网络发送更新的集群地图，将受影响的 OSD 标记为下线。这些 OSD 向监视器回复“我还没死！”，然后循环重复。我们称这种情况为“抖动”，它可能很难隔离和修复。没有私有网络时，这种恼人的动态被避免了：OSD 通常要么上线要么下线，没有抖动。</p><p>如果某些原因导致 OSD “抖动”（被反复标记为下线然后再上线），您可以通过暂时冻结其状态来强制监视器停止抖动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">set</span> noup      <span class="comment"># 防止 OSD 被标记为上线</span></span><br><span class="line">ceph osd <span class="built_in">set</span> nodown    <span class="comment"># 防止 OSD 被标记为下线</span></span><br></pre></td></tr></table></figure><p>这些标志记录在 osdmap 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd dump | grep flags</span><br><span class="line">flags no-up,no-down</span><br></pre></td></tr></table></figure><p>您可以使用以下命令清除这些标志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">unset</span> noup</span><br><span class="line">ceph osd <span class="built_in">unset</span> nodown</span><br></pre></td></tr></table></figure><p>还有两个其他标志 <code>noin</code> 和 <code>noout</code>，它们分别防止启动的 OSD 被标记为分配数据（in）或保护 OSD 被标记为最终移除（out），无论 <code>mon_osd_down_out_interval</code> 的当前值如何。</p><p><strong>注意</strong></p><p><code>noup</code>、<code>noout</code> 和 <code>nodown</code> 是临时性的，因为清除标志后，它们阻止的操作应该很快能够恢复。但是 <code>noin</code> 标志防止 OSD 在启动时被标记为在线，任何在标志设置期间启动的守护进程将保持这种状态。</p><p><strong>注意</strong></p><p>通过仔细调整 <code>mon_osd_down_out_subtree_limit</code>、<code>mon_osd_reporter_subtree_level</code> 和 <code>mon_osd_min_down_reporters</code> 可以在一定程度上缓解抖动的原因和效果。最佳设置的推导取决于集群大小、拓扑结构和使用的 Ceph 版本。这些因素的交互很微妙，超出了本文档的范围。</p><p>由 Ceph 基金会提供</p><p>Ceph 文档是由非营利组织 Ceph 基金会资助和托管的社区资源。如果您想支持我们以及其他努力，请考虑立即加入。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;OSD故障排除&quot;&gt;&lt;a href=&quot;#OSD故障排除&quot; class=&quot;headerlink&quot; title=&quot;OSD故障排除&quot;&gt;&lt;/a&gt;OSD故障排除&lt;/h2&gt;&lt;p&gt;在故障排除集群的 OSD 之前，请先检查监视器和网络。&lt;br&gt;首先，确定监视器是否有法定人数。运行 </summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Cpeh Mon常见故障处理</title>
    <link href="https://watsonlu6.github.io/Ceph-Mon%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"/>
    <id>https://watsonlu6.github.io/Ceph-Mon%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/</id>
    <published>2022-10-06T06:53:31.000Z</published>
    <updated>2024-09-08T09:47:41.847Z</updated>
    
    <content type="html"><![CDATA[<h2 id="监视器故障排查"><a href="#监视器故障排查" class="headerlink" title="监视器故障排查"></a>监视器故障排查</h2><p>即使集群出现与监视器相关的问题，集群也不一定会面临宕机的风险。即使集群丢失了多个监视器，只要还有足够的监视器存活并能够形成法定人数（quorum），集群就能继续运行。<br>如果集群遇到了监视器相关的问题，可以参考以下的故障排查信息。</p><h4 id="初步故障排查"><a href="#初步故障排查" class="headerlink" title="初步故障排查"></a>初步故障排查</h4><p>Ceph 监视器故障排查的第一步是确保监视器正在运行，并且它们能够与网络进行通信。按照本节的步骤操作，以排除监视器故障的最简单原因。</p><ol><li><p><strong>确保监视器正在运行</strong><br>确保监视器守护进程（ceph-mon）正在运行。可能是由于升级后监视器没有重新启动。检查这个简单的疏忽可以节省大量的故障排查时间。同时，也要确保管理守护进程（ceph-mgr）正在运行。记住，典型的集群配置中，每个监视器（ceph-mon）都有一个相应的管理器（ceph-mgr）。<br><strong>注意：</strong> 在 v1.12.5 之前的版本中，Rook 不会运行超过两个管理器。</p></li><li><p><strong>确保可以访问监视器节点</strong><br>在某些罕见情况下，iptables 规则可能会阻止访问监视器节点或 TCP 端口。这些规则可能是以前压力测试或规则开发时遗留下来的。要检查是否存在这样的规则，可以通过 SSH 登录每个监视器节点，并使用 telnet、nc 或类似工具尝试连接到其他监视器节点的 tcp&#x2F;3300 和 tcp&#x2F;6789 端口。</p></li><li><p><strong>确保“ceph status”命令能够运行并从集群接收回复</strong><br>如果 ceph status 命令从集群接收到回复，说明集群正在运行。监视器只有在形成法定人数时才会响应状态请求。确认是否有一个或多个管理器（mgr）守护进程正在运行。在没有任何问题的集群中，ceph status 将报告所有管理器守护进程都在运行。如果 ceph status 命令未能从集群接收到回复，那么很可能是由于没有足够的监视器来形成法定人数。如果运行 ceph -s 命令时未指定进一步的选项，它会连接到任意选择的一个监视器。然而，在某些情况下，添加 -m 标志来连接特定的监视器（或按顺序连接几个特定的监视器）可能更有帮助，例如：ceph status -m mymon1。</p></li></ol><p>如果以上解决方案未能解决问题，可能需要逐一检查每个监视器。即使没有形成法定人数，仍然可以单独联系每个监视器，并使用 ceph tell mon.ID mon_status 命令请求其状态（此处 ID 是监视器的标识符）。对集群中的每个监视器运行 ceph tell mon.ID mon_status 命令。关于此命令的输出。还有另一种联系各个监视器的方法：通过 SSH 登录每个监视器节点并查询守护进程的管理套接字。</p><h4 id="使用监视器的管理套接字"><a href="#使用监视器的管理套接字" class="headerlink" title="使用监视器的管理套接字"></a>使用监视器的管理套接字</h4><p>监视器的管理套接字允许通过 Unix 套接字文件直接与特定的守护进程交互。此套接字文件位于监视器的运行目录中。管理套接字的默认目录是 <code>/var/run/ceph/ceph-mon.ID.asok</code>。可以覆盖管理套接字的默认位置。如果默认位置被覆盖，那么管理套接字会出现在其他位置。这种情况通常发生在集群的守护进程部署在容器中时。<br>要查找管理套接字的目录，请检查<code>ceph.conf</code> 文件以获取备用路径，或运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-conf --name mon.ID --show-config-value admin_socket</span><br></pre></td></tr></table></figure><p>管理套接字只有在监视器守护进程运行时可用。每次监视器正常关闭时，管理套接字都会被移除。如果监视器未运行但管理套接字仍存在，可能是由于监视器未正确关闭。如果监视器未运行，将无法使用管理套接字，并且 ceph 命令很可能返回错误 111：连接被拒绝。</p><p>要访问管理套接字，请运行以下形式的 ceph tell 命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph tell mon.&lt;id&gt; mon_status</span><br></pre></td></tr></table></figure><p>此命令通过管理套接字将帮助命令传递给指定的运行中的监视器守护进程 <code>&lt;id&gt;</code>。如果知道管理套接字文件的完整路径，可以更直接地运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph --admin-daemon &lt;full_path_to_asok_file&gt; &lt;command&gt;</span><br></pre></td></tr></table></figure><p>运行 <code>ceph help</code> 显示通过管理套接字可用的所有受支持的命令。特别参考 <code>config get</code>、<code>config show</code>、<code>mon stat</code> 和 <code>quorum_status</code>。</p><h4 id="理解-mon-status"><a href="#理解-mon-status" class="headerlink" title="理解 mon_status"></a>理解 mon_status</h4><p>监视器的状态（由 <code>ceph tell mon.X mon_status</code> 命令报告）可以通过管理套接字获得。<code>ceph tell mon.X mon_status</code> 命令输出关于监视器的大量信息（包括在 <code>quorum_status</code> 命令输出中找到的信息）。</p><p><strong>注意</strong><br>命令 <code>ceph tell mon.X mon_status</code> 不应被字面输入。运行命令时，mon.X 的 X 部分应替换为 Ceph 集群中的特定值。</p><p>为了理解此命令的输出，考虑以下例子，看到 <code>ceph tell mon.c mon_status</code> 的输出：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span> <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;c&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;rank&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;state&quot;</span><span class="punctuation">:</span> <span class="string">&quot;peon&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;election_epoch&quot;</span><span class="punctuation">:</span> <span class="number">38</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;quorum&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">        <span class="number">2</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;outside_quorum&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;extra_probe_peers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;sync_provider&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;monmap&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="attr">&quot;epoch&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;fsid&quot;</span><span class="punctuation">:</span> <span class="string">&quot;5c4e9d53-e2e1-478a-8061-f543f8be4cf8&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;modified&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2013-10-30 04:12:01.945629&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;created&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2013-10-29 14:14:41.914786&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mons&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;rank&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">              <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;a&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="attr">&quot;addr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;127.0.0.1:6789/0&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;rank&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">              <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;b&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="attr">&quot;addr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;127.0.0.1:6790/0&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span> <span class="attr">&quot;rank&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">              <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;c&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="attr">&quot;addr&quot;</span><span class="punctuation">:</span> <span class="string">&quot;127.0.0.1:6795/0&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>该输出报告 monmap 中有三个监视器（a, b, 和 c），法定人数由两个监视器组成，而 c 是 peon。</p><p><strong>哪个监视器不在法定人数中？</strong><br>答案是 a（即 mon.a）。mon.a 不在法定人数中。</p><p><strong>我们如何知道在此示例中 mon.a 不在法定人数中？</strong><br>我们知道 mon.a 不在法定人数中，因为它的 rank 是 0，而 rank 为 0 的监视器根据定义不在法定人数中。如果我们检查法定人数集，可以看到集合中明显有两个监视器：1 和 2。但这些不是监视器名称，而是当前 monmap 中确立的监视器 rank。法定人数集不包括 rank 为 0 的监视器，根据 monmap，该监视器是 mon.a。</p><p><strong>监视器的 rank 是如何确定的？</strong><br>每当监视器被添加或从集群中移除时，监视器的 rank 会被计算（或重新计算）。rank 的计算遵循一个简单的规则：IP:PORT 组合越大，rank 越低。在此情况下，因为 127.0.0.1:6789（mon.a）在数值上小于其他两个 IP:PORT 组合（即 “监视器 b”的 127.0.0.1:6790 和 “监视器 c”的 127.0.0.1:6795），mon.a 拥有最高的 rank，即 rank 0。</p><h4 id="最常见的监视器问题"><a href="#最常见的监视器问题" class="headerlink" title="最常见的监视器问题"></a>最常见的监视器问题</h4><p><strong>集群有法定人数但至少有一个监视器关闭</strong><br>当集群有法定人数但至少有一个监视器关闭时，<code>ceph health detail</code> 会返回类似以下的消息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ceph health detail</span><br><span class="line">[snip]</span><br><span class="line">mon.a (rank 0) addr 127.0.0.1:6789/0 is down (out of quorum)</span><br></pre></td></tr></table></figure><p><strong>如何排查 Ceph 集群有法定人数但至少有一个监视器关闭的问题？</strong><br>确保 mon.a 正在运行。确保可以从其他监视器节点连接到 mon.a 的节点。也检查 TCP 端口。检查所有节点上的 iptables 和 nf_conntrack，并确保未丢弃&#x2F;拒绝连接。</p><p>如果这些初步故障排查无法解决问题，那么需要进一步调查。</p><p>首先，通过管理套接字检查有问题的监视器的 mon_status，如“使用监视器的管理套接字”和“理解 mon_status”中所述。如果监视器不在法定人数中，则其状态将是以下之一：probing（探测）、electing（选举中）或 synchronizing（同步中）。如果监视器的状态是 leader（领导者）或 peon（跟随者），则监视器认为自己在法定人数中，但集群的其余部分认为它不在法定人数中。排查过程中，可能处于 probing、electing 或 synchronizing 状态的监视器已进入法定人数。再次检查 <code>ceph status</code> 以确定排查过程中监视器是否已进入法定人数。如果监视器仍未进入法定人数，则继续参考本文件中的相关调查。</p><p><strong>监视器状态为 probing 是什么意思？</strong><br>如果 <code>ceph health detail</code> 显示监视器的状态为 probing，则该监视器仍在寻找其他监视器。每个监视器启动时都会在这个状态停留一段时间。当监视器连接到 monmap 中指定的其他监视器时，它就不再处于 probing 状态。监视器处于 probing 状态的时间取决于其所在集群的参数。例如，当监视器是单监视器集群的一部分时（在生产环境中绝对不要这样做），监视器几乎是瞬间通过 probing 状态。在多监视器集群中，监视器保持在 probing 状态，直到找到足够的监视器形成法定人数——这意味着如果集群中的三个监视器中有两个关闭，则剩下的一个监视器将无限期地保持在 probing 状态，直到您启动其他监视器之一。</p><p>如果已建立法定人数，则只要守护进程能够被访问，监视器守护进程应能够快速找到其他监视器。如果监视器卡在 probing 状态，并且您已经完成了上面描述的监视器之间通信的故障排查，那么可能是有问题的监视器尝试以错误地址连接其他监视器。mon_status 会输出监视器已知的 monmap：确定 monmap 中指定的其他监视器的位置是否与网络中监视器的位置匹配。如果不匹配，请参阅“恢复监视器的损坏 monmap”。如果 monmap 中指定的监视器位置与网络中的监视器位置匹配，则持久的 probing 状态可能与监视器节点之间严重的时钟偏差有关。</p><h3 id="监视器的状态为“electing”时意味着什么？"><a href="#监视器的状态为“electing”时意味着什么？" class="headerlink" title="监视器的状态为“electing”时意味着什么？"></a>监视器的状态为“electing”时意味着什么？</h3><p>如果 <code>ceph health detail</code> 显示某个监视器的状态为“electing”，这表明该监视器正在进行选举。选举通常会很快完成，但有时监视器可能会陷入所谓的选举风暴。如果选举状态持续存在，可以将有问题的监视器置于停机状态以进行调查。这只有在集群中有足够的存活监视器以形成仲裁的情况下才可行。</p><h3 id="监视器的状态为“synchronizing”时意味着什么？"><a href="#监视器的状态为“synchronizing”时意味着什么？" class="headerlink" title="监视器的状态为“synchronizing”时意味着什么？"></a>监视器的状态为“synchronizing”时意味着什么？</h3><p>如果 <code>ceph health detail</code> 显示某个监视器的状态为“synchronizing”，这意味着该监视器正在与集群的其他部分同步，以便加入仲裁。监视器与仲裁的其余部分同步所需的时间取决于集群监视器存储的大小、集群的规模以及集群的状态。通常较大且已降级的集群会使监视器在“synchronizing”状态停留的时间比较小且新建的集群更长。<br>如果监视器的状态在“synchronizing”和“electing”之间来回切换，这表明可能存在问题：集群状态可能正在快速推进（即生成新映射），而同步过程无法跟上新映射的生成速度。这个问题在 Cuttlefish 版本之前更为常见，因为同步过程在较新的版本中已经被重构和增强，以避免这种情况。如果您在较新的版本中遇到此问题，请在 Ceph 错误追踪系统中报告问题。准备并提供日志以支持您报告的任何错误。有关日志准备的信息，请参阅《日志准备》。</p><h3 id="监视器的状态为“leader”或“peon”时意味着什么？"><a href="#监视器的状态为“leader”或“peon”时意味着什么？" class="headerlink" title="监视器的状态为“leader”或“peon”时意味着什么？"></a>监视器的状态为“leader”或“peon”时意味着什么？</h3><p>在 Ceph 正常运行期间，当集群处于 HEALTH_OK 状态时，Ceph 集群中的一个监视器处于“leader”状态，其余的监视器处于“peon”状态。可以通过查看命令 <code>ceph tell &lt;mon_name&gt; mon_status</code> 返回的 state 键的值来确定给定监视器的状态。<br>如果 <code>ceph health detail</code> 显示监视器处于“leader”状态或“peon”状态，很可能存在时钟偏移。请遵循《时钟偏移》中的说明。如果您已经按照这些说明进行操作，但 <code>ceph health detail</code> 仍然显示监视器处于“leader”状态或“peon”状态，请在 Ceph 错误追踪系统中报告问题。如果您提出问题，请提供日志以支持它。有关日志准备的信息，请参阅《日志准备》。</p><h3 id="修复监视器的损坏“monmap”"><a href="#修复监视器的损坏“monmap”" class="headerlink" title="修复监视器的损坏“monmap”"></a>修复监视器的损坏“monmap”</h3><p>可以使用类似 <code>ceph tell mon.c mon_status</code> 的命令来检索 monmap。<br>下面是一个 monmap 的示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">epoch 3</span><br><span class="line">fsid 5c4e9d53-e2e1-478a-8061-f543f8be4cf8</span><br><span class="line">last_changed 2013-10-30 04:12:01.945629</span><br><span class="line">created 2013-10-29 14:14:41.914786</span><br><span class="line">0: 127.0.0.1:6789/0 mon.a</span><br><span class="line">1: 127.0.0.1:6790/0 mon.b</span><br><span class="line">2: 127.0.0.1:6795/0 mon.c</span><br></pre></td></tr></table></figure><p>这个 monmap 是正常的，但您可能的 monmap 可能不正常。在某个节点上的 monmap 可能会因为节点长时间宕机，而期间集群的监视器发生了变化，从而变得过时。</p><p>更新监视器过时的 monmap 有两种方法：</p><ol><li><p><strong>报废并重新部署监视器</strong><br>仅在确保不会丢失所报废监视器中保留的信息时使用此方法。确保有其他状态良好的监视器，以便新监视器能够与存活的监视器同步。请记住，如果没有其他监视器内容的副本，销毁监视器可能会导致数据丢失。</p></li><li><p><strong>将 monmap 注入监视器</strong><br>可以通过从集群中存活的监视器中检索最新的 monmap 并将其注入到损坏或缺失 monmap 的监视器中来修复它。<br>实施此解决方案请执行以下步骤：</p><ul><li><p>按以下方式之一检索 monmap：<br> <strong>如果存在监视器仲裁：</strong><br>从仲裁中检索 monmap：</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mon getmap -o /tmp/monmap</span><br></pre></td></tr></table></figure><p> <strong>如果没有监视器仲裁：</strong><br> 直接从已停止的监视器中检索 monmap：</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-mon -i ID-FOO --extract-monmap /tmp/monmap</span><br></pre></td></tr></table></figure><p> 在此示例中，已停止监视器的 ID 是 ID-FOO。</p></li><li><p>停止将注入 monmap 的监视器：</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service ceph -a stop mon.&#123;mon-id&#125;</span><br></pre></td></tr></table></figure></li><li><p>将 monmap 注入已停止的监视器：</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-mon -i ID --inject-monmap /tmp/monmap</span><br></pre></td></tr></table></figure></li><li><p>启动监视器。</p></li></ul></li></ol><p><strong>警告</strong><br>将 monmap 注入监视器可能会引起严重问题。注入 monmap 会覆盖监视器上存储的最新 monmap。请小心操作！</p><h4 id="时钟偏移"><a href="#时钟偏移" class="headerlink" title="时钟偏移"></a>时钟偏移</h4><p>Paxos 共识算法需要紧密的时间同步，这意味着仲裁中的监视器之间的时钟偏移会对监视器的操作产生严重影响，导致一些令人困惑的行为。为避免这种问题，应在监视器节点上运行时钟同步工具，例如 Chrony 或传统的 ntpd 工具。配置每个监视器节点时确保启用了 iburst 选项，并确保每个监视器有多个对等节点，包括以下内容：</p><ul><li>其他监视器</li><li>内部 NTP 服务器</li><li>多个外部公共池服务器</li></ul><p><strong>注意</strong><br>iburst 选项在初始同步时会发送八个数据包，而不是通常的一个。此外，建议将集群中的所有节点与内部和外部服务器同步，甚至与监视器同步。请在物理机上运行 NTP 服务器，因为虚拟机的虚拟化时钟不适合稳定的时间保持。</p><h4 id="时钟偏移问题及解答"><a href="#时钟偏移问题及解答" class="headerlink" title="时钟偏移问题及解答"></a>时钟偏移问题及解答</h4><p><strong>容忍的最大时钟偏移是多少？</strong><br>默认情况下，监视器允许时钟最大漂移 0.05 秒（50 毫秒）。</p><p><strong>我可以增加最大容忍的时钟偏移吗？</strong><br>可以，但我们强烈建议不要这样做。最大容忍的时钟偏移可通过 <code>mon-clock-drift-allowed</code> 选项进行配置，但更改此选项几乎肯定是一个糟糕的决定。设定的时钟偏移上限是因为时钟不同步的监视器不可靠。当前的默认值已证明其在监视器遇到严重问题之前提醒用户方面的有效性。更改此值可能会对监视器的稳定性和整体集群健康状况造成不可预见的影响。</p><p><strong>我如何知道是否存在时钟偏移？</strong><br>当存在时钟偏移时，监视器会通过集群状态 <code>HEALTH_WARN</code> 发出警告。执行 <code>ceph health detail</code> 和 <code>ceph status</code> 命令时，输出类似如下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mon.c addr 10.10.0.1:6789/0 clock skew 0.08235s &gt; max 0.05s (latency 0.0045s)</span><br></pre></td></tr></table></figure><p>在此示例中，监视器 <code>mon.c</code> 被标记为存在时钟偏移。<br>在 Luminous 及更高版本中，可以通过运行 <code>ceph time-sync-status</code> 命令来检查时钟偏移。注意，主监视器通常具有数值最低的 IP 地址。它始终显示 0：其他监视器报告的偏移是相对于主监视器的，而不是任何外部参考源。</p><p><strong>如果存在时钟偏移，我该怎么办？</strong><br>同步时钟。使用 NTP 客户端可能会有所帮助。但是，如果已经在使用 NTP 客户端并且仍然遇到时钟偏移问题，请确定使用的 NTP 服务器是否位于网络之外，还是托管在网络中。托管自己的 NTP 服务器往往可以缓解时钟偏移问题。</p><p><strong>客户端无法连接或挂载</strong><br>如果客户端无法连接到集群或挂载，请检查您的 iptables。一些操作系统安装程序会向 iptables 添加一个 REJECT 规则。iptables 规则会拒绝所有尝试连接到主机的客户端（除了 ssh）。如果您的监视器主机的 iptables 具有 REJECT 规则，则从单独节点连接的客户端会失败，并引发超时错误。检查 iptables 规则，看看是否有任何拒绝尝试连接到 Ceph 守护进程的客户端。例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">REJECT all -- anywhere anywhere reject-with icmp-host-prohibited</span><br></pre></td></tr></table></figure><p>可能还需要在 Ceph 主机上的 iptables 添加规则，以确保客户端能够访问与 Ceph 监视器（默认：端口 6789）和 Ceph OSD（默认：6800 到 7568）关联的 TCP 端口。例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -A INPUT -m multiport -p tcp -s &#123;ip-address&#125;/&#123;netmask&#125; --dports 6789,6800:7568 -j ACCEPT</span><br></pre></td></tr></table></figure><h4 id="监视器存储故障"><a href="#监视器存储故障" class="headerlink" title="监视器存储故障"></a>监视器存储故障</h4><p><strong>存储损坏的症状</strong><br>Ceph 监视器在键值存储中维护集群地图。如果键值存储损坏导致监视器失败，则监视器日志可能包含以下错误消息之一：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Corruption: error in middle of record</span><br></pre></td></tr></table></figure><p>或者：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Corruption: 1 missing files; e.g.: /var/lib/ceph/mon/mon.foo/store.db/1234567.ldb</span><br></pre></td></tr></table></figure><p><strong>使用健康的监视器恢复</strong><br>如果集群中有幸存的监视器，损坏的监视器可以用新的监视器替换。新监视器启动后，它会与健康的对等体同步。新监视器完全同步后，将能够为客户端提供服务。</p><p><strong>使用 OSDs 进行恢复</strong><br>即使所有监视器同时失效，也可以使用存储在 OSDs 中的信息恢复监视器存储。建议在 Ceph 集群中部署至少三个（最好是五个）监视器。在这种部署中，完全的监视器故障是不太可能的。然而，如果数据中心在磁盘设置或文件系统设置配置不当的情况下发生意外断电，可能会导致底层文件系统故障，这可能会导致所有监视器故障。在这种情况下，OSDs 中的数据可用于恢复监视器。以下是可以在这种情况下使用的脚本来恢复监视器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">ms=/root/mon-store</span><br><span class="line"><span class="built_in">mkdir</span> <span class="variable">$ms</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># collect the cluster map from stopped OSDs</span></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> <span class="variable">$hosts</span>; <span class="keyword">do</span></span><br><span class="line">  rsync -avz <span class="variable">$ms</span>/. user@<span class="variable">$host</span>:<span class="variable">$ms</span>.remote</span><br><span class="line">  <span class="built_in">rm</span> -rf <span class="variable">$ms</span></span><br><span class="line">  ssh user@<span class="variable">$host</span> &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">    for osd in /var/lib/ceph/osd/ceph-*; do</span></span><br><span class="line"><span class="string">      ceph-objectstore-tool --data-path \$osd --no-mon-config --op update-mon-db --mon-store-path $ms.remote</span></span><br><span class="line"><span class="string">    done</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">  rsync -avz user@<span class="variable">$host</span>:<span class="variable">$ms</span>.remote/. <span class="variable">$ms</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># rebuild the monitor store from the collected map, if the cluster does not</span></span><br><span class="line"><span class="comment"># use cephx authentication, we can skip the following steps to update the</span></span><br><span class="line"><span class="comment"># keyring with the caps, and there is no need to pass the &quot;--keyring&quot; option.</span></span><br><span class="line"><span class="comment"># i.e. just use &quot;ceph-monstore-tool $ms rebuild&quot; instead</span></span><br><span class="line">ceph-authtool /path/to/admin.keyring -n mon. \</span><br><span class="line">  --<span class="built_in">cap</span> mon <span class="string">&#x27;allow *&#x27;</span></span><br><span class="line">ceph-authtool /path/to/admin.keyring -n client.admin \</span><br><span class="line">  --<span class="built_in">cap</span> mon <span class="string">&#x27;allow *&#x27;</span> --<span class="built_in">cap</span> osd <span class="string">&#x27;allow *&#x27;</span> --<span class="built_in">cap</span> mds <span class="string">&#x27;allow *&#x27;</span></span><br><span class="line"><span class="comment"># add one or more ceph-mgr&#x27;s key to the keyring. in this case, an encoded key</span></span><br><span class="line"><span class="comment"># for mgr.x is added, you can find the encoded key in</span></span><br><span class="line"><span class="comment"># /etc/ceph/$&#123;cluster&#125;.$&#123;mgr_name&#125;.keyring on the machine where ceph-mgr is</span></span><br><span class="line"><span class="comment"># deployed</span></span><br><span class="line">ceph-authtool /path/to/admin.keyring --add-key <span class="string">&#x27;AQDN8kBe9PLWARAAZwxXMr+n85SBYbSlLcZnMA==&#x27;</span> -n mgr.x \</span><br><span class="line">  --<span class="built_in">cap</span> mon <span class="string">&#x27;allow profile mgr&#x27;</span> --<span class="built_in">cap</span> osd <span class="string">&#x27;allow *&#x27;</span> --<span class="built_in">cap</span> mds <span class="string">&#x27;allow *&#x27;</span></span><br><span class="line"><span class="comment"># If your monitors&#x27; ids are not sorted by ip address, please specify them in order.</span></span><br><span class="line"><span class="comment"># For example. if mon &#x27;a&#x27; is 10.0.0.3, mon &#x27;b&#x27; is 10.0.0.2, and mon &#x27;c&#x27; is  10.0.0.4,</span></span><br><span class="line"><span class="comment"># please passing &quot;--mon-ids b a c&quot;.</span></span><br><span class="line"><span class="comment"># In addition, if your monitors&#x27; ids are not single characters like &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, please</span></span><br><span class="line"><span class="comment"># specify them in the command line by passing them as arguments of the &quot;--mon-ids&quot;</span></span><br><span class="line"><span class="comment"># option. if you are not sure, please check your ceph.conf to see if there is any</span></span><br><span class="line"><span class="comment"># sections named like &#x27;[mon.foo]&#x27;. don&#x27;t pass the &quot;--mon-ids&quot; option, if you are</span></span><br><span class="line"><span class="comment"># using DNS SRV for looking up monitors.</span></span><br><span class="line">ceph-monstore-tool <span class="variable">$ms</span> rebuild -- --keyring /path/to/admin.keyring --mon-ids alpha beta gamma</span><br><span class="line"></span><br><span class="line"><span class="comment"># make a backup of the corrupted store.db just in case!  repeat for</span></span><br><span class="line"><span class="comment"># all monitors.</span></span><br><span class="line"><span class="built_in">mv</span> /var/lib/ceph/mon/mon.foo/store.db /var/lib/ceph/mon/mon.foo/store.db.corrupted</span><br><span class="line"></span><br><span class="line"><span class="comment"># move rebuild store.db into place.  repeat for all monitors.</span></span><br><span class="line"><span class="built_in">mv</span> <span class="variable">$ms</span>/store.db /var/lib/ceph/mon/mon.foo/store.db</span><br><span class="line"><span class="built_in">chown</span> -R ceph:ceph /var/lib/ceph/mon/mon.foo/store.db</span><br></pre></td></tr></table></figure><p>该脚本执行以下步骤：</p><ul><li>从每个 OSD 主机收集地图。</li><li>重建存储。</li><li>用适当的权限填充 keyring 文件中的实体。</li><li>用恢复的副本替换 mon.foo 上的损坏存储。</li></ul><p><strong>已知限制</strong><br>上述恢复工具无法恢复以下信息：</p><ul><li><strong>某些添加的 keyring</strong>：所有使用 <code>ceph auth add</code> 命令添加的 OSD keyring 都会从 OSD 的副本中恢复，并且使用 <code>ceph-monstore-tool</code> 导入 client.admin keyring。但是，MDS keyring 和其他所有 keyring 都会在恢复的监视器存储中缺失，可能需要手动重新添加。</li><li><strong>创建池</strong>：如果任何 RADOS 池正在创建过程中，则该状态会丢失。恢复工具假设所有池都已创建。如果恢复后部分创建的池中有 PG 卡在未知状态，可以运行 <code>ceph osd force-create-pg</code> 命令强制创建空 PG。只有当你确定池是空的时才采取此操作。</li><li><strong>MDS 映射</strong>：MDS 映射会丢失。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;监视器故障排查&quot;&gt;&lt;a href=&quot;#监视器故障排查&quot; class=&quot;headerlink&quot; title=&quot;监视器故障排查&quot;&gt;&lt;/a&gt;监视器故障排查&lt;/h2&gt;&lt;p&gt;即使集群出现与监视器相关的问题，集群也不一定会面临宕机的风险。即使集群丢失了多个监视器，只要还有足够</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph Cache Tier源码实现</title>
    <link href="https://watsonlu6.github.io/Ceph-Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://watsonlu6.github.io/Ceph-Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-09-28T12:45:54.000Z</published>
    <updated>2024-09-01T13:34:00.917Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Cache-Tier架构"><a href="#Cache-Tier架构" class="headerlink" title="Cache Tier架构"></a>Cache Tier架构</h2><p>Ceph存储集群如果采用廉价的PC和传统的机械硬盘进行搭建，磁盘的访问速度受到了一定的限制，无法达到理想的IOPS性能水平。为了优化系统的IO性能，可以考虑添加快速的存储设备作为缓存，以减少数据的访问延时。其中，Cache Tier分层存储机制是一种常见的解决方案，在Ceph服务端缓存中被广泛使用，可以有效提升后端存储层的I&#x2F;O性能。Cache Tier需要创建一个由高速且昂贵的存储设备（如SSD）组成的存储池作为缓存层，以及一个相对廉价的设备组成的后端存储池作为经济存储层。缓存层使用多副本模式，存储层可以使用多副本或纠删码模式。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0.png" alt="Cache Tier"><br>Ceph的缓存分层理论基础是数据存在热点，数据访问不均匀。通常，80%的应用只访问20%的数据，这20%的数据被称为热点数据。为了减少响应时间，可以将热点数据保存到性能较高的存储设备（如固态硬盘）中。在Cache Tiering中，有一个分层代理，当保存在缓存层的数据变冷或不再活跃时，该代理会将这些数据刷到存储层，并将其从缓存层中移除。这种操作称为刷新或逐出。在客户端读写数据时，Ceph的对象处理器负责决定对象存储的位置，而Cache Tier则决定何时将缓存层中的对象刷回后端存储层。对于写操作，请求到达缓存层后，完成写操作后直接应答客户端，之后由缓存层的代理线程负责将数据写入存储层。对于读操作，如果命中缓存层，直接在缓存层读取，否则可以重定向到存储层访问。如果数据近期有访问过，说明比较热，可以提升到缓存层中。对于Ceph客户端来说，缓存层和后端存储层是完全透明的。所有Ceph客户端都可以使用缓存层，因此Cache Tier具有提升块设备、Ceph对象存储、Ceph文件系统和原生绑定的I&#x2F;O性能的潜力。</p><h2 id="Ceph-Cache-tier处理流程"><a href="#Ceph-Cache-tier处理流程" class="headerlink" title="Ceph Cache tier处理流程"></a>Ceph Cache tier处理流程</h2><p>使用命令add-cache 可以将一个cachepool作为base pool的tier。这时会设置pool的信息，在pool里面记录了cache pool和base pool的关系。客户端在获取pool信息的时候可知，目标base pool存在一个tier，叫做cache pool，那么操作base pool的请求都会发送给cache pool。请求达到cache pool中时，作为tier的pool会有一些特别的处理maybe_cache_handle，具体的流程如下图：<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B02.png" alt="Cache Tier"></p><ul><li>判断操作的object是否在cache pool中命中，如果命中，则直接在cache pool中处理，和在普通pool的请求一样处理。后续会有agent线程将缓存脏数据刷写到base pool中。</li><li>没有命中缓存的情况下，才会去判断缓存模式。如果命中缓存，不管是什么模式都会在cache pool中处理。下面的处理都是未命中缓存的情况。</li><li>判断是否是writeback模式，读操作，如果可以proxy_read，那就直接do_proxy_read读取数据即可，不可以proxy_read 就使用do_cache_redirect，告诉客户端去base pool中读取。写操作，如果当前是evict_full模式，说明现在缓存中已经达到了阈值，需要等待缓存淘汰一些object，在完成写操作，目前放在等待队列中等待，如果不是evict_full模式，则需要从base pool中promote对应的object到cache pool中，promote结束后继续处理本次的写操作。</li><li>判断是否是forward模式。在forward模式下，不再在cachepool中处理请求，会告诉客户端将请求全部发送到base pool中。</li><li>判断是不是readonly模式。写操作会告诉客户端直接想base pool写即可，如果是读操作，则会从base pool中promote该object。</li><li>判断是不是readforward模式。该模式读操作全部都告诉客户端直接去base pool中读取即可，写操作按着writeback模式处理。</li><li>判断是不是readproxy模式。该模式读操作都采用cachepool的proxy read方法，写操作按着writeback模式处理。</li></ul><p>针对其中涉及到的几个封装好的方法的操作： do_cache_redirect， do_proxy_read， do_proxy_write，promote_object<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B03.png" alt="Cache Tier"></p><ul><li><strong>do_cache_redirect</strong> ：客户端请求cache pool，cache pool告诉客户端你应该去base pool中请求，客户端收到应答后，再次发送请求到base pool中请求数据，由base pool告诉客户端请求完成。</li><li><strong>do_proxy_read</strong>：客户端发送读请求到cache pool，但是未命中，则cache pool自己会发送请求到base pool中，获取数据后，由cache pool将数据发送给客户端，完成读请求。但是值得注意的是，虽然cache pool读取到了该object，但不会保存在cache pool中，下次请求仍然需要调用函数promote_objectbasePool读取该对象请求，然后写入cachePool中。</li><li><strong>do_proxy_write</strong>：直接写数据到basePool中，同样，cachePool中并没有该数据对象，还需要后续调用promote_object函数把数据对象从basePool中读到cachePool中。</li><li><strong>promote_object</strong>：当客户端发送请求到cache pool中，但是cache pool未命中，cache pool会选择将该object从base pool中提升到cache pool中，然后在cache pool进行读写操作，操作完成后告知客户端请求完成，在cache pool会缓存该object，下次直接在cache中处理，和proxy_read存在的区别。构造PromoteCallback回调函数，然后调用函数start_copyk拷贝函数。</li></ul><p>无论是 Proxy Read 还是 Promote Object 操作最终都是调用了 objecter 的 read 方法来从base storage层读取对象数据</p><h2 id="Cache-Tier数据结构"><a href="#Cache-Tier数据结构" class="headerlink" title="Cache Tier数据结构"></a>Cache Tier数据结构</h2><p>由于 Tier cache 在 Ceph 中的存在形式是存储池，pg_pool_t保存了存储池的相关属性。(src&#x2F;osd&#x2F;osd_type.h&#x2F;struct pg_pool_t)</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">set&lt;<span class="type">uint64_t</span>&gt; tiers;   <span class="comment">//如果当前pool是一个basePool，tiers就记录改basepool的cachePool层，一个base pool可以设置多个cachePool</span></span><br><span class="line"><span class="type">int64_t</span> tier_of;            <span class="comment">//如果当前pool是一个cachePool，那么tier_of记录了该cachePool的basePool</span></span><br><span class="line"><span class="type">int64_t</span> read_tier;       <span class="comment">//设置basePool的读缓存层，根据Ceph不同的Cache Tier模式来设置</span></span><br><span class="line"><span class="type">int64_t</span> write_tier;      <span class="comment">//设置basePool的写缓存层，根据Ceph不同的Cache Tier模式来设置</span></span><br><span class="line"><span class="type">cache_mode_t</span> cache_mode;  <span class="comment">//设置Cache Tier模式</span></span><br><span class="line"><span class="type">uint64_t</span> target_max_bytes;   <span class="comment">//设置了cachePool的最大字节数</span></span><br><span class="line"><span class="type">uint64_t</span> target_max_objects; <span class="comment">//设置了cachePool的最大对象数量</span></span><br><span class="line"><span class="type">uint32_t</span> cache_target_dirty_ratio_micro;   <span class="comment">// 目标脏数据率：当脏数据比例达到这个值，后台 agent 开始 flush 数据</span></span><br><span class="line"><span class="type">uint32_t</span> cache_target_dirty_high_ratio_micro;   <span class="comment">// 高目标脏数据率：当脏数据比例达到这个值，后台 agent 开始高速 flush 数据</span></span><br><span class="line"><span class="type">uint32_t</span> cache_target_full_ratio_micro;   <span class="comment">// 数据满的比率：当数据达到这个比例时，认为数据已满，需要进行缓存淘汰</span></span><br><span class="line"><span class="type">uint32_t</span> cache_min_flush_age;      <span class="comment">// 对象在 cache 中被刷入到 storage 层的最小时间</span></span><br><span class="line"><span class="type">uint32_t</span> cache_min_evict_age;   <span class="comment">// 对象在 cache 中被淘汰的最小时间</span></span><br><span class="line">HitSet::Params hit_set_params; <span class="comment">// HitSet 相关参数</span></span><br><span class="line"><span class="type">uint32_t</span> hit_set_period;     <span class="comment">// 每间隔 hit_set_period 一段时间，系统重新产生一个新的 hit_set 对象来记录对象的缓存统计信息</span></span><br><span class="line"><span class="type">uint32_t</span> hit_set_count;      <span class="comment">// 记录系统保存最近的多少个 hit_set 记录</span></span><br><span class="line"><span class="type">bool</span> use_gmt_hitset;        <span class="comment">// hitset archive 对象的命名规则 </span></span><br><span class="line"><span class="type">uint32_t</span> hit_set_grade_decay_rate;    <span class="comment">//当前hit_set在对象温度计数上具有最高优先级，后续hit_set的优先级比预hit_set衰减此参数</span></span><br><span class="line"><span class="type">uint32_t</span> hit_set_search_last_n;      <span class="comment">//为温度累积，最多N次hit_sets</span></span><br></pre></td></tr></table></figure><h4 id="读写IO"><a href="#读写IO" class="headerlink" title="读写IO"></a>读写IO</h4><p><strong>Add Cache</strong><br>在 ceph&#x2F;src&#x2F;mon&#x2F;OSDMonitor.cc 中实现了 add-cache 命令，从命令行中获取对应的参数并绑定 Tier 关系</p><p><strong>选择 Cache Pool</strong><br>Cache Tier的应用主要体现在计算OSD的过程中，通过判断basepool的参数，来决定是否要更新targetpool：读操作时，如果有read_tier，则更新为read_tier pool；写操作时，如果有write_tier，则更新为write_tier pool。read_tier和write_tier与pool是否开启Cache Tier有关。</p><p>在 ceph&#x2F;src&#x2F;osdc&#x2F;Objecter.cc&#x2F;Objecter::_calc_target中指定目标存储池为 Cache Pool，设置之后由后续的代码在该 Pool 中执行 Crush 算法。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先根据base_oloc.pool获取pool信息，获取pg_pool_t对象   </span></span><br><span class="line"><span class="type">const</span> <span class="type">pg_pool_t</span> *pi = osdmap-&gt;<span class="built_in">get_pg_pool</span>(t-&gt;base_oloc.pool);</span><br><span class="line"><span class="comment">// apply tiering 根据读写操作，分别设置需要操作的 tier</span></span><br><span class="line">t-&gt;target_oid = t-&gt;base_oid;         #base_oid        <span class="comment">//读取的对象              #target_oid;          //最终读取的目标对象</span></span><br><span class="line">t-&gt;target_oloc = t-&gt;base_oloc;     #base_oloc       <span class="comment">//对象的pool信息      #//target_oloc      //最终目标对象的pool信息</span></span><br><span class="line"><span class="keyword">if</span> ((t-&gt;flags &amp; CEPH_OSD_FLAG_IGNORE_OVERLAY) == <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">//检查cache tier，如果是读操作，并且有读缓存，就设置t-&gt;target_oloc.pool为该pool的read_tier值。</span></span><br><span class="line"><span class="keyword">if</span> (is_read &amp;&amp; pi-&gt;<span class="built_in">has_read_tier</span>())</span><br><span class="line">    t-&gt;target_oloc.pool = pi-&gt;read_tier;</span><br><span class="line">    <span class="comment">//如果是写操作，并且有写缓存，就设置t-&gt;target_oloc.pool为该pool的write_tier值。</span></span><br><span class="line"><span class="keyword">if</span> (is_write &amp;&amp; pi-&gt;<span class="built_in">has_write_tier</span>())</span><br><span class="line">        t-&gt;target_oloc.pool = pi-&gt;write_tier;</span><br><span class="line">pi = osdmap-&gt;<span class="built_in">get_pg_pool</span>(t-&gt;target_oloc.pool);</span><br><span class="line"><span class="keyword">if</span> (!pi) &#123;</span><br><span class="line">    t-&gt;osd = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span> RECALC_OP_TARGET_POOL_DNE;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>osd 先接收到客户端发送来的请求，然后OSD::dequeue_op()调用 PrimaryLogPG:: do_request()——&gt;PrimaryLogPG::do_op()中处理，这都是正常的一个 pool 处理请求的流程，在 do_op 中来看看不同于其他普通 pool 的处理。如果开启了Cache Tier，将会在do_op中执行以下操作：</p><ol><li>首先判断hit_set中是否包含待操作的对象（hit_set-&gt;contains(obc-&gt;obs.oi.soid)），如果不包含，则把对象添加到hit_set中。添加对象后，如果hit_set满了，或者hit_set超时，则调用hit_set_persist()。</li><li>执行agent_choose_mode()，设置agent相关参数，如flush_mode、num_objects、num_bytes等。</li><li>执行maybe_handle_cache()。这里处理cache执行逻辑。</li><li>如果maybe_handle_cache()中调用maybe_handle_cache_detail()，如果成功处理了op请求，则直接return，否则会继续执行后续操作（说明不需要从datapool读取数据或者转发请求到datapool，可以直接在此osd命中查询的对象），由本OSD执行读取操作。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B01.png" alt="Cache Tier"></li></ol><h4 id="HitSet"><a href="#HitSet" class="headerlink" title="HitSet"></a>HitSet</h4><p>在 write back&#x2F;read forward&#x2F;read proxy 模式下需要 HitSet 来记录缓存命中。</p><p>HitSet 用于跟踪和统计对象的访问行为，记录对象是否存在缓存中。定义了一个缓存查找到抽象接口，目前提供了三种实现方式：ExplicitHashHitSet，ExplicitObjectHitSet，BloomHitSet</p><p>ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h 定义了抽象接口，同时该头文件中包含了具体的 HitSet 实现</p><ul><li><strong>ExplicitHashHitSet</strong><ul><li>ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h&#x2F;class ExplicitHashHitSet</li><li>基于对象的 32 位 HASH 值的 set 来记录对象的命中，每个对象占用 4 bytes 内存空间</li><li>优点：空间占用相对较少，但需要根据 HASH 进行全局的扫描遍历比较</li></ul></li><li><strong>ExplicitObjectHitSet</strong><ul><li>ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h&#x2F;class ExplicitObjectHitSet</li><li>使用一个基于 ceph&#x2F;src&#x2F;common&#x2F;hobject 的 set 来记录对象的命中，占用的内存取决于对象的关键信息的大小</li><li>使用内存中缓存数据结构来进行判断带来的优点就是实现相对简单直观，但占用的内存空间相对较大</li></ul></li><li><strong>BloomHitSet</strong><ul><li>ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h&#x2F;class BloomHitSet</li><li>采用了压缩的 Bloom Filter 的方式来记录对象是否在缓存中，进一步减少了内存占用空间</li></ul></li></ul><h3 id="Cache-Tier的初始化"><a href="#Cache-Tier的初始化" class="headerlink" title="Cache Tier的初始化"></a>Cache Tier的初始化</h3><ul><li>src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::hit_set_setup()用来创建并初始化HisSet对象</li><li>src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_setup()完成agent相关的初始化工作</li></ul><h2 id="Cache-Pool-请求处理"><a href="#Cache-Pool-请求处理" class="headerlink" title="Cache Pool 请求处理"></a>Cache Pool 请求处理</h2><p>Cache 的相关请求处理可以通过do_op()进行梳理，主要包含了 agent_choose_mode()和 maybe_handle_cache() 两个主要方法。(src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;do_op(OpRequestRef &amp;))</p><p><strong>agent_choose_mode(bool restart, OpRequestRef op)</strong></p><ul><li>src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;agent_choose_mode</li><li>该函数主要计算一个 PG 的 flush_mode 和 evic_mode 的参数值。</li><li>返回值如果为 True，表明该请求 Op 被重新加入请求队列（由于 EvictMode 为 Full），其他情况返回 false。</li></ul><p><strong>maybe_handle_cache(…)</strong></p><ul><li>src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;maybe_handle_cache()调用maybe_handle_cache_detail(）</li><li>处理有关cache的读写请求</li></ul><!-- 图解maybe_handle_cache_detail()缓存策略将以上缓存策略的处理流程转换为流程图如下所示（注：流程细节随着Ceph版本的迭代已经有锁改变，此处重点关注最终的调用）![Cache Tier](/images/缓存基础与Ceph分层存储/Cache-Tier源码实现2.png)针对其中涉及到的几个封装好的方法的操作： do_cache_redirect， do_proxy_read， do_proxy_write，promote_object![Cache Tier](/images/缓存基础与Ceph分层存储/Cache-Tier源码实现3.png)- **do_cache_redirect** ：客户端请求cache pool，cache pool告诉客户端你应该去base pool中请求，客户端收到应答后，再次发送请求到base pool中请求数据，由base pool告诉客户端请求完成。- **do_proxy_read**：客户端发送读请求到cache pool，但是未命中，则cache pool自己会发送请求到base pool中，获取数据后，由cache pool将数据发送给客户端，完成读请求。但是值得注意的是，虽然cache pool读取到了该object，但不会保存在cache pool中，下次请求仍然需要调用函数promote_objectbasePool读取该对象请求，然后写入cachePool中。- **do_proxy_write**：直接写数据到basePool中，同样，cachePool中并没有该数据对象，还需要后续调用promote_object函数把数据对象从basePool中读到cachePool中。- **promote_object**：当客户端发送请求到cache pool中，但是cache pool未命中，cache pool会选择将该object从base pool中提升到cache pool中，然后在cache pool进行读写操作，操作完成后告知客户端请求完成，在cache pool会缓存该object，下次直接在cache中处理，和proxy_read存在的区别。构造PromoteCallback回调函数，然后调用函数start_copyk拷贝函数。无论是 Proxy Read 还是 Promote Object 操作最终都是调用了 objecter 的 read 方法来从base storage层读取对象数据 --><h4 id="Cache-flush-evict"><a href="#Cache-flush-evict" class="headerlink" title="Cache flush &amp; evict"></a>Cache flush &amp; evict</h4><p>cachePool空间不够时，需要选择一些脏数据对象会刷到数据层，即flush操作；将一些clean对象从缓存层剔除，以释放更多的缓存空间，即evict操作。这两种操作都是在后台线程完成的。<strong>flush操作和evict操作算法的好坏决定了Cache Tier的缓存命中率</strong>。evict是针对cachepool中已经过期或过冷的数据，只需要把它从cachepool中删除即可，evict操作通常会影响缓存命中率。flush是把脏数据刷新到storagePool，flush操作通常不会直接影响缓存命中率。flush操作是将缓存中的数据写回到持久存储介质中，从而保证数据的一致性，但并不会直接影响缓存的访问，脏数据是只保存在cachePool中，经过修改后，还未写入storagePool的数据。</p><p><strong>数据结构</strong><br>src&#x2F;osd&#x2F;osd.h&#x2F;OSDServices ：定义了 AgentThread 后台线程，用于完成 flush 和 evict 操作：一是把脏对象从cachePool层适时地会刷到basePool层；二是从cachePool层剔除掉一些不经常访问的clean对象。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Mutex agent_lock;     <span class="comment">// agent 线程锁，保护下面所有数据结构</span></span><br><span class="line">Cond agent_cond;     <span class="comment">// 线程相应的条件变量</span></span><br><span class="line">map&lt;<span class="type">uint64_t</span>, set&lt;PGRef&gt; &gt; agent_queue;   <span class="comment">// agent线程的工作队列，保存了OSD中所有归属于cachePool的淘汰或者回刷所需的 PG 集合，根据PG集合的优先级，保存在不同的map中</span></span><br><span class="line">set&lt;PGRef&gt;::iterator agent_queue_pos;   <span class="comment">//当前在扫描的PG集合的一个位置</span></span><br><span class="line"><span class="type">bool</span> agent_valid_iterator;  <span class="comment">//只有agent_valid_iterator为true时，agent_queue_pos指针才有效，否则从集合的起始处开始扫描</span></span><br><span class="line"><span class="type">int</span> agent_ops;            <span class="comment">// 所有正在进行的回刷和淘汰操作</span></span><br><span class="line"><span class="type">int</span> flush_mode_high_count;      <span class="comment">//一旦FLUSH_MODE_HIGH有了一个pg，就可以高速刷新对象</span></span><br><span class="line">set&lt;<span class="type">hobject_t</span>&gt; agent_oids;    <span class="comment">// 所有正在进行的 agent 操作（回刷或者淘汰）的对象</span></span><br><span class="line"><span class="type">bool</span> agent_active;    <span class="comment">// agent 是否有效</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">AgentThread</span> : <span class="keyword">public</span> Thread&#123;&#125; agent_thread;    <span class="comment">// agent 线程，专门用来处理cache tier数据迁移的线程，线程名叫：osd_srv_agent。其作用就是循环遍历agent_queue中的所有pg，并对他们执行agent_work()操作。osd_srv_agent线程是一个OSD上所有PG公用的，为了保证效率，设置了严格的限流参数：osd_pool_default_cache_max_evict_check_size限制依次遍历对象的总数，达到后立刻切换退出循环在osd_srv_agent中切换PG；osd_agent_max_ops设置了一个循环中最多能够处理几次flush或者evict操作。</span></span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> agent_stop_flag;   <span class="comment">// agent 停止的标志</span></span><br><span class="line">    SafeTimer agent_timer;   <span class="comment">//agent相关定时器：当扫描一个 PG 对象时，该对象既没有剔除操作，也没有回刷操作，就停止 PG 的扫描，把该 PG 加入到定时器中，5S 后继续</span></span><br></pre></td></tr></table></figure><p>src&#x2F;osd&#x2F;TierAgentState.h：TierAgentState用来保存PG相关的agent信息。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">hobject_t</span> position;    <span class="comment">//PG内扫描的对象位置</span></span><br><span class="line"><span class="type">int</span> started;    <span class="comment">//PG里所有对象扫描完成后，所发起的所有的agent操作数目。如果没有agent操作，就需要延迟一段时间</span></span><br><span class="line"><span class="type">hobject_t</span> start;    <span class="comment">//本次扫描起始位置</span></span><br><span class="line"><span class="type">bool</span> delaying;    <span class="comment">//是否延迟</span></span><br><span class="line"><span class="type">pow2_hist_t</span> temp_hist;   <span class="comment">//历史统计信息</span></span><br><span class="line"><span class="type">int</span> hist_age;</span><br><span class="line">map&lt;<span class="type">time_t</span>,HitSetRef&gt; hit_set_map;   <span class="comment">//Hitset的历史记录</span></span><br><span class="line">list&lt;<span class="type">hobject_t</span>&gt; recent_clean;   <span class="comment">//最近处于clean的对象</span></span><br><span class="line"><span class="type">unsigned</span> evict_effort;      <span class="comment">//应该驱逐的对象的大致比例（假设它们均匀分布）</span></span><br></pre></td></tr></table></figure><h4 id="flush-evict-执行入口"><a href="#flush-evict-执行入口" class="headerlink" title="flush&#x2F;evict 执行入口"></a>flush&#x2F;evict 执行入口</h4><p>src&#x2F;osd&#x2F;osd.cc&#x2F;OSDService::agent_entry：agent_entry 是 agent_thread 的入口函数，它在后台调用pg-&gt;agent_work()，agent_queue的改变是在PrimaryLogPG::agent_choose_mode函数中改变的</p><p>src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_work：遍历PG中所有对象，去寻找已经过期的、失效的需要flush或者evict的对象并对它们执行相应操作。</p><ol><li><p>扫描本PG的对象，从 agent_state-&gt;position 开始扫描，结果保存在 ls 中</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;<span class="type">hobject_t</span>&gt; ls;</span><br><span class="line"><span class="type">int</span> r = pgbackend-&gt;<span class="built_in">objects_list_partial</span>(agent_state-&gt;position, ls_min, ls_max, &amp;ls, &amp;next); </span><br></pre></td></tr></table></figure></li><li><p>对扫描的 ls 对象做相应的检查，执行 evict 操作和 flush 操作</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (vector&lt;<span class="type">hobject_t</span>&gt;::iterator p = ls.<span class="built_in">begin</span>();p != ls.<span class="built_in">end</span>(); ++p)     </span><br><span class="line"><span class="keyword">if</span> (agent_state-&gt;evict_mode != TierAgentState::EVICT_MODE_IDLE &amp;&amp; <span class="built_in">agent_maybe_evict</span>(obc, <span class="literal">false</span>))</span><br><span class="line">    ++started;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (agent_state-&gt;flush_mode!=TierAgentState::FLUSH_MODE_IDLE&amp;&amp;agent_flush_quota&gt;<span class="number">0</span>&amp;&amp;<span class="built_in">agent_maybe_flush</span>(obc)) &#123;</span><br><span class="line">    ++started;</span><br><span class="line">    --agent_flush_quota;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ol><p>真正执行操作的方法</p><ul><li><strong>evict</strong>：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_maybe_evict</li><li><strong>flush</strong>：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_maybe_flush</li><li><strong>start_flush</strong>：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::start_flush 该函数完成实际的 flush 操作</li><li><strong>start_manifest_flush</strong>：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::start_manifest_flush  真正刷回数据之前的数据准备</li><li><strong>do_manifest_flush</strong>：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::do_manifest_flush 真正刷回数据的过程</li></ul><p>flush 操作最终是以 Op 请求的方式传递到底层存储层的，也就意味着需要再执行一次 Ceph 存储池写数据的相关逻辑。<br>Ceph的Cache Tier功能目前在对象访问频率和热点统计上的实现都比较简单，可以通过基于自学习的Cache算法提升缓存命中率。</p><p><strong>agent_state在每个函数中都起到决定性地位</strong>：在agent_work中，agent_state-&gt;evict_mode和agent_state-&gt;flush_mode的值决定要不要进行evict和flush判断。在agent_maybe_evict和agent_maybe_flush中agent_state-&gt;evict_mode的值决定要不要直接执行evict或者flush。而agent_state值的计算过程是在agent_choose_mode函数中。agent_choose_mode函数计算一个PG的flush和evict行为的相关参数。该函数主要完成以下任务：</p><ul><li>统计当前PG中dirty object数量和当前PG中所有的object数量；（dirty object指的是脏数据对象)</li><li>统计当前PG中dirty object占用的字节数和当前PG中所有object占用的总的字节数；</li><li>分别从object数量角度和object占用的字节数角度计算dirty占比和full占比；</li><li>计算当前flush mode和evict mode；</li><li>更新agent_state-&gt;flush_mode和agent_state-&gt;evict_mode；</li><li>根据当前flush mode和evict mode决定是要将当前PG加入到待处理的PG队列中；</li></ul><p>从agent_choose_mode最后可以看到，如果缓存池需要flush或者evict，需要将待处理的PG加入到agent_queue队列中，这一动作是最终通过调用_enqueue函数实现，该函数主要完成以下任务：</p><ul><li>src&#x2F;osd&#x2F;OSD.h&#x2F;OSDService::_enqueue</li><li>判断是否需要调整agent线程要处理哪个pg set；</li><li>将待处理的pg加入到pg set中；</li><li>唤醒agent线程，执行flush或者evict任务；</li></ul><p>从agent_choose_mode最后可以看到，如果缓存池需不需要flush或者evict，但是如果之前agent线程有处理过该PG，需要将待处理的PG从agent_queue队列中移除掉，这一动作最终通过调用_dequeue函数实现，该函数主要完成以下任务：</p><ul><li>src&#x2F;osd&#x2F;OSD.h&#x2F;OSDService::_dequeue</li><li>根据old_priority从agent_queue队列中获取到相应的pg set；</li><li>在pg set中查找要移除的PG；如果找到了，从pg set中删除，并调整下一个要处理的PG；</li><li>如果删除之后的pg set没有任何一个PG，需要从agent_queue队列中移除，并调整下一个要处理的pg set；</li></ul><p><strong>agent_choose_mode流程图</strong><br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B04.jpg" alt="Cache Tier"><br><strong>agent_entry流程图</strong><br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B05.jpg" alt="Cache Tier"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Cache-Tier架构&quot;&gt;&lt;a href=&quot;#Cache-Tier架构&quot; class=&quot;headerlink&quot; title=&quot;Cache Tier架构&quot;&gt;&lt;/a&gt;Cache Tier架构&lt;/h2&gt;&lt;p&gt;Ceph存储集群如果采用廉价的PC和传统的机械硬盘进行搭建，</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph Cache Tier使用介绍</title>
    <link href="https://watsonlu6.github.io/Ceph-Cache-Tier%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/"/>
    <id>https://watsonlu6.github.io/Ceph-Cache-Tier%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/</id>
    <published>2022-08-25T08:45:47.000Z</published>
    <updated>2024-09-01T13:33:52.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、Ceph-Cache-Tier介绍"><a href="#1、Ceph-Cache-Tier介绍" class="headerlink" title="1、Ceph Cache Tier介绍"></a>1、Ceph Cache Tier介绍</h2><p>缓存层(Ceph Cache Tier)为 Ceph 客户端提供了存储在后备存储层中的一部分数据的更好 I&#x2F;O 性能。缓存分层涉及创建一个相对较快&#x2F;昂贵的存储设备池（例如固态硬盘），配置为缓存层，以及一个后备池，该池由纠删码或相对较慢&#x2F;更便宜的设备组成，配置为经济的存储层。Ceph 对象处理器决定对象的存放位置，而缓存代理负责决定何时将对象从缓存中刷新到后备存储层。因此，缓存层和后备存储层对 Ceph 客户端完全透明。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0.png" alt="Cache Tier"><br>缓存分层代理会自动处理缓存层和后备存储层之间的数据迁移。不过，管理员可以通过设置缓存模式来配置这种迁移方式。</p><ul><li><strong>写回模式 (writeback mode)：</strong> 如果基础层和缓存层被配置为写回模式，Ceph 客户端每次将数据写入时会从基础层接收到 ACK 确认。然后，缓存分层代理会判断是否设置了 <code>osd_tier_default_cache_min_write_recency_for_promote</code>。如果已经设置，并且在指定时间间隔内数据被写入的次数超过设定值，那么数据将被提升到缓存层。当 Ceph 客户端需要访问存储在基础层的数据时，缓存分层代理会从基础层读取数据并返回给客户端。当数据从基础层读取时，缓存分层代理会查阅 <code>osd_tier_default_cache_min_read_recency_for_promote</code> 的值，并决定是否将数据从基础层提升到缓存层。当数据从基础层提升到缓存层后，Ceph 客户端可以通过缓存层对其进行 I&#x2F;O 操作。这种模式非常适合处理可变数据（例如，照片&#x2F;视频编辑，事务性数据）。</li><li><strong>读代理模式 (readproxy mode)：</strong> 该模式将使用缓存层中已存在的对象，但如果缓存中不存在该对象，请求将被代理到基础层。这在从写回模式过渡到禁用缓存的过程中非常有用，因为它允许在缓存被清空的同时，工作负载能够正常运行，而不向缓存添加任何新对象。</li><li><strong>只读模式 (readonly)：</strong> 该模式在读操作时将对象提升到缓存层；写操作则会被直接转发到基础层。该模式适用于无需存储系统强制保持一致性的只读工作负载。（警告：当对象在基础层中被更新时，Ceph 不会尝试将这些更新同步到缓存中的相应对象。由于该模式被认为是实验性的，因此启用时必须传递 <code>--yes-i-really-mean-it</code> 选项。）</li><li><strong>无缓存模式 (none)：</strong> 该模式用于完全禁用缓存。</li></ul><h2 id="2、使用Ceph-Cache-Tier"><a href="#2、使用Ceph-Cache-Tier" class="headerlink" title="2、使用Ceph Cache Tier"></a>2、使用Ceph Cache Tier</h2><p>要设置缓存分层，您必须拥有两个池。一个将作为后备存储，另一个将作为缓存。在后续示例中，我们将缓存池称为 <code>hot-storage</code>，后备池称为 <code>cold-storage</code>。</p><h4 id="2-1、设置后备存储池"><a href="#2-1、设置后备存储池" class="headerlink" title="2.1、设置后备存储池"></a>2.1、设置后备存储池</h4><p>设置后备存储池通常涉及以下两种场景之一：</p><ul><li><strong>标准存储：</strong> 在这种情况下，池在 Ceph 存储集群中存储对象的多个副本。</li><li><strong>纠删码：</strong> 在这种情况下，池使用纠删码来更高效地存储数据，代价是会有一些性能上的折扣。</li></ul><p>在标准存储场景中，可以设置一个 CRUSH 规则来确定故障域（例如，osd、主机、机箱、机架、行等）。当规则中的所有存储驱动器尺寸、速度（包括 RPM 和吞吐量）和类型一致时，Ceph OSD 守护进程的性能最佳。关于创建规则的详细信息，请参阅 CRUSH Maps。一旦创建了规则，就可以创建后备存储池。</p><h4 id="2-2、设置缓存池"><a href="#2-2、设置缓存池" class="headerlink" title="2.2、设置缓存池"></a>2.2、设置缓存池</h4><p>设置缓存池的过程与标准存储场景相同，但有以下不同：缓存层的驱动器通常是高性能驱动器，位于各自的服务器中，并拥有自己的 CRUSH 规则。设置此类规则时，应考虑拥有高性能驱动器的主机，同时排除没有这些驱动器的主机。</p><h4 id="2-3、创建缓存层"><a href="#2-3、创建缓存层" class="headerlink" title="2.3、创建缓存层"></a>2.3、创建缓存层</h4><p>设置缓存层需要将一个后备存储池与一个缓存池关联起来：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier add &#123;storagepool&#125; &#123;cachepool&#125;</span><br></pre></td></tr></table></figure><p>要设置缓存模式，请执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier cache-mode &#123;cachepool&#125; &#123;cache-mode&#125;</span><br></pre></td></tr></table></figure><p>缓存层会覆盖后备存储层，因此还需要执行一个额外的步骤：必须将所有客户端流量从存储池直接引导到缓存池。为此，请执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier set-overlay &#123;storagepool&#125; &#123;cachepool&#125;</span><br></pre></td></tr></table></figure><h4 id="2-4、配置缓存层"><a href="#2-4、配置缓存层" class="headerlink" title="2.4、配置缓存层"></a>2.4、配置缓存层</h4><p>缓存层有多种配置选项。您可以使用以下方式设置缓存层配置选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; &#123;key&#125; &#123;value&#125;</span><br></pre></td></tr></table></figure><h5 id="目标大小和类型"><a href="#目标大小和类型" class="headerlink" title="目标大小和类型"></a>目标大小和类型</h5><p>Ceph 的生产缓存层使用<code>布隆过滤器</code>作为 <code>hit_set_type</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; hit_set_type bloom</span><br></pre></td></tr></table></figure><p><code>hit_set_count</code> 和 <code>hit_set_period</code> 定义了要存储的 HitSets 数量以及每个 HitSet 应覆盖的时间长度：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; hit_set_count 12</span><br><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; hit_set_period 14400</span><br><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; target_max_bytes 1000000000000</span><br></pre></td></tr></table></figure><p><em>注意</em></p><ul><li>较大的 <code>hit_set_count</code> 会消耗更多的 RAM，影响 <code>ceph-osd</code> 进程的内存使用。</li><li>通过对访问时间的分类，Ceph 可以判断 Ceph 客户端是否在一定时间内访问了一个对象至少一次或多次（即“年龄” vs “热度”）。</li></ul><h5 id="读写缓存设置"><a href="#读写缓存设置" class="headerlink" title="读写缓存设置"></a>读写缓存设置</h5><p><code>min_read_recency_for_promote</code> 定义了在处理读操作时检查对象存在性的 HitSets 数量。检查结果用于决定是否异步提升对象。其值应在 0 到 <code>hit_set_count</code> 之间。如果设置为 0，对象将始终被提升；如果设置为 1，则仅检查当前 HitSet，如果对象在当前 HitSet 中，将被提升，否则不提升。对于其他值，将检查对应数量的历史 HitSets，如果对象出现在最近的<code>min_read_recency_for_promote</code> 个 HitSets 中，将会被提升。<br><code>min_write_recency_for_promote</code>定义了写操作可以设置类似的参数。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; min_read_recency_for_promote 2</span><br><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; min_write_recency_for_promote 2</span><br></pre></td></tr></table></figure><p><em>注意</em></p><ul><li>周期越长、<code>min_read_recency_for_promote</code> 和 <code>min_write_recency_for_promote</code> 值越高，<code>ceph-osd</code> 守护进程消耗的 RAM 就越多。特别是在代理活动冲刷或驱逐缓存对象时，所有 <code>hit_set_count</code> 个 HitSets 都会加载到 RAM 中。</li></ul><h5 id="缓存大小设置"><a href="#缓存大小设置" class="headerlink" title="缓存大小设置"></a>缓存大小设置</h5><p>缓存分层代理执行两个主要功能：</p><ul><li><strong>冲刷：</strong> 代理识别已修改（或脏）对象并将其转发到存储池以供长期存储。</li><li><strong>驱逐：</strong> 代理识别未修改（或干净）的对象，并从中驱逐最少最近使用的对象。</li></ul><p><em><strong>绝对大小设置</strong></em><br>缓存分层代理可以基于字节总数或对象总数来冲刷或驱逐对象。要指定最大字节数，请执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; target_max_bytes &#123;bytes&#125;</span><br></pre></td></tr></table></figure><p>例如，要在 1 TB 时进行冲刷或驱逐，请执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> hot-storage target_max_bytes 1099511627776</span><br></pre></td></tr></table></figure><p>要指定最大对象数量，请执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; target_max_objects &#123;objects&#125;</span><br></pre></td></tr></table></figure><p>例如，要在 100 万个对象时进行冲刷或驱逐，请执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> hot-storage target_max_objects 1000000</span><br></pre></td></tr></table></figure><p><em>注意</em></p><ul><li>Ceph 无法自动确定缓存池的大小，因此这里需要对绝对大小进行配置，否则冲刷&#x2F;驱逐将无法正常工作。如果同时指定了两个限制，当任一阈值触发时，缓存分层代理将开始冲刷或驱逐。</li><li>仅当达到 <code>target_max_bytes</code> 或 <code>target_max_objects</code> 时，所有客户端请求才会被阻塞。</li></ul><p><em><strong>相对大小设置</strong></em><br>缓存分层代理可以相对于缓存池的大小（由绝对大小设置中的 <code>target_max_bytes</code> &#x2F; <code>target_max_objects</code> 指定）来冲刷或驱逐对象。当缓存池包含一定比例的已修改（或脏）对象时，缓存分层代理将冲刷它们到存储池。要设置 <code>cache_target_dirty_ratio</code>，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; cache_target_dirty_ratio &#123;0.0~1.0&#125;</span><br></pre></td></tr></table></figure><p>例如，将值设置为 0.4 当已修改（脏）对象达到缓存池容量的 40% 时开始冲刷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> hot-storage cache_target_dirty_ratio 0.4</span><br></pre></td></tr></table></figure><p>当已修改（脏）对象达到一定比例时，以更高速度冲刷这些对象。设置 <code>cache_target_dirty_high_ratio</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; cache_target_dirty_high_ratio &#123;0.0~1.0&#125;</span><br></pre></td></tr></table></figure><p>例如，将值设置为 0.6 当已修改（脏）对象达到缓存池容量的 60% 时开始积极地冲刷脏对象。显然，最好将值设置在 <code>dirty_ratio</code> 和 <code>full_ratio</code> 之间：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> hot-storage cache_target_dirty_high_ratio 0.6</span><br></pre></td></tr></table></figure><p>当缓存池达到一定容量比例时，缓存分层代理将驱逐对象以保持空闲容量。设置 <code>cache_target_full_ratio</code>，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; cache_target_full_ratio &#123;0.0~1.0&#125;</span><br></pre></td></tr></table></figure><p>例如，将值设置为 0.8，当未修改（干净）对象达到缓存池容量的 80% 时开始冲刷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> hot-storage cache_target_full_ratio 0.8</span><br></pre></td></tr></table></figure><h5 id="缓存年龄"><a href="#缓存年龄" class="headerlink" title="缓存年龄"></a>缓存年龄</h5><p>可以指定对象在缓存分层代理将最近修改（或脏）对象冲刷到后备存储池之前的最小年龄：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &#123;cachepool&#125; cache_min_flush_age &#123;seconds&#125;</span><br></pre></td></tr></table></figure><p>例如，在 10 分钟后冲刷修改（或脏）对象，请执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> hot-storage cache_min_flush_age 600</span><br></pre></td></tr></table></figure><p>可以指定对象在从缓存层驱逐之前的最小年龄：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool &#123;cache-tier&#125; cache_min_evict_age &#123;seconds&#125;</span><br></pre></td></tr></table></figure><p>例如，在 30 分钟后驱逐对象，请执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> hot-storage cache_min_evict_age 1800</span><br></pre></td></tr></table></figure><h5 id="移除缓存层"><a href="#移除缓存层" class="headerlink" title="移除缓存层"></a>移除缓存层</h5><p>移除缓存层的步骤取决于缓存是回写类型还是只读类型。<br><strong>移除只读缓存</strong><br>由于只读缓存没有已修改的数据，您可以在不丢失缓存中对象的任何最新更改的情况下禁用并移除它。<br>将缓存模式更改为 <code>none</code> 以禁用它：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier cache-mode &#123;cachepool&#125; none</span><br></pre></td></tr></table></figure><p>从后备池中移除缓存池：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier remove &#123;storagepool&#125; &#123;cachepool&#125;</span><br></pre></td></tr></table></figure><p><strong>移除回写缓存</strong><br>由于回写缓存可能包含已修改的数据，因此在禁用并移除它之前，必须采取措施确保不会丢失缓存中对象的任何最新更改。<br>将缓存模式更改为 <code>proxy</code>，以便新对象和已修改对象将被冲刷到后备存储池：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier cache-mode &#123;cachepool&#125; proxy</span><br></pre></td></tr></table></figure><p>确保缓存池已经被冲刷。此过程可能需要几分钟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados -p &#123;cachepool&#125; <span class="built_in">ls</span></span><br></pre></td></tr></table></figure><p>如果缓存池中仍有对象，可以手动冲刷它们。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados -p &#123;cachepool&#125; cache-flush-evict-all</span><br></pre></td></tr></table></figure><p>移除覆盖，以便客户端不会将流量引导到缓存中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier remove-overlay &#123;storagetier&#125;</span><br></pre></td></tr></table></figure><p>最后，从后备存储池中移除缓存层池：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tier remove &#123;storagepool&#125; &#123;cachepool&#125;</span><br></pre></td></tr></table></figure><h2 id="3、注意事项"><a href="#3、注意事项" class="headerlink" title="3、注意事项"></a>3、注意事项</h2><p>缓存分层会降低大多数工作负载的性能。在使用此功能之前，应格外谨慎。</p><ul><li><strong>依赖于工作负载：</strong> 缓存是否能够提升性能高度依赖于工作负载。由于将对象移入或移出缓存会产生一定的成本，缓存分层只有在数据集的访问模式存在较大偏斜时才有效，例如大多数请求集中访问少量对象。缓存池的大小应足够大，以捕获工作负载的工作集，从而避免缓存抖动。</li><li><strong>难以基准测试：</strong> 大多数用户用来衡量性能的基准测试在启用缓存分层时会显示出较差的性能，部分原因是这些测试很少将请求集中在少量对象上，缓存“预热”需要较长时间，而且预热的成本可能很高。</li><li><strong>通常更慢：</strong> 对于不适合缓存分层的工作负载，其性能通常会比没有启用缓存分层的普通 RADOS 池更慢。</li><li><strong>librados 对象枚举：</strong> librados 级别的对象枚举 API 在存在缓存时并不保证一致性。如果你的应用程序直接使用 librados 并依赖对象枚举，那么缓存分层可能不会如预期工作。（对于 RGW、RBD 或 CephFS，这不是问题。）</li><li><strong>复杂性：</strong> 启用缓存分层意味着 RADOS 集群内将使用大量额外的机制和增加的复杂性。这增加了你可能会遇到其他用户尚未遇到的系统错误的概率，并使你的部署面临更高的风险。</li></ul><h4 id="3-1、已知表现良好的工作负载"><a href="#3-1、已知表现良好的工作负载" class="headerlink" title="3.1、已知表现良好的工作负载"></a>3.1、已知表现良好的工作负载</h4><ul><li><strong>RGW 时间偏斜：</strong> 如果 RGW 工作负载的情况是几乎所有读操作都针对最近写入的对象，那么一个简单的缓存分层配置可以很好地工作，该配置在可配置的时间后将最近写入的对象从缓存层降级到基础层。</li></ul><h4 id="3-2、已知表现不佳的工作负载"><a href="#3-2、已知表现不佳的工作负载" class="headerlink" title="3.2、已知表现不佳的工作负载"></a>3.2、已知表现不佳的工作负载</h4><p>以下配置已知与缓存分层配合表现不佳。</p><ul><li><strong>RBD 与复制缓存和纠删码基础：</strong> 这是一个常见的请求，但通常表现不好。即使是相对倾斜的工作负载，仍然会将一些小的写入发送到冷对象上，而由于纠删码池尚不支持小的写操作，因此必须将整个对象（通常为 4 MB）迁移到缓存中，以满足一个小的（通常为 4 KB）写入请求。只有少数用户成功部署了这种配置，而且对他们而言，这仅在数据极为冷（备份）并且完全不敏感于性能的情况下有效。</li><li><strong>RBD 与复制缓存和基础层：</strong> 使用复制基础层的 RBD 比使用纠删码基础层的表现稍好，但仍然高度依赖于工作负载中的偏斜程度，并且非常难以验证。用户需要对其工作负载有很好的理解，并需要仔细调整缓存分层参数。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1、Ceph-Cache-Tier介绍&quot;&gt;&lt;a href=&quot;#1、Ceph-Cache-Tier介绍&quot; class=&quot;headerlink&quot; title=&quot;1、Ceph Cache Tier介绍&quot;&gt;&lt;/a&gt;1、Ceph Cache Tier介绍&lt;/h2&gt;&lt;p&gt;缓存</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>缓存基础与Ceph分层存储</title>
    <link href="https://watsonlu6.github.io/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/"/>
    <id>https://watsonlu6.github.io/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/</id>
    <published>2022-08-10T11:59:00.000Z</published>
    <updated>2024-09-01T13:34:14.905Z</updated>
    
    <content type="html"><![CDATA[<h2 id="缓存基础"><a href="#缓存基础" class="headerlink" title="缓存基础"></a>缓存基础</h2><ol><li><p><strong>缓存命中率</strong>：表示从缓存中获取数据的成功率，即缓存命中的次数与总访问次数的比值。缓存命中率越高，表示缓存系统的效率越高，能够更快地响应用户的请求。</p></li><li><p><strong>缓存失效率</strong>：表示从缓存中获取数据失败的次数与总访问次数的比值。缓存失效率越高，表示缓存系统的效率越低，需要从持久存储介质中读取数据的次数也越多，可能会导致系统的响应速度变慢。</p></li><li><p><strong>缓存容量</strong>：指缓存系统能够存储数据的最大容量。缓存容量的大小会影响缓存系统的性能和可靠性，如果缓存容量不足，可能会导致缓存系统频繁地进行evict操作，从而影响系统的响应速度和可用性。</p></li><li><p><strong>缓存算法</strong>：指缓存系统用于决定哪些数据被缓存，哪些数据被删除的算法。常见的缓存算法包括LRU（最近最少使用）、LFU（最不经常使用）、FIFO（先进先出）等。</p></li><li><p><strong>脏数据（Dirty Data）</strong>：指缓存中已经被修改但尚未被写回到持久存储介质（如磁盘）中的数据。这些数据需要及时写回到持久存储介质中以保证数据的一致性。常见的处理策略包括写回（write-back）和写直达（write-through）策略。</p></li><li><p><strong>干净数据（Clean Data）</strong>：指缓存中未被修改或已经被写回到持久存储介质中的数据。干净数据在缓存系统中可以快速读取，减少写入操作，优先选择删除干净数据可以避免写回操作带来的额外开销。</p></li><li><p><strong>evict操作</strong>：从缓存中移除某些数据，以释放缓存空间供其他数据使用。常用的策略包括LRU（Least Recently Used）等，根据最近最少使用的数据进行移除。</p></li><li><p><strong>flush操作</strong>：将缓存中的数据立即写回到持久性存储介质（例如硬盘），以确保缓存中的数据与存储介质中的数据保持一致。</p></li></ol><h4 id="数据一致性和性能考虑"><a href="#数据一致性和性能考虑" class="headerlink" title="数据一致性和性能考虑"></a>数据一致性和性能考虑</h4><ul><li><p><strong>脏数据的处理</strong>：存在脏数据可能导致数据一致性问题和性能问题，因此需要及时处理脏数据。选择适当的写回策略可以平衡数据一致性和系统性能。</p></li><li><p><strong>干净数据的优先删除</strong>：在缓存系统中，干净数据的存在可以提高系统性能，因为它们可以快速读取而不需要进行额外的写入操作。当需要从缓存中删除对象时，通常优先选择删除干净数据。</p></li><li><p><strong>flush操作的选择</strong>：通常在以下情况下使用flush操作：</p><ul><li>数据一致性要求高的场景，如数据库应用</li><li>性能要求不高或系统关闭时需要保证数据的持久性</li></ul></li><li><p><strong>evict操作的选择</strong>：通常在以下情况下使用evict操作：</p><ul><li>缓存空间不足，需要释放空间</li><li>数据访问模式固定或数据访问频率低</li><li>基于缓存替换算法（如LRU、LFU、FIFO等）</li></ul></li></ul><h4 id="缓存替换算法"><a href="#缓存替换算法" class="headerlink" title="缓存替换算法"></a>缓存替换算法</h4><ul><li><strong>LRU（Least Recently Used）</strong>：根据最近的访问时间来决定删除哪些数据。</li><li><strong>LFU（Least Frequently Used）</strong>：基于数据的访问频率选择删除数据。</li><li><strong>FIFO（First In First Out）</strong>：按照数据进入缓存的时间顺序移除数据。</li><li><strong>Random（随机）</strong>：随机选择数据进行删除，简单但效果不如其他算法。</li></ul><h4 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h4><ul><li><p><strong>优化evict操作</strong>：通过使用动态策略（基于数据使用情况）和静态策略（基于数据属性），可以提升缓存性能。了解系统的负载和压力情况也有助于优化evict操作。</p></li><li><p><strong>缓存命中率影响</strong>：evict操作可能导致缓存命中率下降，因为被删除的数据可能被访问到。flush操作通常不会直接影响缓存命中率，但需要高效的实现以避免影响系统性能。</p></li></ul><p>这些基本要素和策略可以帮助优化缓存系统的性能和可靠性，根据具体的应用需求进行适当的调整和选择。</p><h2 id="Ceph分层存储Cache-Tier"><a href="#Ceph分层存储Cache-Tier" class="headerlink" title="Ceph分层存储Cache Tier"></a>Ceph分层存储Cache Tier</h2><p>分层存储是存储领域中的一个重要分支，其思想基石是存储的金字塔模型——描述了快速设备通常容量小而性能高，慢速设备通常容量大而性能低。对于数据访问而言，通常在一段时间内，真实数据的访问是具有时间局部性和空间局部性的。时间局部性是指被访问的数据在短时间内可能再次被访问，空间局部性是指与被访问数据临近的数据有更大的概率被访问。故基于时间局部性理论产生了通常所说的缓存，如：cpu缓存、内存等；而基于空间局部性原理，产生了数据预取，如：指令预取（prefetch）、数据预读（read ahead）等。</p><p>目前Ceph的OSD主要可以基于SSD或者HDD的裸盘进行构建，机械盘通常比固态盘容量大、价格比固态盘低、但读写比固态盘慢，如何用机械盘和固态盘来提供一个高可靠、高性能、高性价比的分布式存储是需要解决的重要问题。如果全部基于SSD进行构建，其性能一定会最优，但是SSD价格昂贵，出于成本考虑，不可能全部采用SSD进行构建，那么SSD与HDD混合硬件架构就显得很有必要。</p><p>Ceph的缓存分层理论基础是数据存在热点，数据访问不均匀。通常，80%的应用只访问20%的数据，这20%的数据被称为热点数据。为了减少响应时间，可以将热点数据保存到性能较高的存储设备（如固态硬盘）中。在Cache Tiering中，有一个分层代理，当保存在缓存层的数据变冷或不再活跃时，该代理会将这些数据刷到存储层，并将其从缓存层中移除。这种操作称为刷新或逐出。在客户端读写数据时，Ceph的对象处理器负责决定对象存储的位置，而Cache Tier则决定何时将缓存层中的对象刷回后端存储层。对于写操作，请求到达缓存层后，完成写操作后直接应答客户端，之后由缓存层的代理线程负责将数据写入存储层。对于读操作，如果命中缓存层，直接在缓存层读取，否则可以重定向到存储层访问。如果数据近期有访问过，说明比较热，可以提升到缓存层中。对于Ceph客户端来说，缓存层和后端存储层是完全透明的。所有Ceph客户端都可以使用缓存层，因此Cache Tier具有提升块设备、Ceph对象存储、Ceph文件系统和原生绑定的I&#x2F;O性能的潜力。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8.jpg" alt="Cache Tier"></p><p>Ceph Cache Tier提供了快速存储池与慢速存储池间的分层缓存特性。通常来说，对于块存储用户而言，数据访问会有明显的时间局部性与空间局部性，故可以通过分层存储思想，改善资源配置及效率。Ceph提供了Cache Tier的解决方案，能够融合两种存储，通过合理配比提供容量与性能介于SSD与HDD之间的虚拟存储资源池。对于对象存储而言，目前主要对外提供基于S3与Swift restful api的访问接口。RGW对象存储可以通过对数据池进行Cache Tier，从而提高其访问效率。</p><p>在Ceph中，分层存储系统通过缓存和存储池的方式实现，热资源池可以将数据存储至那些管理SSD磁盘的OSD上，而冷资源池可以将数据存储至那些管理HDD磁盘的OSD上。若客户命中被访问的数据落在热资源池中，可以直接被访问，此时IO速度最快，接近SSD磁盘的性能。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A81.jpg" alt="Cache Tier命中"></p><p>若客户被访问的数据不落在热资源池中，出现缓存丢失的情况，需要转向去HDD盘上读取数据，而HDD盘处理请求访问速度为毫秒级别，故网络延时与请求处理延时可以近似忽略，认为其访问速度接近HDD磁盘的性能。这时候的处理分为两种：代理读写和数据拉取。当读写请求出现缓存丢失时，代理读写向后端请求冷数据，但缓存池不对数据进行缓存，直接将请求内容返回给客户端。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A82.jpg" alt="缓存池的代理读写"></p><p>读写请求出现缓存丢失时，缓存池向后端请求冷数据，在向后端请求冷数据后，会将数据读入缓存池，继续处理客户端请求并返回请求内容。此外，短时间内被多次访问的数据会被认为是热数据而拉取到热池中，这将消耗HDD磁盘的读带宽与SSD磁盘的写入带宽。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A83.jpg" alt="缓存池的数据拉取"></p><p>另一方面，在热池中的数据，需要定期回写入冷池，此时，回写数据将暂用SSD与HDD磁盘的部分带宽，这个过程叫数据回写。<br><img src="/images/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A84.jpg" alt="缓存池的数据回写"></p><p>还未回写入冷资源池的数据，在热资源池中再次被修改，这种情况越多，缓存效率越高，即相当于热资源池带宽充分利用，帮助冷资源池挡掉了大量的写入带宽。可以简单的认为，有1%的数据是需要脏回刷的（即回刷后的1%数据为clean状态，所以后续的命中会是非脏命中），如果所有数据都不脏回刷，且都访问命中的话，那么脏命中率为100%。</p><p>根据上述原理，不难发现，Ceph Cache Tier的性能取决于访问命中率。访问命中率越高时，存储系统越接近SSD磁盘的性能；反之，访问命中率越低时，越接近HDD磁盘的性能。另一方面，在Ceph中，缓存粒度以对象方式进行拉取与回写，故在实际情况下，如果缓存丢失过多，将会有大量的数据会被拉取，从而占用SSD磁盘的带宽，使得其访问带宽比SATA磁盘更差。然而，在实际生产使用过程中，数据总使用量总是逐步增加的，与此同时，热数据的量也将逐步的增加。那么，在整个使用周期中，随着数据量的增加，就必然会经历以下过程：首先刚刚开始使用时，数据量还很少。此时，所有数据全部能够被缓存，数据命中率为100%，效果很好。随着总数据量与热数据量不断的增加，缓存池已经无法容纳所有数据，只能容纳较多的热数据，此时缓存命中率会随之逐步的下降。随着数据的进一步增加，缓存命中率低于某个临界值了，此时保持同样大小的缓存池已经无法给使用带来足够好的收益。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;缓存基础&quot;&gt;&lt;a href=&quot;#缓存基础&quot; class=&quot;headerlink&quot; title=&quot;缓存基础&quot;&gt;&lt;/a&gt;缓存基础&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;缓存命中率&lt;/strong&gt;：表示从缓存中获取数据的成功率，即缓存命中的次数与总访问次数的</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>存储系统缓存/分层相关论文</title>
    <link href="https://watsonlu6.github.io/%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E7%BC%93%E5%AD%98%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/"/>
    <id>https://watsonlu6.github.io/%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E7%BC%93%E5%AD%98%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/</id>
    <published>2022-02-10T07:31:33.000Z</published>
    <updated>2024-08-04T07:59:18.239Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TDC-Pool-level-object-cache-replacement-algorithm-based-on-temperature-density"><a href="#TDC-Pool-level-object-cache-replacement-algorithm-based-on-temperature-density" class="headerlink" title="TDC: Pool-level object cache replacement algorithm based on temperature density"></a>TDC: Pool-level object cache replacement algorithm based on temperature density</h2><ul><li>在原生Ceph系统的基础上，提出缓存池的基于热度密度缓存替换算法，计算每个对象消耗空间的热度密度，并以最低的热度密度驱逐对象。通过驱逐对命中率贡献不大的对象，提高缓存池的命中率以及分层存储性能。</li><li>在 Ceph 中，Cache Tier通过缓存机制和存储池方式的实现，其将热数据存储SSD池，冷数据存储HDD池。若客户访问的数据直接命中落在SSD池中，可以直接被访问，此时IO速度最快 ，接近SSD磁盘的性能。但如果被访问的数据不在SSD池中，需要转向去HDD池上读取数据，而HDD盘处理请求访问速度为毫秒级别，则认为其访问速度接近HDD磁盘的性能。</li><li>目前 Cache Tier 采用的是基于频率估计的类 LRU缓存替换算法，并未充分利用数据对象所携带的元数据信息，这种频率估计概率存在误差，缓存命中率性能有限，且很难达到理论极限。因此实际使用时缓存命中率较低，导致较长的 IO 路径，使得Cache Tier性能表现较差。为进一步提升缓存命中率，考虑 Cache Tier 数据对象可以携带更多信息的特点，提出基于热度密度的缓存替换算法(TDC)。对象的热度计算基于访问频率，而热度密度计算基于热度和缓存对象占用时空的比例。通过引入热度密度计算，可以更准确地评估对象对命中率贡献，从而更有效地驱逐对命中率贡献不大的对象。</li><li>在用户文件上传到Ceph集群时，Ceph通过调用file_to_extents()函数将文件分割成若干个对象。面对数目庞大的对象，为了计算每个对象的访问热度值，采用Multiple Bloomfilter记录存储池每个对象访问的频率，通过这种方式有效地捕获更细粒度的最近性和频率。</li><li>将数据对象在缓存池中花费的时间和占用缓存空间视为缓存成本，这样，将缓存池中的对象对缓存命中率的贡献转化成一种成本效益分析。热度密度就是一种综合考虑对象大小、访问频率和访问时间等因素的指标，可以用来评估每个对象对缓存命中率的贡献，并根据热度密度来进行缓存替换。整个图形的热度密度可以表示为所有命中对象的贡献之和除以所有对象的占用缓存资源之和。通过比较已缓存对象的热度密度，并按一定比例删除热度密度较低的对象。</li></ul><h2 id="A-Survey-on-Tiering-and-Caching-in-High-Performance-Storage-Systems"><a href="#A-Survey-on-Tiering-and-Caching-in-High-Performance-Storage-Systems" class="headerlink" title="A Survey on Tiering and Caching in High-Performance Storage Systems"></a>A Survey on Tiering and Caching in High-Performance Storage Systems</h2><ul><li>论文讨论了针对高性能存储系统的缓存和分层解决方案研究。在第2节中，简要介绍了存储设备及其技术。在第3节中，讨论关于缓存解决方案及缓存算法的研究。在第4节中，讨论关于存储分层解决方案的研究。缓存和分层已被长期用于隐藏存储层次结构中慢速设备的长延迟。</li><li>基于响应时间，计算机存储系统被设计为有组织的多级层次结构，旨在提高整体性能和存储管理。不同类型的存储介质根据其性能、容量和控制技术指定为级别。通常，层次结构中级别越低，其带宽越小，存储容量越大。层次结构中有四个主要级别：处理器寄存器和缓存（SRAM、触发器或锁存缓冲区）、主存储器（DRAM）、辅助存储（SSD、HDD）、第三级存储（可移动存储设备）。图1概述了当前可用的和新兴的存储技术。表1比较了不同的计算机存储技术。</li><li>硬盘驱动器由固定在主轴周围的刚性快速旋转磁盘和使用致动器臂重新定位的磁头组成。数字数据以磁材料薄膜磁化转变的形式存储在每个磁盘上。HDD的机电方面和存储数据的串行化使得HDD比所提到的非易失性存储器技术慢几个数量级。然而，它的低价格和极高的密度使它成为二级和三级存储级别的理想选择。根据表1，HDD的容量可以比DRAM大1000倍，而操作延迟大约慢106倍。</li><li>IDC报告[58]预测到2020年，云将只触及数字世界的24%，13%将存储在云中，63%可能根本无法触及[58]。需要保护的数据的速度超过40%，甚至比数字世界本身还要快。因此，大部分数据通常存储在更便宜、更可靠和更大的设备中，而未经处理的部分数据则保存在快速存储介质中。因此，无疑需要具有缓存&#x2F;分层机制的混合存储系统</li><li>为了减轻慢设备的长延迟，可以在混合存储系统中使用缓存机制。缓存子系统有两个主要原则：1）在将原始数据保持在层次结构的中等级别时，缓存中存在一个处理不足的数据副本；以及2）缓存层中数据的生命周期短，并且它是临时的。</li><li>分层ARC（H-ARC）[12]缓存是一种基于NVM的缓存，它优化了ARC算法，以考虑最近、频率、脏和干净四种状态，并首先将缓存拆分为脏&#x2F;干净页缓存，然后将每个部分拆分为最近&#x2F;频率页缓存。基于与ARC类似的机制，它在每个级别上按层次调整每个部分的大小。因此，H-ARC在缓存中保持较高频率的脏页的时间更长。</li><li>如今，多层存储系统中使用了许多具有不同特性和容量的存储介质。缓存和分层之间的主要区别在于，在缓存系统中，数据的副本保存在缓存中，而在分层系统中，原始数据通过升级和降级两种操作在多个层之间迁移。数据根据应用程序需求和可用层的特性进行分类，通常分为热层和冷层。热数据驻留在性能层，冷数据留在容量层。考虑到随机性、传输速度等多种因素，可能有两个以上的层。</li></ul><h2 id="eMRC-Efficient-Miss-Ratio-Approximation-for-Multi-Tier-Caching"><a href="#eMRC-Efficient-Miss-Ratio-Approximation-for-Multi-Tier-Caching" class="headerlink" title="eMRC: Efficient Miss Ratio Approximation for Multi-Tier Caching"></a>eMRC: Efficient Miss Ratio Approximation for Multi-Tier Caching</h2><ul><li>研究高效的多层缓冲命中率分析技术</li><li>未命中率曲线（MRC）是捕获工作负载特性和调整系统行为的有用工具。MRC表示缓存大小和相应的缓存未命中率之间的关系。假设随着时间的推移，工作负载相对稳定，从观察到的IO跟踪得出的MRC对于单层缓存有效工作[13]。</li><li>许多存储缓存分配方法使用未命中率曲线（MRC）来提高缓存效率。然而，他们只关注单层缓存架构，并需要整个MRC作为缓存管理的输入，而现代数据中心采用分层缓存架构以最大限度地提高资源利用率。由于每个缓存层的逐出策略和容量不同，为多层缓存生成MRC（我们称之为未命中率函数）要困难得多。我们引入eMRC，一种多维未命中率近似技术，以实现多层缓存的高效MRC生成。论文使用了一种新颖的多维性能悬崖去除方法和凸包近似技术，以使用少量采样点有效地生成没有悬崖的多维MRC。</li><li>多层缓存需要一种有效的、低开销的缓存管理方案，因为多层的任意缓存配置可能会因其副作用而对租户不利[27]。为具有不同服务级别目标（SLO）的租户配置缓存需要对缓存的每一层进行高效、准确的缓存性能分析。</li><li>扩展：已经有很多关于MRC(miss ratio curves)的理论，来对上层缓存进行建模，得出程序分配缓存大小和性能的关系模型：MissRatio&#x3D;F(capacity)。基本分为两种方法：1、通过数据重新访问距离(reuse distance)，来建立MRC模型。2、通过数据重新访问时间(reuse time)，来建立MRC模型。</li></ul><h2 id="PHOEBE-Reuse-Aware-Online-Caching-with-Reinforcement-Learning-for-Emerging-Storage-Models"><a href="#PHOEBE-Reuse-Aware-Online-Caching-with-Reinforcement-Learning-for-Emerging-Storage-Models" class="headerlink" title="PHOEBE: Reuse-Aware Online Caching with Reinforcement Learning for Emerging Storage Models"></a>PHOEBE: Reuse-Aware Online Caching with Reinforcement Learning for Emerging Storage Models</h2><ul><li>NVMe和SSD是新兴存储技术的公认代表，具有数据持久性、高访问速度、低功耗和字节寻址能力。高性能采用这些技术的一个关键问题是如何正确定义智能缓存层，以便能够很好地弥补新兴技术和主存储器之间的性能差距。快速的主机端内存和慢速的后端存储驱动器之间的延迟差异很大，存储I&#x2F;O仍然是性能瓶颈，这导致了难以置信的长I&#x2F;O等待时间和大量CPU闲置浪费。为了缓解这种延迟差异，缓存层被广泛用于驻留在主存储器和后端存储驱动器之间。缓存系统的性能通常受到三个因素的影响：数据分配策略、数据热识别的准确性和数据驱逐策略。数据分配策略基本上控制数据流，并确定各种数据的接纳，例如只读、只读或两者兼而有之；数据热识别的高精度可以防止不必要数据对缓存的污染，通过局部保护提高缓存性能；数据驱逐策略决定在缓存已满时驱逐哪个数据块，从而间接增加了缓存的有效容量。这三个因素关注三个不同的方面，具有高度相关性。值得一提的是，传统的缓存策略，如LRU和LFU，并不是一个符合所有这些因素的通用解决方案（Li等人2019；Li和Gu 2020；Liu等人2020）。</li><li>PHOEBE是一种基于强化学习的在线缓存的可重用优化方案，适用于广泛的新兴存储模型。通过与缓存环境和数据流的持续交互，PHOEBE能够从单个跟踪中提取关键的时间数据依赖性和相对位置信息，随着时间的推移变得越来越智能。实验结果表明，PHOEBE能够将LRU和最先进的基于在线学习的缓存策略Belady最优策略之间的缓存未命中率差距分别缩小70.3%和52.6%。</li><li>PHOEBE预测了一个新定义的指标，即停留优先级，以表示每个数据块的相对重要性，而不是显式预测重用距离，重用距离可能是无界值，从而增加了高精度预测的难度和复杂性。当驱逐事件发生时，缓存根据其停留优先级值替换数据块（驱逐具有最低停留优先级的数据块），旨在最大化缓存命中率。停留优先级值具有时间滞后特性，在过时时间戳处的高优先级值不能在最新时间戳处保持同样高，因此它们通常与相应的时间戳组合以得出最终驱逐决定。PHOEBE关注9个特征来提取同时考虑全局和局部模式的重用信息。前六个特征和最后一个是全局特征；第7和第8特征是从当前数据块之前的滑动窗口收集的局部特征。数据块地址、数据块地址增量、频率、重复使用距离、最终重复使用距离、平均重复使用距离、滑动窗口中的频率、滑动窗口中的缓存未命中数、优先级值</li><li>将在线缓存问题建模为马尔可夫决策过程</li><li>LeCaR（Vietri等人，2018）：一种基于在线学习的缓存策略，根据学习到的概率在LRU和LFU缓存算法之间切换。</li><li>相关工作：将机器学习应用于缓存优化有两个主要途径：设计智能预取策略或改进缓存替换策略。</li></ul><h2 id="Sprout-A-functional-caching-approach-to-minimize-service-latency-in-erasure-coded-storage"><a href="#Sprout-A-functional-caching-approach-to-minimize-service-latency-in-erasure-coded-storage" class="headerlink" title="Sprout: A functional caching approach to minimize service latency in erasure-coded storage"></a>Sprout: A functional caching approach to minimize service latency in erasure-coded storage</h2><ul><li>利用缓存优化纠删码存储的性能，没有优化缓存策略。</li><li>论文提出一种新的具有纠删码存储的缓存框架，称为功能缓存。功能缓存涉及在缓存中使用纠删码块，使得存储节点中的块和缓存组合形成的代码是最大距离可分离（MDS）纠删码。</li></ul><h2 id="SSD-HDD混合存储中基于顺序封装的缓存取出"><a href="#SSD-HDD混合存储中基于顺序封装的缓存取出" class="headerlink" title="SSD-HDD混合存储中基于顺序封装的缓存取出"></a>SSD-HDD混合存储中基于顺序封装的缓存取出</h2><ul><li>论文提出了一种基于顺序打包的缓存逐出技术，该技术将垃圾收集（GC）块中的相邻冷数据页与位于其他SSD块中的相邻冷数据页数据分组。然后，打包这些数据页一起flush到较低级别的HDD存储中，以充分利用HDD的高顺序带宽。该方法可以减少写放大对SSD缓存的负面影响，并有助于提高SSD-HDD混合存储的I&#x2F;O性能。</li><li>Shi等人[13]提出了SSDUP+，该SSDUP＋采用SSD设备来缓冲随机访问的数据，并且顺序访问的数据被刷新到磁盘的低级别存储。在SSDUP+中，需要SSD缓存来缓存热读数据，尽管它们揭示了序列特征并直接刷新到HDD上。也就是说，如果SSD缓存已满，SSDUP+必须使用最近最少使用（LRU）策略来处理缓存逐出。</li><li>将热数据放在快速存储中，将冷数据放在慢速存储中并不是一个新想法，分层存储管理通常采用快速存储作为慢速存储的缓冲[5]。已经推出了许多结合HDD和SSD的混合存储解决方案，以提高读写吞吐量[11、12、14、15]。Chen等人[11]提出Hystor将低成本HDD和高速SSD相结合，以识别关键数据（例如元数据），从而将其保存在SSD中以快速响应。此外，它利用SSD作为回写缓冲区来吸收写请求，从而产生更好的写性能。HotDataTrap[20]建议仅缓冲SSD缓存中的热数据，并将冷数据直接刷新到HDD，这为热数据存储在缓存空间中提供了更多机会。类似地，Zhang等人[21]提出了一种基于机器学习的混合存储系统写策略。具体来说，它使用机器学习来识别只写数据，并将其直接刷新到HDD中，以最大限度地减少SSD的写入流量。混合存储系统的性能受到数据刷新例程或缓存逐出方案的严重影响[2，8]。</li><li>SeqPack的Hot Read&#x2F;Write Separate模块维护两个固定长度的LRU链接列表，以分别记录最近读取和写入的数据页，用于筛选热读取数据页而不是热写入。具体而言，在发生读或写访问之后，可以将数据页插入或移动到相应链接列表的开头。然后，两个LRU链接列表都维护最近访问的读写页，这样我们可以筛选热读数据页，但不筛选热写数据页，它们可以始终缓存在SSD缓存中，同时其他数据页被视为顺序打包的候选页。sequential packer模块依赖于所提出的顺序打包模型，通过将GC块中的弹出页面与其他SSD块中的冷数据页面打包，将许多随机写入分组为大型顺序写入。GC Selector模块以较小的成本释放SSD空间，引入了一种基于成本的选择方法来定位GC目标块，<ul><li>性能指标：I&#x2F;O响应时间、缓存命中率、长尾延迟。</li></ul></li></ul><h2 id="Improving-in-memory-file-system-reading-performance-by-fine-grained-user-space-cache-mechanisms（大数据场景的缓存优化）"><a href="#Improving-in-memory-file-system-reading-performance-by-fine-grained-user-space-cache-mechanisms（大数据场景的缓存优化）" class="headerlink" title="Improving in-memory file system reading performance by fine-grained user-space cache mechanisms（大数据场景的缓存优化）"></a>Improving in-memory file system reading performance by fine-grained user-space cache mechanisms（大数据场景的缓存优化）</h2><ul><li>随着服务器的内存容量越来越大，分布式内存文件系统已被广泛使用，该系统使应用程序能够快速与数据交互。然而，现有的分布式内存文件系统在小数据读取中仍然面临数据访问性能低的问题（非混合存储），这严重降低了它们在许多重要的大数据场景中的可用性。论文分析了影响内存文件读取性能的因素，并提出了一种两层用户空间缓存管理机制：在第一层，我们缓存数据包引用以减少频繁的页面故障中断（包级缓存）；在第二层，我们缓存和管理小文件数据单元，以避免冗余的进程间通信（对象级缓存）。设计了一个基于子模块函数优化理论的细粒度缓存模型，以有效地管理客户端具有部分重叠片段的可变长度缓存单元，更准确地识别热碎片，避免不必要的RPC通信。。重点是设计可变长度缓存块的管理机制和替换策略。</li><li>[15]提出了一种缓存模型，该模型使用不同的LRU队列来管理不同大小的文件。因此，它可以减少小文件被部分锁定的频率。为了使应用程序能够从客户端节点读取和写入数据，而不会失去全局文件系统命名空间的优势。传统的缓存替换策略包括FIFO、LRU[17]、LFU[18]、ARC[19]、FBR[20]和2Q[21]。根据[19]的结果，ARC通过复杂的自调整机制改进了基本的LRU策略，在各种工作负载上优于上述算法。然而，对大数据工作负载的实验表明，即使使用非常大的缓存空间，传统的缓存方法仍然遭受相对较低的命中率[22]。为了进一步提高I&#x2F;O性能，研究人员提出了大量方法，这些方法通过专门的指标来替代缓存候选[23–26]。</li><li>许多智能技术应用于广泛领域[33-35]。受此启发，一些研究人员采用了先进的数学方法或机器学习模型来描述和分析缓存问题。例如，[24，36]考虑马尔可夫决策过程背景下的缓存替换问题，[37，38]提出了基于强化学习和长短期记忆神经网络的更复杂场景下的智能缓存替换框架。特别是，在这些算法中，基于概率模型的EVA[24]通过充分利用所有缓存单元的命中、逐出和年龄分布，实现了最佳性能。</li><li>数据压缩是另一种常用于优化缓存空间利用率的策略。通常，压缩用于优化存储级别的访问性能。事实上，压缩也可以用于优化缓存性能[45–48]。压缩可以扩大有效的缓存容量，因此，通过保存更多对象可以减少缓存未命中。然而，由于压缩&#x2F;解压缩过程，压缩方案引入了额外的访问延迟。更糟糕的是，由于压缩比不可预测，这可能会导致性能下降。</li><li>数据包级缓存机制：在分布式文件系统中，要读取的数据被拆分为数据包，以便在多次小规模读取时容错和提高性能。当读取文件时，客户端调用mmap()将数据包映射到进程的虚拟地址空间，并调用munmap()读取后立即释放映射区域。在某种情况下，每个计算任务都将调用mmap()和munmap()，导致多页错误中断。为了提高读取性能，论文设计了一种包级缓存机制：在读取后不会立即释放它们，而是设法存储读取数据包的引用以供后续使用。数据包级缓存机制维护缓存队列。如果数据包的引用被减少到0，则该数据包的参考被放入队列。当队列大小超过阈值, 包将由munmap()根据缓存迁移策略调用。（带有优先级计数和缓存队列的数据包级缓存机制）</li><li>对象级缓存机制：哈希表：在第一级，整个文件被分成几个桶；每个文件片段根据其起始地址和结束地址被放入特定的存储桶中。红黑树：当单个存储桶中的单元数量超过限制时，缓存模型会根据当前存储桶中所有元素的地址将其放入红黑树中。当存储桶中的单元数小于限制时，原始红黑树将被删除。双重链接列表：文件片段自然按其起始地址和结束地址排序。每个缓存单元包含指向最近单元的前指针和后指针。</li><li>为了获得对象级缓存模型中缓存管理问题的近似解，论文实现了[53]提出的两种Greedy和ISK算法</li></ul><h2 id="Improving-NAND-Flash-Based-Disk-Caches"><a href="#Improving-NAND-Flash-Based-Disk-Caches" class="headerlink" title="Improving NAND Flash Based Disk Caches"></a>Improving NAND Flash Based Disk Caches</h2><ul><li>论文介绍了Flash在当今的服务器平台中用作磁盘缓存的研究。提出了两项改进。第一种方法通过将基于Flash的磁盘缓存拆分为单独的读写区域来提高性能和可靠性。第二种通过采用可编程闪存控制器来提高可靠性。它可以根据应用的需求改变错误码强度（可纠正位的数量）和存储单元可以存储的位的数量（单元密度）。Flash的可管理性和可靠性是一个具有挑战性的问题，需要解决这些问题才能将Flash完全集成到数据中心。提出了一种用于NAND闪存的硬件辅助软件管理磁盘缓存。</li><li>为了减轻磨损，对基于闪存的磁盘缓存的闪存擦除执行磨损级别管理。对于读缓存和写缓存，首先使用LRU策略选择要逐出的块，该策略针对磁盘缓存容量未命中（对于读缓存）或容量写入（对于写缓存，需要首先擦除块的异地写入）。然而，如果该块的磨损超过最新块的磨损预定阈值，则驱逐与最小磨损相对应的块（最新块）以平衡磨损水平。从整个闪存块集合中选择最新的块。在驱逐最新的块之前，它的内容被迁移到旧块。</li></ul><h2 id="Optimizing-the-SSD-Burst-Buffer-by-Traffic-Detection（细粒度缓存替换策略）"><a href="#Optimizing-the-SSD-Burst-Buffer-by-Traffic-Detection（细粒度缓存替换策略）" class="headerlink" title="Optimizing the SSD Burst Buffer by Traffic Detection（细粒度缓存替换策略）"></a>Optimizing the SSD Burst Buffer by Traffic Detection（细粒度缓存替换策略）</h2><ul><li>HPC存储系统仍然使用硬盘驱动器（HDD）作为其主要存储设备。固态驱动器（SSD）被广泛部署为HDD的缓冲区。还提出了突发缓冲器来管理突发写入请求的SSD缓冲。虽然突发缓冲区在许多情况下可以提高I&#x2F;O性能，但它具有一些限制，例如需要大的SSD容量以及计算阶段和数据flush阶段之间的和谐重叠。提出了一种称为SSDUP+的方案。SSDUP+旨在通过解决上述限制来改善突发缓冲区。首先，为了减少对SSD容量的需求，只选择一部分数据写入SSD，而其余数据则直接写入HDD，而不牺牲I&#x2F;O性能。开发了一种新的方法来检测和量化写入流量中的数据随机性。此外，提出了一种自适应算法来动态地对随机写入进行分类。通过这样做，需要更少的SSD容量来实现与其他突发缓冲方案类似的性能。然后，为了克服计算阶段和flush阶段完美重叠的困难，提出了SSD缓冲区的流水线机制，在流水线机制中，SSD缓冲区被分成两半。当一半接收写入数据时，另一半完全占用将数据从SSD刷新到HDD。其中数据缓冲和刷新在流水线中执行。为了提高I&#x2F;O吞吐量，采用了流量感知刷新策略来减少HDD中的I&#x2F;O干扰。最后，为了进一步提高SSD中缓冲随机写入的性能，SSDUP+通过使用日志结构存储数据，将SSD中的随机写入转换为顺序写入。此外，SSDUP+使用AVL树结构来存储数据的序列信息。SSDUP+以减少满足突发性大规模I&#x2F;O访问性能所需的SSD容量，从另一个角度来看，在相同的SSD容量下提高I&#x2F;O性能。</li><li>硬盘驱动器（HDD）仍然被用作HPC存储系统中的主要永久存储设备，部分原因是其成本低，可以在访问大型连续数据块时提供高带宽。然而，HDD有一个主要缺点：当随机访问数据时，由于磁盘头的缓慢机械移动，它们的性能很差。固态驱动器（SSD）等新的存储设备由于其接近零的寻道延迟和优异的性能（特别是对于随机访问）而被广泛部署在HPC环境中。然而，SSD比HDD昂贵得多。因此，在大规模生产HPC系统中使用SSD作为唯一的存储设备并不是一个经济高效的解决方案，更不用说SSD的技术限制，例如磨损和寿命有限的问题。解决HDD随机数据访问问题的一个流行解决方案是使用SSD缓冲HDD和计算节点之间的数据流。另一方面，对HDD的突发随机写入可能会显著降低HPC存储系统上运行的数据密集型应用程序的性能。为了解决上述问题，引入了突发缓冲器，它使用SSD缓冲器作为计算节点和基于HDD的存储服务器之间的中间层，以吸收突发写入请求。</li></ul><h2 id="Exploration-and-Exploitation-for-Buffer-Controlled-HDD-Writes-for-SSD-HDD-Hybrid-Storage-Server（细粒度缓存替换策略）"><a href="#Exploration-and-Exploitation-for-Buffer-Controlled-HDD-Writes-for-SSD-HDD-Hybrid-Storage-Server（细粒度缓存替换策略）" class="headerlink" title="Exploration and Exploitation for Buffer-Controlled HDD-Writes for SSD-HDD Hybrid Storage Server（细粒度缓存替换策略）"></a>Exploration and Exploitation for Buffer-Controlled HDD-Writes for SSD-HDD Hybrid Storage Server（细粒度缓存替换策略）</h2><ul><li>结合固态驱动器（SSD）和硬盘驱动器（HDD）的混合存储服务器为应用程序提供了成本效益和μ级响应能力。会导致HDD通常利用不足，而SSD使用过度，特别是在密集写入下。这会导致SSD的快速磨损和高尾部延迟。HDD的一系列顺序和连续写入呈现出周期性、阶梯状的写入延迟模式，即低（35μs）、中（55μs）和高延迟（12毫秒），这是由HDD控制器内的缓冲写入导致的。可以利用HDD的潜在μs级IO延迟，以吸收过多的SSD写入，而不会降低性能。论文建立了一个描述阶梯行为的HDD写入模型，并设计了一个配置过程来初始化和动态重新校准模型参数。然后，提出了一种缓冲区控制写入方法（BCW），以主动控制缓冲区写入，从而用应用程序数据调度低延迟和中延迟时段，并用填充数据填充高延迟时段。利用BCW，设计了一个混合IO调度器（MIOS），以自适应地将传入数据引导到SSD和HDD。进一步设计了多HDD调度以最小化HDD写入延迟</li></ul><h2 id="Cache-Replacement-Policy-Based-on-Expected-Hit-Count"><a href="#Cache-Replacement-Policy-Based-on-Expected-Hit-Count" class="headerlink" title="Cache Replacement Policy Based on Expected Hit Count"></a>Cache Replacement Policy Based on Expected Hit Count</h2><ul><li>现有处理器采用最近最少使用（LRU）策略的变体来确定替换的缓存块。不幸的是，LRU提供的服务与Belady的MIN之间存在很大差距，这是最佳的更换策略。Belady的MIN要求选择具有最长重用距离的缓存块，因此，由于需要了解未来，这是不可行的。在论文研究中，发现缓存块的预期命中数与其重用距离的倒数之间存在很强的相关性。论文提出了用于替换缓存中的缓存块的预期命中计数（EHC）策略，在现有低成本替换策略的基础上，采用基于命中计数的缓存块选择程序，以显著提高最后一级缓存中缓存块选择的质量，而无需相应的区域开销。</li><li>现代处理器经常需要从最后一级缓存中移出一段数据，以便为新数据留出空间。替换策略决定了在所有可能的候选项中，在新数据到达时应该从缓存中删除哪个候选项。</li><li>使用第二届缓存替换锦标赛（CRC2）发布的模拟框架评估预期命中计数（EHC）策略。</li></ul><h2 id="Hystor-Making-the-best-use-of-solid-state-drives-in-high-performance-storage-systems（分层存储）"><a href="#Hystor-Making-the-best-use-of-solid-state-drives-in-high-performance-storage-systems（分层存储）" class="headerlink" title="Hystor: Making the best use of solid state drives in high performance storage systems（分层存储）"></a>Hystor: Making the best use of solid state drives in high performance storage systems（分层存储）</h2><ul><li>由于SSD相对较高的价格和较低的容量，需要解决的一个主要系统研究问题是如何以成本和性能有效的方式使SSD在高性能存储系统中发挥最有效的作用。论文设计和实现Hystor高性能混合存储系统，Hystor将SSD和HDD作为一个单块设备进行管理，Hystor可以有效地识别（1）可能导致长延迟或（2）语义关键的块（例如文件系统元数据），并将其存储在SSD中以供将来访问，从而实现显著的性能改进。为了进一步利用最先进SSD中极高的写入性能，Hystor还充当回写缓冲区，以加快写入请求。</li><li>将高容量SSD视为存储的一部分，而不是缓存位置。相应地，与基于缓存的传统策略不同，基于缓存的策略在每次数据访问时频繁更新缓存内容，论文只定期和异步地重新组织设备之间的块布局，以实现长期优化。Hystor通过三个主要组件实现其数据管理的优化目标。首先，通过实时监控I&#x2F;O流量，Hystor自动学习工作负载访问模式并识别性能关键块。只有能够带来最大性能优势的块才能从HDD重新映射到高速SSD。第二，通过有效利用现有接口中可用的高级信息，Hystor识别语义关键块（例如文件系统元数据），并及时为它们提供高优先级，使其留在SSD中，这进一步提高了系统性能。第三，传入的写入被缓冲到低延迟SSD中，以提高写入密集型工作负载的性能。</li></ul><h2 id="Back-to-the-Future-Leveraging-Belady’s-Algorithm-for-Improved-Cache-Replacement"><a href="#Back-to-the-Future-Leveraging-Belady’s-Algorithm-for-Improved-Cache-Replacement" class="headerlink" title="Back to the Future: Leveraging Belady’s Algorithm for Improved Cache Replacement"></a>Back to the Future: Leveraging Belady’s Algorithm for Improved Cache Replacement</h2><ul><li>缓存是减少数据访问的长延迟的重要机制，其有效性受到其替换策略的显著影响。论文解释了缓存替换算法如何通过将其应用于过去的缓存访问来学习Belady的算法，以告知未来的缓存替换决策。并提出了基于Belady的缓存替换算法，将Belady方法的变体应用于过去的内存访问历史。如果过去的行为是未来行为的良好预测，论文提出的策略将接近Belady算法的行为。新缓存替换策略由两部分组成。第一个使用OPTgen算法重构了Belady对过去缓存访问的最佳解决方案。第二个是一个预测器，它可以学习OPT对过去PC的行为，以告知同一PC对未来负载的驱逐决定。</li><li>在缺乏明确反馈的情况下，现有的替换策略基于启发式方法，如最近最少使用（LRU）和最近最多使用（MRU），这两种方法都适用于不同的工作负载。然而，即使使用越来越聪明的技术来优化和组合这些策略，这些基于启发式的解决方案也仅限于特定类别的访问模式，无法在更复杂的场景中表现良好。</li><li>将缓存替换视为一个二进制分类问题，其目标是确定传入的行是缓存友好的还是缓存厌恶的：缓存友好的行以高优先级插入，而缓存厌恶的行被标记为未来冲突的驱逐候选行。为了确定传入线路应如何分类，Hawkeye重构了Belady对过去访问的最优解决方案，以了解单个加载指令的行为。</li></ul><h2 id="Performance-Evaluation-of-Traditional-Caching-Policies-on-A-Large-System-with-Petabytes-of-Data"><a href="#Performance-Evaluation-of-Traditional-Caching-Policies-on-A-Large-System-with-Petabytes-of-Data" class="headerlink" title="Performance Evaluation of Traditional Caching Policies on A Large System with Petabytes of Data"></a>Performance Evaluation of Traditional Caching Policies on A Large System with Petabytes of Data</h2><ul><li>大多数现有的缓存性能研究都评估填充相对较小缓存的、相当小的文件。很少有报告讨论了传统缓存替换策略在超大系统上的性能。论文在PB级存储系统中，全面评估了几种缓存策略的性能，包括先进先出（FIFO）、最近最少使用（LRU）和最不频繁使用（LFU）。研究表明当应用于大型数据集和小型数据集时，传统缓存策略能够提高性能。</li><li>在整个评估过程中，FIFO缓存替换策略经常导致比LRU或LFU显著更低的命中率，尽管有一小部分数据点的FIFO命中率较高。LRU缓存替换策略在所有测试的替换策略中获得了最高的比率，但与LFU策略相比，LRU导致平均命中率的标准偏差更高。当排除攻击性用户时，LFU缓存替换策略的平均命中率最高，而当LRU包含攻击性用户后，该策略的命中率仅超过0.29%。</li><li>将可用缓存的大小增加一倍，最多可以提高12%的命中率。论文建议额外的需求可以通过简单地扩展缓存大小来降低性能增益的价值。论文认为，对专用缓存策略进行更彻底的检查能够专注于大规模缓存大小的优化。通过将所使用的缓存大小增加一倍，命中率发生了相对较小的变化，这表明，在使用更有效的缓存策略的同时，缩小总体缓存大小将节省大量空间，并减少用作缓存所需的活动磁盘数。</li><li>预取是另一种有可能显著提高缓存性能的技术[24]，[25]，[26]，[27]，[28]。事实上，预取比简单地用流行文档加载缓存更有效[33]。有效的预取策略可以帮助缓存将命中率提高50%[32]。智能地预加载数据可以在不增加成本的情况下实现性能提高，因为使用预取的缓存可以与不使用预取缓存的两倍缓存一样有效[30]。已经证明，使用有效的预取方案可以显著减少不同缓存替换策略的命中率之间的差异，增强了格式良好的预取算法的重要性[33]。</li></ul><h2 id="Improving-Cache-Management-Policies-Using-Dynamic-Reuse-Distances"><a href="#Improving-Cache-Management-Policies-Using-Dynamic-Reuse-Distances" class="headerlink" title="Improving Cache Management Policies Using Dynamic Reuse Distances"></a>Improving Cache Management Policies Using Dynamic Reuse Distances</h2><ul><li>论文提出了一种新的PDP缓存管理策略，一种使用动态重用距离来进一步改进缓存替换策略，该策略防止替换缓存线，直到对其缓存集进行一定数量的访问，称为保护距离（PD）。该策略保护缓存线足够长，可以重复使用，但不能超过该长度，以避免缓存污染。这可以与旁路机制相结合，该机制也依赖于动态重用分析，以绕过预期重用较少的管线。如果没有未保护的行，则忽略未命中提取。提出了一种基于动态重用历史的命中率模型，并动态计算了使命中率最大的PD。PD会定期重新计算，以跟踪程序的内存访问行为和阶段。</li></ul><h2 id="Optimum-Caching-versus-LRU-and-LFU-Comparison-and-Combined-Limited-Look-Ahead-Strategies"><a href="#Optimum-Caching-versus-LRU-and-LFU-Comparison-and-Combined-Limited-Look-Ahead-Strategies" class="headerlink" title="Optimum Caching versus LRU and LFU: Comparison and Combined Limited Look-Ahead Strategies"></a>Optimum Caching versus LRU and LFU: Comparison and Combined Limited Look-Ahead Strategies</h2><ul><li>将基于最近最少使用（LRU）和最不频繁使用（LFU）替换原则的web缓存策略与根据Belady算法的最佳缓存进行比较。研究了一种结合LRU、LFU或其他非预测方法的有限前瞻最优策略的组合方法。、通过模拟，根据请求跟踪和独立参考模型（IRM）的前瞻性程度来评估命中率增益，并对观察到的行为进行分析确认。</li><li>将常用缓存策略的命中率和更新工作量与最佳缓存作为性能上限进行比较。缓存策略性能评估的三种基本方法是通过跟踪模拟、根据综合模型模拟运行生成的请求模式和分析。</li><li>对一种组合缓存方法的评估表明，优化缓存不仅可以提供缓存命中率上限，而且可以部分用于视频流的缓存和服务于巨大请求工作负载的缓存。对缓存和请求特定参数对有限前瞻方案适用性的影响进行更详细的分析，以供将来研究。</li></ul><h2 id="A-Distributed-Block-Storage-Optimization-Mechanism-Based-on-Ceph"><a href="#A-Distributed-Block-Storage-Optimization-Mechanism-Based-on-Ceph" class="headerlink" title="A Distributed Block Storage Optimization Mechanism Based on Ceph"></a>A Distributed Block Storage Optimization Mechanism Based on Ceph</h2><ul><li>为了应对企业在提高块存储服务的资源利用率和读&#x2F;写速率方面面临的挑战，Ceph提供了缓存分层，以提高异构存储环境中的群集性能。然而，由于缓存污染，缓存分层中最近最少使用的（LRU）算法会驱逐更多有价值的数据，这会导致某些请求的延迟更高；同时，当在存储节点上分配数据时，可扩展哈希下的受控复制（CRUSH）算法只考虑存储节点容量，这使得Ceph无法动态平衡节点的I&#x2F;O负载。为了解决这些问题，提出了一种基于预测模型的存储选择策略，以提高缓存池中对象访问的命中率，提高集群的整体I&#x2F;O性能；此外，还提出了缓存池I&#x2F;O负载平衡策略。与原生机制相比，所提出的块存储优化机制可以实现更高的I&#x2F;O吞吐量和更均衡的I&#x2F;O负载。</li><li>当数据在缓存层被逐出时，缓存分层中的LRU算法仅基于最近的访问记录逐出数据，这可能会由于偶尔的冷数据访问而导致逐出更有价值的热数据[3]。基于Ceph的缓存分层机制，论文提出了一种基于预测模型的存储选择策略，该策略根据对象访问频率确定对象请求是访问SSD OSD池还是访问后端HDD OSD池。该策略可以减少冷数据处理所造成的不必要开销，从而提高集群的总体I&#x2F;O性能。</li><li>为了有效利用缓存分层中有限的缓存池资源，冷数据应该存储在后端存储池中，而热数据应该存储到缓存池中。因此，在海量数据存储的背景下，区分数据的热量（即访问频率）并采用不同的处理策略可以充分利用Cache Tiering中的存储资源，并减少冷数据处理（例如冷数据从后端存储池进入缓存池，LRU驱逐冷数据）所造成的不必要开销。提出了一种基于预测模型的存储选择策略。该策略适应海量数据存储的特点，分析存储对象的长期访问记录。同时，根据某一时间段内的对象热度，判断是选择访问SSD缓存池还是后端HDD存储池，以减少冷数据处理带来的不必要开销，最终提高集群性能。</li></ul><h2 id="Maximizing-Cache-Performance-Under-Uncertainty（提出EVA）2017-HPCA"><a href="#Maximizing-Cache-Performance-Under-Uncertainty（提出EVA）2017-HPCA" class="headerlink" title="Maximizing Cache Performance Under Uncertainty（提出EVA）2017 HPCA"></a>Maximizing Cache Performance Under Uncertainty（提出EVA）2017 HPCA</h2><ul><li>指出Belady理论假设了对未来的完全了解，但这在实践中是不可用的，其明显在信息不完善的情况下是次优的。并建议：对于实际的缓存替换，应该根据其经济增加值（即其预期命中率与平均值的差异）来替代。缓存替换中的两个主要权衡：命中概率和缓存空间，并描述了EVA如何在一个直观的度量中协调它们。通过借鉴马尔可夫决策过程（MDP）理论，证明了EVA最大化了缓存命中率。</li><li>最常见的缓存替换策略是使用最近性和频率启发式。大多数缓存替换策略采用某种形式的最近性，有利于最近被引用的候选人：例如，LRU仅使用最近性，而RRIP[17，39]预测较老的候选人需要更长的时间才能被引用。类似地，一些不假设最近的政策仍然基于候选人最后被引用的时间：PDP[14]保护候选人直到某个年龄；IRGD[35]使用年龄的启发式函数。另一种常见的缓存替换策略解释动态行为的方式是通过频率，倾向于先前重用的候选：例如，LFU单独使用频率，ARC[26]和SRRIP[17]等“抗扫描”策略倾向于至少重用一次的候选。</li><li>EVA缓存替换策略：本质上是一种成本效益分析，即候选数据的命中概率是否值得其所消耗的缓存空间。EVA将每个候选数据选视为一项投资，试图留住利润最高的候选候选（以命中率衡量）。首先，EVA奖励每个候选数据预期的未来命中率。然后，由于缓存空间是一种稀缺资源，EVA需要考虑每个候选将消耗多少空间。EVA通过对每个候选数据在缓存中花费的时间“收费”来实现这一点。具体而言，EVA以单行的平均命中率（即缓存的命中率除以其大小）对候选项收费，因为这是消耗缓存空间的长期机会成本。<br>EVA &#x3D; Expected hits - (Cache hit rate&#x2F; Cache size) * Expected time</li><li>EVA策略的实现主要包括以下几个步骤：<ol><li>计算每个缓存行的经济增值（EVA）：EVA是一个衡量缓存行价值的指标，它考虑了缓存行的命中概率和占用缓存空间的时间成本。具体地，EVA等于缓存行的期望命中次数减去缓存的命中率乘以缓存行在缓存中的时间。 </li><li>选择EVA最小的缓存行进行替换：当需要替换缓存行时，EVA策略会选择EVA最小的缓存行进行替换。这是因为EVA最小的缓存行对缓存的贡献最小，替换它可以最大化缓存的命中率。</li><li>更新缓存行的EVA值：当缓存行被访问时，EVA策略会更新它的EVA值。具体地，EVA策略会根据缓存行的命中情况和占用缓存空间的时间，重新计算缓存行的EVA值。 </li><li>调整缓存大小：EVA策略还可以根据缓存的命中率和缓存行的EVA值，动态调整缓存的大小。具体地，当缓存的命中率较低时，EVA策略会增加缓存的大小；当缓存的命中率较高时，EVA策略会减小缓存的大小。 EVA策略的实现比较简单，只需要对每个缓存行维护一个EVA值，并选择EVA最小的缓存行进行替换即可。</li></ol></li></ul><h2 id="LHD-Improving-Cache-Hit-Rate-by-Maximizing-Hit-Density-2018-NSDI"><a href="#LHD-Improving-Cache-Hit-Rate-by-Maximizing-Hit-Density-2018-NSDI" class="headerlink" title="LHD: Improving Cache Hit Rate by Maximizing Hit Density  2018 NSDI"></a>LHD: Improving Cache Hit Rate by Maximizing Hit Density  2018 NSDI</h2><ul><li>云应用程序的性能严重依赖于数据中心键值缓存的命中率。键值缓存通常使用最近最少使用（LRU）作为其逐出策略，但在实际工作负载下，LRU的命中率远不是最佳的。论文提出最小命中密度（LHD）缓存替换算法，这是一种针对键值缓存的新驱逐策略。LHD预测每个对象每消耗空间的预期命中率（命中密度），过滤对缓存命中率贡献不大的对象。与先前的驱逐策略不同，LHD不依赖启发式，而是使用条件概率严格地模拟对象的行为，以实时调整其行为。</li><li>缓存命中率的小幅增加会对应用程序性能产生巨大影响。例如，将命中率从98%提高到99%，只需1%，就可以将对数据库的请求数量减半。使用上面使用的延迟数，这将平均服务时间从210µs减少到110µs（接近2倍），并且对于云应用程序来说，重要的是，将长延迟请求的尾部减半[21]。为了提高缓存命中率，云提供商通常会扩展服务器数量，从而增加缓存总容量[37]。从长远来看，添加缓存容量是不可行的，因为命中率随着缓存容量的增加呈对数增长[3，13，20]。需要大量内存才能显著影响命中率。在一定的缓存空间条件下，可以采用高效的缓存替换策略来提高缓存命中率。</li><li>流行的内存缓存使用最近最少使用（LRU）或LRU的变体作为其逐出策略。然而，LRU远不是缓存工作负载的最佳选择，因为：当工作负载具有可变的对象大小时，LRU的性能会受到影响，以及常见的访问模式暴露了LRU中的病态，导致命中率低。LRU的这些缺点已经得到了充分的记录，先前的工作已经提出了许多针对LRU的驱逐政策[4，14，16，25，35，38，40]。然而，这些策略并没有被广泛采用，因为它们通常需要大量的参数调整，这使得它们的性能不可靠，并且全局同步状态会影响它们的请求吞吐量。</li><li>论文提出命中密度的概念，用它衡量对象对缓存命中率的贡献程度。根据每个对象的信息（其年龄或大小）推断出每个对象的命中密度，然后以最小的命中密度（LHD）驱逐该对象。最小命中密度（LHD）是一种基于命中密度的缓存替换策略。LHD在线监控对象，并使用条件概率预测其可能的行为。LHD利用了许多不同的对象特性（例如，年龄、频率、应用程序id和大小），并且很容易支持其他对象。动态排名使LHD能够随时间调整其替换策略，以适应不同的应用程序工作负载，而无需任何手动调整。例如，在某个工作负载上，LHD可能最初接近LRU，然后切换到最近使用的（MRU）、最不频繁使用的（LFU）或其组合。LHD动态预测每个对象每消耗空间的预期命中率或命中密度，并以最低的命中率驱逐对象。通过过滤掉对缓存命中率贡献不大的对象，LHD逐渐提高了平均命中率。</li><li>根据Memcachier[36]提供的为期一周的商业memcached跟踪和Microsoft Research提供的存储跟踪对LHD进行了评估[48]。LHD显著提高了先前策略的命中率，例如，与LRU相比，将未命中率减少了一半，与最近的策略相比，减少了四分之一，并且还避免了诸如影响先前策略的性能悬崖等问题。图1显示了实现与LHD相同命中率所需的缓存大小，Memcachier上为256 MB，Microsoft跟踪上为64 GB。LHD需要的空间比以前的驱逐策略少得多，从而节省了现代数据中心数千台服务器的成本。</li><li>先前的缓存替换策略以许多不同的方式改进了LRU。几乎所有的政策都通过额外的机制来改善其最坏的病理状况。例如，ARC[35]使用两个LRU列表来区分新进入的对象，并限制来自不常访问对象的污染。类似地，AdaptSize[9]在LRU列表前面添加了一个概率过滤器，以限制大型物体的污染。最近的一些策略将访问划分为多个LRU列表，以消除性能悬崖[6，18，51]或在不同大小的对象之间分配空间[10，17，18，37，41，43，49]。所有这些策略都使用LRU列表作为核心机制，因此保留了最近性作为内置假设。此外，他们增加的机制可以引入新的假设和病理。例如，ARC通过将频繁访问的对象与新允许的对象放在一个单独的LRU列表中，并倾向于驱逐新允许的物体，从而假设频繁访问的物体更有价值。这通常是LRU的改进，但可能表现为病态。</li><li>EVA，一种最近针对处理器缓存的驱逐策略[7，8]，引入了使用条件概率来平衡命中与消耗的资源的想法。LHD和EVA之间有几个显著的差异，使LHD能够在关键价值工作负载上表现出色。首先，LHD和EVA使用不同的排名功能。EVA根据对象的命中率（而不是命中密度）对其进行排名。</li><li>LHD算法的实现过程如下：<ol><li>首先，需要为每个对象计算其期望的命中率。这可以通过以下公式计算： Hit density &#x3D; Hit probability * Object size &#x2F; Expected time in cache 其中，Hit probability是对象在其生命周期内被访问的概率，Object size是对象的大小，Expected time in cache是对象在缓存中的期望时间。在LHD算法中，Expected time in cache是通过对象的访问模式和缓存的大小等因素来计算的。具体地，可以使用以下公式计算对象的Expected time in cache： Expected time in cache &#x3D; (Cache size &#x2F; Object size) * (1 &#x2F; Hit probability) 其中，Cache size是缓存的大小，Object size是对象的大小，Hit probability是对象在其生命周期内被访问的概率。这个公式的意思是，如果缓存中有足够的空间来存储对象，那么对象在缓存中的期望时间就是对象被访问的平均间隔时间的倒数。这个期望时间可以用来计算对象的期望命中率，从而帮助LHD算法更好地预测对象的命中率。</li><li>然后，需要为每个对象维护一个命中率分布。这可以通过记录对象的命中和驱逐时间来实现。当对象被命中时，将其命中时间添加到命中率分布中。当对象被驱逐时，将其驱逐时间添加到驱逐率分布中。 </li><li>当需要驱逐一个对象时，LHD算法会选择命中率分布最小的对象进行驱逐。这可以通过计算每个对象的命中率分布的加权平均值来实现。具体地，对于每个对象，将其命中率分布的每个时间点乘以其命中率，然后将所有时间点的乘积相加，得到该对象的加权平均命中率。然后，选择加权平均命中率最小的对象进行驱逐。</li><li>在实现过程中，还可以使用其他技术来优化LHD算法的性能。例如，可以使用分类来改进预测，以便更好地考虑对象的特征。还可以使用并发技术来提高算法的吞吐量。 总之，LHD算法的实现过程包括计算对象的期望命中率，维护命中率分布，选择命中率分布最小的对象进行驱逐等步骤。通过这些步骤，LHD算法可以更好地预测对象的命中率，从而提高缓存</li></ol></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;TDC-Pool-level-object-cache-replacement-algorithm-based-on-temperature-density&quot;&gt;&lt;a href=&quot;#TDC-Pool-level-object-cache-replacement-al</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Ceph PG介绍</title>
    <link href="https://watsonlu6.github.io/ceph-PG%E4%BB%8B%E7%BB%8D/"/>
    <id>https://watsonlu6.github.io/ceph-PG%E4%BB%8B%E7%BB%8D/</id>
    <published>2021-11-10T10:22:25.000Z</published>
    <updated>2024-09-01T13:29:00.495Z</updated>
    
    <content type="html"><![CDATA[<p>Ceph并不是直接通过CRUSH算法将数据对象一一映射到OSD中的，这样做将非常复杂与低效。而且，Ceph用户并不需要了解实际的CRUSH算法是怎么运行的，只需要关心数据保存在哪里即可。Ceph通过存储池Pool和放置组Placement Groups (PGs)来实现CRUSH算法进行数据寻址。<br><img src="/images/PG%E4%BB%8B%E7%BB%8D_1.png" alt="Cache Tier"><br>PG聚合了池中的对象。按对象跟踪 RADOS 对象的位置和元数据在计算上是昂贵的。对于具有数百万个 RADOS 对象的系统，按对象跟踪位置是不切实际的。Ceph 客户端计算一个 RADOS 对象应位于哪个 PG。作为计算的一部分，客户端会对对象 ID 进行哈希处理，并执行涉及池中 PG 数量和池 ID 的操作。</p><p>属于 PG 的 RADOS 对象的内容存储在一组 OSD 中。例如，在一个大小为 2 的复制池中，每个 PG 将在两个 OSD 上存储对象，如下所示：<br><img src="/images/PG%E4%BB%8B%E7%BB%8D_2.png" alt="Cache Tier"><br>如果 OSD #2 发生故障，另一个 OSD 将被分配到 Placement Group #1，然后用 OSD #1 中所有对象的副本填充。如果池大小从 2 更改为 3，将分配一个额外的 OSD 到 PG，并接收 PG 中所有对象的副本。<br>分配给 PG 的 OSD 并不是专门属于该 PG 的；相反，OSD 也与来自同一池或其他池的其他 PG 共享。在我们的例子中，OSD #2 由 Placement Group #1 和 Placement Group #2 共享。如果 OSD #2 发生故障，那么 Placement Group #2 必须恢复对象副本（利用 OSD #3）。</p><p>PG是 Ceph 中的逻辑池的子集。PG 执行将对象（作为一组）放置到 OSDs 中的功能。Ceph 以 PG 为单位管理数据, 这比管理单个 RADOS 对象更具扩展性。在集群中拥有较多的 PG的集群，相比具有较少 PG 的相同集群，数据分布更均衡。每个 Ceph 的内部 RADOS 对象都会映射到特定的 PG，而每个 PG 又只属于一个 Ceph 池。</p><p>当 PG 数量增加时，会发生几个后果。新的 PG 被分配到 OSD。CRUSH 函数的结果发生变化，这意味着一些来自已经存在的 PG 的对象被复制到新的 PG 中，并从旧的 PG 中删除。</p><p>在理想条件下，对象会在PG之间均匀分布。由于CRUSH计算每个对象的PG，但不知道每个与PG关联的OSD中存储了多少数据，因此PG和OSD的数量比例可能对数据分布产生显著影响。</p><p>例如，假设在一个有三个副本的池中，只有一个PG分配给十个OSD。在这种情况下，由于CRUSH没有其他选择，只会使用三个OSD。然而，如果有更多PG可用，RADOS对象更有可能在OSD之间均匀分布。CRUSH会尽力使OSD在所有现有PG之间均匀分布。</p><p>只要PG的数量比OSD多一个或两个数量级，分布通常会比较均匀。例如：3个OSD对应256个PG，10个OSD对应512个PG，或者10个OSD对应1024个PG。</p><p>但是，不均匀的数据分布可能由于PG与OSD比例之外的因素而出现。例如，由于CRUSH不考虑RADOS对象的大小，一些非常大的RADOS对象的存在可能会造成不平衡。例如，假设有一百万个4 KB的RADOS对象，总计4 GB，均匀分布在10个OSD的1024个PG中。这些RADOS对象将会在每个OSD上消耗4 GB &#x2F; 10 &#x3D; 400 MB。如果再往池中添加一个400 MB的RADOS对象，则在该RADOS对象所在的PG的三个OSD上，每个OSD将被填充到400 MB + 400 MB &#x3D; 800 MB，而其他七个OSD仍然只有400 MB。</p><p><strong>每个PG对OSD和MON增加了内存、网络和CPU的需求</strong>。这些需求必须始终得到满足，并在恢复期间增加。实际上，PG的主要作用之一是通过将对象聚集在一起分担这些开销。<br>因此，减少PG的数量可以节省大量资源。</p><h2 id="自动缩放PG"><a href="#自动缩放PG" class="headerlink" title="自动缩放PG"></a>自动缩放PG</h2><p>PG是 Ceph 用于分配数据的内部实现细节。自动缩放提供了一种管理 PG，特别是管理不同池中 PG 数量的方法。当启用 pg-autoscaling 时，集群可以根据预期的集群和池使用情况，对每个池的 PG 数量（pgp_num）进行建议或自动调整。<br>每个池都有一个 <code>pg_autoscale_mode</code> 属性，可以设置为以下值：</p><ul><li><strong>off</strong>：禁用该池的自动缩放。管理员需要为每个池选择合适的 <code>pgp_num</code>。有关更多信息，请参见选择 PG 数量。</li><li><strong>on</strong>：启用对给定池的 PG 数量的自动调整。</li><li><strong>warn</strong>：当 PG 数量需要调整时发出健康检查警告。</li></ul><p>要设置现有池的自动缩放模式，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &lt;pool-name&gt; pg_autoscale_mode &lt;mode&gt;</span><br></pre></td></tr></table></figure><p>对于在集群初始设置后创建的池，也有一个 <code>pg_autoscale_mode</code> 设置。要更改此设置，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global osd_pool_default_pg_autoscale_mode &lt;mode&gt;</span><br></pre></td></tr></table></figure><p>可以使用 <code>noautoscale</code> 标志来禁用或启用所有池的自动缩放。默认情况下，该标志设置为 <code>off</code>，但可以通过以下命令将其设置为 <code>on</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set noautoscale</span><br></pre></td></tr></table></figure><p>要将 <code>noautoscale</code> 标志设置为 <code>off</code>，请运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool unset noautoscale</span><br></pre></td></tr></table></figure><p>要获取标志的值，请运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get noautoscale</span><br></pre></td></tr></table></figure><p><strong>查看 PG 缩放状态</strong><br>要查看每个池、其相对利用率以及对 PG 数量的任何推荐更改，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool autoscale-status</span><br></pre></td></tr></table></figure><p>输出将类似于：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">POOL    SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO BIAS PG_NUM  NEW PG_NUM  AUTOSCALE BULK</span><br><span class="line">a     12900M                3.0        82431M  0.4695                                          8         128  warn      True</span><br><span class="line">c         0                 3.0        82431M  0.0000        0.2000           0.9884  1.0      1          64  warn      True</span><br><span class="line">b         0        953.6M   3.0        82431M  0.0347                                          8              warn      False</span><br></pre></td></tr></table></figure><ul><li><strong>POOL</strong>：池的名称。</li><li><strong>SIZE</strong>：池中存储的数据量。</li><li><strong>TARGET SIZE</strong>（如果存在）：预计池中存储的数据量，由管理员指定。系统使用两个值中的较大者进行计算。</li><li><strong>RATE</strong>：池的倍数，决定了消耗多少原始存储容量。例如，一个三副本池的比例为 3.0，一个 k&#x3D;4 m&#x3D;2 的纠删编码池的比例为 1.5。</li><li><strong>RAW CAPACITY</strong>：负责存储池数据（以及可能的其他池数据）的特定 OSD 上的总原始存储容量。</li><li><strong>RATIO</strong>：池使用的存储量与总原始存储容量的比率。换句话说，RATIO 定义为 (SIZE * RATE) &#x2F; RAW CAPACITY。</li><li><strong>TARGET RATIO</strong>（如果存在）：池的预期存储（即管理员指定的池预计消耗的存储量）与设置了目标比例的所有其他池的预期存储之比。如果同时指定了 <code>target_size_bytes</code> 和 <code>target_size_ratio</code>，则 <code>target_size_ratio</code> 优先。</li><li><strong>EFFECTIVE RATIO</strong>：对目标比例进行两次调整后的结果：<ul><li>减去预期被使用的目标容量。</li><li>在设置了目标比例的池之间对目标比例进行归一化，以便它们集体瞄准集群容量。例如，四个目标比例为 1.0 的池将具有 0.25 的有效比例。</li></ul></li><li><strong>BIAS</strong>：用作倍数，根据之前有关特定池应有多少 PG 的信息手动调整池的 PG 数量。</li><li><strong>PG_NUM</strong>：池当前的 PG 数量，或如果正在进行 pg_num 更改，则为池正在努力达到的 PG 数量。</li><li><strong>NEW PG_NUM</strong>（如果存在）：系统建议的池的 pg_num 值。它始终是 2 的幂，并且仅在推荐值与当前值的差异大于默认的 3 倍时出现。</li><li><strong>AUTOSCALE</strong>：池的 <code>pg_autoscale_mode</code>，设置为 <code>on</code>、<code>off</code> 或 <code>warn</code>。</li><li><strong>BULK</strong>：确定池是否是批量池。它的值为 True 或 False。批量池预计会很大，应该最初具有大量 PG，以避免性能问题。另一方面，非批量池预计会很小（例如，一个 .mgr 池或元数据池）。</li></ul><p><em>注意</em><br>如果 <code>ceph osd pool autoscale-status</code> 命令没有返回任何输出，可能至少有一个池跨越了多个 CRUSH 根。这种“跨越池”问题可能发生在以下场景中：当新部署自动在默认 CRUSH 根上创建 .mgr 池时，后续池的创建规则将它们约束到特定的阴影 CRUSH 树。例如，如果您创建一个限制为 <code>deviceclass = ssd</code> 的 RBD 元数据池和一个限制为 <code>deviceclass = hdd</code> 的 RBD 数据池，您将遇到此问题。要解决此问题，将跨越池约束到仅一个设备类。在上述场景中，可能会有一个 <code>replicated-ssd</code> CRUSH 规则生效，可以通过运行以下命令将 .mgr 池约束到 ssd 设备：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set .mgr crush_rule replicated-ssd</span><br></pre></td></tr></table></figure><p>此干预将导致少量的回填，但通常这些流量很快就会完成。</p><p><strong>自动扩缩容</strong><br>在最简单的自动扩缩容方法中，集群允许根据使用情况自动调整 <code>pgp_num</code>。Ceph 会考虑整个系统的总可用存储和目标 PG 数量，考虑每个池中存储的数据量，并相应地分配 PG。系统采取保守的方法，仅在当前 PG 数量（pg_num）与推荐数量的差异超过 3 倍时，才对池进行更改。<br>每个 OSD 的目标 PG 数量由 <code>mon_target_pg_per_osd</code> 参数确定（默认值：100），可以通过以下命令调整：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set global mon_target_pg_per_osd 100</span><br></pre></td></tr></table></figure><p>自动缩放器会分析池并在每个子树基础上进行调整。由于每个池可能映射到不同的 CRUSH 规则，每条规则可能将数据分配到不同的设备，Ceph 将独立考虑层次结构中每个子树的利用率。例如，映射到 ssd OSD 的池和映射到 hdd OSD 的池将根据这两种不同设备类型的数量来确定各自的最佳 PG 数量。<br>如果一个池使用了两个或更多 CRUSH 根（例如，同时包含 ssd 和 hdd 设备的阴影树），自动缩放器会在管理器日志中发出警告。警告中会列出池的名称和重叠的根集。自动缩放器不会对具有重叠根的池进行扩缩容，因为这种情况可能会导致扩缩容过程出现问题。我们建议将每个池约束到仅一个根（即一个 OSD 类别），以消除警告并确保扩缩容过程成功。</p><p><strong>管理标记为批量的池</strong><br>如果一个池被标记为批量池，则自动缩放器会为池分配完整的 PG 数量，然后仅在池的使用比例不均时才缩减 PG 数量。然而，如果一个池未被标记为批量池，则自动缩放器会以最小 PG 数量启动池，仅在池中使用量增加时才创建额外的 PG。<br>要创建一个标记为批量池的池，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create &lt;pool-name&gt; --bulk</span><br></pre></td></tr></table></figure><p>要设置或取消设置现有池的批量标志，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &lt;pool-name&gt; bulk &lt;true/false/1/0&gt;</span><br></pre></td></tr></table></figure><p>要获取现有池的批量标志，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get &lt;pool-name&gt; bulk</span><br></pre></td></tr></table></figure><p><strong>指定预期池大小</strong><br>当集群或池首次创建时，它只消耗了集群总容量的一小部分，并且系统认为它只需要少量 PG。然而，在某些情况下，集群管理员知道哪些池在长期内可能会消耗大部分系统容量。当 Ceph 提供了这些信息时，可以从一开始就使用更合适的 PG 数量，从而避免后续更改 <code>pg_num</code> 和相关的数据迁移开销。</p><p>池的目标大小可以通过两种方式指定：一种是与池的绝对大小（以字节为单位）相关，另一种是作为与所有其他设置了 <code>target_size_ratio</code> 的池的权重关系。</p><p>例如，要告诉系统 <code>mypool</code> 预计将消耗 100 TB 的容量，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set mypool target_size_bytes 100T</span><br></pre></td></tr></table></figure><p>或者，要告诉系统 <code>mypool</code> 预计将消耗相对于设置了 <code>target_size_ratio</code> 的其他池的 1.0 比例，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set mypool target_size_ratio 1.0</span><br></pre></td></tr></table></figure><p>如果 <code>mypool</code> 是集群中唯一的池，则预计它将使用集群总容量的 100%。然而，如果集群中包含一个目标比例设置为 1.0 的第二个池，则两个池都预计将使用集群总容量的 50%。</p><p><code>ceph osd pool create</code> 命令具有两个命令行选项，可以在创建池时设置目标大小：<code>--target-size-bytes &lt;bytes&gt;</code> 和 <code>--target-size-ratio &lt;ratio&gt;</code>。</p><p>注意，如果指定的目标大小值不合理（例如，大于集群总容量），则会引发健康检查（POOL_TARGET_SIZE_BYTES_OVERCOMMITTED）。</p><p>如果为池同时指定了 <code>target_size_ratio</code> 和 <code>target_size_bytes</code>，则会忽略后者，前者将用于系统计算，并会引发健康检查（POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO）。</p><p><strong>指定池的 PG 范围</strong></p><p>可以指定池的 PG 最小值和最大值。</p><p><strong>设置 PG 的最小值和最大值</strong></p><p>如果设置了最小值，则 Ceph 不会自行将 PG 数量减少（也不会建议减少）到低于配置值的水平。设置最小值是为了在 I&#x2F;O 期间即使池大部分为空时，也能为客户端提供一定程度的并行性。</p><p>如果设置了最大值，则 Ceph 不会自行将 PG 数量增加（也不会建议增加）到高于配置值的水平。</p><p>要设置池的 PG 最小值，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &lt;pool-name&gt; pg_num_min &lt;num&gt;</span><br></pre></td></tr></table></figure><p>要设置池的 PG 最大值，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &lt;pool-name&gt; pg_num_max &lt;num&gt;</span><br></pre></td></tr></table></figure><p>此外，<code>ceph osd pool create</code> 命令具有两个命令行选项，可用于在创建池时指定池的最小或最大 PG 数量：<code>--pg-num-min &lt;num&gt;</code> 和 <code>--pg-num-max &lt;num&gt;</code>。</p><h2 id="预选择PG-NUM"><a href="#预选择PG-NUM" class="headerlink" title="预选择PG_NUM"></a>预选择PG_NUM</h2><p>在创建池时，可以通过以下命令预选择 <code>pg_num</code> 参数的值：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create &#123;pool-name&#125; [pg_num]</span><br></pre></td></tr></table></figure><p>如果在命令中选择不指定 <code>pg_num</code>，集群会使用 PG 自动缩放器根据池中存储的数据量自动配置该参数（见上文的自动扩缩容部分）。<br>无论是否在创建时指定 <code>pg_num</code>，都不会影响集群之后是否会自动调整该参数。PG 自动缩放的启用或禁用可以通过以下命令设置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pg_autoscale_mode (on|off|warn)</span><br></pre></td></tr></table></figure><p>没有平衡器时，建议的目标是在每个 OSD 上大约 100 个 PG 副本。使用平衡器时，初始目标是每个 OSD 上大约 50 个 PG 副本。<br>自动缩放器尝试满足以下条件：</p><ul><li>每个 OSD 上的 PG 数量应与池中的数据量成正比。</li><li>每个池应有 50-100 个 PG，考虑到每个 PG 副本在 OSD 之间的复制开销或纠删编码扩展。</li></ul><h2 id="指定-PG-NUM-的相关因素"><a href="#指定-PG-NUM-的相关因素" class="headerlink" title="指定 PG_NUM 的相关因素"></a>指定 PG_NUM 的相关因素</h2><p>在某种程度上，数据高可用和在 OSD 之间均匀分配的标准倾向于更高的 PG 数量。另一方面，节省 CPU 资源和最小化内存使用的标准则倾向于较低的 PG 数量。<br><strong>数据高可用</strong><br>当一个 OSD 发生故障时，数据丢失的风险增加，直到恢复到配置的复制水平。为了说明这一点，假设以下场景导致一个 PG 的永久数据丢失：</p><ol><li>OSD 发生故障，所有其包含的对象副本丢失。对于 PG 中的每个对象，其副本数量从三降到两。</li><li>Ceph 开始为这个 PG 选择一个新的 OSD，以重新创建每个对象的第三个副本。</li><li>在新的 OSD 完全填充第三个副本之前，另一个 OSD 在同一 PG 内发生故障。某些对象将只有一个剩余副本。</li><li>Ceph 选择另一个 OSD 并继续复制对象，以恢复所需的副本数量。</li><li>在恢复完成之前，PG 内的第三个 OSD 发生故障。如果这个 OSD 恰好包含对象的唯一剩余副本，则对象将永久丢失。</li></ol><p>在一个包含 10 个 OSD 的集群中，具有 512 个 PG 和三副本池的情况下，CRUSH 会将每个 PG 分配给三个 OSD。因此，当第一个 OSD 发生故障时，将同时为所有 150 个 PG 开始恢复。<br>被恢复的 150 个 PG 可能会均匀分布在 9 个剩余的 OSD 上。每个剩余的 OSD 因此可能会将对象副本发送到所有其他 OSD，并且还可能接收一些新对象，因为它已成为新的 PG 的一部分。</p><p>恢复完成所需的时间取决于 Ceph 集群的架构。比较两种设置：<br>（1）每个 OSD 由一台机器上的 1 TB SSD 托管，所有 OSD 连接到 10 Gb&#x2F;s 交换机，单个 OSD 的恢复在一定时间内完成。<br>（2）每台机器上有两个 OSD，使用 HDD，没有 SSD WAL+DB 和 1 Gb&#x2F;s 交换机。在第二种设置中，恢复将慢至少一个数量级。</p><p>在这样的集群中，PG 数量对数据高可用几乎没有影响。无论每个 OSD 有 128 个 PG 还是 8192 个 PG，恢复不会更快或更慢。<br>然而，增加 OSD 的数量可以提高恢复速度。假设我们的 Ceph 集群从 10 个 OSD 扩展到 20 个 OSD。每个 OSD 现在只参与大约 75 个 PG，而不是大约 150 个 PG。所有 19 个剩余的 OSD 仍然需要复制相同数量的对象以进行恢复。但现在有 20 个 OSD 必须复制仅 50 GB 的每个对象，而不是 10 个 OSD 复制 100 GB 的每个对象。如果网络以前是瓶颈，则恢复现在的速度加倍。</p><p>类似地，假设我们的集群增长到 40 个 OSD。每个 OSD 仅托管大约 38 个 PG。如果 OSD 发生故障，恢复将比以前更快，除非被其他瓶颈阻碍。然而，假设我们的集群增长到 200 个 OSD。每个 OSD 将只托管大约 7 个 PG。如果 OSD 发生故障，恢复将在最多 7 个 OSD 上进行，这意味着恢复时间将比 40 个 OSD 时长。因此，应该增加 PG 数量。</p><p>无论恢复时间多么短，在恢复过程中始终有额外 OSD 失败的可能性。考虑上面的 10 个 OSD 的集群：如果任何 OSD 失败，则大约 150 除以 9 的 PG 将只有一个剩余副本。如果剩余的 8 个 OSD 中的任何一个失败，则大约 17 除以 8 的 PG 可能会丢失其剩余的对象。这是为什么设置 <code>size=2</code> 是有风险的一个原因。</p><p>当集群中的 OSD 数量增加到 20 时，损失3个OSD 会显著减少损坏的 PG 数量。第二个 OSD 的损失仅导致大约 17 除以 8 个 PG 损坏，而第三个 OSD 的损失仅导致如果它是包含剩余副本的 4 个 OSD 之一，则才会丢失数据。这意味着——假设恢复期间失去一个 OSD 的概率是 0.0001%——在 10 个 OSD 的集群中丢失三个 OSD 的概率为 <code>X</code>，而在 20 个 OSD 的集群中为 <code>Y</code>。<br>总之，OSD 数量越多，恢复速度越快，因级联故障而永久丢失 PG 的风险越低。就数据高可用而言，在少于 50 个 OSD 的集群中，512 或 4096 个 PG 的数量差异几乎没有影响。</p><p><em>注意</em><br>最近添加到集群中的 OSD 可能需要较长时间来填充分配给它的 PG。但是，这一过程的缓慢不会导致对象退化或对数据耐用性产生影响，因为 Ceph 会在从旧 PG 中移除数据之前，将数据填充到新的 PG 中。</p><h2 id="选择PG的数量"><a href="#选择PG的数量" class="headerlink" title="选择PG的数量"></a>选择PG的数量</h2><p>如果OSD数量超过50个，我们建议每个OSD大约设置50-100个PG，以平衡资源使用、数据耐久性和数据分布。如果OSD数量少于50个，请参阅预选择部分的指导。对于单个池，使用以下公式获取基线值：<br>[<br>\text{Total PGs} &#x3D; \text{OSD数量} \times \text{池大小}<br>]</p><p>其中池大小是复制池的副本数量或编码池的K+M总和。要检索这个总和，请运行命令 <code>ceph osd erasure-code-profile get</code>。</p><p>接下来，检查结果的基线值是否与您设计Ceph集群的方式一致，以最大化数据耐久性和对象分布，并最小化资源使用。</p><p>这个值应四舍五入到最接近的2的幂。</p><p>每个池的 <code>pg_num</code> 应该是2的幂。其他值可能会导致数据在OSD之间分布不均。最好只在可行且期望的情况下增加 <code>pg_num</code> 到下一个最高的2的幂。注意，这个2的幂规则是针对每个池的；对所有池的 <code>pg_num</code> 的总和对齐到2的幂既不是必要的，也不容易。</p><p>例如，如果您有一个200个OSD的集群和一个副本大小为3的单个池，估算PG的数量如下：<br>[<br>\text{Total PGs} &#x3D; 200 \times 3 &#x3D; 600<br>]</p><p>四舍五入到最接近的2的幂：8192。</p><p>当使用多个数据池存储对象时，确保平衡每个池的PG数量和每个OSD的PG数量，以便得到一个合理的PG总数。找到一个为每个OSD提供合理低方差的数字，而不会对系统资源造成过大压力或使配对过程过慢是很重要的。</p><p>例如，假设您有一个包含10个池的集群，每个池有512个PG，分布在10个OSD上。这意味着总共有5120个PG分布在10个OSD上，每个OSD上有512个PG。这种集群不会使用过多的资源。然而，在一个包含1000个池的集群中，每个池有512个PG，OSD将处理大约50000个PG。这种集群将需要显著更多的资源和更多的配对时间。</p><h2 id="设置PG的数量"><a href="#设置PG的数量" class="headerlink" title="设置PG的数量"></a>设置PG的数量</h2><p>设置池中初始PG数量必须在创建池时进行。然而，即使在池创建之后，如果未使用 <code>pg_autoscaler</code> 管理 <code>pg_num</code> 值，您仍然可以通过运行以下命令更改PG数量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pg_num &#123;pg_num&#125;</span><br></pre></td></tr></table></figure><p>如果您增加PG数量，集群将不会重新平衡，直到您增加用于放置的PG数量（<code>pgp_num</code>）。<code>pgp_num</code> 参数指定了CRUSH算法在放置时要考虑的PG数量。增加 <code>pg_num</code> 会拆分集群中的PG，但数据不会迁移到新的PG，直到 <code>pgp_num</code> 被增加。<code>pgp_num</code> 参数应与 <code>pg_num</code> 参数相等。要增加用于放置的PG数量，请运行以下命令：<br>shell</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pgp_num &#123;pgp_num&#125;</span><br></pre></td></tr></table></figure><p>如果您减少PG数量，则 <code>pgp_num</code> 会自动调整。在Nautilus及以后的版本中，当未使用 <code>pg_autoscaler</code> 时，<code>pgp_num</code> 会自动调整以匹配 <code>pg_num</code>。这个过程表现为PG的重新映射和回填，这是正常的预期行为。</p><h2 id="获取PG数量"><a href="#获取PG数量" class="headerlink" title="获取PG数量"></a>获取PG数量</h2><p>要获取池中的PG数量，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get &#123;pool-name&#125; pg_num</span><br></pre></td></tr></table></figure><h2 id="获取集群的PG统计信息"><a href="#获取集群的PG统计信息" class="headerlink" title="获取集群的PG统计信息"></a>获取集群的PG统计信息</h2><p>要查看集群中PG的详细信息，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump [--format &#123;format&#125;]</span><br></pre></td></tr></table></figure><p>有效的格式有plain（默认）和json。</p><h2 id="获取卡住PG的统计信息"><a href="#获取卡住PG的统计信息" class="headerlink" title="获取卡住PG的统计信息"></a>获取卡住PG的统计信息</h2><p>要查看所有处于指定状态的卡住PG的统计信息，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format &lt;format&gt;] [-t|--threshold &lt;seconds&gt;]</span><br></pre></td></tr></table></figure><p>Inactive PGs 不能处理读写操作，因为它们在等待足够的OSD来获得最新的数据。</p><p>Undersized PGs 包含未被复制到所需次数的对象。在正常情况下，可以假设这些PG正在恢复。</p><p>Stale PGs 处于未知状态 — 托管它们的OSD在一定时间内（由 <code>mon_osd_report_timeout</code> 决定）没有向监视集群报告。</p><p>有效的格式有plain（默认）和json。阈值定义PG卡住的最少秒数，超过该时间PG会被包括在返回的统计信息中（默认：300秒）。</p><h2 id="获取PG映射"><a href="#获取PG映射" class="headerlink" title="获取PG映射"></a>获取PG映射</h2><p>要获取特定PG的映射，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg map &#123;pg-id&#125;</span><br></pre></td></tr></table></figure><p>Ceph会返回PG映射、PG和OSD状态。输出类似于以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]</span><br></pre></td></tr></table></figure><h2 id="获取PG的统计信息"><a href="#获取PG的统计信息" class="headerlink" title="获取PG的统计信息"></a>获取PG的统计信息</h2><p>要查看特定PG的统计信息，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg &#123;pg-id&#125; query</span><br></pre></td></tr></table></figure><h2 id="对PG进行清理"><a href="#对PG进行清理" class="headerlink" title="对PG进行清理"></a>对PG进行清理</h2><p>要对PG进行清理，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg scrub &#123;pg-id&#125;</span><br></pre></td></tr></table></figure><p>Ceph检查主OSD和副本OSD，生成PG中所有对象的目录，并将对象进行对比，以确保没有对象丢失或不匹配，并且其内容是一致的。如果副本全部匹配，则进行最终的语义扫描，以确保所有与快照相关的对象元数据一致。错误会记录在日志中。</p><p>要对特定池中的所有PG进行清理，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool scrub &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><h2 id="优先恢复-回填PG"><a href="#优先恢复-回填PG" class="headerlink" title="优先恢复&#x2F;回填PG"></a>优先恢复&#x2F;回填PG</h2><p>如果遇到多个PG需要恢复或回填的情况，但某些PG中的数据比其他PG中的数据更重要（例如，有些PG包含正在运行的机器使用的镜像数据，而其他PG用于非活动机器，包含的数据不太相关），您可能希望优先恢复或回填特别重要的数据所在的PG，以便尽快恢复集群的性能和数据的可用性。要将特定PG标记为恢复优先，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br></pre></td></tr></table></figure><p>要将特定PG标记为回填优先，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br></pre></td></tr></table></figure><p>这些命令指示Ceph在处理</p><p>其他PG之前，优先对指定的PG进行恢复或回填。优先级不会中断当前的回填或恢复，但会将指定的PG置于队列顶部，以便下一个被处理。如果您改变主意或意识到优先级设置错误，请运行以下命令之一：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph pg cancel-force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br><span class="line">ceph pg cancel-force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</span><br></pre></td></tr></table></figure><p>这些命令会从指定PG中移除强制标记，使PG按常规顺序处理。与添加强制标记的情况一样，这只会影响那些仍在排队的PG，而不会影响当前正在恢复的PG。</p><p>强制标记会在PG的恢复或回填完成后自动清除。</p><p>同样，要指示Ceph优先处理指定池中的所有PG（即，首先对这些PG进行恢复或回填），请运行以下命令之一：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool force-recovery &#123;pool-name&#125;</span><br><span class="line">ceph osd pool force-backfill &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><p>这些命令也可以被取消。要恢复到默认顺序，请运行以下命令之一：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool cancel-force-recovery &#123;pool-name&#125;</span><br><span class="line">ceph osd pool cancel-force-backfill &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><h2 id="警告"><a href="#警告" class="headerlink" title="警告"></a>警告</h2><p>这些命令可能会打破Ceph内部优先级计算的顺序，因此使用时请小心！如果您有多个池当前共享相同的底层OSD，并且某些池中的数据比其他池中的数据更重要，则建议运行以下命令来为所有池安排自定义恢复&#x2F;回填优先级：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; recovery_priority &#123;value&#125;</span><br></pre></td></tr></table></figure><p>例如，如果您有二十个池，您可以将最重要的池的优先级设为20，次重要的池设为19，以此类推。<br>另一种选择是仅为适当的池子子集设置恢复&#x2F;回填优先级。在这种情况下，可能会将三个重要的池都分配为优先级1，而所有其他池将没有分配恢复&#x2F;回填优先级。另一种可能性是选择三个重要的池，并将它们的恢复&#x2F;回填优先级分别设置为3、2和1。</p><h2 id="重要"><a href="#重要" class="headerlink" title="重要"></a>重要</h2><p>使用 <code>ceph osd pool set &#123;pool-name&#125; recovery_priority &#123;value&#125;</code> 设置恢复&#x2F;回填优先级时，数值越大优先级越高。例如，优先级为30的池比优先级为15的池具有更高的优先级。</p><h2 id="恢复丢失的RADOS对象"><a href="#恢复丢失的RADOS对象" class="headerlink" title="恢复丢失的RADOS对象"></a>恢复丢失的RADOS对象</h2><p>如果集群丢失了一个或多个RADOS对象，并且您决定放弃对丢失数据的搜索，您必须将未找到的对象标记为丢失。</p><p>如果已经查询了所有可能的位置，并且所有OSD都正常，但某些RADOS对象仍然丢失，您可能不得不放弃这些对象。当罕见和异常的故障组合允许集群了解到在写入操作恢复之前执行的写入时，可能会出现这种情况。</p><p>标记RADOS对象丢失的命令只有一个支持的选项：revert。revert选项将回滚到RADOS对象的先前版本（如果它足够旧以拥有先前版本），或者完全忽略它（如果它太新而没有先前版本）。要标记“未找到”的对象为丢失，请运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg &#123;pg-id&#125; mark_unfound_lost revert|delete</span><br></pre></td></tr></table></figure><h2 id="平衡模块"><a href="#平衡模块" class="headerlink" title="平衡模块"></a>平衡模块</h2><p>平衡模块可以优化放置组（PG）在OSD之间的分配，以实现平衡分布。平衡器可以自动操作，也可以在监督下操作。</p><h4 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h4><p>要检查平衡器的当前状态，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer status</span><br></pre></td></tr></table></figure><h4 id="自动平衡"><a href="#自动平衡" class="headerlink" title="自动平衡"></a>自动平衡</h4><p>当平衡器处于 upmap 模式时，自动平衡功能默认启用。要禁用平衡器，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer off</span><br></pre></td></tr></table></figure><p>平衡器模式可以从 upmap 模式更改为 crush-compat 模式。crush-compat 模式与较旧的客户端向后兼容。在 crush-compat 模式下，平衡器会自动对数据分布进行小的调整，以确保 OSD 被均等利用。</p><h4 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h4><p>如果集群处于降级状态（即，OSD 已失败且系统尚未自愈），则平衡器不会对 PG 分布进行任何调整。</p><p>当集群处于健康状态时，平衡器将逐步移动一小部分不平衡的 PG，以改善分布。这个比例不会超过默认的 5% 阈值。要调整这个 target_max_misplaced_ratio 阈值设置，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr target_max_misplaced_ratio .07   # 7%</span><br></pre></td></tr></table></figure><p>平衡器在运行之间会休眠。要设置此休眠间隔的秒数，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr mgr/balancer/sleep_interval 60</span><br></pre></td></tr></table></figure><p>要设置自动平衡开始的时间（HHMM 格式），请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr mgr/balancer/begin_time 0000</span><br></pre></td></tr></table></figure><p>要设置自动平衡结束的时间（HHMM 格式），请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr mgr/balancer/end_time 2359</span><br></pre></td></tr></table></figure><p>自动平衡可以限制在特定的星期几。要将其限制为特定的星期几或之后的时间（如 crontab 中，0 是星期天，1 是星期一，以此类推），请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr mgr/balancer/begin_weekday 0</span><br></pre></td></tr></table></figure><p>要限制自动平衡为特定的星期几或之前的时间（同样，0 是星期天，1 是星期一，以此类推），请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr mgr/balancer/end_weekday 6</span><br></pre></td></tr></table></figure><p>自动平衡可以限制在特定的池。默认情况下，这个设置的值是空字符串，这样所有池都会自动平衡。要将自动平衡限制到特定的池，请获取它们的数字池 ID（通过运行 <code>ceph osd pool ls detail</code> 命令），然后运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config set mgr mgr/balancer/pool_ids 1,2,3</span><br></pre></td></tr></table></figure><h4 id="模式"><a href="#模式" class="headerlink" title="模式"></a>模式</h4><p>支持两种平衡器模式：</p><ol><li><strong>crush-compat</strong>：此模式使用兼容权重集功能（在 Luminous 中引入）来管理 CRUSH 层次结构中设备的备用权重集。当平衡器在此模式下运行时，正常权重应保持为设备的大小，以反映计划存储在设备上的数据量。然后，平衡器将优化权重集值，通过小的增量进行调整，以实现尽可能接近目标分布的分布。（由于 PG 放置是伪随机过程，因此会受到自然变异的影响；优化权重有助于对抗这种自然变异。）<ul><li>请注意，此模式与较旧的客户端完全向后兼容：当 OSD Map 和 CRUSH 图与较旧的客户端共享时，Ceph 将优化后的权重呈现为“真实”权重。</li><li>此模式的主要限制是，平衡器无法处理具有不同放置规则的多个 CRUSH 层次结构，如果层次结构的子树共享任何 OSD。（这种 OSD 共享是不典型的，并且由于管理共享 OSD 上的空间利用的困难，通常不推荐。）</li></ul></li><li><strong>upmap</strong>：在 Luminous 及更高版本中，OSDMap 可以存储对单个 OSD 的显式映射，作为正常 CRUSH 放置计算的例外。这些 upmap 条目提供了对 PG 映射的细粒度控制。此平衡器模式优化单个 PG 的放置，以实现平衡分布。在大多数情况下，结果分布几乎是完美的：即，每个 OSD 上的 PG 数量相等（±1 PG，因为总数可能无法均匀分割）。<ul><li>要使用 upmap，所有客户端必须是 Luminous 或更高版本。</li><li>默认模式是 upmap。可以通过运行以下命令将模式更改为 crush-compat： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer mode crush-compat</span><br></pre></td></tr></table></figure></li></ul></li></ol><h4 id="监控优化"><a href="#监控优化" class="headerlink" title="监控优化"></a>监控优化</h4><p>监督使用平衡器可以分为三个不同的阶段：</p><ol><li><strong>制定计划</strong></li><li><strong>评估数据分布的质量，无论是当前的 PG 分布还是执行计划后将产生的 PG 分布</strong></li><li><strong>执行计划</strong></li></ol><p>要评估当前分布，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer eval</span><br></pre></td></tr></table></figure><p>要评估单个池的分布，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer eval &lt;pool-name&gt;</span><br></pre></td></tr></table></figure><p>要详细查看评估，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer eval-verbose ...</span><br></pre></td></tr></table></figure><p>要指示平衡器生成一个计划（使用当前配置的模式），为计划起个名字（任何有用的标识字符串），并运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer optimize &lt;plan-name&gt;</span><br></pre></td></tr></table></figure><p>要查看计划的内容，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer show &lt;plan-name&gt;</span><br></pre></td></tr></table></figure><p>要显示所有计划，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer ls</span><br></pre></td></tr></table></figure><p>要丢弃旧的计划，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer rm &lt;plan-name&gt;</span><br></pre></td></tr></table></figure><p>要查看当前记录的计划，请检查以下状态命令的输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer status</span><br></pre></td></tr></table></figure><p>要评估执行特定计划后将产生的分布，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer eval &lt;plan-name&gt;</span><br></pre></td></tr></table></figure><p>如果计划预计会改善分布（即计划的评分低于当前集群状态的评分），可以通过运行以下命令执行该计划：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph balancer execute &lt;plan-name&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Ceph并不是直接通过CRUSH算法将数据对象一一映射到OSD中的，这样做将非常复杂与低效。而且，Ceph用户并不需要了解实际的CRUSH算法是怎么运行的，只需要关心数据保存在哪里即可。Ceph通过存储池Pool和放置组Placement Groups (PGs)来实现CR</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>存储性能测试工具</title>
    <link href="https://watsonlu6.github.io/%E5%AD%98%E5%82%A8%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    <id>https://watsonlu6.github.io/%E5%AD%98%E5%82%A8%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</id>
    <published>2021-10-11T05:26:35.000Z</published>
    <updated>2024-08-03T06:56:02.287Z</updated>
    
    <content type="html"><![CDATA[<h2 id="FIO简介"><a href="#FIO简介" class="headerlink" title="FIO简介"></a>FIO简介</h2><p>FIO是Linux下开源的一款IOPS测试工具，主要用来对磁盘进行压力测试和性能验证。它可以产生许多线程或进程来执行用户特定类型的I&#x2F;O操作，通过编写作业文件或者直接命令去执行测试动作，相当于是一个 多线程的io生成工具，用于生成多种IO模式来测试硬盘设备的性能（大多情况用于测试裸盘性能）。<br>硬盘I&#x2F;O测试主要有以下类型：</p><ul><li>随机读</li><li>随机写</li><li>顺序读</li><li>顺序写</li><li>混合读写 （可根据需求设置70%读，30%写或100%读等等）</li></ul><h2 id="FIO的安装与使用"><a href="#FIO的安装与使用" class="headerlink" title="FIO的安装与使用"></a>FIO的安装与使用</h2><p>github地址：github.com&#x2F;axboe&#x2F;fio<br>下载安装方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">yum -i install fio</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">yum -y install libaio-devel   <span class="comment">#安装libaio引擎，不然执行fio会报“fio: engine libaio not loadable”，必须要在fio安装前安装，不然还要重新编译安装一遍fio</span></span><br><span class="line">wget https://github.com/axboe/fio/archive/refs/tags/fio-3.10.zip</span><br><span class="line"><span class="built_in">cd</span> /root/fio-fio-3.10</span><br><span class="line">./configure</span><br><span class="line">mke &amp;&amp; make install</span><br></pre></td></tr></table></figure><h2 id="常用参数介绍"><a href="#常用参数介绍" class="headerlink" title="常用参数介绍"></a>常用参数介绍</h2><ul><li>filename&#x3D;&#x2F;dev&#x2F;sdb  要测试盘的名称，支持文件系统或者裸设备，&#x2F;dev&#x2F;sda2或&#x2F;dev&#x2F;sdb</li><li>direct&#x3D;1  测试过程绕过机器自带的buffer，使测试结果更真实（Linux在读写时，数据会先写到缓存，再在后台写到硬盘，读的时候也是优先从缓存中读，这样访问速度会加快，但是一旦掉电，缓存中数据就会清空，所有一种模式为DirectIO，可以跳过缓存，直接读写硬盘）</li><li>ioengine&#x3D;libaio  定义使用什么io引擎去下发io请求<ul><li>sync：同步IO引擎，使用Linux系统调用实现IO操作，可以测试磁盘性能的上限。</li><li>mmap：使用内存映射技术实现IO操作，可以测试文件系统的缓存和文件的预读能力。</li><li>libaio：异步IO引擎，使用Linux系统调用libaio实现IO操作，可以测试磁盘的随机读写性能。</li><li>posixaio：类似于libaio的异步IO引擎，但使用POSIX AIO接口实现IO操作。</li><li>pvsync：使用Linux系统调用实现IO操作，但对写操作进行缓存，并且只在需要时进行刷新，可以提高IO性能。</li><li>rbd：用于测试Ceph集群中rados block device (RBD)的性能，支持异步IO和同步IO操作。</li></ul></li><li>iodepth&#x3D;16  队列的深度为16，在异步模式下，CPU不能一直无限的发命令到硬盘设备。比如SSD执行读写如果发生了卡顿，那有可能系统会一直不停的发命令，几千个，甚至几万个，这样一方面SSD扛不住，另一方面这么多命令会很占内存，系统也要挂掉了。这样，就带来一个参数叫做队列深度。</li><li>bs&#x3D;4k   单次io的块文件大小为4k</li><li>numjobs&#x3D;10   并发工作线程数</li><li>size&#x3D;5G      每个线程读写的数据量是5GB</li><li>runtime&#x3D;60   测试时间为60秒，可以设置2m为两分钟。如果不配置此项，会将设置的size大小全部写入或者读取完为止</li><li>rw&#x3D;randread   测试随机读的I&#x2F;O</li><li>rw&#x3D;randwrite  测试随机写的I&#x2F;O</li><li>rw&#x3D;randrw     测试随机混合写和读的I&#x2F;O</li><li>rw&#x3D;read       测试顺序读的I&#x2F;O</li><li>rw&#x3D;write      测试顺序写的I&#x2F;O</li><li>rw&#x3D;rw         测试顺序混合写和读的I&#x2F;O</li><li>thread        使用pthread_create创建线程，另一种是fork创建进程。进程的开销比线程要大，一般都采用thread测试</li><li>rwmixwrite&#x3D;30   在混合读写的模式下，写占30%（即rwmixread读为70%，单独配置这样的一个参数即可）</li><li>group_reporting 关于显示结果的，汇总每个进程的信息</li><li>name&#x3D;”TDSQL_4KB_read_test”  定义测试任务名称<br>扩展</li><li>lockmem&#x3D;1g       只使用1g内存进行测试</li><li>zero_buffers     用全0初始化缓冲区，默认是用随机数据填充缓冲区</li><li>random_distribution&#x3D;random    #默认情况下，fio 会在询问时使用完全均匀的随机分布，有需要的话可以自定义访问区域，zipf、pareto、normal、zoned</li><li>nrfiles&#x3D;8        每个进程生成文件的数量</li></ul><h2 id="测试场景示例"><a href="#测试场景示例" class="headerlink" title="测试场景示例"></a>测试场景示例</h2><p>100%随机读，5G大小，4k块文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=randread -group_reporting -name=<span class="string">&quot;TDSQL_4KB_randread_test&quot;</span></span><br></pre></td></tr></table></figure><p>100%顺序读，5G大小，4k块文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=<span class="built_in">read</span> -group_reporting -name=<span class="string">&quot;TDSQL_4KB_write_test&quot;</span></span><br></pre></td></tr></table></figure><p>70%随机读，30%随机写，5G大小，4k块文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=randrw -rwmixread=70 -group_reporting -name=<span class="string">&quot;TDSQL_4KB_randread70-write_test&quot;</span></span><br></pre></td></tr></table></figure><p>70%顺序读，30%随机写，5G大小，4k块文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=rw -rwmixread=70 -group_reporting -name=<span class="string">&quot;TDSQL_4KB_read70-write_test&quot;</span></span><br></pre></td></tr></table></figure><h2 id="输出报告"><a href="#输出报告" class="headerlink" title="输出报告"></a>输出报告</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost <span class="built_in">test</span>]# fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=randrw -rwmixread=70 -group_reporting -name=<span class="string">&quot;local_randrw_test&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">local_randrw_test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=16</span><br><span class="line">...</span><br><span class="line">fio-3.10</span><br><span class="line">Starting 10 threads</span><br><span class="line">Jobs: 10 (f=10): [m(10)][100.0%][r=19.4MiB/s,w=8456KiB/s][r=4969,w=2114 IOPS][eta 00m:00s]</span><br><span class="line">local_randrw_test: (groupid=0, <span class="built_in">jobs</span>=10): err= 0: pid=11189: Mon Oct 25 11:01:46 2021</span><br><span class="line">   <span class="built_in">read</span>: IOPS=5230, BW=20.4MiB/s (21.4MB/s)(1226MiB/60031msec)</span><br><span class="line">    slat (usec): min=2, max=342637, avg=1266.82, stdev=7241.29</span><br><span class="line">    clat (usec): min=4, max=459544, avg=20056.81, stdev=24888.90</span><br><span class="line">     lat (usec): min=134, max=459586, avg=21329.16, stdev=25378.16</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  1467],  5.00th=[  1844], 10.00th=[  2147], 20.00th=[  2606],</span><br><span class="line">     | 30.00th=[  3032], 40.00th=[  3556], 50.00th=[  4359], 60.00th=[  6063],</span><br><span class="line">     | 70.00th=[ 36439], 80.00th=[ 46924], 90.00th=[ 51643], 95.00th=[ 59507],</span><br><span class="line">     | 99.00th=[105382], 99.50th=[117965], 99.90th=[137364], 99.95th=[152044],</span><br><span class="line">     | 99.99th=[219153]</span><br><span class="line">   bw (  KiB/s): min=  795, max= 4494, per=9.91%, avg=2072.23, stdev=744.04, samples=1195</span><br><span class="line">   iops        : min=  198, max= 1123, avg=517.74, stdev=186.00, samples=1195</span><br><span class="line">  write: IOPS=2243, BW=8972KiB/s (9188kB/s)(526MiB/60031msec)</span><br><span class="line">    slat (usec): min=2, max=311932, avg=1272.76, stdev=7272.09</span><br><span class="line">    clat (usec): min=6, max=458031, avg=20206.30, stdev=24897.71</span><br><span class="line">     lat (usec): min=974, max=459755, avg=21484.12, stdev=25400.41</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  1500],  5.00th=[  1860], 10.00th=[  2147], 20.00th=[  2606],</span><br><span class="line">     | 30.00th=[  3064], 40.00th=[  3621], 50.00th=[  4424], 60.00th=[  6194],</span><br><span class="line">     | 70.00th=[ 36439], 80.00th=[ 46924], 90.00th=[ 51643], 95.00th=[ 59507],</span><br><span class="line">     | 99.00th=[105382], 99.50th=[117965], 99.90th=[137364], 99.95th=[149947],</span><br><span class="line">     | 99.99th=[200279]</span><br><span class="line">   bw (  KiB/s): min=  357, max= 1944, per=9.90%, avg=888.57, stdev=325.49, samples=1195</span><br><span class="line">   iops        : min=   89, max=  486, avg=221.80, stdev=81.37, samples=1195</span><br><span class="line">  lat (usec)   : 10=0.01%, 50=0.01%, 100=0.01%, 250=0.02%, 500=0.01%</span><br><span class="line">  lat (usec)   : 750=0.01%, 1000=0.01%</span><br><span class="line">  lat (msec)   : 2=7.45%, 4=38.36%, 10=18.10%, 20=1.09%, 50=22.31%</span><br><span class="line">  lat (msec)   : 100=11.42%, 250=1.24%, 500=0.01%</span><br><span class="line">  cpu          : usr=0.26%, sys=19.41%, ctx=12026, majf=0, minf=18</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=313975,134655,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=16</span><br><span class="line"></span><br><span class="line">Run status group 0 (all <span class="built_in">jobs</span>):</span><br><span class="line">   READ: bw=20.4MiB/s (21.4MB/s), 20.4MiB/s-20.4MiB/s (21.4MB/s-21.4MB/s), io=1226MiB (1286MB), run=60031-60031msec</span><br><span class="line">  WRITE: bw=8972KiB/s (9188kB/s), 8972KiB/s-8972KiB/s (9188kB/s-9188kB/s), io=526MiB (552MB), run=60031-60031msec</span><br><span class="line"></span><br><span class="line">Disk stats (<span class="built_in">read</span>/write):</span><br><span class="line">  sdb: ios=314008/134653, merge=0/0, ticks=189470/89778, in_queue=279286, util=99.75%</span><br></pre></td></tr></table></figure><p>输出报告分析<br>下面是每个执行的数据方向的I&#x2F;O统计数据信息的代表值含义</p><ul><li><p>read&#x2F;write： 读&#x2F;写的IO操作（还有一个trim没用过）</p><ul><li>salt： 提交延迟，这是提交I&#x2F;O所花费的时间（min:最小值，max:最大值，avg:平均值，stdev:标准偏差）</li><li>chat： 完成延迟，表示从提交到完成I&#x2F;O部分的时间</li><li>lat： 相应时间，表示从fio创建I&#x2F;O单元到完成I&#x2F;O操作的时间</li><li>bw： 带宽统计</li><li>iops： IOPS统计</li></ul></li><li><p>lat(nsec&#x2F;usec&#x2F;msec)： I&#x2F;O完成延迟的分布。这是从I&#x2F;O离开fio到它完成的时间。与上面单独的读&#x2F;写&#x2F;修剪部分不同，这里和其余部分的数据适用于报告组的所有I&#x2F; o。10&#x3D;0.01%意味着0.01%的I&#x2F;O在250us以下完成。250&#x3D;0.02%意味着0.02%的I&#x2F;O需要10到250us才能完成。</p></li><li><p>cpu： cpu使用率</p></li><li><p>IO depths： I&#x2F;O深度在作业生命周期中的分布</p><ul><li>IO submit： 在一个提交调用中提交了多少个I&#x2F;O。每一个分录表示该数额及其以下，直到上一分录为止——例如，4&#x3D;100%意味着我们每次提交0到4个I&#x2F;O调用</li><li>IO complete： 和上边的submit一样，不过这个是完成了多少个</li><li>IO issued rwt： 发出的read&#x2F;write&#x2F;trim请求的数量，以及其中有多少请求被缩短或删除</li><li>IO latency： 满足指定延迟目标所需的I&#x2F;O深度</li></ul></li><li><p>bw： 总带宽以及最小和最大带宽</p></li><li><p>io： 该组中所有线程执行的累计I&#x2F;O</p></li><li><p>run： 这组线程中最小和最长的运行时。</p></li><li><p>ios： 所有组的I&#x2F; o个数</p></li><li><p>merge： I&#x2F;O调度器执行的总合并数</p></li><li><p>ticks： 使磁盘繁忙的滴答数（仅供参考，原文是Number of ticks we kept the disk busy）</p></li><li><p>in_queue： 在磁盘队列中花费的总时间</p></li><li><p>util： 磁盘利用率。值为100%意味着我们保留了磁盘，如果一直很忙，那么50%的时间磁盘就会闲置一半的时间</p></li></ul><h2 id="FIO通过配置文件运行"><a href="#FIO通过配置文件运行" class="headerlink" title="FIO通过配置文件运行"></a>FIO通过配置文件运行</h2><p>除了命令行直接执行命令外，也可以通过写配置到xxx.fio文件中，每次只用修改配置即可，使用更方便些，执行方式为fio xxx.fio</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost <span class="built_in">jobs</span>]# <span class="built_in">cat</span> test.fio</span><br><span class="line">[global]</span><br><span class="line">filename=/dev/sdb</span><br><span class="line">ioengine=libaio</span><br><span class="line">direct=1</span><br><span class="line">thread</span><br><span class="line">group_reporting</span><br><span class="line"></span><br><span class="line">[randread-4k-128M]</span><br><span class="line">rw=randread</span><br><span class="line">bs=4k</span><br><span class="line">size=128M</span><br><span class="line">numjobs=5</span><br><span class="line"></span><br><span class="line">[randwrite-4k-128M]</span><br><span class="line">rw=randwrite</span><br><span class="line">bs=4k</span><br><span class="line">size=128M</span><br><span class="line">numjobs=5</span><br><span class="line"></span><br><span class="line">[write-4k-128M]</span><br><span class="line">rw=write</span><br><span class="line">bs=4k</span><br><span class="line">size=128M</span><br><span class="line">numjobs=5</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行fio命令测试</span></span><br><span class="line">[root@localhost <span class="built_in">jobs</span>]# fio test.fio</span><br><span class="line">randread-4k-128M: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1</span><br><span class="line">...</span><br><span class="line">randwrite-4k-128M: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1</span><br><span class="line">...</span><br><span class="line">write-4k-128M: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1</span><br><span class="line">...</span><br><span class="line">fio-3.10</span><br><span class="line">Starting 15 threads</span><br><span class="line">Jobs: 6 (f=6): [_(3),r(1),_(1),w(5),E(1),_(4)][92.1%][r=10.8MiB/s,w=29.2MiB/s][r=2777,w=7483 IOPS][eta 00m:05s]</span><br></pre></td></tr></table></figure><h2 id="性能关注的重点"><a href="#性能关注的重点" class="headerlink" title="性能关注的重点"></a>性能关注的重点</h2><ol><li><strong>顺序读写和随机读写</strong>：顺序读写指对大块数据进行读写操作，随机读写则是对随机位置的小块数据进行读写操作。顺序读写通常比随机读写更快，因为它可以利用SSD的顺序读写优势。然而，随机读写需要处理更多的数据以找到所需数据，因此其IOPS和带宽通常低于顺序读写。</li><li><strong>SSD与HDD性能比较</strong>：<ul><li><strong>HDD</strong>：顺硬盘驱动器 (HDD) 的顺序读写和随机读写性能是不同的。顺序读写操作是指读写大块连续的数据，这是硬盘驱动器的强项。因为磁头可以直接读写连续的数据块，因此顺序读写速度快。随机读写操作是指读写随机位置的小块数据，这是硬盘驱动器的弱点。因为磁头需要频繁移动以读写不同的数据块，因此随机读写速度较慢。因此，硬盘驱动器的顺序读写速度通常比随机读写速度快。在评估硬盘驱动器性能时，需要考虑两种读写方式的结果。</li><li><strong>SSD</strong>：SSD 具有很高的随机读写性能，但顺序读写性能仍然不如随机读写性能。因为 SSD 需要管理大量的块，因此对大块连续的数据的读写可能不如对随机位置的小块数据的读写快。但需要注意的是，SSD 的顺序读写性能仍然高于硬盘驱动器 (HDD)。因此，即使是 SSD 的顺序读写性能不如随机读写性能，它仍然具有很高的性能。</li></ul></li></ol><h3 id="测试配置建议"><a href="#测试配置建议" class="headerlink" title="测试配置建议"></a>测试配置建议</h3><ol><li><strong>numjobs和iodepth设置</strong>：<ul><li>numjobs过大可能导致任务等待时间过长，建议设置为1、2、4、8。</li><li>iodepth可以设置大一些，建议为64、128，以增加IO任务队列。</li></ul></li><li><strong>利用率观察</strong>：通过<code>iostat -x -m 5 /dev/sdb</code>查看磁盘的utilize是否达到百分百。如果未达到，可继续增加numjobs直到utilize达到百分百。</li><li><strong>关闭写缓存</strong>：<ul><li>关闭写缓存可以提高数据持久性测试的准确性。</li><li>使用<code>hdparm -W /dev/device</code>查看当前状态，<code>hdparm -W 0 /dev/device</code>关闭写缓存。</li></ul></li></ol><h3 id="其他建议"><a href="#其他建议" class="headerlink" title="其他建议"></a>其他建议</h3><ol><li><strong>获取设备信息</strong>：使用<code>smartctl -a /dev/device</code>获取设备型号、连接版本和速度。</li><li><strong>numjobs大小</strong>:FIO的多线程调度其实还是一个进程，numjobs过大的话会导致任务等待时间过长，任务一直在排队。numjobs不能开太大，建议是1、2、4、8；iodepth任务可以开大点，建议是64、128，因为队列任务相当于IO任务，进行压测时，numjobs从1、2、4、8往上调，同时使用<code>iostat -x -m 5 /dev/sdb</code>查看磁盘的utilize是否达到百分百；如果当前numjobs的utilize还不是百分百，表示不是压测，numjobs再往上加，直到utilize达到百分百为止。</li><li><strong>多个fio进程测一个SSD和一个fio进程多线程测一个SSD的区别</strong><ul><li>多个fio进程测一个SSD和一个fio进程多线程测一个SSD的主要区别在于并发性和资源利用率。多个fio进程可以并发地访问SSD并生成更多的负载，这可以更好地测试SSD的并行读写能力和响应时间，但同时也会占用更多的CPU和内存资源。此外，由于多个fio进程之间的I&#x2F;O请求存在竞争关系，可能会影响测试结果的准确性和一致性。</li><li>相比之下，一个fio进程多线程测一个SSD可以更好地利用系统资源并模拟真实的应用程序I&#x2F;O模式。在这种情况下，每个线程可以并发地访问SSD并生成负载，同时避免了多个fio进程之间的竞争关系。这可以更好地测试SSD的性能和稳定性，并提供更准确的测试结果。</li><li>总的来说，两种方法都有其优缺点和适用场景。需要根据实际情况选择适当的测试方法来评估SSD的性能和可靠性。</li></ul></li><li><strong>磁盘性能摸底时需要关闭写缓存？</strong><ul><li>在使用fio进行性能测试时，是否需要关闭写缓存取决于具体的测试需求和测试方案。如果测试场景需要模拟真实应用程序中的写操作，并希望测试结果反映出SSD的真实性能水平，那么建议关闭写缓存，以便更准确地衡量SSD的写性能和数据持久性。然而，在某些情况下，为了测试SSD的I&#x2F;O性能而不是数据持久性，或者为了测试SSD的读性能，可能需要保持写缓存打开。因此，是否需要关闭写缓存取决于具体的测试需求和测试方案，需要根据实际情况进行决定。</li><li>关闭写缓存会使得磁盘的性能更具可预测性，因为每次写入都会立即被持久化到磁盘上，可以更准确地测试磁盘的写入性能和数据持久性。然而，关闭写缓存会使得写入操作变慢，因为每个写入操作都必须等待磁盘确认数据已经被永久写入。</li><li>不关闭写缓存会使得磁盘的写入性能更高，因为数据可以先被缓存起来，减少了写入操作对磁盘的访问次数，从而提高了写入性能。然而，数据可能会在缓存中存储一段时间，而不是立即写入磁盘，这可能会导致数据丢失或不一致。</li></ul></li></ol><p>通过合理配置fio参数，可以有效测试存储系统的性能。需要根据实际需求选择适当的测试方法和参数设置，确保测试结果的准确性和代表性。在测试过程中，需特别注意顺序读写和随机读写性能的差异，以及SSD和HDD在不同读写模式下的表现。</p><h1 id="Ceph-Rados性能测试工具"><a href="#Ceph-Rados性能测试工具" class="headerlink" title="Ceph Rados性能测试工具"></a>Ceph Rados性能测试工具</h1><p>Ceph 提供了 <code>rados bench</code> 和 <code>rados load-gen</code> 两个命令，用于测试和评估集群的性能。以下是这两个命令的用法和选项。</p><h2 id="rados-bench-命令"><a href="#rados-bench-命令" class="headerlink" title="rados bench 命令"></a>rados bench 命令</h2><p><code>rados bench</code> 命令用于对 Ceph 集群进行基准测试，以评估集群的读写性能。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados bench &#123;seconds&#125; &#123;operation&#125; [options]</span><br></pre></td></tr></table></figure><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li><code>&#123;seconds&#125;</code>：测试运行的持续时间，单位为秒。</li><li><code>&#123;operation&#125;</code>：指定测试的操作类型，包括 <code>write</code>、<code>seq</code>（顺序读）、<code>rand</code>（随机读）。</li></ul><h3 id="常用选项"><a href="#常用选项" class="headerlink" title="常用选项"></a>常用选项</h3><ul><li><code>-p &#123;pool&#125;</code> 或 <code>--pool &#123;pool&#125;</code>：指定使用的存储池名称。</li><li><code>-b &#123;block_size&#125;</code> 或 <code>--block-size &#123;block_size&#125;</code>：指定块大小，默认值为 4MB。</li><li><code>-t &#123;threads&#125;</code> 或 <code>--threads &#123;threads&#125;</code>：指定使用的线程数，默认值为 16。</li><li><code>-n &#123;num_objects&#125;</code> 或 <code>--num-objects &#123;num_objects&#125;</code>：指定创建的对象数量。</li><li><code>-c</code> 或 <code>--no-cleanup</code>：在测试结束后保留测试数据。</li><li><code>-D</code> 或 <code>--verify</code>：启用数据验证。</li></ul><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="写入测试"><a href="#写入测试" class="headerlink" title="写入测试"></a>写入测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados bench 60 write --pool testpool</span><br></pre></td></tr></table></figure><h4 id="顺序读测试"><a href="#顺序读测试" class="headerlink" title="顺序读测试"></a>顺序读测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados bench 60 <span class="built_in">seq</span> --pool testpool</span><br></pre></td></tr></table></figure><h4 id="随机读测试"><a href="#随机读测试" class="headerlink" title="随机读测试"></a>随机读测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados bench 60 rand --pool testpool</span><br></pre></td></tr></table></figure><h4 id="使用自定义块大小和线程数"><a href="#使用自定义块大小和线程数" class="headerlink" title="使用自定义块大小和线程数"></a>使用自定义块大小和线程数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados bench 60 write --pool testpool --block-size 8192 --threads 32</span><br></pre></td></tr></table></figure><h2 id="rados-load-gen-命令"><a href="#rados-load-gen-命令" class="headerlink" title="rados load-gen 命令"></a>rados load-gen 命令</h2><p><code>rados load-gen</code> 命令用于生成负载，以测试和评估 Ceph 集群在不同负载下的性能。</p><h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados load-gen [options]</span><br></pre></td></tr></table></figure><h3 id="常用选项-1"><a href="#常用选项-1" class="headerlink" title="常用选项"></a>常用选项</h3><ul><li><code>-b &#123;bytes&#125;</code> 或 <code>--block-size &#123;bytes&#125;</code>：指定块大小，默认值为 4096 字节。</li><li><code>-t &#123;threads&#125;</code> 或 <code>--threads &#123;threads&#125;</code>：指定使用的线程数，默认值为 16。</li><li><code>-o &#123;objects&#125;</code> 或 <code>--objects &#123;objects&#125;</code>：指定创建的对象数量，默认值为 100。</li><li><code>-p &#123;pool&#125;</code> 或 <code>--pool &#123;pool&#125;</code>：指定使用的存储池名称，默认值为 <code>rbd</code>。</li><li><code>-c &#123;clients&#125;</code> 或 <code>--clients &#123;clients&#125;</code>：指定客户端数量，默认值为 1。</li><li><code>-d &#123;seconds&#125;</code> 或 <code>--duration &#123;seconds&#125;</code>：指定测试运行的持续时间，单位为秒。</li><li><code>--num-objects</code>：指定对象的总数</li><li><code>--min-object-size</code>：指定最小object尺寸</li><li><code>--max-object-size</code>：指定最大object尺寸</li><li><code>--min-op-len</code>：指定操作的最小 io 长度</li><li><code>--max-op-len</code>：指定操作的最大 io 长度</li><li><code>--max-ops</code>：指定最大操作数</li><li><code>--max-backlog</code>：指定最大压测规模</li><li><code>--read-percent</code>：指定读取操作的百分比</li><li><code>--target-throughput</code>：指定目标吞吐量（以字节为单位）</li><li><code>--run-length</code>：指定总时间（以秒为单位）</li></ul><h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><h4 id="使用默认参数测试"><a href="#使用默认参数测试" class="headerlink" title="使用默认参数测试"></a>使用默认参数测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados load-gen</span><br></pre></td></tr></table></figure><h4 id="指定块大小和对象数量测试"><a href="#指定块大小和对象数量测试" class="headerlink" title="指定块大小和对象数量测试"></a>指定块大小和对象数量测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados load-gen --block-size 8192 --objects 500</span><br></pre></td></tr></table></figure><h4 id="在指定存储池中测试"><a href="#在指定存储池中测试" class="headerlink" title="在指定存储池中测试"></a>在指定存储池中测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados load-gen --pool mypool</span><br></pre></td></tr></table></figure><h4 id="使用多个线程和客户端测试"><a href="#使用多个线程和客户端测试" class="headerlink" title="使用多个线程和客户端测试"></a>使用多个线程和客户端测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados load-gen --threads 32 --clients 4</span><br></pre></td></tr></table></figure><h4 id="指定时间的测试"><a href="#指定时间的测试" class="headerlink" title="指定时间的测试"></a>指定时间的测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados load-gen --duration 60</span><br></pre></td></tr></table></figure><h4 id="指定object大小范围的测试"><a href="#指定object大小范围的测试" class="headerlink" title="指定object大小范围的测试"></a>指定object大小范围的测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados -p hdd_pool load-gen --num-objects 200 --min-object-size 4K --max-object-size 4M --max-ops 20 --read-percent 0 --min-op-len 4K --max-op-len 1M --target-throughput 20G --run-length 20 --num-threads 64</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;FIO简介&quot;&gt;&lt;a href=&quot;#FIO简介&quot; class=&quot;headerlink&quot; title=&quot;FIO简介&quot;&gt;&lt;/a&gt;FIO简介&lt;/h2&gt;&lt;p&gt;FIO是Linux下开源的一款IOPS测试工具，主要用来对磁盘进行压力测试和性能验证。它可以产生许多线程或进程来执行</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="存储基础" scheme="https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>rbd相关运维命令</title>
    <link href="https://watsonlu6.github.io/rbd%E7%9B%B8%E5%85%B3%E8%BF%90%E7%BB%B4%E5%91%BD%E4%BB%A4/"/>
    <id>https://watsonlu6.github.io/rbd%E7%9B%B8%E5%85%B3%E8%BF%90%E7%BB%B4%E5%91%BD%E4%BB%A4/</id>
    <published>2021-09-22T16:31:00.000Z</published>
    <updated>2024-08-04T08:03:25.937Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建-rbd-镜像"><a href="#创建-rbd-镜像" class="headerlink" title="创建 rbd 镜像"></a>创建 rbd 镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd create &#123;pool-name&#125;/&#123;image-name&#125; [--size &#123;size&#125;] [--image-format &#123;format&#125;] [--features &#123;feature-list&#125;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参数说明</p><ul><li>{pool-name}&#x2F;{image-name}：指定存储池名称和镜像名称。</li><li>–size {size}：设置镜像的大小。</li><li>–image-format {format}：指定镜像的格式。有效的格式包括 1（兼容旧版格式）和 2（支持更多特性）。</li><li>–features {feature-list}：启用镜像特性，特性名称之间用逗号分隔。例如，layering,exclusive-lock。</li></ul><h4 id="查看-rbd-镜像列表"><a href="#查看-rbd-镜像列表" class="headerlink" title="查看 rbd 镜像列表"></a>查看 rbd 镜像列表</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">ls</span> &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><h4 id="获取-rbd-镜像信息"><a href="#获取-rbd-镜像信息" class="headerlink" title="获取 rbd 镜像信息"></a>获取 rbd 镜像信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd info &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="删除-rbd-镜像"><a href="#删除-rbd-镜像" class="headerlink" title="删除 rbd 镜像"></a>删除 rbd 镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">rm</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="映射-rbd-镜像到本地设备"><a href="#映射-rbd-镜像到本地设备" class="headerlink" title="映射 rbd 镜像到本地设备"></a>映射 rbd 镜像到本地设备</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd map &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="取消映射-rbd-镜像"><a href="#取消映射-rbd-镜像" class="headerlink" title="取消映射 rbd 镜像"></a>取消映射 rbd 镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd unmap /dev/rbd/&#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="扩展-rbd-镜像大小"><a href="#扩展-rbd-镜像大小" class="headerlink" title="扩展 rbd 镜像大小"></a>扩展 rbd 镜像大小</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd resize &#123;pool-name&#125;/&#123;image-name&#125; --size &#123;new-size-in-MB&#125;</span><br></pre></td></tr></table></figure><h4 id="从ceph导出-RBD-镜像"><a href="#从ceph导出-RBD-镜像" class="headerlink" title="从ceph导出 RBD 镜像"></a>从ceph导出 RBD 镜像</h4><p>用于将镜像的数据导出到一个本地文件中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">export</span> &#123;pool-name&#125;/&#123;image-name&#125; &#123;output-file&#125;</span><br></pre></td></tr></table></figure><h4 id="向ceph导入-RBD-镜像"><a href="#向ceph导入-RBD-镜像" class="headerlink" title="向ceph导入 RBD 镜像"></a>向ceph导入 RBD 镜像</h4><p>用于将本地文件导入到ceph rbd中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd import &#123;input-file&#125; &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="启用rbd特性"><a href="#启用rbd特性" class="headerlink" title="启用rbd特性"></a>启用rbd特性</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd feature <span class="built_in">enable</span> &#123;pool-name&#125;/&#123;image-name&#125; &#123;feature-name&#125;</span><br></pre></td></tr></table></figure><p><strong>rbd镜像特性</strong></p><ul><li>layering：支持图层（Layering）</li><li>exclusive-lock：独占锁（Exclusive Locking）</li><li>object-map：对象映射（Object Map）</li></ul><h4 id="禁用rbd特性"><a href="#禁用rbd特性" class="headerlink" title="禁用rbd特性"></a>禁用rbd特性</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd feature <span class="built_in">disable</span> &#123;pool-name&#125;/&#123;image-name&#125; &#123;feature-name&#125;</span><br></pre></td></tr></table></figure><h4 id="启用和禁用所有特性"><a href="#启用和禁用所有特性" class="headerlink" title="启用和禁用所有特性"></a>启用和禁用所有特性</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd feature <span class="built_in">enable</span> &#123;pool-name&#125;/&#123;image-name&#125; --all</span><br><span class="line">rbd feature <span class="built_in">disable</span> &#123;pool-name&#125;/&#123;image-name&#125; --all</span><br></pre></td></tr></table></figure><h4 id="RBD-复制命令"><a href="#RBD-复制命令" class="headerlink" title="RBD 复制命令"></a>RBD 复制命令</h4><p>用于复制 RBD 镜像到同一存储池中的新镜像，或复制到不同存储池中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">cp</span> &#123;source-pool-name&#125;/&#123;source-image-name&#125; &#123;destination-pool-name&#125;/&#123;destination-image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="启RBD-深度复制命令"><a href="#启RBD-深度复制命令" class="headerlink" title="启RBD 深度复制命令"></a>启RBD 深度复制命令</h4><p>用于执行深度复制，包括镜像的所有快照和元数据。这个命令是 RADOS 镜像的完整复制工具。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd deep-copy &#123;source-pool-name&#125;/&#123;source-image-name&#125; &#123;destination-pool-name&#125;/&#123;destination-image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="RBD-差异命令"><a href="#RBD-差异命令" class="headerlink" title="RBD 差异命令"></a>RBD 差异命令</h4><p>用于查看两个 RBD 镜像之间的差异，包括哪些块已更改、已删除或已新增。它可以帮助识别镜像之间的更改。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd diff &#123;pool-name&#125;/&#123;image-name&#125; [--snap &#123;snapshot-name&#125;] [--diff &#123;other-image&#125;]</span><br></pre></td></tr></table></figure><h4 id="RBD-磁盘使用命令"><a href="#RBD-磁盘使用命令" class="headerlink" title="RBD 磁盘使用命令"></a>RBD 磁盘使用命令</h4><p>用于显示 RBD 镜像的磁盘使用情况，包括镜像占用的总磁盘空间和其他相关信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">du</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="RBD-状态命令"><a href="#RBD-状态命令" class="headerlink" title="RBD 状态命令"></a>RBD 状态命令</h4><p>用于显示 RBD 镜像的状态信息，包括镜像的健康状态和其他相关信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd status &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="RBD-观察命令"><a href="#RBD-观察命令" class="headerlink" title="RBD 观察命令"></a>RBD 观察命令</h4><p>用于观察镜像的实时更改，这个命令允许用户跟踪镜像的变化，包括数据的写入、删除和其他操作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd watch &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="RBD-稀疏化命令"><a href="#RBD-稀疏化命令" class="headerlink" title="RBD 稀疏化命令"></a>RBD 稀疏化命令</h4><p>用于将 RBD 镜像的已分配但未使用的空间标记为稀疏。通过稀疏化，可以释放磁盘上未使用的空间，从而优化存储资源。<br><strong>注意事项</strong></p><ul><li>影响：稀疏化操作会扫描镜像并更新其内部元数据，可能会占用一定的 I&#x2F;O 带宽和计算资源。</li><li>稀疏化条件：只有在镜像的写入操作完成后，才建议执行稀疏化，以避免在镜像空间仍在使用时进行操作。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd sparsify &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="创建-rbd-快照"><a href="#创建-rbd-快照" class="headerlink" title="创建 rbd 快照"></a>创建 rbd 快照</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap create &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125;</span><br></pre></td></tr></table></figure><h4 id="保护-rbd-快照"><a href="#保护-rbd-快照" class="headerlink" title="保护 rbd 快照"></a>保护 rbd 快照</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap protect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125;</span><br></pre></td></tr></table></figure><h4 id="取消保护-rbd-快照"><a href="#取消保护-rbd-快照" class="headerlink" title="取消保护 rbd 快照"></a>取消保护 rbd 快照</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap unprotect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125;</span><br></pre></td></tr></table></figure><h4 id="列出-rbd-快照"><a href="#列出-rbd-快照" class="headerlink" title="列出 rbd 快照"></a>列出 rbd 快照</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap <span class="built_in">ls</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="回滚到-rbd-快照"><a href="#回滚到-rbd-快照" class="headerlink" title="回滚到 rbd 快照"></a>回滚到 rbd 快照</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap rollback &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125;</span><br></pre></td></tr></table></figure><h4 id="删除-rbd-快照"><a href="#删除-rbd-快照" class="headerlink" title="删除 rbd 快照"></a>删除 rbd 快照</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap <span class="built_in">rm</span> &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125;</span><br></pre></td></tr></table></figure><h2 id="rbd-trash"><a href="#rbd-trash" class="headerlink" title="rbd trash"></a>rbd trash</h2><p>rbd trash 功能允许管理员在删除 rbd 镜像时先将其移动到 trash，而不是立即永久删除。这为误删除的镜像提供了恢复的机会。以下是 rbd trash 的详细说明和常用操作指南。<br>rbd trash 功能的主要特点包括：</p><ul><li>安全性：防止意外删除镜像。</li><li>可恢复性：在一定时间内可以恢复已删除的镜像。</li><li>定期清理：可以设置镜像在 trash 中的过期时间，自动清理过期的镜像。</li></ul><h4 id="查看-rbd-trash-中的镜像"><a href="#查看-rbd-trash-中的镜像" class="headerlink" title="查看 rbd trash 中的镜像"></a>查看 rbd trash 中的镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash <span class="built_in">ls</span> &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><h4 id="将-rbd-镜像移动到-trash"><a href="#将-rbd-镜像移动到-trash" class="headerlink" title="将 rbd 镜像移动到 trash"></a>将 rbd 镜像移动到 trash</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash <span class="built_in">mv</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="恢复-trash-中的-rbd-镜像"><a href="#恢复-trash-中的-rbd-镜像" class="headerlink" title="恢复 trash 中的 rbd 镜像"></a>恢复 trash 中的 rbd 镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash restore &#123;pool-name&#125;/&#123;image-id or image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="永久删除-trash-中的-rbd-镜像"><a href="#永久删除-trash-中的-rbd-镜像" class="headerlink" title="永久删除 trash 中的 rbd 镜像"></a>永久删除 trash 中的 rbd 镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash <span class="built_in">rm</span> &#123;pool-name&#125;/&#123;image-id or image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="查看-trash-中镜像的详细信息"><a href="#查看-trash-中镜像的详细信息" class="headerlink" title="查看 trash 中镜像的详细信息"></a>查看 trash 中镜像的详细信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash info &#123;pool-name&#125;/&#123;image-id or image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="设置-trash-镜像的过期时间"><a href="#设置-trash-镜像的过期时间" class="headerlink" title="设置 trash 镜像的过期时间"></a>设置 trash 镜像的过期时间</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash <span class="built_in">mv</span> &#123;pool-name&#125;/&#123;image-name&#125; --expire &#123;time-spec&#125;</span><br></pre></td></tr></table></figure><p>其中，<code>&#123;time-spec&#125;</code> 可以是以下格式之一：</p><ul><li><code>1d</code>：1天</li><li><code>1h</code>：1小时</li><li><code>1m</code>：1分钟</li></ul><h4 id="查看-trash-镜像的过期时间"><a href="#查看-trash-镜像的过期时间" class="headerlink" title="查看 trash 镜像的过期时间"></a>查看 trash 镜像的过期时间</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash list &#123;pool-name&#125; --long</span><br></pre></td></tr></table></figure><h4 id="清空-trash"><a href="#清空-trash" class="headerlink" title="清空 trash"></a>清空 trash</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash purge &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><h4 id="克隆-rbd-镜像"><a href="#克隆-rbd-镜像" class="headerlink" title="克隆 rbd 镜像"></a>克隆 rbd 镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">clone</span> &#123;pool-name&#125;/&#123;parent-image&#125;@&#123;snap-name&#125; &#123;pool-name&#125;/&#123;clone-name&#125;</span><br></pre></td></tr></table></figure><h4 id="合并克隆的-rbd-镜像"><a href="#合并克隆的-rbd-镜像" class="headerlink" title="合并克隆的 rbd 镜像"></a>合并克隆的 rbd 镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd flatten &#123;pool-name&#125;/&#123;clone-name&#125;</span><br></pre></td></tr></table></figure><h4 id="启用镜像同步"><a href="#启用镜像同步" class="headerlink" title="启用镜像同步"></a>启用镜像同步</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd mirror image <span class="built_in">enable</span> &#123;pool-name&#125;/&#123;image-name&#125; &#123;mode&#125;</span><br></pre></td></tr></table></figure><h4 id="禁用镜像同步"><a href="#禁用镜像同步" class="headerlink" title="禁用镜像同步"></a>禁用镜像同步</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd mirror image <span class="built_in">disable</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="查看镜像同步状态"><a href="#查看镜像同步状态" class="headerlink" title="查看镜像同步状态"></a>查看镜像同步状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd mirror image status &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="查看镜像配置"><a href="#查看镜像配置" class="headerlink" title="查看镜像配置"></a>查看镜像配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd config image list &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h4 id="修改镜像配置"><a href="#修改镜像配置" class="headerlink" title="修改镜像配置"></a>修改镜像配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd config image <span class="built_in">set</span> &#123;pool-name&#125;/&#123;image-name&#125; &#123;config-key&#125; &#123;value&#125;</span><br></pre></td></tr></table></figure><h4 id="删除镜像配置"><a href="#删除镜像配置" class="headerlink" title="删除镜像配置"></a>删除镜像配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd config image <span class="built_in">rm</span> &#123;pool-name&#125;/&#123;image-name&#125; &#123;config-key&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;创建-rbd-镜像&quot;&gt;&lt;a href=&quot;#创建-rbd-镜像&quot; class=&quot;headerlink&quot; title=&quot;创建 rbd 镜像&quot;&gt;&lt;/a&gt;创建 rbd 镜像&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph相关运维命令</title>
    <link href="https://watsonlu6.github.io/Ceph%E5%B8%B8%E8%A7%81%E8%BF%90%E7%BB%B4%E5%91%BD%E4%BB%A4/"/>
    <id>https://watsonlu6.github.io/Ceph%E5%B8%B8%E8%A7%81%E8%BF%90%E7%BB%B4%E5%91%BD%E4%BB%A4/</id>
    <published>2021-09-07T12:01:00.000Z</published>
    <updated>2024-08-02T17:38:18.260Z</updated>
    
    <content type="html"><![CDATA[<h4 id="查看-Ceph-的守护进程"><a href="#查看-Ceph-的守护进程" class="headerlink" title="查看 Ceph 的守护进程"></a>查看 Ceph 的守护进程</h4><p>使用以下命令查看所有 Ceph 守护进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl list-unit-files | grep ceph</span><br></pre></td></tr></table></figure><h4 id="按类型在-Ceph-节点上启动特定类型的所有守护进程"><a href="#按类型在-Ceph-节点上启动特定类型的所有守护进程" class="headerlink" title="按类型在 Ceph 节点上启动特定类型的所有守护进程"></a>按类型在 Ceph 节点上启动特定类型的所有守护进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl start/restart/stop ceph-osd.target</span><br><span class="line">systemctl start/restart/stop ceph-mon.target</span><br><span class="line">systemctl start/restart/stop ceph-mds.target</span><br><span class="line">systemctl start/restart/stop ceph-radosgw.target</span><br></pre></td></tr></table></figure><h4 id="启动特定守护进程实例"><a href="#启动特定守护进程实例" class="headerlink" title="启动特定守护进程实例"></a>启动特定守护进程实例</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl start/status/restart/stop ceph-osd@&#123;<span class="built_in">id</span>&#125;</span><br><span class="line">systemctl start/status/restart/stop ceph-mon@&#123;hostname&#125;</span><br><span class="line">systemctl start/status/restart/stop ceph-mds@&#123;hostname&#125;</span><br><span class="line">systemctl start/restart/stop ceph-radosgw@&#123;hostname&#125;</span><br></pre></td></tr></table></figure><h4 id="查看-Ceph-集群状态"><a href="#查看-Ceph-集群状态" class="headerlink" title="查看 Ceph 集群状态"></a>查看 Ceph 集群状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph -s</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">ceph health detail</span><br></pre></td></tr></table></figure><p>输出信息包括：</p><ul><li>集群的 ID</li><li>集群健康状况</li><li>monitor map 版本和 mon 法定人数状态</li><li>OSD map 版本和 OSD 状态摘要</li><li>PG map 版本</li><li>PG 和 Pool 的数量</li><li>集群存储的数据量，对象的总量，以及集群的已用容量&#x2F;总容量&#x2F;可用容量</li><li>客户端的 IOPS 信息</li></ul><h4 id="观察集群中的状态"><a href="#观察集群中的状态" class="headerlink" title="观察集群中的状态"></a>观察集群中的状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph -w</span><br></pre></td></tr></table></figure><p>输出信息包含：</p><ul><li>集群的 ID</li><li>集群健康状况</li><li>monitor map 版本和 mon 法定人数状态</li><li>OSD map 版本和 OSD 状态摘要</li><li>PG map 版本</li><li>PG 和 Pool 的数量</li><li>集群存储的数据量，对象的总量，以及集群的已用容量&#x2F;总容量&#x2F;可用容量</li><li>客户端的 IOPS 信息</li></ul><h4 id="检查集群的容量情况"><a href="#检查集群的容量情况" class="headerlink" title="检查集群的容量情况"></a>检查集群的容量情况</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph <span class="built_in">df</span></span><br></pre></td></tr></table></figure><h3 id="修改集群配置"><a href="#修改集群配置" class="headerlink" title="修改集群配置"></a>修改集群配置</h3><h4 id="查看默认配置"><a href="#查看默认配置" class="headerlink" title="查看默认配置"></a>查看默认配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph --show-config</span><br></pre></td></tr></table></figure><h4 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h4><p>Ceph 支持在运行时更改 ceph-osd、ceph-mon、ceph-mds 守护进程的配置。</p><h4 id="使用-tell-的方式"><a href="#使用-tell-的方式" class="headerlink" title="使用 tell 的方式"></a>使用 tell 的方式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph tell &#123;daemon-type&#125;.&#123;<span class="built_in">id</span> or *&#125; injectargs --&#123;name&#125; &#123;value&#125; [--&#123;name&#125; &#123;value&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：</span></span><br><span class="line">ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1</span><br></pre></td></tr></table></figure><h4 id="使用-daemon-的方式设置"><a href="#使用-daemon-的方式设置" class="headerlink" title="使用 daemon 的方式设置"></a>使用 daemon 的方式设置</h4><p>在设置的角色所在主机上进行设置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看配置</span></span><br><span class="line">ceph daemon osd.1 config get mon_osd_full_ratio</span><br><span class="line"><span class="comment"># 修改配置</span></span><br><span class="line">ceph daemon osd.1 config <span class="built_in">set</span> mon_osd_full_ratio 0.97</span><br></pre></td></tr></table></figure><h4 id="在线调整日志级别"><a href="#在线调整日志级别" class="headerlink" title="在线调整日志级别"></a>在线调整日志级别</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph tell osd.0 injectargs --debug-osd 0/5</span><br></pre></td></tr></table></figure><h4 id="修改配置文件进行调整"><a href="#修改配置文件进行调整" class="headerlink" title="修改配置文件进行调整"></a>修改配置文件进行调整</h4><p>编辑 <code>/etc/ceph/ceph.conf</code> 中的 [global] 字段添加配置，重启相应服务生效。</p><h4 id="查看-mon-状态"><a href="#查看-mon-状态" class="headerlink" title="查看 mon 状态"></a>查看 mon 状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mon <span class="built_in">stat</span></span><br></pre></td></tr></table></figure><h4 id="查看-mon-的详细状态"><a href="#查看-mon-的详细状态" class="headerlink" title="查看 mon 的详细状态"></a>查看 mon 的详细状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph daemon mon.ceph-xx-mon00 mon_status</span><br></pre></td></tr></table></figure><h4 id="mon-法定人数状态"><a href="#mon-法定人数状态" class="headerlink" title="mon 法定人数状态"></a>mon 法定人数状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph quorum_status -f json-pretty</span><br></pre></td></tr></table></figure><h4 id="查看-mon-选举状态"><a href="#查看-mon-选举状态" class="headerlink" title="查看 mon 选举状态"></a>查看 mon 选举状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph quorum_status</span><br></pre></td></tr></table></figure><h4 id="查看-mon-的映射信息"><a href="#查看-mon-的映射信息" class="headerlink" title="查看 mon 的映射信息"></a>查看 mon 的映射信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mon dump</span><br></pre></td></tr></table></figure><h4 id="查看-mon-的-admin-socket"><a href="#查看-mon-的-admin-socket" class="headerlink" title="查看 mon 的 admin socket"></a>查看 mon 的 admin socket</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-conf --name mon.ceph-xx-mon00 --show-config-value admin_socket</span><br><span class="line">/var/run/ceph/ceph-mon.ceph-xx-mon00.asok</span><br></pre></td></tr></table></figure><h2 id="CRUSH-Map"><a href="#CRUSH-Map" class="headerlink" title="CRUSH Map"></a>CRUSH Map</h2><h4 id="创建-bucket"><a href="#创建-bucket" class="headerlink" title="创建 bucket"></a>创建 bucket</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush add-bucket host-xx host</span><br></pre></td></tr></table></figure><h4 id="移动-bucket"><a href="#移动-bucket" class="headerlink" title="移动 bucket"></a>移动 bucket</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush move host-xx room=default</span><br></pre></td></tr></table></figure><h4 id="提取-CRUSH-Map"><a href="#提取-CRUSH-Map" class="headerlink" title="提取 CRUSH Map"></a>提取 CRUSH Map</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd getcrushmap -o crush</span><br></pre></td></tr></table></figure><h4 id="反编译-crush-map"><a href="#反编译-crush-map" class="headerlink" title="反编译 crush map"></a>反编译 crush map</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crushtool -d crush -o de_crush </span><br></pre></td></tr></table></figure><h4 id="编译-crush-map"><a href="#编译-crush-map" class="headerlink" title="编译 crush map"></a>编译 crush map</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crushtool -c de_crush -o new_crush</span><br></pre></td></tr></table></figure><h4 id="测试新的-CRUSH-Map"><a href="#测试新的-CRUSH-Map" class="headerlink" title="测试新的 CRUSH Map"></a>测试新的 CRUSH Map</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crushtool --<span class="built_in">test</span> -i new_crush --num-rep 3 --rule 1 --show-mappings </span><br></pre></td></tr></table></figure><h4 id="注入-CRUSH-Map"><a href="#注入-CRUSH-Map" class="headerlink" title="注入 CRUSH Map"></a>注入 CRUSH Map</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd setcrushmap -i new_crush</span><br></pre></td></tr></table></figure><h4 id="列出-crush-rule"><a href="#列出-crush-rule" class="headerlink" title="列出 crush_rule"></a>列出 crush_rule</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush rule <span class="built_in">ls</span></span><br></pre></td></tr></table></figure><h4 id="查看-crush-rule"><a href="#查看-crush-rule" class="headerlink" title="查看 crush_rule"></a>查看 crush_rule</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd crush rule dump &#123;rule&#125;</span><br></pre></td></tr></table></figure><h2 id="PG-和-PGP"><a href="#PG-和-PGP" class="headerlink" title="PG 和 PGP"></a>PG 和 PGP</h2><h4 id="查看-PG-状态"><a href="#查看-PG-状态" class="headerlink" title="查看 PG 状态"></a>查看 PG 状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg <span class="built_in">stat</span></span><br></pre></td></tr></table></figure><h4 id="查看-PG-组映射信息"><a href="#查看-PG-组映射信息" class="headerlink" title="查看 PG 组映射信息"></a>查看 PG 组映射信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump</span><br></pre></td></tr></table></figure><h4 id="查看一个-PG-的-map"><a href="#查看一个-PG-的-map" class="headerlink" title="查看一个 PG 的 map"></a>查看一个 PG 的 map</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg map 1.2f6</span><br></pre></td></tr></table></figure><h4 id="查看-PG-详细信息"><a href="#查看-PG-详细信息" class="headerlink" title="查看 PG 详细信息"></a>查看 PG 详细信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg 1.2f6 query</span><br></pre></td></tr></table></figure><h4 id="显示集群所有-PG-统计"><a href="#显示集群所有-PG-统计" class="headerlink" title="显示集群所有 PG 统计"></a>显示集群所有 PG 统计</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump --format plain</span><br></pre></td></tr></table></figure><h4 id="显示非正常状态的-PG"><a href="#显示非正常状态的-PG" class="headerlink" title="显示非正常状态的 PG"></a>显示非正常状态的 PG</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump_stuck inactive|unclean|stale</span><br></pre></td></tr></table></figure><h2 id="OSD"><a href="#OSD" class="headerlink" title="OSD"></a>OSD</h2><h4 id="查看-OSD-状态"><a href="#查看-OSD-状态" class="headerlink" title="查看 OSD 状态"></a>查看 OSD 状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">stat</span></span><br></pre></td></tr></table></figure><h4 id="检查-OSD-容量是否均衡"><a href="#检查-OSD-容量是否均衡" class="headerlink" title="检查 OSD 容量是否均衡"></a>检查 OSD 容量是否均衡</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">df</span> tree</span><br></pre></td></tr></table></figure><h4 id="查看-OSD-映射信息"><a href="#查看-OSD-映射信息" class="headerlink" title="查看 OSD 映射信息"></a>查看 OSD 映射信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd dump</span><br></pre></td></tr></table></figure><h4 id="查看-OSD-目录树"><a href="#查看-OSD-目录树" class="headerlink" title="查看 OSD 目录树"></a>查看 OSD 目录树</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tree</span><br></pre></td></tr></table></figure><h4 id="定位-OSD-在集群中的节点位置"><a href="#定位-OSD-在集群中的节点位置" class="headerlink" title="定位 OSD 在集群中的节点位置"></a>定位 OSD 在集群中的节点位置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd find [osd]</span><br></pre></td></tr></table></figure><h4 id="查看对象在哪些-OSD-上"><a href="#查看对象在哪些-OSD-上" class="headerlink" title="查看对象在哪些 OSD 上"></a>查看对象在哪些 OSD 上</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd map test-pool object1</span><br></pre></td></tr></table></figure><h4 id="下线某个-OSD"><a href="#下线某个-OSD" class="headerlink" title="下线某个 OSD"></a>下线某个 OSD</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd down 0</span><br></pre></td></tr></table></figure><h4 id="拉起某个-OSD"><a href="#拉起某个-OSD" class="headerlink" title="拉起某个 OSD"></a>拉起某个 OSD</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd up 0</span><br></pre></td></tr></table></figure><h4 id="将某个-OSD-逐出集群"><a href="#将某个-OSD-逐出集群" class="headerlink" title="将某个 OSD 逐出集群"></a>将某个 OSD 逐出集群</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd out 0</span><br></pre></td></tr></table></figure><h4 id="将某个-OSD-加入集群"><a href="#将某个-OSD-加入集群" class="headerlink" title="将某个 OSD 加入集群"></a>将某个 OSD 加入集群</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="keyword">in</span> 0</span><br></pre></td></tr></table></figure><h4 id="删除某个-OSD"><a href="#删除某个-OSD" class="headerlink" title="删除某个 OSD"></a>删除某个 OSD</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd <span class="built_in">rm</span> 0</span><br></pre></td></tr></table></figure><h4 id="查看-OSD-延迟"><a href="#查看-OSD-延迟" class="headerlink" title="查看 OSD 延迟"></a>查看 OSD 延迟</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd perf</span><br></pre></td></tr></table></figure><h4 id="查看当前-OSD-的状态"><a href="#查看当前-OSD-的状态" class="headerlink" title="查看当前 OSD 的状态"></a>查看当前 OSD 的状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph daemon osd.14 perf dump</span><br></pre></td></tr></table></figure><h2 id="Pool"><a href="#Pool" class="headerlink" title="Pool"></a>Pool</h2><h4 id="查看-pool-信息"><a href="#查看-pool-信息" class="headerlink" title="查看 pool 信息"></a>查看 pool 信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd lspools</span><br></pre></td></tr></table></figure><h4 id="查看-pool-详细信息"><a href="#查看-pool-详细信息" class="headerlink" title="查看 pool 详细信息"></a>查看 pool 详细信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">ls</span> detail</span><br></pre></td></tr></table></figure><h4 id="查看-pool-状态"><a href="#查看-pool-状态" class="headerlink" title="查看 pool 状态"></a>查看 pool 状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool stats</span><br></pre></td></tr></table></figure><h3 id="创建-pool"><a href="#创建-pool" class="headerlink" title="创建 pool"></a>创建 pool</h3><h4 id="创建副本-pool"><a href="#创建副本-pool" class="headerlink" title="创建副本 pool"></a>创建副本 pool</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; &#123;pgp-num&#125; replicated &#123;crush-ruleset-name&#125; </span><br></pre></td></tr></table></figure><h4 id="创建-EC-pool"><a href="#创建-EC-pool" class="headerlink" title="创建 EC pool"></a>创建 EC pool</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; &#123;pgp-num&#125; erasure &#123;erasure-code-profile&#125;</span><br></pre></td></tr></table></figure><h4 id="创建-erasure-code-profile"><a href="#创建-erasure-code-profile" class="headerlink" title="创建 erasure-code-profile"></a>创建 erasure-code-profile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd erasure-code-profile <span class="built_in">set</span> ec-4-2 k=4 m=2 ruleset-failure-domain=host ruleset-root=hddRoom</span><br></pre></td></tr></table></figure><h4 id="列出-erasure-code-profile"><a href="#列出-erasure-code-profile" class="headerlink" title="列出 erasure-code-profile"></a>列出 erasure-code-profile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd erasure-code-profile <span class="built_in">ls</span></span><br></pre></td></tr></table></figure><h4 id="查看-erasure-code-profile"><a href="#查看-erasure-code-profile" class="headerlink" title="查看 erasure-code-profile"></a>查看 erasure-code-profile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd erasure-code-profile get [name]</span><br></pre></td></tr></table></figure><h4 id="删除-pool"><a href="#删除-pool" class="headerlink" title="删除 pool"></a>删除 pool</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除 pool 前需要执行</span></span><br><span class="line">ceph tell mon.* injectargs --mon-allow-pool-delete=<span class="literal">true</span></span><br><span class="line"><span class="comment"># 删除pool</span></span><br><span class="line">ceph osd pool delete test_pool test_pool --yes-i-really-really-mean-it  <span class="comment">#pool的名字需要重复两次</span></span><br></pre></td></tr></table></figure><h4 id="设置-pool-的-PG-数量"><a href="#设置-pool-的-PG-数量" class="headerlink" title="设置 pool 的 PG 数量"></a>设置 pool 的 PG 数量</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> test_pool pg_num 100</span><br></pre></td></tr></table></figure><h4 id="查看-pool-的-PG-数量"><a href="#查看-pool-的-PG-数量" class="headerlink" title="查看 pool 的 PG 数量"></a>查看 pool 的 PG 数量</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get test_pool pg_num</span><br></pre></td></tr></table></figure><h4 id="设置-pool-的-PGP-数量"><a href="#设置-pool-的-PGP-数量" class="headerlink" title="设置 pool 的 PGP 数量"></a>设置 pool 的 PGP 数量</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> test_pool pgp_num 100</span><br></pre></td></tr></table></figure><h4 id="查看-pool-的-PGP-数量"><a href="#查看-pool-的-PGP-数量" class="headerlink" title="查看 pool 的 PGP 数量"></a>查看 pool 的 PGP 数量</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get test_pool pgp_num</span><br></pre></td></tr></table></figure><h4 id="设置-pool-池副本数"><a href="#设置-pool-池副本数" class="headerlink" title="设置 pool 池副本数"></a>设置 pool 池副本数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> test_pool size 3</span><br></pre></td></tr></table></figure><h4 id="查看-pool-池副本数"><a href="#查看-pool-池副本数" class="headerlink" title="查看 pool 池副本数"></a>查看 pool 池副本数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get test_pool size</span><br></pre></td></tr></table></figure><h4 id="设置存储池-crush-rule"><a href="#设置存储池-crush-rule" class="headerlink" title="设置存储池 crush rule"></a>设置存储池 crush rule</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &lt;poolname&gt; crush_ruleset &lt;ruleset&gt;</span><br></pre></td></tr></table></figure><h4 id="查看存储池-crush-rule"><a href="#查看存储池-crush-rule" class="headerlink" title="查看存储池 crush rule"></a>查看存储池 crush rule</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get &lt;poolname&gt; crush_rule</span><br></pre></td></tr></table></figure><h2 id="RADOS"><a href="#RADOS" class="headerlink" title="RADOS"></a>RADOS</h2><h4 id="查看对象信息"><a href="#查看对象信息" class="headerlink" title="查看对象信息"></a>查看对象信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados -p test_pool <span class="built_in">stat</span> test-object-1</span><br></pre></td></tr></table></figure><h4 id="获取对象内容"><a href="#获取对象内容" class="headerlink" title="获取对象内容"></a>获取对象内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados -p test_pool get test-object-1 test.txt</span><br></pre></td></tr></table></figure><h4 id="将指定文件作为对象写入到资源池"><a href="#将指定文件作为对象写入到资源池" class="headerlink" title="将指定文件作为对象写入到资源池"></a>将指定文件作为对象写入到资源池</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados -p test_pool put test-object-2 test.txt</span><br></pre></td></tr></table></figure><h4 id="删除对象"><a href="#删除对象" class="headerlink" title="删除对象"></a>删除对象</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados -p test_pool <span class="built_in">rm</span> test-object-1</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;查看-Ceph-的守护进程&quot;&gt;&lt;a href=&quot;#查看-Ceph-的守护进程&quot; class=&quot;headerlink&quot; title=&quot;查看 Ceph 的守护进程&quot;&gt;&lt;/a&gt;查看 Ceph 的守护进程&lt;/h4&gt;&lt;p&gt;使用以下命令查看所有 Ceph 守护进程：&lt;/p&gt;
</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph线程池实现</title>
    <link href="https://watsonlu6.github.io/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0/"/>
    <id>https://watsonlu6.github.io/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-08-27T14:26:42.000Z</published>
    <updated>2024-07-28T10:16:50.698Z</updated>
    
    <content type="html"><![CDATA[<p>线程池和工作队列是紧密相连的，基本流程就是将任务送入到对应的工作队列中，线程池中的线程从工作队列中取出任务并进行处理。Ceph 为了支持高并发读写，源码设计中大量采用线程池来进行io的推进。Ceph的线程池实现了多种不同的工作队列。一般情况下，一个线程池对应一个类型的工作队列。在要求不高的情况下，也可以一个线程池对应多种类型的工作队列，让线程池处理不同类型的任务。</p><h2 id="mutex的实现"><a href="#mutex的实现" class="headerlink" title="mutex的实现"></a>mutex的实现</h2><p>src&#x2F;common&#x2F;mutex.h<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B01.png"><br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B02.png"></p><p>condition variable的实现<br>src&#x2F;common&#x2F;cond.h<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B03.png"></p><p><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B04.png"></p><h2 id="线程的实现"><a href="#线程的实现" class="headerlink" title="线程的实现"></a>线程的实现</h2><p>Ceph中线程的在src&#x2F;common&#x2F;Thread.h中定义<br>线程编程接口中，一个线程在创建时调用pthread_create函数来传入entry函数，杀死线程调用pthread_kill函数，当线程被杀死之后，必须调用pthread_join函数来进行线程资源的回收，如果不调用此函数，就会出现类似zombie process。如果要想让系统自己回收线程资源，就要将线程与父线程分离即调用pthread_detach。通过接口对比，src&#x2F;common&#x2F;Thread.h中定义的class thread，实际上是Ceph自己封装了一个线程类，这个线程类其实就是对Linux线程接口的一层封装。<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B05.png"></p><p>Ceph中所有要用的线程必须继承Thread类，通过查找发现如下一些线程：</p><ol><li>Accepter.h (src\msg)：class Accepter : public Thread  &#x2F;&#x2F;用来socket bind的线程,   accepter线程入口函数里定义了poll的网络通讯结构，用来放入管道</li><li>Admin_socket.h (src\common)：class AdminSocket : public Thread</li><li>Ceph_context.cc (src\common)：class CephContextServiceThread : public Thread</li><li>DispatchQueue.h (src\msg):  class DispatchThread : public Thread   &#x2F;&#x2F;用来进行消息分发的线程，  在simpleMessenger中有dispatch_queue成员变量,</li><li>FileJournal.h (src\os):  class Writer : public Thread     &#x2F;&#x2F;用来进行写数据到journal中的线程</li><li>FileJournal.h (src\os):  class WriteFinisher : public Thread   &#x2F;&#x2F;当用aio异步模式写数据到journal完成后，此线程用来接管其他剩余操作</li><li>FileStore.h (src\os):  struct SyncThread : public Thread    &#x2F;&#x2F;用来同步数据执行同步的线程，主要是将已经完成的journal的序列号写入到文件中</li><li>Finisher.h (src\common):  struct FinisherThread : public Thread   &#x2F;&#x2F;公用的finisher线程，用来查看某些特定的操作是否结束，结束后进行后续处理工作</li><li>MDLog.h (src\mds):  class ReplayThread : public Thread </li><li>OSD.h (src\osd):  struct T_Heartbeat : public Thread   &#x2F;&#x2F;维系osd进程之间互相心跳连接的线程</li><li>OutputDataSocket.h (src\common):class OutputDataSocket : public Thread</li><li>Pipe.h (src\msg): class Reader : public Thread   &#x2F;&#x2F;用来处理所有对socket的读操作，由acepter线程将socket accept以后打入到SimpleMessenger::dispatch_queue中交由此线程处理</li><li>Pipe.h (src\msg): class Writer : public Thread   &#x2F;&#x2F;用来处理所有对socket的写操作，由acepter线程将socket accept以后打入到SimpleMessenger::dispatch_queue中交由此线程处理</li><li>Pipe.h (src\msg):    class DelayedDelivery: public Thread    &#x2F;&#x2F;用来处理所有对socket的延时操作</li><li>Signal_handler.cc (src\global)：struct SignalHandler : public Thread </li><li>SimpleMessenger.h (src\msg):  class ReaperThread : public Thread &#x2F;&#x2F;用来进行消息通信的主要线程 reaper是用来在通讯完成时拆除管道，其中成员有accepter线程（用来bind，accept socket文件放入管道），还有dispatch_queue线程</li><li>Throttle.cc (src\test\common):  class Thread_get : public Thread </li><li>Timer.cc (src\common)：class SafeTimerThread : public Thread </li><li>WorkQueue.h (src\common):  struct WorkThread : public Thread</li></ol><p>可以将这些线程分为四类线程</p><ol><li>普通类线程：<br> 使用此类线程类直接申明继承自Thread，重写一个entry函数，在进程启动最初时，调用了create函数创建了线程，同时使用它的人必须自己定义消息队列。上面大部分线程都是此类，比如FileJournal::write_thread就是一个FileJournal::Writer类对象，它自己定义了消息队列FileJournal::writeq</li><li>SafeTimerThread类线程:<br> 此类线程使用者可以直接申明一个SafeTimer成员变量，因为SafeTimer中已经封装了SafeTimerThread类和一个消息队列（成员是Context回调类），并完成了entry函数的逻辑流程。使用者使用方法，就是设置回调函数，通过SafeTimer::add_event_after函数将钩子埋入，等待规定时间到达后执行。</li><li>FinisherThread类线程:<br> 此类线程使用者可以直接申明一个Finisher成员变量，因为Finsher中已经封装了FinisherThread类和一个消息队列（成员是Context回调类），并完成entry函数的逻辑流程。使用者使用方法，就是设置回调函数，通过Finisher::queue函数将钩子埋入，等待某类操作完成后执行。</li><li>ThreadPool内部线程：<br> 这类线程由于是具体工作类线程，所以他们一般都是以线程池形式一下创建多个。ThreadPool类内部有多个线程set&lt;WorkThread*&gt;和多个消息队列vector&lt;WorkQueue_*&gt;组成。工作流程就是线程不断的轮询从队列中拿去数据进行操作。</li></ol><p>可以看到Ceph线程的所有接口都只是对相应的Linux接口的封装。继承其的子类主要在于实现entry()函数：<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B06.png"></p><p><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B07.png"></p><p><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B08.png"></p><h2 id="线程池的实现"><a href="#线程池的实现" class="headerlink" title="线程池的实现"></a>线程池的实现</h2><p>Ceph中线程池的在src&#x2F;common&#x2F;WorkQueue.h中定义<br>线程池和工作队列其实是密不可分的，从Ceph的代码中也可以看出来。让任务推入工作队列，而线程池中的线程负责从工作队列中取出任务进行处理。工作队列和线程池的关系，类似于狡兔和走狗的关系，正是因为有任务，所以才需要雇佣线程来完成任务，没有了狡兔，走狗也就失去了存在的意义。而线程必须要可以从工作队列中认领任务并完成，这就类似于猎狗要有追捕狡兔的功能。正因为两个数据结构拥有如此紧密的关系，因此，Ceph中他们的相关函数都位于WorkQueue.cc和WorkQueue.h中。</p><p><strong>void ThreadPool::start()</strong><br>函数ThreadPool::start()用来启动线程池，其在加锁的情况下，调用函数start_threads()，start_threads()检查当前的线程数，如果小于配置的线程池线程数，就创建新的工作线程。<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B09.png"><br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B010.png"></p><p><strong>struct WorkThread : public Thread</strong><br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B011.png"></p><p><strong>ThreadPool::worker()</strong><br>线程池的关键在于线程的主函数做的事情。首先是工作线程。线程池中会有很多的WorkThread，它的基类就是Thread。线程的主函数为pool-&gt;worker，即ThreadPool::worker函数。其entry函数其实就是调用线程池的worker函数进行具体的工作。</p><p>ThreadPool::worker函数内定义了WorkThread类线程的操作逻辑。基本流程就是轮询所有WorkQueue_，当发现某种类型WorkQueue_中有数据时拿出，然后依次调用该WorkQueue_自己定义的函数_void_process和_void_process_finish等函数来顺序执行操作。（worker函数的主要实现其实很常规，就是遍历work_queues，从其中找出每一个消息队列实例，并调用WorkQueue_自己定义的函数_void_process和_void_process_finish等函数来顺序执行操作。）<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B012.png"><br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B013.png"></p><p>线程池是支持动态调整线程个数的。所谓调整，有两种可能性，一种是线程个数增加，一种线程个数减少。当添加OSD的时候，数据会重分布，恢复的速度可以调节，其中一个重要的参数为osd-max-recovery-threads，该值修改可以实时生效。</p><p><strong>ThreadPool::join_old_threads()</strong><br>线程本身是一个loop，不停地处理WorkQueue中的任务，在一个loop的开头，线程个数是否超出了配置的个数，如果超出了，就需要自杀，所谓自杀即将自身推送到_old_threads中，然后跳出loop，直接返回了。线程池中的其他兄弟在busy-loop开头的join_old_threads函数会判断是否存在自杀的兄弟，如果存在的话，执行join，为兄弟收尸。<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B014.png"></p><p><strong>ThreadPool::start_threads()</strong><br>start_threads函数不仅仅可以用在初始化时启动所有工作线程，而且可以用于动态增加，它会根据配置要求的线程数_num_threads和当前线程池中线程的个数，来创建WorkThread，当然了，他会调整线程的io优先级。<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B015.png"></p><p><strong>ThreadPool::handle_conf_change()</strong><br>线程池的线程个数如果不够用，也可以动态的增加，通过配置的变化来做到：<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B016.png"></p><p><strong>ThreadPool::pause()</strong><br>线程池的工作线程，绝大部分时间内，自然是busy－loop中处理工作队列上的任务，但是有一种场景是，需要让工作暂时停下来，停止工作，不要处理WorkQueue中的任务。线程池提供了一个标志为_pause,只要_pause不等于0，那么线程池中线程就在loop中就不会处理工作队列中的任务，而是空转。为了能够及时的醒来，也不是sleep，而是通过条件等待，等待执行的时间。</p><p>当下达pause指令的时候，很可能线程池中的某几个线程正在处理工作队列中的任务，这种情况下并不是立刻就能停下的，只有处理完手头的任务，在下一轮loop中检查_pause标志位才能真正地停下。那么pause指令就面临选择，要不要等工作线程WorkThread处理完手头的任务。pause函数是等，pauser_new函数并不等，pause_new函数只负责设置标志位，当其返回的时候，某几个线程可能仍然在处理工作队列中的任务。<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B017.png"></p><p><strong>struct WorkQueue_</strong><br>在ThreadPool这个类中，set&lt;WorkThread*&gt; _threads保存着线程池中的多个线程，vector&lt;WorkQueue_*&gt; work_queues保存着线程池中的待线程处理的消息队列。整个线程池的原理思想比较简单就是生成一定数目的线程，然后线程从队列中遍历获取队列实例，调用实例自带的处理函数_void_process和_void_process_finish处理。 ThreadPool中的WorkQueue_，这是一种抽象的类，只定义了一个队列应该有的一些特定的函数，这些函数几乎都是虚函数，目的是为了调用到自己三个子类BatchWorkQueue、WorkQueueVal、WorkQueue自己定义的函数。而在三个子类中对应函数_void_process、_void_process_finish中又分别调用了使用者自己继承它们而自己实现的具体操作函数如_process,_process_finish。存放在work_queues里面的WorkQueue_类：<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B018.png"></p><p>这是一个纯虚基类，也就是说不同的线程池要实现自己的队列，继承WorkQueues_并且实现其接口。线程池已经有4个纯虚基类继承这个类：</p><ul><li>BatchWorkQueue<br>  批量处理队列<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B019.png"></li><li>WorkQueueVal<br>  存值队列<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B020.png"></li><li>WorkQueue<br>  存指针队列<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B021.png"></li><li>PointerWQ<br>  存指针队列<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B022.png"></li></ul><p><strong>add_work_queue()&#x2F;remove_work_queue()</strong><br>ThreadPool中的add_work_queue和remove_work_queue就是用来建立和移除与WorkQueue关联的函数<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B023.png"></p><p><strong>TPHandle</strong><br>超时检查，每次线程函数执行时，都会设置一个grace超时时间，当线程执行超过该时间，就认为是unhealthy的状态。当执行时间超过suicide_grace时，OSD就会产生断言而导致自杀。heartbeat_handle_d记录了相关信息，并把该结构添加到HeartbeatMap的系统链表中保存。OSD会有一个定时器，定时检查是否超时。<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B024.png"></p><p>线程池使用步骤<br>先创建线程池，然后创建WorkQueue的时候，将线程池作为参数传递给WorkQueue，就能建立关系。</p><ol><li>声明线程池成员ThreadPool *_tp</li><li>声明队列类型ThreadPool::WorkQueue_*_wq</li><li>重写WorkQueue中对应函数_void_process,_void_process_finish</li><li>调用*_tp.add_work_queue(*_wq)将队列传入</li></ol><h2 id="基本线程池扩展"><a href="#基本线程池扩展" class="headerlink" title="基本线程池扩展"></a>基本线程池扩展</h2><p>在Ceph中有不少线程池会实现继承以上基类：<br>ThreadPool op_tp: 处理client请求<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B025.png"></p><p>struct recovery_tp: 处理recovery_tp操作<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B026.png"></p><p>struct command_tp: 处理命令行来的操作<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B027.png"></p><p>ShardedThreadPool: Ceph还实现了另外一种线程池ShardedThreadPool，这种线程池与上面的线程池不同之处在于这种线程池是多线程共享队列的方式。只有一个队列，多个线程同时对这个队列进行处理。<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B028.png"><br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B029.png"></p><p>SharededWQ: shardedThreadPool类型线程池内部有个比较重要的消息队列SharededWQ，该队列将多种OP放入其中<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B030.png"></p><p>Ceph 在实际使用中，会用到这种线程池<br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B031.png"><br><img src="/images/thread/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B032.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;线程池和工作队列是紧密相连的，基本流程就是将任务送入到对应的工作队列中，线程池中的线程从工作队列中取出任务并进行处理。Ceph 为了支持高并发读写，源码设计中大量采用线程池来进行io的推进。Ceph的线程池实现了多种不同的工作队列。一般情况下，一个线程池对应一个类型的工作队</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph_rbd客户端实现</title>
    <link href="https://watsonlu6.github.io/Ceph-rbd%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://watsonlu6.github.io/Ceph-rbd%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-08-15T14:26:55.000Z</published>
    <updated>2024-09-08T10:08:22.331Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ceph-RBD介绍"><a href="#Ceph-RBD介绍" class="headerlink" title="Ceph RBD介绍"></a>Ceph RBD介绍</h2><p>随着云计算的发展，Ceph已经成为目前最为流行的分布式存储系统，俨然存储界的Linux操作系统。Ceph集块存储、文件存储和对象存储于一身，适用场景广泛，用户众多。RBD是 Ceph 分布式存储系统中提供的块存储服务，Ceph的块存储通过一个客户端模块实现，这个客户端可以直接从数据守护进程读写数据（不需要经过一个网关）。根据客户端整合生态系统的差异，使用Ceph的块设备有两种实现方式：librbd (用户态)和krbd (内核态)。RBD：RADOS Block Devices. Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster.<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B01.png"></p><p>使用Ceph的块设备有两种路径（内核态与用户态）：(rbd map就是内核使用ceph块设备，调用librbd&#x2F;librados API访问ceph块设备是用户态)</p><ul><li>通过Kernel Module(内核态RBD)：即创建了RBD设备后，把它映射到内核中（使用rbd map命令映射到操作系统上），成为一个虚拟的块设备，这时这个块设备同其他通用块设备一样，设备文件一般为&#x2F;dev&#x2F;rbd0，后续直接使用这个块设备文件就可以了，可以把&#x2F;dev&#x2F;rbd0格式化后挂载到某目录，也可以直接作为裸设备进行使用。krbd是一个内核模块。其在内核中以一个块设备的方式加以实现。这整个Ceph客户端都是以内核模块的方式实现（没有与之相关的用户态进程或者守护进程）。krbd在内核的源码目录源文件:drivers&#x2F;block&#x2F;rbd.c、drivers&#x2F;block&#x2F;rbd_types.h、net&#x2F;ceph&#x2F;、include&#x2F;linux&#x2F;ceph<ul><li><a href="https://www.likecs.com/show-203739919.html">https://www.likecs.com/show-203739919.html</a></li><li><a href="https://github.com/torvalds/linux/blob/cfb92440ee71adcc2105b0890bb01ac3cddb8507/drivers/block/rbd.c">https://github.com/torvalds/linux/blob/cfb92440ee71adcc2105b0890bb01ac3cddb8507/drivers/block/rbd.c</a></li><li><a href="https://github.com/torvalds/linux/tree/85c7000fda0029ec16569b1eec8fd3a8d026be73/include/linux/ceph">https://github.com/torvalds/linux/tree/85c7000fda0029ec16569b1eec8fd3a8d026be73/include/linux/ceph</a></li></ul></li><li>通过librbd(用户态RBD)：即创建了RBD设备后，使用librbd&#x2F;librados库访问和管理块设备。这种方式直接调用librbd提供的接口，实现对RBD设备的访问和管理，不会在客户端产生设备文件，这种方式主要是为虚拟机提供块存储设备，在虚拟机场景中，一般会用QEMU&#x2F;KVM中的RBD驱动部署Ceph块设备，宿主机通过librbd向客户端提供块存储服务。应用方案有：SPDK+librbd&#x2F;librados<ul><li><a href="https://github.com/ceph/ceph/tree/acf835db0376b1b71152949fdfec36e68f4a8474/src/librbd">https://github.com/ceph/ceph/tree/acf835db0376b1b71152949fdfec36e68f4a8474/src/librbd</a></li><li><a href="https://github.com/spdk/spdk/tree/cff525d336fb2c4c087413d4c53474b9e61cbdbe/module/bdev/rbd">https://github.com/spdk/spdk/tree/cff525d336fb2c4c087413d4c53474b9e61cbdbe/module/bdev/rbd</a><br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B02.png"></li></ul></li></ul><p>RBD 的块设备由于元数据信息少而且访问不频繁，故 RBD 在 Ceph 集群中不需要单独的守护进程将元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的 Monitor 服务和 OSD 服务进行交互。</p><h2 id="RBD-模块相关IO流图"><a href="#RBD-模块相关IO流图" class="headerlink" title="RBD 模块相关IO流图"></a>RBD 模块相关IO流图</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B03.png"><br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B04.png"><br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B05.png"></p><p>客户端写数据osd过程：</p><ol><li>采用的是 librbd 的形式，使用 librbd 创建一个块设备，向这个块设备中写入数据</li><li>在客户端本地同过调用 librados 接口，然后经过 pool，rbd，object，pg 进行层层映射（CRUSH 算法）,在 PG 这一层中，可以知道数据保存在哪几个 OSD 上，这几个 OSD 分为主从的关系</li><li>客户端与 primary OSD 建立 SOCKET 通信，将要写入的数据传给 primary OSD，由 primary OSD 再将数据发送给其他 replica OSD 数据节点。</li></ol><h2 id="IO-时序图"><a href="#IO-时序图" class="headerlink" title="IO 时序图"></a>IO 时序图</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B06.png"><br>librbd提供了针对image的数据读写和管理操作两种访问接口，其中数据读写请求入io_work_queue，然后由线程池中的线程将io请求以object粒度切分并分别调用rados层的aio接口（IoCtxImpl）下发，当所有的object请求完成时，调用librbd io回调（librbd::io::AioCompletion）完成用户层的数据io。而对image的管理操作通常需要涉及单个或多个对象的多次访问以及对内部状态的多次更新，其第一次访问将从用户线程调用至rados层 aio 接口或更新状态后入 op_work_queue 队列进行异步调用，当 rados aio 层回调或 Context 完成时再根据实现逻辑调用新的 rados aio 或构造 Context 回调，如此反复，最后调用应用层的回调完成管理操作请求。<br>      此外为了支持多客户端共享访问 image，librbd 提供了构建于 rados watch&#x2F;notify 之上的通知、远程执行以及 exclusive lock 分布式锁机制。每个 librbd 客户端在打开 image 时（以非只读方式打开）都会 watch image 的 header 对象，从远程发往本地客户端的通知消息或者内部的 watch 错误消息会通过 RadosClient 的 Finisher 线程入 op_work_queue 队列进行异步处理。</p><h2 id="RBD读写流程"><a href="#RBD读写流程" class="headerlink" title="RBD读写流程"></a>RBD读写流程</h2><p>对于任何RBD客户端的读写都要经过以下步骤：</p><ol><li>集群句柄创建、读取配置<br> 集群句柄的创建即是librados:Rados的创建，初始化，读取配置<br> 创建：librados::Rados rados;<br> 初始化：librados::Rados::init(const char * const id)<br>     主要是初始化librados::RadosClient<br>     读取配置：<br>     librados::Rados::conf_read_file(const char * const path) const<br>     librados::Rados::conf_parse_argv(int argc, const char ** argv) const</li><li>集群连接<br> librados::Rados::connect()</li><li>IO上下文环境初始化（pool创建读写等）<br> librados::Rados::ioctx_create(const char *name, IoCtx &amp;io)<br> 主要是IoCtxImpl即librados::IoCtx</li><li>rbd创建<br> librbd::RBD rbd;<br> RBD::create2(IoCtx&amp; io_ctx, const char *name, uint64_t size,uint64_t features, int *order)</li><li>rbd的读写<br> librbd::Image image;<br> RBD::open(IoCtx&amp; io_ctx, Image&amp; image, const char *name)<br> Image::write(uint64_t ofs, size_t len, bufferlist&amp; bl)<br> Image::read(uint64_t ofs, size_t len, bufferlist&amp; bl)</li><li>IO上下文环境关闭<br> librbd::Image::close()<br> librados::IoCtx::close()</li><li>集群句柄关闭<br> librados::Rados::shutdown()</li></ol><h2 id="RBD源码介绍"><a href="#RBD源码介绍" class="headerlink" title="RBD源码介绍"></a>RBD源码介绍</h2><p>librbd以及librados都是属于ceph 的客户端，其提供ceph的接口向上提供块存储服务。<br>librados提供客户端访问Ceph集群的原生态统一接口。其它接口或者命令行工具都基于该动态库实现。在librados中实现了Crush算法和网络通信等公共功能，数据请求操作在librados计算完成后可以直接与对应的OSD交互进行数据传输。<br>librbd 是Ceph提供的在librados上封装的块存储接口的抽象。</p><p>librados主要的类是Rados和IoCtx<br>librados::Rados负责初始化集群、读取配置、连接集群<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B07.jpg"></p><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B08.jpg"></p><p>librados::IoCtx负责创建IO上下文环境<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B09.jpg"></p><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B010.jpg"></p><p>librados::bufferlist负责读写缓存<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B011.jpg"></p><p>librbd最主要的两个类是：RBD和Image<br>librbd::rbd主要负责 Image 的创建、删除、重命名、克隆映像等操作，包括对存储池的元数据的管理，针对部分操作提供异步接口<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B012.jpg"></p><p>librbd::image负责image的读写(read&#x2F;write)，以及快照相关的操作等等。同时提供了相关异步操作的接口。<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B013.png"></p><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B014.png"></p><h2 id="rbd-Image的创建"><a href="#rbd-Image的创建" class="headerlink" title="rbd Image的创建"></a>rbd Image的创建</h2><p>rbd卷的创建接口：<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B015.jpg"></p><p>函数输入参数：</p><ul><li>io_ctx: 针对pool的上下文环境，对pool的操作都要首先建立一个相应的上下文环境</li><li>*name：rbd卷名字</li><li>size：rbd卷大小</li><li>features: rbd卷的特性</li><li>order: rbd卷的分块大小<br>其具体实现在internal.cc中：<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B016.jpg"></li></ul><p>继续往下调用：<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B017.jpg"><br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B018.jpg"></p><p>根据format格式调用不同的创建接口，现在主流采用新的format2，所用调用新的接口：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">create_v2</span><span class="params">(IoCtx&amp; io_ctx, <span class="type">const</span> <span class="type">char</span> *imgname, <span class="type">uint64_t</span> bid, <span class="type">uint64_t</span> size,<span class="type">int</span> order, <span class="type">uint64_t</span> features, <span class="type">uint64_t</span> stripe_unit,<span class="type">uint64_t</span> stripe_count, <span class="type">uint8_t</span> journal_order,<span class="type">uint8_t</span> journal_splay_width, <span class="type">const</span> std::string &amp;journal_pool,<span class="type">const</span> std::string &amp;non_primary_global_image_id,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="type">const</span> std::string &amp;primary_mirror_uuid,<span class="type">bool</span> negotiate_features)</span></span></span><br></pre></td></tr></table></figure><p>这个接口会做如下工作：<br>创建rbd_id.{volume_name}的object：<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B019.png"></p><p>然后想这个object写入block_name_prefix中的id号：<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B020.png"></p><p>然后向rbd_directory写入卷名和id的一一映射。<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B021.jpg"></p><p>创建名为rbd_header.id的object，并向这个object写入size,order,features,RBD_DATA_PREFIX等信息。<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B022.png"></p><p>如果有条带化，则会设置条带化信息：<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B023.png"></p><p>创建名为rbd_object_map.{id}的对象：<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B024.jpg"></p><h2 id="rbd-Image的打开"><a href="#rbd-Image的打开" class="headerlink" title="rbd Image的打开"></a>rbd Image的打开</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B025.jpg"><br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B026.jpg"></p><p>其实就是生成一个ImageCtx实例，调用其open接口。</p><h2 id="rbd-Image的写"><a href="#rbd-Image的写" class="headerlink" title="rbd Image的写"></a>rbd Image的写</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B027.jpg"></p><h2 id="rbd-Image的读"><a href="#rbd-Image的读" class="headerlink" title="rbd Image的读"></a>rbd Image的读</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B028.jpg"></p><h2 id="rbd-Image的快照"><a href="#rbd-Image的快照" class="headerlink" title="rbd Image的快照"></a>rbd Image的快照</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B029.jpg"></p><h2 id="rbd-Image的克隆"><a href="#rbd-Image的克隆" class="headerlink" title="rbd Image的克隆"></a>rbd Image的克隆</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B030.jpg"></p><h2 id="rbd-Image的删除"><a href="#rbd-Image的删除" class="headerlink" title="rbd Image的删除"></a>rbd Image的删除</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B031.jpg"></p><h2 id="rbd的读写"><a href="#rbd的读写" class="headerlink" title="rbd的读写"></a>rbd的读写</h2><p><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B032.png"></p><p>要使用librbd, 需要先安装下面两个包。可以通过yum安装, 也可以通过下载ceph源码编译后, 通过make install进行安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">  yum list | grep librbd</span></span><br><span class="line">librbd1.x86_64                  1:0.80.7-3.el7                    base</span><br><span class="line">librbd1-devel.x86_64            1:0.80.7-3.el7                    base</span><br></pre></td></tr></table></figure><p>至于如何使用librbd来编程, 请参考下面的代码, 这是使用librbd的一般流程。<br>编译时记得加上链接参数: g++ librbdtest.cpp -lrados -lrbd。<br>更多函数的使用请参考 librbd.hpp。 另外 这里 有一些不错的示例。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;rbd/librbd.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;rados/librados.hpp&gt;</span></span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Ceph-RBD介绍&quot;&gt;&lt;a href=&quot;#Ceph-RBD介绍&quot; class=&quot;headerlink&quot; title=&quot;Ceph RBD介绍&quot;&gt;&lt;/a&gt;Ceph RBD介绍&lt;/h2&gt;&lt;p&gt;随着云计算的发展，Ceph已经成为目前最为流行的分布式存储系统，俨然存储界的</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph_rbd介绍与使用</title>
    <link href="https://watsonlu6.github.io/Ceph-rbd%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>https://watsonlu6.github.io/Ceph-rbd%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/</id>
    <published>2021-08-10T14:26:55.000Z</published>
    <updated>2024-09-08T11:25:39.559Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ceph-RBD介绍"><a href="#Ceph-RBD介绍" class="headerlink" title="Ceph RBD介绍"></a>Ceph RBD介绍</h2><p>随着云计算的发展，Ceph已经成为目前最为流行的分布式存储系统，俨然存储界的Linux操作系统。Ceph集块存储、文件存储和对象存储于一身，适用场景广泛，用户众多。RBD是 Ceph 分布式存储系统中提供的块存储服务，Ceph的块存储通过一个客户端模块实现，这个客户端可以直接从数据守护进程读写数据（不需要经过一个网关）。根据客户端整合生态系统的差异，使用Ceph的块设备有两种实现方式：librbd (用户态)和krbd (内核态)。RBD：RADOS Block Devices. Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster.<br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B01.png"></p><p>使用Ceph的块设备有两种路径（内核态与用户态）：(rbd map就是内核使用ceph块设备，调用librbd&#x2F;librados API访问ceph块设备是用户态)</p><ul><li>通过Kernel Module(内核态RBD)：即创建了RBD设备后，把它映射到内核中（使用rbd map命令映射到操作系统上），成为一个虚拟的块设备，这时这个块设备同其他通用块设备一样，设备文件一般为&#x2F;dev&#x2F;rbd0，后续直接使用这个块设备文件就可以了，可以把&#x2F;dev&#x2F;rbd0格式化后挂载到某目录，也可以直接作为裸设备进行使用。krbd是一个内核模块。其在内核中以一个块设备的方式加以实现。这整个Ceph客户端都是以内核模块的方式实现（没有与之相关的用户态进程或者守护进程）。krbd在内核的源码目录源文件:drivers&#x2F;block&#x2F;rbd.c、drivers&#x2F;block&#x2F;rbd_types.h、net&#x2F;ceph&#x2F;、include&#x2F;linux&#x2F;ceph<ul><li><a href="https://www.likecs.com/show-203739919.html">https://www.likecs.com/show-203739919.html</a></li><li><a href="https://github.com/torvalds/linux/blob/cfb92440ee71adcc2105b0890bb01ac3cddb8507/drivers/block/rbd.c">https://github.com/torvalds/linux/blob/cfb92440ee71adcc2105b0890bb01ac3cddb8507/drivers/block/rbd.c</a></li><li><a href="https://github.com/torvalds/linux/tree/85c7000fda0029ec16569b1eec8fd3a8d026be73/include/linux/ceph">https://github.com/torvalds/linux/tree/85c7000fda0029ec16569b1eec8fd3a8d026be73/include/linux/ceph</a></li></ul></li><li>通过librbd(用户态RBD)：即创建了RBD设备后，使用librbd&#x2F;librados库访问和管理块设备。这种方式直接调用librbd提供的接口，实现对RBD设备的访问和管理，不会在客户端产生设备文件，这种方式主要是为虚拟机提供块存储设备，在虚拟机场景中，一般会用QEMU&#x2F;KVM中的RBD驱动部署Ceph块设备，宿主机通过librbd向客户端提供块存储服务。应用方案有：SPDK+librbd&#x2F;librados<ul><li><a href="https://github.com/ceph/ceph/tree/acf835db0376b1b71152949fdfec36e68f4a8474/src/librbd">https://github.com/ceph/ceph/tree/acf835db0376b1b71152949fdfec36e68f4a8474/src/librbd</a></li><li><a href="https://github.com/spdk/spdk/tree/cff525d336fb2c4c087413d4c53474b9e61cbdbe/module/bdev/rbd">https://github.com/spdk/spdk/tree/cff525d336fb2c4c087413d4c53474b9e61cbdbe/module/bdev/rbd</a><br><img src="/images/rbd/rbd%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B02.png"></li></ul></li></ul><p>RBD 的块设备由于元数据信息少而且访问不频繁，故 RBD 在 Ceph 集群中不需要单独的守护进程将元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的 Monitor 服务和 OSD 服务进行交互。</p><h2 id="rbd基本块设备命令"><a href="#rbd基本块设备命令" class="headerlink" title="rbd基本块设备命令"></a>rbd基本块设备命令</h2><p><code>rbd</code>命令使你能够创建、列出、检查和删除块设备镜像。还可以使用它来克隆镜像、创建快照、将镜像回滚到快照、查看快照等。</p><p><strong>1. 创建块设备池</strong><br>使用 <code>ceph</code> 工具创建一个池，使用 <code>rbd</code> 工具初始化池以供 RBD 使用：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd pool init &lt;pool-name&gt;</span><br></pre></td></tr></table></figure><p><strong>2. 创建块设备用户</strong><br>除非另有说明，<code>rbd</code> 命令使用 Ceph 用户 ID <code>admin</code> 来访问 Ceph 集群。<code>admin</code> Ceph 用户 ID 允许对集群进行完全的管理访问。建议使用权限比 <code>admin</code> Ceph 用户 ID 少的 Ceph 用户 ID 访问 Ceph 集群。<br>要创建 Ceph 用户，请使用 <code>ceph auth get-or-create</code> 命令指定 Ceph 用户 ID 名称、监视器权限（capabilities）和 OSD 权限（capabilities）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.&#123;ID&#125; mon &#x27;profile rbd&#x27; osd &#x27;profile &#123;profile name&#125; [pool=&#123;pool-name&#125;][, profile ...]&#x27; mgr &#x27;profile rbd [pool=&#123;pool-name&#125;]&#x27;</span><br></pre></td></tr></table></figure><p>例如，要创建一个名为 <code>qemu</code> 的 Ceph 用户 ID，该用户对池 <code>vms</code> 具有读写权限，对池 <code>images</code> 具有只读权限，运行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.qemu mon &#x27;profile rbd&#x27; osd &#x27;profile rbd pool=vms, profile rbd-read-only pool=images&#x27; mgr &#x27;profile rbd pool=images&#x27;</span><br></pre></td></tr></table></figure><p><code>ceph auth get-or-create</code> 命令的输出是指定 Ceph 用户 ID 的密钥环，可以写入 <code>/etc/ceph/ceph.client.&#123;ID&#125;.keyring</code>。</p><p><strong>3. 创建块设备镜像</strong><br>在将块设备添加到节点之前，你必须在 Ceph 存储集群中创建一个镜像。要创建块设备镜像，请运行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd create --size &#123;megabytes&#125; &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p>如果在创建镜像时未指定池，则镜像将存储在默认池 <code>rbd</code> 中。例如，如果运行以下命令，将创建一个大小为 1GB、名称为 <code>foo</code> 的镜像，并将其存储在默认池 <code>rbd</code> 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd create --size 1024 foo</span><br></pre></td></tr></table></figure><p><strong>4. 列出块设备镜像</strong><br>要列出 <code>rbd</code> 池中的块设备，请运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">ls</span></span><br></pre></td></tr></table></figure><p><code>rbd</code> 是默认的池名称，<code>rbd ls</code> 列出默认池中的命令。要列出特定池中的块设备，请运行以下命令，但将 <code>&#123;poolname&#125;</code> 替换为池的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">ls</span> &#123;poolname&#125;</span><br></pre></td></tr></table></figure><p>要列出 <code>rbd</code> 池中的“推迟删除”块设备，请运行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash <span class="built_in">ls</span> &#123;poolname&#125;</span><br></pre></td></tr></table></figure><p><strong>5.检索镜像信息</strong><br>要从特定镜像中检索信息，请运行以下命令，但将 <code>&#123;image-name&#125;</code> 替换为镜像的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd info &#123;image-name&#125;</span><br></pre></td></tr></table></figure><p>要从池中的镜像中检索信息，请运行以下命令，但将 <code>&#123;image-name&#125;</code> 替换为镜像的名称，将 <code>&#123;pool-name&#125;</code> 替换为池的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd info &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p><strong>6.调整块设备镜像大小</strong><br>Ceph 块设备镜像是按需配置的。在开始保存数据之前，它们不会实际使用任何物理存储。但是，它们确实有一个最大容量，你可以使用 <code>--size</code> 选项来设置。如果你想增加（或减少）Ceph 块设备镜像的最大大小，请运行以下命令之一：</p><ul><li><p>增加块设备镜像的大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd resize --size 2048 foo</span><br></pre></td></tr></table></figure></li><li><p>减少块设备镜像的大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd resize --size 2048 foo --allow-shrink</span><br></pre></td></tr></table></figure></li></ul><p><strong>7. 删除块设备镜像</strong><br>要删除一个块设备，请运行以下命令，但将 <code>&#123;image-name&#125;</code> 替换为你想删除的镜像的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">rm</span> &#123;image-name&#125;</span><br></pre></td></tr></table></figure><p><strong>8. 从池中删除块设备</strong><br>要从池中删除块设备，请运行以下命令，但将 <code>&#123;image-name&#125;</code> 替换为要删除的镜像的名称，将 <code>&#123;pool-name&#125;</code> 替换为要从中删除镜像的池的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">rm</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p><strong>9. “推迟删除”块设备</strong><br>要推迟删除块设备（即将其移动到“垃圾桶”中并稍后删除），请运行以下命令，但将 <code>&#123;image-name&#125;</code> 替换为要移动到垃圾桶中的镜像的名称，将 <code>&#123;pool-name&#125;</code> 替换为池的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash <span class="built_in">mv</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p><strong>10. 从池中删除推迟的块设备</strong><br>要从池中删除推迟的块设备，请运行以下命令，但将 <code>&#123;image-id&#125;</code> 替换为要删除的镜像的 ID，将 <code>&#123;pool-name&#125;</code> 替换为要从中删除镜像的池的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash <span class="built_in">rm</span> &#123;pool-name&#125;/&#123;image-id&#125;</span><br></pre></td></tr></table></figure><p>即使镜像有快照或被克隆使用，你也可以将其移动到垃圾桶中。但是，在这些条件下，你不能从垃圾桶中删除它。你可以使用 <code>--expires-at</code> 设置推迟时间（默认为现在）。如果推迟时间尚未到达，除非使用 <code>--force</code>，否则你不能删除镜像。</p><p><strong>11.恢复块设备镜像</strong><br>要恢复 <code>rbd</code> 池中的推迟删除块设备，请运行以下命令，但将 <code>&#123;image-id&#125;</code> 替换为镜像的 ID：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash restore &#123;image-id&#125;</span><br></pre></td></tr></table></figure><p><strong>12.恢复特定池中的块设备镜像</strong><br>要恢复特定池中的推迟删除块设备，请运行以下命令，但将 <code>&#123;image-id&#125;</code> 替换为镜像的 ID，将 <code>&#123;pool-name&#125;</code> 替换为池的名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash restore &#123;pool-name&#125;/&#123;image-id&#125;</span><br></pre></td></tr></table></figure><p><strong>13.恢复镜像时重命名</strong><br>你还可以在恢复镜像时使用 <code>--image</code> 选项进行重命名。<br>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd trash restore swimmingpool/2bf4474b0dc51 --image new-name</span><br></pre></td></tr></table></figure><h2 id="rbd快照"><a href="#rbd快照" class="headerlink" title="rbd快照"></a>rbd快照</h2><p>快照是某个时间点上镜像的只读逻辑副本：一个检查点。Ceph 块设备的一个高级功能是你可以创建镜像的快照，以保留时间点状态历史。Ceph 还支持快照分层，这允许你快速而轻松地克隆镜像（例如，虚拟机镜像）。Ceph 块设备快照是通过 <code>rbd</code> 命令和多个高级接口（包括 QEMU、libvirt、OpenStack、OpenNebula 和 CloudStack）进行管理的。</p><p>由于 RBD 对镜像（卷）内的文件系统没有感知，快照仅在发生崩溃时保持一致，除非它们在挂载（附加）操作系统内进行协调。因此，我们建议你在创建快照之前暂停或停止 I&#x2F;O 操作。</p><p>如果卷包含文件系统，则在创建快照之前，文件系统应该处于内部一致的状态。没有进行写操作冻结的快照在重新挂载之前可能需要进行 fsck 检查。要冻结 I&#x2F;O 操作，可以使用 <code>fsfreeze</code> 命令。有关更多详细信息，请参见 <code>fsfreeze(8)</code> 手册页。</p><p>对于虚拟机，可以使用 <code>qemu-guest-agent</code> 自动冻结文件系统以创建快照。</p><p><strong>1.创建快照</strong><br>要创建快照，请使用 <code>rbd snap create</code> 命令，并指定池名称、镜像名称和快照名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap create &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125;</span><br></pre></td></tr></table></figure><p><strong>2.列出快照</strong><br>要列出镜像的快照，请使用 <code>rbd snap ls</code> 命令，并指定池名称和镜像名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap <span class="built_in">ls</span> &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p><strong>3.回滚快照</strong><br>要回滚到快照，请使用 <code>rbd snap rollback</code> 命令，并指定池名称、镜像名称和快照名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap rollback &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125;</span><br></pre></td></tr></table></figure><p>将镜像回滚到快照意味着用快照中的数据覆盖镜像的当前版本。执行回滚所需的时间随着镜像大小的增加而增加。从快照克隆比回滚镜像到快照要快。克隆快照是恢复到先前状态的首选方法。</p><p><strong>4.删除快照</strong><br>要删除快照，请使用 <code>rbd snap rm</code> 命令，并指定池名称、镜像名称和快照名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap <span class="built_in">rm</span> &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125;</span><br></pre></td></tr></table></figure><p>Ceph OSD 异步删除数据，因此删除快照不会立即释放底层 OSD 的容量。这个过程被称为“snaptrim”，在 <code>ceph status</code> 输出中也称为此。</p><p><strong>5.清除快照</strong><br>要删除所有快照，请使用 <code>rbd snap purge</code> 命令，并指定池名称和镜像名称：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap purge &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><h2 id="rbd分层"><a href="#rbd分层" class="headerlink" title="rbd分层"></a>rbd分层</h2><p>Ceph 支持创建多个基于写时复制（COW）的块设备快照克隆。快照分层使 Ceph 块设备客户端能够非常快速地创建镜像。例如，你可以创建一个写入 Linux 虚拟机的块设备镜像，快照该镜像，保护快照，然后创建任意数量的基于写时复制的克隆。快照是只读的，因此克隆快照简化了语义，使得创建克隆变得迅速。</p><p>术语“父” 和 “子” 指的是 Ceph 块设备快照（父）及从快照克隆的相应镜像（子）。这些术语在下面的命令行用法中很重要。</p><p>每个克隆的镜像（子）存储对其父镜像的引用，这使得克隆镜像能够打开父快照并读取它。</p><p>基于写时复制的快照克隆表现得完全像任何其他 Ceph 块设备镜像。你可以读取、写入、克隆和调整克隆镜像的大小。克隆镜像没有特殊限制。然而，基于写时复制的快照克隆依赖于快照，因此你必须在克隆之前保护快照。下图描述了这个过程。</p><p>Ceph 仅支持克隆 “RBD 格式 2” 镜像（即，没有指定 <code>--image-format 1</code> 创建的镜像）。Linux 内核客户端从 3.10 版本开始支持克隆镜像。</p><p>Ceph 块设备分层是一个简单的过程。必须有一个镜像。必须创建镜像的快照。必须保护快照。在执行这些步骤后，你可以开始克隆快照。</p><p>克隆的镜像有一个对父快照的引用，并包括池 ID、镜像 ID 和快照 ID。池 ID 的包含意味着你可以将快照从一个池克隆到另一个池中的镜像。</p><p><strong>镜像模板</strong>：块设备分层的常见用例是创建一个基础镜像和一个作为克隆模板的快照。例如：用户可以创建一个 Linux 发行版的镜像（例如，Ubuntu 22.04）并创建其快照。用户可以定期更新镜像并创建新的快照（通过运行诸如 <code>sudo apt-get update</code>、<code>sudo apt-get upgrade</code> 或 <code>sudo apt-get dist-upgrade</code> 以及 <code>rbd snap create</code> 的命令）。随着镜像的成熟，用户可以克隆任何一个快照。</p><p><strong>扩展模板</strong>：更高级的用例包括扩展模板镜像以提供比基础镜像更多的信息。例如，用户可以克隆一个镜像（例如，一个虚拟机模板），安装其他软件（例如，数据库、内容管理系统、分析系统），然后对扩展的镜像进行快照，该镜像本身可以像基础镜像一样进行更新。</p><p><strong>模板池</strong>：使用块设备分层的一种方式是创建一个池，其中包含（1）作为模板的基础镜像和（2）这些模板的快照。然后你可以扩展只读权限给用户，以便他们可以克隆快照，即使他们没有允许在池中写入或执行的权限。</p><p><strong>镜像迁移&#x2F;恢复</strong>：使用块设备分层的一种方式是将数据从一个池迁移或恢复到另一个池中。</p><p><strong>保护快照</strong><br>克隆访问父快照。如果用户不小心删除了父快照，所有克隆都会中断。为了防止数据丢失，你必须在克隆之前保护快照：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap protect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125;</span><br></pre></td></tr></table></figure><p><strong>克隆快照</strong><br>要克隆快照，请指定父池、父镜像和父快照，以及子池和镜像名称。你必须在克隆之前保护快照：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd <span class="built_in">clone</span> &#123;pool-name&#125;/&#123;parent-image-name&#125;@&#123;snap-name&#125; &#123;pool-name&#125;/&#123;child-image-name&#125;</span><br></pre></td></tr></table></figure><p>可以将快照从一个池克隆到另一个池中的镜像。例如，你可以在一个池中维护只读镜像和快照作为模板，而在另一个池中维护可写的克隆。</p><p><strong>取消保护快照</strong><br>在删除快照之前，你必须先取消保护它。此外，你不能删除有克隆引用的快照。在取消保护快照之前，你必须先扁平化或删除每个克隆：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd snap unprotect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125;</span><br></pre></td></tr></table></figure><p><strong>列出快照的子镜像</strong><br>要列出快照的子镜像，请使用 <code>rbd children</code> 命令，并指定池名称、镜像名称和快照名称：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd children &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125;</span><br></pre></td></tr></table></figure><p><strong>扁平化克隆镜像</strong><br>克隆镜像保留对父快照的引用。当你从克隆中移除对父快照的引用时，你实际上是通过将快照中存储的数据复制到克隆中来“扁平化”克隆。扁平化克隆所需的时间随着快照的大小增加。要删除快照，你必须首先扁平化子镜像（或删除它们）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd flatten &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p>由于扁平化镜像包含快照中存储的所有数据，因此扁平化镜像比分层克隆占用更多的存储空间。</p><h2 id="RBD独占锁"><a href="#RBD独占锁" class="headerlink" title="RBD独占锁"></a>RBD独占锁</h2><p>独占锁是一种机制，用于防止多个进程以未协调的方式访问同一个 Rados 块设备（RBD）。独占锁在虚拟化中使用广泛（它们防止虚拟机互相覆盖写操作），以及在 RBD 镜像中（它们是基于日志的镜像中的日志记录和基于快照的镜像中增量差异快速生成的先决条件）。</p><p>独占锁功能在新创建的镜像上默认启用。这个默认设置可以通过 <code>rbd_default_features</code> 配置选项或 <code>rbd create</code> 命令的 <code>--image-feature</code> 和 <code>--image-shared</code> 选项进行覆盖。</p><p>许多镜像功能，包括对象映射和快速差异，依赖于独占锁定。禁用独占锁功能将会对某些操作的性能产生负面影响。</p><p>为了维护多客户端访问，独占锁功能实现了客户端之间的自动协作锁过渡。它确保在任何给定时间只有一个客户端可以写入 RBD 镜像，从而保护像对象映射、日志或 PWL 缓存这样的内部镜像结构免受并发修改。</p><p>独占锁对用户基本上是透明的：</p><ul><li><p>每当一个客户端（一个 librbd 进程或在 krbd 客户端的情况下，一个客户端节点的内核）需要处理对启用了独占锁的 RBD 镜像的写操作时，它首先会对该镜像获取一个独占锁。如果锁已被其他客户端持有，则会请求该客户端释放锁。</p></li><li><p>每当持有 RBD 镜像独占锁的客户端收到释放锁的请求时，它会停止处理写操作，刷新其缓存，并释放锁。</p></li><li><p>每当持有 RBD 镜像独占锁的客户端正常终止时，锁也会被正常释放。</p></li></ul><p>独占锁的正常释放（无论是由于请求还是客户端终止）使得另一个后续客户端能够获取锁并开始处理写操作。</p><p>默认情况下，独占锁功能并不防止两个或更多并发运行的客户端轮流打开同一个 RBD 镜像并写入（无论是在同一节点上还是在不同节点上）。实际上，它们的写入只是被线性化，因为锁会以协作方式自动过渡来回。</p><p>要禁用客户端之间的自动锁过渡，可以在获取独占锁时指定 <code>RBD_LOCK_MODE_EXCLUSIVE</code> 标志。这通过 <code>rbd device map</code> 命令的 <code>--exclusive</code> 选项暴露。</p><p>独占锁功能与 RBD 顾问锁（<code>rbd lock add</code> 和 <code>rbd lock rm</code> 命令）不兼容。</p><p><strong>阻止列表</strong></p><p>有时，之前持有 RBD 镜像独占锁的客户端不会正常终止，而是突然终止。这可能是因为客户端进程收到了 KILL 或 ABRT 信号，或者客户端节点经历了硬重启或遭遇了电力故障。在这种情况下，锁从未正常释放。这意味着任何新客户端在启动并尝试写入镜像时，必须打破先前持有的独占锁。</p><p>然而，进程（或内核线程）可能会挂起或仅仅丧失与 Ceph 集群的网络连接。在这种情况下，打破锁可能是灾难性的：挂起的进程或连接问题可能会自行解决，原始进程可能会与中间启动的新进程竞争，从而以未协调和破坏性的方式访问 RBD 数据。</p><p>如果锁无法以标准的正常方式获取，超越的进程不仅打破锁，还会将之前的锁持有者列入阻止列表。这是新客户端进程与 Ceph 监视器之间的协商结果。</p><p>在接收到阻止列表请求后，监视器会指示相关的 OSD 不再服务于旧客户端进程的请求；</p><p>在完成相关的 OSD 映射更新后，新客户端可以打破先前持有的锁；</p><p>在新客户端获取到锁后，它可以开始向镜像写入数据。</p><p>阻止列表因此是一种存储级别的资源隔离形式。</p><p>为了使阻止列表功能正常工作，客户端必须具备 <code>osd blocklist</code> 能力。这个能力包含在 <code>rbd</code> 能力配置文件中，通常应在所有使用 RBD 的 Ceph 客户端身份上设置。</p><h2 id="RBD-镜像同步"><a href="#RBD-镜像同步" class="headerlink" title="RBD 镜像同步"></a>RBD 镜像同步</h2><p>RBD 镜像可以在两个 Ceph 集群之间异步镜像。这一功能有两种模式：</p><ul><li><strong>基于日志的同步</strong>：该模式使用 RBD 日志镜像功能来确保集群之间的时间点、崩溃一致性复制。每次写入 RBD 镜像时，首先将写入记录到相关日志中，然后再修改实际的镜像。远程集群将从这个相关日志中读取并重放更新到其本地镜像副本。由于每次写入 RBD 镜像会导致 Ceph 集群中发生两次写入，因此在使用 RBD 日志镜像功能时，写入延迟几乎会增加一倍。</li><li><strong>基于快照的同步</strong>：该模式使用定期计划或手动创建的 RBD 镜像镜像快照，在集群之间复制崩溃一致的 RBD 镜像。远程集群将确定两个镜像快照之间的任何数据或元数据更新，并将差异复制到其本地镜像副本。借助 RBD 快速差异图像功能，可以快速确定更新的数据块，而无需扫描整个 RBD 镜像。由于此模式不如日志模式那样精细，因此在故障转移场景中，必须在使用之前同步两个快照之间的完整差异。任何部分应用的差异集将在故障转移时被回滚。</li></ul><p><strong>注意：</strong></p><ul><li>基于日志的同步需要 Ceph Jewel 版本或更高版本；基于快照的同步需要 Ceph Octopus 版本或更高版本。</li><li>镜像同步在对等集群内按池进行配置，并且可以在池内的特定图像子集上进行配置。使用基于日志的同步时，您还可以镜像池内的所有图像。镜像同步使用 <code>rbd</code> 命令配置。<code>rbd-mirror</code> 守护进程负责从远程对等集群提取镜像更新，并将其应用于本地集群中的镜像。</li></ul><p>根据复制的需求，RBD 镜像同步可以配置为单向或双向复制：</p><ul><li><strong>单向复制</strong>：当数据仅从主集群镜像到辅助集群时，<code>rbd-mirror</code> 守护进程仅在辅助集群上运行。</li><li><strong>双向复制</strong>：当数据从一个集群的主镜像镜像到另一个集群的非主镜像（反之亦然）时，<code>rbd-mirror</code> 守护进程会在两个集群上运行。</li></ul><p>每个 <code>rbd-mirror</code> 守护进程必须能够同时连接到本地和远程 Ceph 集群（即所有监视器和 OSD 主机）。此外，网络必须在两个数据中心之间具有足够的带宽，以处理镜像工作负载。</p><p><strong>1.池配置</strong><br>以下演示了如何执行基本的管理任务，以使用 <code>rbd</code> 命令配置镜像同步。镜像同步是在池级别配置的。这些池配置步骤应在两个对等集群上执行。这些过程假设名为“site-a”和“site-b”的两个集群从单个主机访问，以便清晰。</p><p>以下示例中的集群名称对应于具有相同名称的 Ceph 配置文件（例如 <code>/etc/ceph/site-b.conf</code>）。有关如何配置多个集群的信息，请参见 <code>ceph-conf</code> 文档。请注意，<code>rbd-mirror</code> 不要求源集群和目标集群具有唯一的内部名称；两者都可以并且应称为 ceph。<code>rbd-mirror</code> 需要的本地和远程集群的配置文件可以任意命名，容器化守护进程是一种将它们保持在 <code>/etc/ceph</code> 之外以避免混淆的策略。</p><p><strong>2.启用镜像同步</strong><br>要启用池的镜像同步，请使用 <code>rbd</code> 发出 <code>mirror pool enable</code> 子命令，指定池名称、镜像模式和可选的友好站点名称来描述本地集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd mirror pool enable [--site-name &#123;local-site-name&#125;] &#123;pool-name&#125; &#123;mode&#125;</span><br></pre></td></tr></table></figure><p>镜像模式可以是 <code>image</code> 或 <code>pool</code>：</p><ul><li><strong>image</strong>：在镜像模式下配置时，必须显式启用每个镜像的同步。</li><li><strong>pool（默认）</strong>：在池模式下配置时，池中所有启用了日志功能的镜像都会被镜像。</li></ul><p>创建或导入新的引导令牌时也可以指定站点名称。<br>站点名称可以使用相同的 <code>mirror pool enable</code> 子命令进行更改，但请注意，本地站点名称和远程集群使用的相应站点名称通常必须匹配。</p><p><strong>3.禁用镜像同步</strong><br>要禁用池上的镜像同步，请指定 <code>mirror pool disable</code> 命令和池名称：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd mirror pool disable &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><p>当以这种方式禁用池上的镜像同步时，池内所有显式启用镜像同步的图像也将禁用镜像同步。<br>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd --cluster site-a mirror pool <span class="built_in">disable</span> image-pool</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd --cluster site-b mirror pool <span class="built_in">disable</span> image-pool</span></span><br></pre></td></tr></table></figure><p><strong>4.引导对等集群</strong><br>为了使 <code>rbd-mirror</code> 守护进程发现其对等集群，必须注册对等集群并创建一个用户帐户。可以使用 <code>rbd</code> 和 <code>mirror pool peer bootstrap create</code> 及 <code>mirror pool peer bootstrap import</code> 命令自动完成此过程。</p><p>要使用 <code>rbd</code> 手动创建新的引导令牌，请发出 <code>mirror pool peer bootstrap create</code> 子命令，指定池名称和可选的友好站点名称来描述本地集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd mirror pool peer bootstrap create [--site-name &#123;local-site-name&#125;] &#123;pool-name&#125;</span><br></pre></td></tr></table></figure><p><code>mirror pool peer bootstrap create</code> 的输出将是一个令牌，应该提供给 <code>mirror pool peer bootstrap import</code> 命令。例如，在 site-a 上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd --cluster site-a mirror pool peer bootstrap create --site-name site-a image-pool</span></span><br><span class="line">eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</span><br></pre></td></tr></table></figure><p>要使用 <code>rbd</code> 手动导入由另一个集群创建的引导令牌，请指定 <code>mirror pool peer bootstrap import</code> 命令，池名称、创建的令牌的文件路径（或使用 <code>-</code> 从标准输入读取），以及可选的友好站点名称来描述本地集群和镜像方向（默认为双向镜像，但也可以设置为单向镜像）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd mirror pool peer bootstrap import [--site-name &#123;local-site-name&#125;] [--direction &#123;rx-only or rx-tx&#125;] &#123;pool-name&#125; &#123;token-path&#125;</span><br></pre></td></tr></table></figure><p>例如，在 site-b 上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF &gt; token</span></span></span><br><span class="line">eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="string">rbd --cluster site-b mirror pool peer bootstrap import --site-name site-b image-pool token</span></span></span><br></pre></td></tr></table></figure><p><strong>5.手动添加集群对等体</strong><br>如果需要或当前安装的 Ceph 版本不支持上述引导命令，可以手动指定集群对等体。<br>远程 <code>rbd-mirror</code> 守护进程需要访问本地集群以执行镜像同步。应为远程守护进程创建一个新的本地 Ceph 用户。</p><p>用户可以是具有最低权限的 <code>client.radosgw</code>，例如，但最好为 <code>rbd-mirror</code> 创建一个新用户。要创建该用户，请使用以下 <code>ceph auth</code> 命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth add client.rbd-mirror mon &#x27;allow r&#x27; osd &#x27;allow rwx&#x27; mgr &#x27;allow rw&#x27; mds &#x27;allow r&#x27;</span><br></pre></td></tr></table></figure><p>远程集群应有一个对等集群条目以允许从远程 Ceph 集群接收镜像更新。要创建本地 Ceph 集群的对等体条目，请使用 <code>ceph</code> 命令添加对等集群的配置信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config-key put client.radosgw.rgw_mirror.pool.&#123;pool-name&#125;.peers.&#123;remote-site-name&#125; &#123;remote-hostname&#125;:&#123;remote-port&#125; &#123;remote-client-key&#125;</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph config-key put client.radosgw.rgw_mirror.pool.image-pool.peers.site-b 192.168.1.1:6800 AQBzYvU5y5JYkFABx8T/8E6bErfHk8m1IoV9EA== </span><br></pre></td></tr></table></figure><ul><li><code>remote-site-name</code> 是要向其发送镜像更新的远程集群的站点名称（相当于远程集群上的站点名称）。</li><li><code>remote-hostname</code> 和 <code>remote-port</code> 是远程 Ceph 集群的主机名和端口。</li><li><code>remote-client-key</code> 是用来连接远程 Ceph 集群的密钥。</li></ul><p>一旦完成，<code>rbd-mirror</code> 守护进程将开始从对等集群同步镜像。</p><p><strong>6.启动镜像守护进程</strong><br>要启动 <code>rbd-mirror</code> 守护进程，请在每个 Ceph 集群的所有监视器节点上运行 <code>radosgw</code> 守护进程。通过发出 <code>systemctl</code> 命令来完成这一操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start ceph-radosgw</span><br></pre></td></tr></table></figure><p>要检查 <code>radosgw</code> 守护进程是否正在运行，请使用以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status ceph-radosgw</span><br></pre></td></tr></table></figure><p>完成这些步骤后，<code>rbd-mirror</code> 守护进程将在 Ceph 集群之间镜像 RBD 镜像。</p><h2 id="镜像实时迁移"><a href="#镜像实时迁移" class="headerlink" title="镜像实时迁移"></a>镜像实时迁移</h2><p>RBD 镜像可以在同一个 Ceph 集群内的不同池、镜像格式和&#x2F;或布局之间进行实时迁移；也可以从另一个 Ceph 集群的镜像进行迁移；或从外部数据源进行迁移。当迁移开始时，源镜像将被深度复制到目标镜像，同时保留所有快照历史，并尽可能保持数据的稀疏分配。</p><p>默认情况下，当在同一个 Ceph 集群内进行 RBD 镜像的实时迁移时，源镜像将被标记为只读，所有客户端将重定向 I&#x2F;O 到新的目标镜像。此外，这种模式还可以选择保留源镜像的父级链接以保留稀疏性，或者在迁移过程中将镜像扁平化，以消除对源镜像父级的依赖。</p><p>实时迁移过程还可以在仅导入模式下使用，其中源镜像保持不变，目标镜像可以链接到另一个 Ceph 集群中的镜像或外部数据源，如备份文件、HTTP(s) 文件、S3 对象或 NBD 导出。</p><p>实时迁移复制过程可以在新的目标镜像被使用时安全地在后台运行。在不使用仅导入模式的情况下，目前要求在准备迁移之前暂时停止使用源镜像。这有助于确保使用镜像的客户端被更新为指向新的目标镜像。</p><p>实时迁移需要 Ceph Nautilus 版本或更高版本。对外部数据源的支持需要 Ceph Pacific 版本或更高版本。krbd 内核模块目前不支持实时迁移。</p><p><strong>实时迁移过程包含三个步骤：</strong></p><ul><li><p><strong>准备迁移</strong>：初始步骤创建新的目标镜像并将其链接到源镜像。如果没有配置为仅导入模式，源镜像也会被链接到目标镜像，并标记为只读。类似于分层镜像，尝试读取目标镜像内的未初始化数据区段将内部重定向读取到源镜像，写入未初始化区段将内部深度复制重叠的源镜像块到目标镜像。</p></li><li><p><strong>执行迁移</strong>：这是一个后台操作，将源镜像中的所有初始化块深度复制到目标镜像。这一步可以在客户端正在积极使用新目标镜像时运行。</p></li><li><p><strong>完成迁移</strong>：一旦后台迁移过程完成，可以提交或中止迁移。提交迁移将移除源镜像和目标镜像之间的交叉链接，并在未配置为仅导入模式时移除源镜像。中止迁移将移除交叉链接，并移除目标镜像。</p></li></ul><p><strong>1.准备迁移</strong><br>在同一个 Ceph 集群内的镜像进行实时迁移的默认过程是通过运行 <code>rbd migration prepare</code> 命令来启动，提供源镜像和目标镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd migration prepare migration_source [migration_target]</span></span><br></pre></td></tr></table></figure><p><code>rbd migration prepare</code> 命令接受与 <code>rbd create</code> 命令相同的布局选项，这允许更改不可变的镜像磁盘布局。如果目标镜像是仅用于更改磁盘布局而保持原始镜像名称，则可以跳过 <code>migration_target</code>。</p><p>在准备实时迁移之前，必须停止所有使用源镜像的客户端。如果在准备步骤中发现任何客户端以读写模式打开镜像，则准备步骤将失败。一旦准备步骤完成，客户端可以使用新的目标镜像名称重新启动。尝试使用源镜像名称重新启动客户端将导致失败。</p><p><code>rbd status</code> 命令将显示实时迁移的当前状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd status migration_target</span></span><br><span class="line">Watchers: none</span><br><span class="line">Migration:</span><br><span class="line">            source: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: prepared</span><br></pre></td></tr></table></figure><p>请注意，为避免在迁移过程中误用，源镜像将被移动到 RBD 垃圾箱中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd info migration_source</span></span><br><span class="line">rbd: error opening image migration_source: (2) No such file or directory</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd trash <span class="built_in">ls</span> --all</span></span><br><span class="line">5e2cba2f62e migration_source</span><br></pre></td></tr></table></figure><p><strong>2.准备仅导入迁移</strong><br>仅导入的实时迁移过程是通过运行相同的 <code>rbd migration prepare</code> 命令来启动，但添加 <code>--import-only</code> 可选项，并提供 JSON 编码的源规格来描述如何访问源镜像数据。这个源规格可以通过 <code>--source-spec</code> 可选项直接传递，或通过 <code>--source-spec-path</code> 可选项通过文件或标准输入传递：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd migration prepare --import-only --source-spec <span class="string">&quot;&lt;JSON&gt;&quot;</span> migration_target</span></span><br></pre></td></tr></table></figure><p><code>rbd migration prepare</code> 命令接受与 <code>rbd create</code> 命令相同的布局选项。</p><p><code>rbd status</code> 命令将显示实时迁移的当前状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd status migration_target</span></span><br><span class="line">Watchers: none</span><br><span class="line">Migration:</span><br><span class="line">        source: &#123;&quot;stream&quot;:&#123;&quot;file_path&quot;:&quot;/mnt/image.raw&quot;,&quot;type&quot;:&quot;file&quot;&#125;,&quot;type&quot;:&quot;raw&quot;&#125;</span><br><span class="line">        destination: rbd/migration_target (ac69113dc1d7)</span><br><span class="line">        state: prepared</span><br></pre></td></tr></table></figure><p>源规格 JSON 的一般格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;format-type&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    &lt;format unique parameters&gt;</span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;stream-type&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        &lt;stream unique parameters&gt;</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>当前支持的格式包括：native、qcow 和 raw。当前支持的流类型包括：file、http、s3 和 nbd。</p><p><strong>格式</strong></p><ul><li><strong>native 格式</strong> 可以用来描述 Ceph 集群内的本地 RBD 镜像作为源镜像。其源规格 JSON 编码如下：</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;native&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">[</span><span class="attr">&quot;cluster_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;cluster-name&gt;&quot;</span><span class="punctuation">,</span><span class="punctuation">]</span> (如果镜像在其他集群中，指定，要求 `&lt;cluster-name&gt;.conf` 文件)</span><br><span class="line">    <span class="punctuation">[</span><span class="attr">&quot;client_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;client-name&gt;&quot;</span><span class="punctuation">,</span><span class="punctuation">]</span> (用于连接到其他集群，默认为 `client.admin`)</span><br><span class="line">    <span class="attr">&quot;pool_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;pool-name&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">[</span><span class="attr">&quot;pool_id&quot;</span><span class="punctuation">:</span> &lt;pool-id&gt;<span class="punctuation">,</span><span class="punctuation">]</span> (可选，作为 <span class="string">&quot;pool_name&quot;</span> 的替代)</span><br><span class="line">    <span class="punctuation">[</span><span class="attr">&quot;pool_namespace&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;pool-namespace&quot;</span><span class="punctuation">,</span><span class="punctuation">]</span> (可选)</span><br><span class="line">    <span class="attr">&quot;image_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;image-name&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">[</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;image-id&gt;&quot;</span><span class="punctuation">,</span><span class="punctuation">]</span> (如果镜像在垃圾箱中，指定)</span><br><span class="line">    <span class="attr">&quot;snap_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;snap-name&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">[</span><span class="attr">&quot;snap_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;snap-id&gt;&quot;</span><span class="punctuation">,</span><span class="punctuation">]</span> (可选，作为 <span class="string">&quot;snap_name&quot;</span> 的替代)</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>注意，native 格式不包括 stream 对象，因为它利用了 Ceph 的本地操作。例如，要从镜像 <code>rbd/ns1/image1@snap1</code> 导入，源规格可以编码为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;native&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;pool_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;rbd&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;pool_namespace&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ns1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;image_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;image1&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;snap_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;snap1&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>qcow 格式</strong> 可用于描述 QCOW（QEMU copy-on-write）块设备。目前支持 QCOW（v1）和 QCOW2 格式，但不支持高级功能，如压缩、加密、备份文件和外部数据文件。未来版本可能会添加对这些缺失功能的支持。qcow 格式数据可以链接到任何支持的流源。例如，其基本源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;qcow&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        &lt;stream unique parameters&gt;</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>raw 格式</strong> 可用于描述厚配置的原始块设备导出（即 <code>rbd export --export-format 1 &lt;snap-spec&gt;</code>）。raw 格式数据可以链接到任何支持的流源。例如，其基本源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        &lt;stream unique parameters for HEAD<span class="punctuation">,</span> non-snapshot revision&gt;</span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;snapshots&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;snapshot-name&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                &lt;stream unique parameters for snapshot&gt;</span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">]</span> (可选，按从旧到新的顺序排列的快照)</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>快照数组的包含是可选的，目前仅支持厚配置的原始快照导出。未来版本将添加更多格式，如 RBD export-format v2 和 RBD export-diff 快照。</p></li><li><p><strong>file 流</strong> 可用于从本地可访问的 POSIX 文件源导入。其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    &lt;format unique parameters&gt;</span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;file&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;file_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;file-path&gt;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>例如，要从位于 <code>/mnt/image.raw</code> 的文件导入 raw 格式镜像，其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;file&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;file_path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/mnt/image.raw&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>http 流</strong> 可用于从远程 HTTP 或 HTTPS 网络服务器导入。其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    &lt;format unique parameters&gt;</span><br><span class="line">    <span class="string">&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;url-path&gt;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>例如，要从位于 <code>http://download.ceph.com/image.raw</code> 的文件导入 raw 格式镜像，其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://download.ceph.com/image.raw&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>s3 流</strong> 可用于从远程 S3 存储桶导入。其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    &lt;format unique parameters&gt;</span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;s3&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;url-path&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;access_key&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;access-key&gt;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;secret_key&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;secret-key&gt;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>例如，要从位于 <code>http://s3.ceph.com/bucket/image.raw</code> 的文件导入 raw 格式镜像，其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;s3&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://s3.ceph.com/bucket/image.raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;access_key&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NX5QOQKC6BH2IDN8HC7A&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;secret_key&quot;</span><span class="punctuation">:</span> <span class="string">&quot;LnEsqNNqZIpkzauboDcLXLcYaWwLQ3Kop0zAnKIn&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li></ul><p><code>access_key</code> 和 <code>secret_key</code> 参数支持将密钥存储在 MON 配置密钥存储中，可以通过在密钥值前加上 <code>config://</code> 前缀和路径来实现。可以通过 <code>ceph config-key set &lt;key-path&gt; &lt;value&gt;</code> 来存储值（例如 <code>ceph config-key set rbd/s3/access_key NX5QOQKC6BH2IDN8HC7A</code>）。</p><p><strong>nbd 流</strong> 可用于从远程 NBD 导出导入。其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    &lt;format unique parameters&gt;</span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nbd&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;uri&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&lt;nbd-uri&gt;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>例如，要从位于 <code>nbd://nbd.ceph.com</code> 的 NBD 导出中导入 raw 格式镜像，导出名称为 <code>image.raw</code>，其源规格 JSON 编码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;stream&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nbd&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;uri&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nbd://nbd.ceph.com/image.raw&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><code>nbd-uri</code> 参数应遵循 NBD URI 规范。默认 NBD 端口是 10809。</p><p><strong>3.执行迁移</strong><br>准备好实时迁移后，必须将源镜像的块复制到目标镜像。这是通过运行 <code>rbd migration execute</code> 命令来完成的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd migration execute migration_target</span></span><br><span class="line">Image migration: 100% complete...done.</span><br></pre></td></tr></table></figure><p><code>rbd status</code> 命令还会提供迁移块深度复制过程的进度反馈：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd status migration_target</span></span><br><span class="line">Watchers:</span><br><span class="line">    watcher=1.2.3.4:0/3695551461 client.123 cookie=123</span><br><span class="line">Migration:</span><br><span class="line">            source: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: executing (32% complete)</span><br></pre></td></tr></table></figure><p><strong>4.提交迁移</strong><br>一旦实时迁移完成了从源镜像到目标镜像的所有数据块的深度复制，可以提交迁移：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd status migration_target</span></span><br><span class="line">Watchers: none</span><br><span class="line">Migration:</span><br><span class="line">            source: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: executed</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd migration commit migration_target</span></span><br><span class="line">Commit image migration: 100% complete...done.</span><br></pre></td></tr></table></figure><p>如果 <code>migration_source</code> 镜像是一个或多个克隆的父镜像，则需要在确保所有后代克隆镜像未使用的情况下指定 <code>--force</code> 选项。</p><p>提交实时迁移将移除源镜像和目标镜像之间的交叉链接，并移除源镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd trash list --all</span></span><br></pre></td></tr></table></figure><p><strong>5.中止迁移</strong><br>如果希望撤销准备或执行步骤，可以运行 <code>rbd migration abort</code> 命令以撤销迁移过程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd migration abort migration_target</span></span><br><span class="line">Abort image migration: 100% complete...done.</span><br></pre></td></tr></table></figure><p>中止迁移将导致目标镜像被删除，并恢复对原始源镜像的访问：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd <span class="built_in">ls</span></span></span><br><span class="line">migration_source</span><br></pre></td></tr></table></figure><h2 id="RBD-持久只读缓存"><a href="#RBD-持久只读缓存" class="headerlink" title="RBD 持久只读缓存"></a>RBD 持久只读缓存</h2><h4 id="共享只读父镜像缓存"><a href="#共享只读父镜像缓存" class="headerlink" title="共享只读父镜像缓存"></a>共享只读父镜像缓存</h4><p>克隆的 RBD 镜像通常只修改父镜像的一小部分。例如，在 VDI 使用场景中，虚拟机是从相同的基础镜像克隆的，最初的区别仅在于主机名和 IP 地址。在启动过程中，所有这些虚拟机读取的是相同父镜像数据的不同部分。如果我们有一个本地的父镜像缓存，这将加速缓存主机上的读取操作，同时减少客户端到集群的网络流量。RBD 缓存必须在 ceph.conf 中显式启用。ceph-immutable-object-cache 守护进程负责在本地磁盘上缓存父内容，未来对该数据的读取将从本地缓存中提供。<br>RBD 共享只读父镜像缓存需要 Ceph Nautilus 版本或更高版本。</p><p><strong>启用 RBD 共享只读父镜像缓存</strong><br>要启用 RBD 共享只读父镜像缓存，需要在 ceph.conf 文件的 [client] 部分中添加以下 Ceph 设置：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd parent cache <span class="attr">enabled</span> = <span class="literal">true</span></span><br><span class="line">rbd <span class="attr">plugins</span> = parent_cache</span><br></pre></td></tr></table></figure><h4 id="不可变对象缓存守护进程"><a href="#不可变对象缓存守护进程" class="headerlink" title="不可变对象缓存守护进程"></a>不可变对象缓存守护进程</h4><p>ceph-immutable-object-cache 守护进程负责在其本地缓存目录中缓存父镜像内容。建议使用 SSD 作为底层存储，因为这样可以提供更好的性能。<br>守护进程的主要组件包括：</p><ul><li>基于域套接字的进程间通信（IPC）：守护进程在启动时监听本地域套接字，并等待来自 librbd 客户端的连接。</li><li>基于 LRU 的提升&#x2F;降级策略：守护进程维护每个缓存文件的内存中缓存命中统计。如果容量达到配置的阈值，则降级冷缓存。</li><li>基于文件的缓存存储：守护进程维护一个简单的文件缓存存储。在提升时，RADOS 对象从 RADOS 集群中获取并存储在本地缓存目录中。</li></ul><p>当每个克隆的 RBD 镜像被打开时，librbd 尝试通过其 Unix 域套接字连接到缓存守护进程。在 librbd 成功连接后，它在每次后续读取时与守护进程协调。在未缓存读取的情况下，守护进程将 RADOS 对象提升到本地缓存目录中，下一个读取操作将从缓存中服务。守护进程维护简单的 LRU 统计信息，用于在需要时逐出冷缓存文件（例如，当缓存达到容量并且受到压力时）。</p><p>以下是一些重要的缓存配置设置：</p><ul><li><p><strong>immutable_object_cache_sock</strong><br>描述：用于 librbd 客户端和 ceph-immutable-object-cache 守护进程之间通信的域套接字路径。<br>类型：字符串<br>必需：否<br>默认值：<code>/var/run/ceph/immutable_object_cache_sock</code></p></li><li><p><strong>immutable_object_cache_path</strong><br>描述：不可变对象缓存数据目录。<br>类型：字符串<br>必需：否<br>默认值：<code>/tmp/ceph_immutable_object_cache</code></p></li><li><p><strong>immutable_object_cache_max_size</strong><br>描述：不可变缓存的最大大小。<br>类型：大小<br>必需：否<br>默认值：<code>1G</code></p></li><li><p><strong>immutable_object_cache_watermark</strong><br>描述：缓存的高水位线。值在 (0, 1) 之间。如果缓存大小达到此阈值，守护进程将开始根据 LRU 统计信息删除冷缓存。<br>类型：浮点数<br>必需：否<br>默认值：<code>0.9</code></p></li></ul><p>ceph-immutable-object-cache 守护进程可以在可选的 ceph-immutable-object-cache 发行包中找到。<br>ceph-immutable-object-cache 守护进程需要能够连接到 RADOS 集群。</p><p><strong>运行不可变对象缓存守护进程</strong><br>ceph-immutable-object-cache 守护进程应使用唯一的 Ceph 用户 ID。要创建 Ceph 用户，可以使用 ceph 指定 <code>auth get-or-create</code> 命令、用户名、监视器权限和 OSD 权限：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.ceph-immutable-object-cache.&#123;unique id&#125; mon &#x27;allow r&#x27; osd &#x27;profile rbd-read-only&#x27;</span><br></pre></td></tr></table></figure><p>ceph-immutable-object-cache 守护进程可以通过 systemd 管理，将用户 ID 作为守护进程实例指定：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable ceph-immutable-object-cache@ceph-immutable-object-cache.&#123;unique id&#125;</span><br></pre></td></tr></table></figure><p>ceph-immutable-object-cache 也可以通过 ceph-immutable-object-cache 命令在前台运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-immutable-object-cache -f --log-file=&#123;log_path&#125;</span><br></pre></td></tr></table></figure><p><strong>QOS 设置</strong><br>不可变对象缓存支持限流，由以下设置控制：</p><ul><li><p><strong>immutable_object_cache_qos_schedule_tick_min</strong><br>描述：不可变对象缓存的最小调度间隔。<br>类型：毫秒<br>必需：否<br>默认值：<code>50</code></p></li><li><p><strong>immutable_object_cache_qos_iops_limit</strong><br>描述：每秒所需的不可变对象缓存 IO 操作限制。<br>类型：无符号整数<br>必需：否<br>默认值：<code>0</code></p></li><li><p><strong>immutable_object_cache_qos_iops_burst</strong><br>描述：不可变对象缓存 IO 操作的突发限制。<br>类型：无符号整数<br>必需：否<br>默认值：<code>0</code></p></li><li><p><strong>immutable_object_cache_qos_iops_burst_seconds</strong><br>描述：不可变对象缓存 IO 操作的突发持续时间（秒）。<br>类型：秒<br>必需：否<br>默认值：<code>1</code></p></li><li><p><strong>immutable_object_cache_qos_bps_limit</strong><br>描述：每秒所需的不可变对象缓存 IO 字节限制。<br>类型：无符号整数<br>必需：否<br>默认值：<code>0</code></p></li><li><p><strong>immutable_object_cache_qos_bps_burst</strong><br>描述：不可变对象缓存 IO 字节的突发限制。<br>类型：无符号整数<br>必需：否<br>默认值：<code>0</code></p></li><li><p><strong>immutable_object_cache_qos_bps_burst_seconds</strong><br>描述：不可变对象缓存 IO 字节的突发持续时间（秒）。<br>类型：秒<br>必需：否<br>默认值：<code>1</code></p></li></ul><h2 id="RBD-持久写日志缓存"><a href="#RBD-持久写日志缓存" class="headerlink" title="RBD 持久写日志缓存"></a>RBD 持久写日志缓存</h2><p>持久写日志缓存（PWL）为基于 librbd 的 RBD 客户端提供了一个持久且容错的写回缓存。</p><p>该缓存采用日志顺序写回设计，内部维护检查点，以确保写入被刷新回集群时始终保持崩溃一致性。即使客户端缓存完全丢失，磁盘镜像仍然是一致的，但数据可能会显得过时。</p><p>此缓存可以使用 PMEM 或 SSD 作为缓存设备。对于 PMEM，缓存模式称为副本写日志（rwl）。目前仅支持本地缓存，副本功能尚在开发中。对于 SSD，缓存模式称为 ssd。</p><p><strong>使用</strong><br>PWL 缓存在持久设备中管理缓存数据。它会在配置的目录中查找并创建缓存文件，然后将数据缓存到文件中。<br>PWL 缓存依赖于独占锁功能。只有在获取了独占锁后，缓存才能加载。</p><p>缓存提供两种不同的持久性模式。在持久写模式下，写入操作仅在写入被持久化到缓存设备后才会完成，并且在崩溃后仍然可读。在持久刷新模式下，写入操作一旦不再需要调用者的数据缓冲区即可完成，但不保证在崩溃后写入数据可读。数据在收到刷新请求时持久化到缓存设备。<br>默认情况下，缓存模式为持久写模式，在第一次刷新请求后切换到持久刷新模式。</p><p><strong>1.启用缓存</strong><br>要启用 PWL 缓存，请设置以下配置项：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">rbd_persistent_cache_mode</span> = &#123;cache-mode&#125;</span><br><span class="line"><span class="attr">rbd_plugins</span> = pwl_cache</span><br></pre></td></tr></table></figure><p><code>&#123;cache-mode&#125;</code> 的值可以是 rwl、ssd 或 disabled。默认情况下，缓存是禁用的。</p><p>rwl 缓存模式依赖于 libpmem 库（PMDK 的一部分）。它在 x86_64 架构上应普遍可用，并且在某些发行版的 ppc64le 和 aarch64 架构上也可能可用。在 s390x 架构上不可用。</p><p>以下是一些缓存配置项：</p><ul><li><p><strong>rbd_persistent_cache_path</strong><br>描述：缓存数据的文件夹。在使用 rwl 模式时，这个文件夹必须启用 DAX（参见 DAX），以避免性能下降。<br>类型：字符串<br>必需：否<br>默认值：无</p></li><li><p><strong>rbd_persistent_cache_size</strong><br>描述：每个镜像的缓存大小。最小缓存大小为 1 GB。<br>类型：大小<br>必需：否<br>默认值：无</p></li></ul><p>上述配置可以按主机、池、镜像等进行设置。例如，要按主机设置，请将覆盖项添加到主机的 ceph.conf 文件的相应部分。要按池、镜像等设置，请参阅 rbd 配置命令。</p><p><strong>2.缓存状态</strong><br>当获取到独占锁时，PWL 缓存被启用；当释放独占锁时，缓存被关闭。要检查缓存状态，可以使用命令 <code>rbd status</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd status &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p>状态信息将显示，包括当前状态、清理状态、缓存大小、位置以及一些基本指标。<br>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd status rbd/foo</span></span><br><span class="line">Watchers:</span><br><span class="line">        watcher=10.10.0.102:0/1061883624 client.25496 cookie=140338056493088</span><br><span class="line">Persistent cache state:</span><br><span class="line">        host: sceph9</span><br><span class="line">        path: /mnt/nvme0/rbd-pwl.rbd.101e5824ad9a.pool</span><br><span class="line">        size: 1 GiB</span><br><span class="line">        mode: ssd</span><br><span class="line">        stats_timestamp: Sun Apr 10 13:26:32 2022</span><br><span class="line">        present: true   empty: false    clean: false</span><br><span class="line">        allocated: 509 MiB</span><br><span class="line">        cached: 501 MiB</span><br><span class="line">        dirty: 338 MiB</span><br><span class="line">        free: 515 MiB</span><br><span class="line">        hits_full: 1450 / 61%</span><br><span class="line">        hits_partial: 0 / 0%</span><br><span class="line">        misses: 924</span><br><span class="line">        hit_bytes: 192 MiB / 66%</span><br><span class="line">        miss_bytes: 97 MiB</span><br></pre></td></tr></table></figure><p><strong>3.刷新缓存</strong><br>要刷新 RBD 缓存文件，请指定持久缓存刷新命令、池名称和镜像名称：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd persistent-cache flush &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p>如果应用程序意外崩溃，此命令也可以用于将缓存刷新回 OSDs。<br>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd persistent-cache flush rbd/foo</span></span><br></pre></td></tr></table></figure><p><strong>4.使缓存失效</strong><br>要使 RBD 缓存文件失效（丢弃），请指定持久缓存失效命令、池名称和镜像名称：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd persistent-cache invalidate &#123;pool-name&#125;/&#123;image-name&#125;</span><br></pre></td></tr></table></figure><p>该命令会移除对应镜像的缓存元数据，禁用缓存功能，并删除本地缓存文件（如果存在）。<br>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rbd persistent-cache invalidate rbd/foo</span></span><br></pre></td></tr></table></figure><h2 id="镜像加密"><a href="#镜像加密" class="headerlink" title="镜像加密"></a>镜像加密</h2><p>从 Pacific 版本开始，镜像级别的加密可以由 RBD 客户端内部处理。这意味着你可以设置一个秘密密钥，用于加密特定的 RBD 镜像。此页面描述了 RBD 加密功能的范围。</p><p>当前 krbd 内核模块不支持加密。也可以使用外部工具（如 dm-crypt、QEMU）来加密 RBD 镜像，这些工具的功能集和限制可能与这里描述的有所不同。</p><h4 id="加密格式"><a href="#加密格式" class="headerlink" title="加密格式"></a>加密格式</h4><p>默认情况下，RBD 镜像是未加密的。要加密 RBD 镜像，必须将其格式化为支持的加密格式之一。格式化操作会将加密元数据持久化到镜像中。加密元数据通常包括加密格式和版本、加密算法和模式规范，以及用于保护加密密钥的信息。加密密钥本身由用户保管的秘密（通常是密码短语）保护，该秘密不会持久化。基本的加密格式操作需要指定加密格式和一个秘密。</p><p>一些加密元数据可能作为镜像数据的一部分存储，通常一个加密头会写入原始镜像数据的开头。这意味着加密镜像的有效镜像大小可能低于原始镜像大小。有关更多详细信息，请参见支持的格式部分。</p><p>除非明确（重新）格式化，否则加密镜像的克隆会使用相同的格式和秘密进行加密。</p><p>加密镜像的克隆始终会被加密。重新格式化为明文是不支持的。</p><p>格式化前写入镜像的数据可能变得不可读，尽管它仍然可能占用存储资源。</p><p>启用了日志功能的镜像不能由 RBD 客户端格式化和加密。</p><h4 id="加密加载"><a href="#加密加载" class="headerlink" title="加密加载"></a>加密加载</h4><p>格式化镜像是启用加密的必要前提。然而，格式化的镜像仍将被所有 RBD API 视为原始未加密镜像。特别是，加密的 RBD 镜像可以通过与其他镜像相同的 API 打开，并可以读取&#x2F;写入原始未加密数据。这种原始 IO 可能会影响加密格式的完整性，例如覆盖位于镜像开头的加密元数据。</p><p>为了安全地对格式化的镜像执行加密 IO，应在打开镜像后应用额外的加密加载操作。加密加载操作需要提供加密格式和一个秘密，以解锁镜像及其所有显式格式化祖先镜像的加密密钥。成功的加密加载操作后，所有对已打开镜像的 IO 将被加密&#x2F;解密。对于克隆镜像，这也包括祖先镜像的 IO。加密密钥将由 RBD 客户端保存在内存中，直到镜像被关闭。</p><p>一旦加载了加密，不能对已打开镜像的上下文应用其他加密加载&#x2F;格式化操作。<br>一旦加载了加密，使用打开的镜像上下文的 API 调用将返回有效镜像大小和有效父重叠。<br>一旦加载了加密，调整镜像大小的 API 调用将把指定的目标大小解释为有效镜像大小。</p><p>如果加密镜像的克隆被显式格式化，则克隆镜像的扁平化操作不再透明，因为父数据必须根据克隆镜像的格式重新加密，因为它是从父快照中复制的。如果在执行扁平化操作之前没有加载加密，则克隆镜像中之前可访问的任何父数据可能变得不可读。</p><p>如果加密镜像的克隆被显式格式化，则克隆镜像的缩小操作不再透明，因为在某些情况下（例如，如果克隆镜像有快照或克隆镜像被缩小到不与对象大小对齐的大小），它涉及从父快照复制一些数据，类似于扁平化。如果在执行缩小操作之前没有加载加密，则克隆镜像中之前可访问的任何父数据可能变得不可读。</p><p>当通过 rbd-nbd 挂载 RBD 镜像作为块设备时，可以自动应用加密加载。</p><h4 id="支持的格式"><a href="#支持的格式" class="headerlink" title="支持的格式"></a>支持的格式</h4><p><strong>LUKS</strong><br>支持 LUKS1 和 LUKS2。数据布局完全符合 LUKS 规范。因此，RBD 格式化的镜像可以使用外部 LUKS 支持工具（如 dm-crypt 或 QEMU）加载。此外，现有的 LUKS 数据（在 RBD 之外创建的）可以导入（通过将原始 LUKS 数据复制到镜像中）并由 RBD 加密加载。</p><p>LUKS 格式仅在基于 Linux 的系统上受支持。 当前，仅支持 AES-128 和 AES-256 加密算法。此外，xts-plain64 目前是唯一支持的加密模式。</p><p>要使用 LUKS 格式，请首先格式化镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd encryption format [--cipher-alg &#123;aes-128|aes-256&#125;] &#123;image-spec&#125; &#123;luks1|luks2&#125; &#123;passphrase-file&#125;</span><br></pre></td></tr></table></figure><p>加密格式操作会生成一个 LUKS 头并将其写入镜像的开头。头部附加了一个密钥槽，其中包含一个随机生成的加密密钥，并由从 passphrase-file 读取的密码短语保护。</p><p>在较早版本中，如果 passphrase-file 的内容以换行符结束，则会被去除。</p><p>默认情况下，将使用 AES-256 的 xts-plain64 模式（这是当前推荐的模式，也是其他工具的通常默认模式）。格式化操作也允许选择 AES-128。目前，RBD 不支持添加&#x2F;移除密码短语，但可以使用兼容工具（如 cryptsetup）应用于原始 RBD 数据。</p><p>LUKS 头的大小可能会有所不同（在 LUKS2 中最大为 136MiB），但通常为 16MiB，具体取决于安装的 libcryptsetup 版本。为了优化性能，加密格式会将数据偏移量设置为与镜像条带周期大小对齐。例如，如果使用配置为 8MiB 对象大小的镜像，则期望最小开销为 8MiB；如果使用配置为 4MiB 对象大小且条带计数为 3 的镜像，则期望最小开销为 12MiB。</p><p>在 LUKS1 中，扇区是最小的加密单位，固定为 512 字节。LUKS2 支持更大的扇区，为了获得更好的性能，我们将默认扇区大小设置为最大 4KiB。写入小于扇区的内容或不对齐的写入将触发客户端的受保护读写链，并带来相当大的延迟。这样的未对齐写入的批次可能导致 IO 竞争，进一步恶化性能。因此，建议在无法保证传入写入与扇区对齐的情况下，避免使用 RBD 加密。</p><p>要映射一个 LUKS 格式化的镜像，请运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd device map -t nbd -o encryption-passphrase-file=&#123;passphrase-file&#125; &#123;image-spec&#125;</span><br></pre></td></tr></table></figure><p>请注意，出于安全原因，加密格式和加密加载操作都是 CPU 密集型的，可能需要几秒钟才能完成。对于实际镜像 IO 的加密操作，假设启用了 AES-NI，相对较小的微秒延迟应被添加，以及少量的 CPU 利用率增加。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>创建一个 LUKS2 格式化的镜像，实际大小为 50GiB：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rbd create --size 50G mypool/myimage</span><br><span class="line">rbd encryption format mypool/myimage luks2 passphrase.bin</span><br><span class="line">rbd resize --size 50G --encryption-passphrase-file passphrase.bin mypool/myimage</span><br></pre></td></tr></table></figure><p><code>rbd resize</code> 命令的最后一步将镜像扩展以补偿与 LUKS2 头相关的开销。</p><p>给定一个 LUKS2 格式化的镜像，创建一个具有相同有效大小的 LUKS2 格式化克隆：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rbd snap create mypool/myimage@snap</span><br><span class="line">rbd snap protect mypool/myimage@snap</span><br><span class="line">rbd clone mypool/myimage@snap mypool/myclone</span><br><span class="line">rbd encryption format mypool/myclone luks2 clone-passphrase.bin</span><br></pre></td></tr></table></figure><p>给定一个 LUKS2 格式化的镜像，实际大小为 50GiB，创建一个具有相同有效大小的 LUKS1 格式化克隆：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rbd snap create mypool/myimage@snap</span><br><span class="line">rbd snap protect mypool/myimage@snap</span><br><span class="line">rbd clone mypool/myimage@snap mypool/myclone</span><br><span class="line">rbd encryption format mypool/myclone luks1 clone-passphrase.bin</span><br><span class="line">rbd resize --size 50G --allow-shrink --encryption-passphrase-file clone-passphrase.bin --encryption-passphrase-file passphrase.bin mypool</span><br><span class="line"></span><br><span class="line">/myclone</span><br></pre></td></tr></table></figure><p>由于 LUKS1 头通常小于 LUKS2 头，<code>rbd resize</code> 命令的最后一步将克隆镜像缩小，以去除不需要的空间分配。</p><p>给定一个 LUKS1 格式化的镜像，实际大小为 50GiB，创建一个具有相同有效大小的 LUKS2 格式化克隆：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rbd resize --size 51G mypool/myimage</span><br><span class="line">rbd snap create mypool/myimage@snap</span><br><span class="line">rbd snap protect mypool/myimage@snap</span><br><span class="line">rbd clone mypool/myimage@snap mypool/myclone</span><br><span class="line">rbd encryption format mypool/myclone luks2 clone-passphrase.bin</span><br><span class="line">rbd resize --size 50G --allow-shrink --encryption-passphrase-file passphrase.bin mypool/myimage</span><br><span class="line">rbd resize --size 50G --allow-shrink --encryption-passphrase-file clone-passphrase.bin --encryption-passphrase-file passphrase.bin mypool/myclone</span><br></pre></td></tr></table></figure><p>由于 LUKS2 头通常大于 LUKS1 头，<code>rbd resize</code> 命令的开始步骤会暂时扩展父镜像，以保留一些额外的空间在父快照中，从而使克隆镜像中的所有父数据可访问。<code>rbd resize</code> 命令的最后一步将父镜像缩小回其原始大小（这不会影响父快照），并将克隆镜像缩小，以去除未使用的保留空间。</p><p>这同样适用于创建格式化克隆未格式化（明文）镜像，因为未格式化镜像完全没有头。</p><p>要映射一个格式化的克隆，请为克隆本身和所有显式格式化的父镜像提供加密格式和密码短语。加密格式和密码短语文件选项的顺序应基于镜像层次结构：从克隆镜像开始，然后是其父镜像，依此类推。</p><p>以下是映射格式化克隆的命令示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd device map -t nbd -o encryption-passphrase-file=clone-passphrase.bin,encryption-passphrase-file=passphrase.bin mypool/myclone</span><br></pre></td></tr></table></figure><h2 id="RBD-配置设置"><a href="#RBD-配置设置" class="headerlink" title="RBD 配置设置"></a>RBD 配置设置</h2><h4 id="通用设置"><a href="#通用设置" class="headerlink" title="通用设置"></a>通用设置</h4><p><strong>1.rbd_compression_hint</strong><br>写操作时发送给 OSD 的提示。如果设置为 <code>compressible</code> 并且 OSD 的 <code>bluestore_compression_mode</code> 设置为 <code>passive</code>，则 OSD 会尝试压缩数据。如果设置为 <code>incompressible</code> 并且 OSD 的压缩设置为 <code>aggressive</code>，则 OSD 不会尝试压缩数据。</p><ul><li>类型：字符串 (str)</li><li>默认值：none</li><li>可选值：<ul><li>none</li><li>compressible</li><li>incompressible</li></ul></li></ul><p><strong>2.rbd_read_from_replica_policy</strong><br>确定哪个 OSD 接收读取操作的策略。如果设置为 <code>default</code>，每个 PG 的主 OSD 将始终用于读取操作。如果设置为 <code>balance</code>，读取操作将发送到副本集中的随机选择的 OSD。如果设置为 <code>localize</code>，读取操作将发送到由 CRUSH 图确定的最接近的 OSD。与 <code>rbd_balance_snap_reads</code> 和 <code>rbd_localize_snap_reads</code> 或 <code>rbd_balance_parent_reads</code> 和 <code>rbd_localize_parent_reads</code> 不同，它影响所有读取操作，而不仅仅是快照或父镜像。注意：此功能需要集群配置为最低兼容 OSD 版本 Octopus。</p><ul><li>类型：字符串 (str)</li><li>默认值：default</li><li>可选值：<ul><li>default</li><li>balance</li><li>localize</li></ul></li></ul><p><strong>3.rbd_default_order</strong><br>配置新镜像的默认对象大小。值作为二的幂使用，意味着 <code>default_object_size = 2 ^ rbd_default_order</code>。配置一个介于 12 和 25 之间的值（包含 12 和 25），对应于 4KiB 下限和 32MiB 上限。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：22</li></ul><h4 id="缓存设置"><a href="#缓存设置" class="headerlink" title="缓存设置"></a>缓存设置</h4><p><strong>1.内核缓存</strong><br>Ceph 块设备的内核驱动程序可以使用 Linux 页面缓存来提高性能。</p><p>Ceph 块设备的用户空间实现（即 librbd）无法利用 Linux 页面缓存，因此它包含自己的内存缓存，称为“RBD 缓存”。RBD 缓存的行为类似于良好工作的硬盘缓存。当操作系统发送障碍或刷新请求时，所有脏数据会被写入 OSD。这意味着使用回写缓存的安全性与使用良好工作的物理硬盘一样（即 Linux 内核 &gt;&#x3D; 2.6.32）。缓存使用最少最近使用（LRU）算法，在回写模式下，它可以合并连续请求以提高吞吐量。</p><p>librbd 缓存默认启用，支持三种不同的缓存策略：写绕过、回写和写直达。在写绕过和回写策略下，写入会立即返回，除非有超过 <code>rbd_cache_max_dirty</code> 的未写入字节到存储集群。写绕过策略与回写策略的不同之处在于它不会尝试从缓存中服务读取请求，因此对于高性能写工作负载更快。在写直达策略下，写入仅在数据在所有副本上都存在时返回，但读取可能来自缓存。</p><p>在收到刷新请求之前，缓存表现为写直达缓存，以确保旧操作系统（不会发送刷新以确保崩溃一致性行为）安全操作。</p><p>如果禁用 librbd 缓存，写入和读取会直接到达存储集群，写入仅在数据在所有副本上都存在时返回。</p><p>缓存位于客户端的内存中，每个 RBD 镜像都有自己的缓存。由于缓存是客户端本地的，如果有其他人访问镜像，缓存之间不会保持一致。运行 GFS 或 OCFS 在 RBD 上时，启用缓存将无法正常工作。</p><p>RBD 的选项设置应在配置文件或中央配置存储的 [client] 部分中设置。这些设置包括：</p><p><strong>2.rbd_cache</strong><br>启用 RADOS 块设备（RBD）的缓存。</p><ul><li>类型：布尔值 (bool)</li><li>默认值：true</li></ul><p><strong>3.rbd_cache_policy</strong><br>选择 librbd 的缓存策略。</p><ul><li>类型：字符串 (str)</li><li>默认值：writearound</li><li>可选值：<ul><li>writethrough</li><li>writeback</li><li>writearound</li></ul></li></ul><p><strong>4.rbd_cache_writethrough_until_flush</strong><br>开始时使用写直达模式，并在收到第一个刷新请求后切换到回写模式。启用此选项是一种保守但安全的策略，适用于运行在 RBD 卷上的虚拟机（如 Linux 内核版本低于 2.6.32 的 virtio 驱动程序）不能发送刷新请求的情况。</p><ul><li>类型：布尔值 (bool)</li><li>默认值：true</li></ul><p><strong>5.rbd_cache_size</strong><br>每卷 RBD 客户端缓存的大小（以字节为单位）。</p><ul><li>类型：大小 (size)</li><li>默认值：32Mi</li><li>策略：回写和写直达</li></ul><p><strong>6.rbd_cache_max_dirty</strong><br>缓存触发回写的脏数据限制（以字节为单位）。如果为 0，则使用写直达缓存。</p><ul><li>类型：大小 (size)</li><li>默认值：24Mi</li><li>约束：必须小于 <code>rbd_cache_size</code></li><li>策略：写绕过和回写</li></ul><p><strong>7.rbd_cache_target_dirty</strong><br>缓存开始将数据写入数据存储之前的脏数据目标值。不会阻止写入缓存。</p><ul><li>类型：大小 (size)</li><li>默认值：16Mi</li><li>约束：必须小于 <code>rbd_cache_max_dirty</code></li><li>策略：回写</li></ul><p><strong>8.rbd_cache_max_dirty_age</strong><br>脏数据在缓存中存在的秒数，然后开始回写。</p><ul><li>类型：浮点数 (float)</li><li>默认值：1.0</li><li>策略：回写</li></ul><h4 id="预读设置"><a href="#预读设置" class="headerlink" title="预读设置"></a>预读设置</h4><p>librbd 支持预读&#x2F;预取，以优化小的、顺序的读取。这通常应该由虚拟机的操作系统处理，但引导加载程序可能不会发出有效的读取。如果缓存被禁用或策略为写绕过，则预读会自动禁用。<br><strong>1.rbd_readahead_trigger_requests</strong><br>触发预读所需的顺序请求数量。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：10</li></ul><p><strong>2.rbd_readahead_max_bytes</strong><br>预读请求的最大大小。如果为零，则禁用预读。</p><ul><li>类型：大小 (size)</li><li>默认值：512Ki</li></ul><p><strong>3.rbd_readahead_disable_after_bytes</strong><br>从 RBD 镜像中读取此字节数后，预读对该镜像将被禁用，直到它关闭。这允许来宾操作系统在启动后接管预读。如果为零，则预读保持启用。</p><ul><li>类型：大小 (size)</li><li>默认值：50Mi</li></ul><h4 id="镜像特性"><a href="#镜像特性" class="headerlink" title="镜像特性"></a>镜像特性</h4><p>RBD 支持通过命令行创建镜像时指定高级功能，或通过 <code>rbd_default_features = &lt;特性数值总和&gt;</code> 或 <code>rbd_default_features = &lt;逗号分隔的 CLI 值列表&gt;</code> 配置默认特性。</p><p><strong>layering</strong><br>描述<br>层叠启用克隆。</p><ul><li>内部值：1</li><li>CLI 值：layering</li><li>添加于：v0.52 (Bobtail)</li><li>KRBD 支持：从 v3.10 起</li><li>默认值：是</li></ul><p><strong>striping v2</strong><br>描述<br>条带将数据分散到多个对象上。条带有助于顺序读&#x2F;写工作负载的并行性。</p><ul><li>内部值：2</li><li>CLI 值：striping</li><li>添加于：v0.55 (Bobtail)</li><li>KRBD 支持：从 v3.10 起（仅默认条带，v4.17 添加了“Fancy”条带）</li><li>默认值：是</li></ul><p><strong>exclusive-lock</strong><br>描述<br>启用时，要求客户端在进行写入之前对对象进行锁定。排他锁应仅在任何时候只有一个客户端访问镜像时启用。</p><ul><li>内部值：4</li><li>CLI 值：exclusive-lock</li><li>添加于：v0.92 (Hammer)</li><li>KRBD 支持：从 v4.9 起</li><li>默认值：是</li></ul><p><strong>object-map</strong><br>描述<br>对象映射支持依赖于排他锁支持。块设备是瘦配置的，这意味着它们仅存储实际写入的数据，即它们是稀疏的。对象映射支持有助于跟踪哪些对象实际存在（在设备上存储有数据）。启用对象映射支持可以加快克隆、导入和导出稀疏填充镜像以及删除的 I&#x2F;O 操作。</p><ul><li>内部值：8</li><li>CLI 值：object-map</li><li>添加于：v0.93 (Hammer)</li><li>KRBD 支持：从 v5.3 起</li><li>默认值：是</li></ul><p><strong>fast-diff</strong><br>描述<br>快速差异支持依赖于对象映射支持和排他锁支持。它为对象映射添加了另一个属性，使生成镜像快照之间的差异变得更快。它还更快地计算快照或卷的实际数据使用量（<code>rbd du</code>）。</p><ul><li>内部值：16</li><li>CLI 值：fast-diff</li><li>添加于：v9.0.1 (Infernalis)</li><li>KRBD 支持：从 v5.3 起</li><li>默认值：是</li></ul><p><strong>deep-flatten</strong><br>描述<br>深度扁平化使 <code>rbd flatten</code> 能在镜像的所有快照上工作，除了镜像本身。没有它，镜像的快照仍将依赖于父镜像，因此在删除父镜像之前必须先删除快照。深度扁平化使父镜像独立于其克隆，即使它们有快照，也不会受到影响，但会使用额外的 OSD 设备空间。</p><ul><li>内部值：32</li><li>CLI 值：deep-flatten</li><li>添加于：v9.0.2 (Infernalis)</li><li>KRBD 支持：从 v5.1 起</li><li>默认值：是</li></ul><p><strong>journaling</strong><br>描述<br>日志记录支持依赖于排他锁支持。日志记录按发生顺序记录对镜像的所有修改。RBD 镜像可以利用日志记录将崩溃一致的镜像复制到远程集群。最好让 <code>rbd-mirror</code> 仅在需要时管理此功能，因为长期启用可能导致大量额外的 OSD 空间消耗。</p><ul><li>内部值：64</li><li>CLI 值：journaling</li><li>添加于：v10.0.1 (Jewel)</li><li>KRBD 支持：无</li><li>默认值：否</li></ul><p><strong>Data pool</strong><br>描述<br>在纠删码池上，镜像数据块对象需要存储在与镜像元数据不同的池中。</p><ul><li>内部值：128</li><li>添加于：v11.1.0 (Kraken)</li><li>KRBD 支持：从 v4.11 起</li><li>默认值：否</li></ul><p><strong>Operations</strong><br>描述<br>用于限制旧客户端对镜像执行某些维护操作（例如克隆、创建快照）。</p><ul><li>内部值：256</li><li>添加于：v13.0.2 (Mimic)</li><li>KRBD 支持：从 v4.16 起</li></ul><p><strong>Migrating</strong><br>描述<br>用于限制旧客户端在镜像处于迁移状态时打开镜像。</p><ul><li>内部值：512</li><li>添加于：v14.0.1 (Nautilus)</li><li>KRBD 支持：无</li></ul><p><strong>Non-primary</strong><br>描述<br>用于限制使用基于快照的镜像镜像的非主副本进行更改。</p><ul><li>内部值：1024</li><li>添加于：v15.2.0 (Octopus)</li><li>KRBD 支持：无</li></ul><h4 id="QOS-设置"><a href="#QOS-设置" class="headerlink" title="QOS 设置"></a>QOS 设置</h4><p>librbd 支持以几种方式限制每个镜像的 I&#x2F;O。这些限制适用于给定进程中的给定镜像——例如，在多个地方使用的同一镜像（例如两个独立的虚拟机）会有独立的限制。</p><ul><li>IOPS：每秒 I&#x2F;O 操作数（任何类型的 I&#x2F;O）</li><li>读取 IOPS：每秒读取 I&#x2F;O 操作数</li><li>写入 IOPS：每秒写入 I&#x2F;O 操作数</li><li>bps：每秒字节数（任何类型的 I&#x2F;O）</li><li>读取 bps：每秒读取字节数</li><li>写入 bps：每秒写入字节数</li></ul><p>这些限制彼此独立操作。默认情况下，它们都处于关闭状态。每种限制类型使用令牌桶算法来限制 I&#x2F;O，能够配置限制（时间上的平均速度）和较高的速率（突发）在短时间内（<code>burst_seconds</code>）。当达到这些限制并且没有剩余的突发容量时，librbd 会将该类型 I&#x2F;O 的速率降低到限制值。</p><p>例如，如果配置了 100MB 的读取 bps 限制，但写入没有限制，则写入可以尽可能快地进行，而读取将被限制为 100MB&#x2F;s。如果设置了 150MB 的读取突发和 5 秒的读取突发时间，则读取可以以 150MB&#x2F;s 的速度进行长达 5 秒，然后恢复到 100MB&#x2F;s 的限制。</p><p>以下选项配置这些限制：</p><h5 id="rbd-qos-iops-limit"><a href="#rbd-qos-iops-limit" class="headerlink" title="rbd_qos_iops_limit"></a>rbd_qos_iops_limit</h5><p>每秒 I&#x2F;O 操作数的期望限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-iops-burst"><a href="#rbd-qos-iops-burst" class="headerlink" title="rbd_qos_iops_burst"></a>rbd_qos_iops_burst</h5><p>I&#x2F;O 操作的期望突发限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-iops-burst-seconds"><a href="#rbd-qos-iops-burst-seconds" class="headerlink" title="rbd_qos_iops_burst_seconds"></a>rbd_qos_iops_burst_seconds</h5><p>I&#x2F;O 操作的期望突发持续时间（以秒为单位）。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：1</li><li>最小值：1</li></ul><h5 id="rbd-qos-read-iops-limit"><a href="#rbd-qos-read-iops-limit" class="headerlink" title="rbd_qos_read_iops_limit"></a>rbd_qos_read_iops_limit</h5><p>每秒读取操作数的期望限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-read-iops-burst"><a href="#rbd-qos-read-iops-burst" class="headerlink" title="rbd_qos_read_iops_burst"></a>rbd_qos_read_iops_burst</h5><p>读取操作的期望突发限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-read-iops-burst-seconds"><a href="#rbd-qos-read-iops-burst-seconds" class="headerlink" title="rbd_qos_read_iops_burst_seconds"></a>rbd_qos_read_iops_burst_seconds</h5><p>读取操作的期望突发持续时间（以秒为单位）。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：1</li><li>最小值：1</li></ul><h5 id="rbd-qos-write-iops-limit"><a href="#rbd-qos-write-iops-limit" class="headerlink" title="rbd_qos_write_iops_limit"></a>rbd_qos_write_iops_limit</h5><p>每秒写入操作数的期望限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-write-iops-burst"><a href="#rbd-qos-write-iops-burst" class="headerlink" title="rbd_qos_write_iops_burst"></a>rbd_qos_write_iops_burst</h5><p>写入操作的期望突发限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-write-iops-burst-seconds"><a href="#rbd-qos-write-iops-burst-seconds" class="headerlink" title="rbd_qos_write_iops_burst_seconds"></a>rbd_qos_write_iops_burst_seconds</h5><p>写入操作的期望突发持续时间（以秒为单位）。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：1</li><li>最小值：1</li></ul><h5 id="rbd-qos-bps-limit"><a href="#rbd-qos-bps-limit" class="headerlink" title="rbd_qos_bps_limit"></a>rbd_qos_bps_limit</h5><p>每秒 I&#x2F;O 字节数的期望限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-bps-burst"><a href="#rbd-qos-bps-burst" class="headerlink" title="rbd_qos_bps_burst"></a>rbd_qos_bps_burst</h5><p>I&#x2F;O 字节的期望突发限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-bps-burst-seconds"><a href="#rbd-qos-bps-burst-seconds" class="headerlink" title="rbd_qos_bps_burst_seconds"></a>rbd_qos_bps_burst_seconds</h5><p>I&#x2F;O 字节的期望突发持续时间（以秒为单位）。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：1</li><li>最小值：1</li></ul><h5 id="rbd-qos-read-bps-limit"><a href="#rbd-qos-read-bps-limit" class="headerlink" title="rbd_qos_read_bps_limit"></a>rbd_qos_read_bps_limit</h5><p>每秒读取字节数的期望限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-read-bps-burst"><a href="#rbd-qos-read-bps-burst" class="headerlink" title="rbd_qos_read_bps_burst"></a>rbd_qos_read_bps_burst</h5><p>读取字节的期望突发限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-read-bps-burst-seconds"><a href="#rbd-qos-read-bps-burst-seconds" class="headerlink" title="rbd_qos_read_bps_burst_seconds"></a>rbd_qos_read_bps_burst_seconds</h5><p>读取字节的期望突发持续时间（以秒为单位）。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：1</li><li>最小值：1</li></ul><h5 id="rbd-qos-write-bps-limit"><a href="#rbd-qos-write-bps-limit" class="headerlink" title="rbd_qos_write_bps_limit"></a>rbd_qos_write_bps_limit</h5><p>每秒写入字节数的期望限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-write-bps-burst"><a href="#rbd-qos-write-bps-burst" class="headerlink" title="rbd_qos_write_bps_burst"></a>rbd_qos_write_bps_burst</h5><p>写入字节的期望突发限制。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：0</li></ul><h5 id="rbd-qos-write-bps-burst-seconds"><a href="#rbd-qos-write-bps-burst-seconds" class="headerlink" title="rbd_qos_write_bps_burst_seconds"></a>rbd_qos_write_bps_burst_seconds</h5><p>写入字节的期望突发持续时间（以秒为单位）。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：1</li><li>最小值：1</li></ul><h5 id="rbd-qos-schedule-tick-min"><a href="#rbd-qos-schedule-tick-min" class="headerlink" title="rbd_qos_schedule_tick_min"></a>rbd_qos_schedule_tick_min</h5><p>此设置决定了 I&#x2F;O 在限制被触发时可以解锁的最小时间（以毫秒为单位）。就令牌桶算法而言，这是添加令牌到桶中的最小间隔。</p><ul><li>类型：无符号整数 (uint)</li><li>默认值：50</li><li>最小值：1</li></ul><h5 id="rbd-qos-exclude-ops"><a href="#rbd-qos-exclude-ops" class="headerlink" title="rbd_qos_exclude_ops"></a>rbd_qos_exclude_ops</h5><p>可选地从 QoS 排除操作。此设置接受整数位掩码值或逗号分隔的操作名称字符串。此设置始终作为整数位掩码值内部存储。操作位掩码值与操作名称的映射如下：+1 -&gt; 读取，+2 -&gt; 写入，+4 -&gt; 丢弃，+8 -&gt; write_same，+16 -&gt; compare_and_write</p><ul><li>类型：字符串 (str)</li></ul><h2 id="RBD-重放"><a href="#RBD-重放" class="headerlink" title="RBD 重放"></a>RBD 重放</h2><p>RBD 重放是一组用于捕获和重放 RADOS 块设备（RBD）工作负载的工具。要捕获 RBD 工作负载，客户端必须安装 lttng-tools，并且客户端上的 librbd 必须是 v0.87（Giant）版本或更高版本。要重放 RBD 工作负载，客户端上的 librbd 必须是 Giant 版本或更高版本。<br>捕获和重放的步骤如下：</p><ol><li><p><strong>捕获跟踪数据</strong>。确保捕获 pthread_id 上下文：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p traces</span><br><span class="line">lttng create -o traces librbd</span><br><span class="line">lttng enable-event -u <span class="string">&#x27;librbd:*&#x27;</span></span><br><span class="line">lttng add-context -u -t pthread_id</span><br><span class="line">lttng start</span><br><span class="line"><span class="comment"># 在这里运行 RBD 工作负载</span></span><br><span class="line">lttng stop</span><br></pre></td></tr></table></figure></li><li><p><strong>处理跟踪数据</strong>，使用 <code>rbd-replay-prep</code>：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd-replay-prep traces/ust/uid/*/* replay.bin</span><br></pre></td></tr></table></figure></li><li><p><strong>重放跟踪数据</strong>，使用 <code>rbd-replay</code>。在确认它做了你想要的操作之前，使用只读模式：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd-replay --read-only replay.bin</span><br></pre></td></tr></table></figure></li></ol><p><strong>重要提示</strong></p><ul><li><code>rbd-replay</code> 默认情况下会销毁数据。除非使用 <code>--read-only</code> 选项，否则不要对希望保留的镜像使用。</li><li>重放的工作负载不必与捕获的工作负载使用相同的 RBD 镜像或相同的集群。为了处理差异，可能需要使用 <code>--pool</code> 和 <code>--map-image</code> 选项。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Ceph-RBD介绍&quot;&gt;&lt;a href=&quot;#Ceph-RBD介绍&quot; class=&quot;headerlink&quot; title=&quot;Ceph RBD介绍&quot;&gt;&lt;/a&gt;Ceph RBD介绍&lt;/h2&gt;&lt;p&gt;随着云计算的发展，Ceph已经成为目前最为流行的分布式存储系统，俨然存储界的</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph_crush算法实现</title>
    <link href="https://watsonlu6.github.io/Ceph-crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
    <id>https://watsonlu6.github.io/Ceph-crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-08-02T14:26:17.000Z</published>
    <updated>2024-07-28T09:23:25.673Z</updated>
    
    <content type="html"><![CDATA[<p>分布式存储系统的数据分布算法要解决数据如何分布到集群中的各个节点和磁盘上，其面临： 数据分布和负载均衡、灵活应对集群伸缩、大规模集群计算速率三方面的挑战。 </p><ol><li>数据分布和负载均衡：数据分布均衡，使数据能均匀地分布在各个节点和磁盘上，使数据访问的负载在各个节点和磁盘上。</li><li>灵活应对集群伸缩：系统可以方便地增加或者删除存储设备，当增加或删除存储设备后，能自动实现数据的均衡，并且迁移的数据尽可能减少。</li><li>大规模集群算法计算速率：要求数据分布算法维护的元数据相对较小，并且计算量不能太大。</li></ol><p>在分布式存储系统中，数据分布算法由两种基本实现方法，一种是<code>基于集中式的元数据查询的方式</code>，如HDFS的实现：文件的分布信息是通过访问集中元数据服务器获得；另一种是<code>基于哈希算法计算的方式</code>。例如一致性哈希算法(DHT)。Ceph的数据分布算法CRUSH属于后者。CRUSH(Controlled Replication Under Scalable Hashing)，是一种基于哈希的数据分布算法。与另一种基于集中式的元数据查询的存储方式(文件的分布信息需要先通过访问集中元数据服务器获得)不同。CRUSH算法以数据唯一标识符、当前存储集群的拓扑结构以及数据分布策略作为CRUSH的输入，经过计算获得数据分布位置，直接与OSD进行通信，从而避免集中式查询操作，实现去中心化和高度并发。</p><p>Ceph 作为分布式存储系统，采用多节点多副本的数据存放方式，必然要解决数据如何分布到集群中各个节点和磁盘上。Ceph使用CRUSH数据分布算法。例如一个Ceph集群三副本，就存在着如何映射3个OSD存储这3个副本的数据，Ceph写数据时，即写object时，首先需要计算出object属于哪个PG，然后根据PG id 计算出存放的OSD位置。过程分两步：PG id的计算 ；OSD位置的计算。结合rbd的代码介绍这两个过程：</p><h2 id="数据分片"><a href="#数据分片" class="headerlink" title="数据分片"></a>数据分片</h2><p>rbd的写接口（src&#x2F;linrbd&#x2F;librbd.cc）<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B01.png"><br>接口传入的参数是起始写位置（ofs）以及写数据大小（len）和要写入的数据（bl），调用io_work_queue-&gt;write()，生成Object写入请求对象，发送到ImageRequestWQ任务队列中，等待工作线程处理。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B02.png"><br>现在看看ImageRequest的数据类型<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B03.png"></p><p>因为Image的ImageWriteRequest继承AbstractImageWriteRequest类，重点关注AbstractImageWriteRequest类<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B04.png"><br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B05.png"></p><p>发送写请求时调用void AbstractImageWriteRequest<I>::send_request()函数，在这个函数进行切分数据，分成大小同等（可设定，一般为4M）的object(最后一块object可能大小小于块大小)。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B06.png"><br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B07.png"></p><p>file_to_extents就是将数据段切分各个object，具体怎么分割就不深入看源码了。然后调用send_object_requests()将分片各个object分别构造写请求<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B08.png"></p><h2 id="Op请求处理"><a href="#Op请求处理" class="headerlink" title="Op请求处理"></a>Op请求处理</h2><p>此后会构造objecter的Op请求，发送出去；转到src&#x2F;librados&#x2F;IoCtxImpl.cc，深入了解Op请求的处理。类IoCtxImpl是pool相关的上下文信息，一个pool对应一个IoCtxImpl对象，可以在该pool里创建、删除对象，完成对象数据读写等各种操作，包括同步和异步的实现。类IoCtxImpl把请求封装成ObjectOperation类。然后再添加pool的地址信息，封装成Obejcter::Op对象。Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。类IoCtxImpl的write&#x2F;read等同步操作函数通过调用operate()来调用op_submit()，类IoCtxImpl的aio_write&#x2F;aio_read&#x2F;aio_operate等异步函数直接调用了op_submit(），说明op_submit(）是object读写操作的入口。调用函数objeter-&gt;op_submit发送给相应的OSD，如果是同步操作，就等待操作完成。如果是异步操作，就不用等待，直接返回，当操作完成后，调用相应的回调函数通知。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B09.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B010.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B011.png"></p><p>Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B012.png"></p><h2 id="发送数据op-submit"><a href="#发送数据op-submit" class="headerlink" title="发送数据op_submit"></a>发送数据op_submit</h2><p>在op_submit()调用_op_submit_with_budget()处理Throttle相关流量的限制<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B013.png"><br>在_op_submit_with_budget()中，如果osd_timeout大于0，就是设置定时器，当操作超时，就调用定时器回调函数op_ cancel取消操作，然后通过调用_op_submit(op, sul, ptid)。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B014.png"></p><p>_op_submit函数完成了关键的地址寻址和发送工作，比如_calc_target()、_get_session()、_send_op()等，调用函数_calc_target()计算对象的目标OSD；调用函数_get_session()获取目标OSD的链接，如果返回值为-EAGAIN，就升级为写锁，重新获取。检查当前的状态标志，如果当前是CEPH_OSDMAP_PAUSEWR或者OSD空间满，就暂时不发送，否则调用函数_prepare_osd_op准备请求的信息，调用函数_send_op发送出去。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B015.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B016.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B017.png"></p><h2 id="对象寻址-calc-target"><a href="#对象寻址-calc-target" class="headerlink" title="对象寻址_calc_target"></a>对象寻址_calc_target</h2><p>重点详细分析下_calc_target函数：首先调用函数osdmap-&gt;get_pg_pool()根据t-&gt;base_oloc.pool获取pool信息，获取pg_pool_t对象；检查pi-&gt;last_force_op_resend是否强制重发，如果强制重发，force_resend设置为true；检查cache tier，如果是读操作，并且有读缓存，就设置t-&gt;target_oloc.pool为该pool的read_tier值；如果是写操作，并且有写缓存，就设置t-&gt;target_oloc.pool为该pool的write_tier值；调用函数osdmap-&gt;object_locator_to_pg()获取目标对象所在的PG；调用函数osdmap-&gt;pg_to_up_acting_osds()通过CRUSH算法，获取该PG对应的OSD列表，即pg_to_up_acting_osds()通过CRUSH算法计算OSD；判断读写操作：读操作，如果设置了CEPH_OSD_FLAG_BALANCE_READS标志，调用rand() 取余随机选择一个副本读取；读操作，如果设置了CEPH_OSD_FLAG_LOCALIZE_READS标志，尽可能从本地副本读取；写操作，target的OSD就设置为主OSD。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B018.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B019.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B020.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B021.png"></p><p>首先获取pool信息，判断是否有效：<br>        <code>const pg_pool_t *pi = osdmap-&gt;get_pg_pool(t-&gt;base_oloc.pool);</code><br>然后根据获取pgid，注意pgid是一个结构体pg_t<br>pg_t 的结构如下：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B022.png"></p><p>m_pool 是pool id， m_seed是函数根据object id算出来的哈希值，m_preferred赋值-1。<br>接下来就是调用osdmap-&gt;pg_to_up_acting_osds()，获取该PG对应的OSD列表，即选择OSD：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B023.png"></p><p>pg_to_up_acting_osds()函数在src\osd\OSDMap.cc中，函数功能是选出up osds以及 acting osds, 两个都是数组类型，大小为副本数<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B024.png"></p><p>继续跟踪这个函数：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B025.png"></p><p>进入_pg_to_raw_osds：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B026.png"></p><p>上面函数crush-&gt;do_rule()就是真正调用crush算法计算出相应的osd列表。<br>这里重点解释下参数pps：对象到PG的映射：任何程序通过客户端访问集群时，首先由客户端生成一个字符串形式的对象名，然后基于对象名和命名空间计算得出一个32位哈希值。针对此哈希值，对该存储池的PG总数量pg_num取模(掩码计算)，得到该对象所在的PG的id号。<br><code>ps_t pps = pool.raw_pg_to_pps(pg);  // placement ps</code><br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B027.jpg"></p><p>可以看出pps这是一个哈希值，这个哈希值根据pool id，函数中pg.ps()就是我们object哈希算出的m_seed：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B028.jpg"></p><h2 id="调用CRUSH算法"><a href="#调用CRUSH算法" class="headerlink" title="调用CRUSH算法"></a>调用CRUSH算法</h2><p>下面就是进入do_rule 进行CRUSH算法的处理了：src&#x2F;crush&#x2F;CrushWrapper.h<br>调用crush_do_rule()函数<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B029.png"></p><p>继续调用crush_do_rule()算法，执行CEUSH算法<br><strong>CRUSH算法：</strong>针对指定输入x(要计算PG的pg_id)，CRUSH将输出一个包含n个不同目标存储对象(例如磁盘)的集合(OSD列表)。CRUSH的计算过程使用x、cluster map、placement rule作为哈希函数输入。因此如果cluster map不发生变化(一般placement rule不会轻易变化)，那么结果就是确定的。算法输入需要3个输入参数：</p><ol><li>输入x 即PG id的哈希值</li><li>crush_map即集群的拓扑结构，集群的层级化描述，形如”数据中心-&gt;机架-&gt;主机-&gt;磁盘”这样的层级拓扑。用树来表示，每个叶子节点都是真实的最小物理存储设备，称为devices；所有中间节点统称为bucket，每个bucket可以是一些devices的集合，也可以是低一级的buckets集合；根节点称为root，是整个集群的入口。</li><li>ruleno 即选择策略，就rule规则，这里用编号表示；它决定一个PG的对象副本如何选择(从定义的cluster map的拓扑结构中)的规则，以此完成数据映射。palcement rule可以包含多个操作，这些操作共有3种类型：take(root)、select(replicas, type)、emit(void)<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B030.png"></li></ol><p>crush 算法输入需要3个输入参数：</p><ol><li>输入x 即PG id的哈希值</li><li>crush_map即集群的拓扑结构</li><li>ruleno 即选择策略，就rule规则，这里用编号表示</li></ol><p>可以通过集群输出crush_map:<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B031.jpg"></p><p>vim crush_map如下：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B032.png"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B033.jpg"></p><p><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B034.jpg"></p><p>显示的结构和代码中的结构还是有着映射的关系：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B035.jpg"></p><p>其中crush_bucket:<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B036.jpg"><br>对应：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B037.0.jpg"><br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B037.jpg"></p><p>crush_rule:<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B038.jpg"><br>对应于：<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B039.png"></p><p>逐一对比分析其数据结构。<br>这里分析下其选择OSD的过程：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> *a = scratch;</span><br><span class="line"><span class="type">int</span> *b = scratch + result_max;</span><br><span class="line"><span class="type">int</span> *c = scratch + result_max*<span class="number">2</span>;</span><br><span class="line">w = a;</span><br><span class="line">o= b;</span><br></pre></td></tr></table></figure><p>a, b, c 分别指向 scratch向量的0, 1, 2的位置.<br>w &#x3D; a; o &#x3D; b; </p><ul><li>w被用作一个先入先出队列来在CRUSH map中进行横向优先搜索(BFS traversal). </li><li>o存储crush_choose_firstn选择的结果. </li><li>c存储最终的OSD选择结果.</li></ul><p>crush_do_rule函数里面最重要的是函数里面的for循环，这个循环就是筛选osd的过程，</p><p>for循环中：</p><ol><li>首先从rule规则中当前执行的步骤，首次就执行第一条步骤：<br> <code>struct crush_rule_step *curstep = &amp;rule-&gt;steps[step];</code></li><li>然后根据当前执行步骤的操作类型，选择不同的分支操作，首先一般是take操作，而且是take fault。即crush map树根节点。这个过程就是根据step 逐步选择bucket 知道知道叶子节点，即OSD。</li><li>这个过程中，crush_choose_firstn 函数, 递归的选择特定bucket或者设备,并且可以处理冲突,失败的情况.</li><li>如果当前是choose过程,通过调用crush_bucket_choose来直接选择. </li><li>如果当前是chooseleaf选择叶子节点的过程,该函数将递归直到得到叶子节点.<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B040.png"></li></ol><p>在for循环中的crush_choose_firstn()计算后如果结果不是OSD类型, o 交给w。以便于 w成为下次crush_choose_firstn的输入参数。在crush_choose_firstn()中，for(){}：副本选择循环判断条件rep是否等于副本数numrep，rep叠加。do{}while (retry_descent)：选择OSD冲突或故障域失效时循环，随机因子r改变。do{}while (retry_bucket)：进行bucket层级选择，当前item type不是OSD时循环，当前进行选择的bucket，即in改变。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B041.png"></p><p>在crush_choose_firstn()函数中有crush_bucket_choose函数，这个函数根据bucket类型选择不同的权重计算方法刷选出bucket。如果采用straw2，就会采用bucket_straw2_choose接口进行筛选。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B042.jpg"></p><p>bucket_straw2_choose()功能是通过调用伪随机算法计算伪随机数，以伪随机数最高的作为选择出的节点<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B043.png"></p><p>generate_exponential_distribution()产生随机数的思想是：采用逆变换采样的思想，先调用crush_hash32_3()计算哈希值，然后取随机数的低16位。计算指数随机变量。作为参考，请参阅指数分布示例：<a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling#Examples%E3%80%82">https://en.wikipedia.org/wiki/Inverse_transform_sampling#Examples。</a> 由于某种原因，略小于 0x10000 会产生更准确的分布……可能是舍入效果。 自然对数查找表映射 [0,0xffff]（对应实数 [1&#x2F;0x10000, 1] 到 [0, 0xffffffffffff]（对应实数 [-11.090355,0]）。除以 16.16 定点权重。 请注意，ln 值为负数，因此较大的权重意味着较大的（较小的负数）draw值。<br><img src="/images/crush/crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B044.png"></p><p>CRUSH算法的一些缺陷： </p><ol><li>CRUSH算法提供了uniform、list和tree等bucket类型作为straw bucket类型的替代方案，但这些算法在添加或删除服务器时需要进行不必要的重排，这使它们不适合用于大规模存储系统。 </li><li>CRUSH算法的查找函数需要进行O(log n)的二分查找，以找到与给定对象ID最接近的虚拟ID。这个计算对于系统中的每个对象都需要进行，因此在系统中有大量对象时，计算成本会很高。 </li><li>CRUSH算法在重建过程中可能会出现瓶颈，因为它需要在placement groups中进行数据放置，这可能会导致数据重建速度变慢。</li><li>CRUSH算法的计算复杂度较高，需要进行大量的计算，这可能会影响系统的性能。 综上所述，CRUSH算法虽然是一种灵活的对象放置算法，但它也存在一些缺陷，需要进一步改进和优化。</li></ol><p>由于CRUSH算法的计算复杂度较高，需要进行大量的计算，因此使用多线程来加速计算是一种可行的方法。具体来说，可以将CRUSH算法的计算任务分配给多个线程，每个线程负责计算一部分任务，然后将结果合并起来。这样可以充分利用多核处理器的计算能力，提高计算效率。但是，需要注意的是，多线程计算也会带来一些额外的开销，如线程间的同步和通信开销，因此需要进行合理的线程调度和优化，以达到最佳的性能提升效果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;分布式存储系统的数据分布算法要解决数据如何分布到集群中的各个节点和磁盘上，其面临： 数据分布和负载均衡、灵活应对集群伸缩、大规模集群计算速率三方面的挑战。 &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据分布和负载均衡：数据分布均衡，使数据能均匀地分布在各个节点和磁盘上，使数据访问的负载在</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph_Bufferlist的设计与使用</title>
    <link href="https://watsonlu6.github.io/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>https://watsonlu6.github.io/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8/</id>
    <published>2021-07-14T05:19:06.000Z</published>
    <updated>2024-07-27T14:37:12.169Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ceph-Bufferlist的设计与使用"><a href="#Ceph-Bufferlist的设计与使用" class="headerlink" title="Ceph Bufferlist的设计与使用"></a>Ceph Bufferlist的设计与使用</h2><p>做为主要和磁盘、网络打交道的分布式存储系统，序列化是最基础的功能之一。当一个结构通过网络发送或写入磁盘时，它被编码为一串字节。可序列化结构具encode 和 decode方法，将结构体<strong>序列化</strong>后存入bufferlist和从bufferlist读出字节串<strong>反序列化</strong>出结构体。bufferlist是ceph的底层组件，用于存储二进制数据，其存储的数据可以直接写入磁盘，在代码中有很广泛的使用。</p><p><strong>为什么要用bufferlist？</strong></p><p>为了免拷贝。发送数据时，传统的socket接口通常需要读取一段连续的内存。但是我们要发的数据内存不连续，所以以前的做法是申请一块大的内存，然后将不连续的内存内的数据拷贝到大内存块中，然后将大内存块地址给发送接口。但是找一块连续的大内存并不容易，系统可能会为此做各种腾挪操作，而将数据拷贝的大内存中，又是一个拷贝操作。RDMA的发送支持聚散表，不需要读取连续的内存。有bufferlist之后，我们可以通过bufferlist，将不连续的物理内存管理起来，形成一段“连续”的虚拟内存，然后将bufferlist的内存指针传递给聚散表，再把聚散表交给RDMA 发送接口即可。整个过程免去了内存拷贝操作。大大降低了CPU的消耗。</p><p>在ceph中经常需要将一个bufferlist编码(encode)到另一个bufferlist中，例如在msg发送消息的时候，通常msg拿到的osd等逻辑层传递给它的bufferlist，然后msg还需要给这个bufferlist加上消息头和消息尾，而消息头和消息尾也是用bufferlist表示的。这时候，msg通常会构造一个空的bufferlist，然后将消息头、消息尾、内容都encode到这个空的bufferlist。而bufferlist之间的encode实际只需要做ptr的copy，而不涉及到系统内存的申请和copy，效率较高。</p><p>补充：</p><ol><li>传统内存访问需要通过CPU进行数据copy来移动数据，通过CPU将内存中的Buffer1移动到Buffer2中。</li><li>DMA(直接内存访问)是一种能力，允许在计算机主板上的设备直接把数据发送到内存中去，数据搬运不需要CPU的参与。</li><li>DMA模式：可以同DMA Engine之间通过硬件将数据从Buffer1移动到Buffer2，而不需要操作系统CPU的参与，大大降低了CPU Copy的开销。</li><li>RDMA是一种概念，在两个或者多个计算机进行通讯的时候使用DMA， 从一个主机的内存直接访问另一个主机的内存。RDMA是一种新的直接内存访问技术，RDMA让计算机可以直接存取其他计算机的内存，而不需要经过处理器的处理。RDMA将数据从一个系统快速移动到远程系统的内存中，而不对操作系统造成任何影响。</li></ol><h2 id="bufferlist的设计"><a href="#bufferlist的设计" class="headerlink" title="bufferlist的设计"></a>bufferlist的设计</h2><p>Bufferlist负责管理Ceph中所有的内存。整个Ceph中所有涉及到内存的操作，无论是msg分配内存接收消息，还是OSD构造各类数据结构的持久化表示（encode&#x2F;decode），再到实际磁盘操作，都将bufferlist作为基础。bufferlist对应的类为buffer::list(using bufferlist &#x3D; buffer::list;)，而buffer::list又基于buffer::ptr和buffer::raw实现，探讨buffer::list的实现，不能跳过它们。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> ceph &#123;</span><br><span class="line">    <span class="keyword">namespace</span> buffer &#123;</span><br><span class="line">    <span class="keyword">inline</span> <span class="keyword">namespace</span> v14_2_0 &#123;</span><br><span class="line">        <span class="keyword">class</span> <span class="title class_">ptr</span>;</span><br><span class="line">        <span class="keyword">class</span> <span class="title class_">list</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">hash</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">using</span> bufferptr = buffer::ptr;</span><br><span class="line">    <span class="keyword">using</span> bufferlist = buffer::list;</span><br><span class="line">    <span class="keyword">using</span> bufferhash = buffer::hash;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ceph::buffer是ceph非常底层的实现，负责管理ceph的内存。ceph::buffer的设计较为复杂，但本身没有任何内容，主要包含buffer::list、 buffer::ptr、 buffer::raw、 buffer::hash。这三个类都定义在src&#x2F;include&#x2F;buffer.h和src&#x2F;common&#x2F;buffer.cc中。</p><ol><li>buffer::raw：负责维护物理内存的引用计数nref和释放操作。</li><li>buffer::ptr：指向buffer::raw的指针。</li><li>buffer::list：表示一个ptr的列表（std::list<bufferptr>），相当于将N个ptr构成一个更大的虚拟的连续内存。</li><li>buffer::hash：一个或多个bufferlist的有效哈希。</li></ol><p>buffer这三个类的相互关系可以用下面这个图来表示：<br><img src="/images/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8_1.png"><br>图中蓝色的表示bufferlist，橙色表示bufferptr，绿色表示bufferraw。</p><pre><code>在这个图中，实际占用的系统内存一共就三段，分别是raw0，raw1和raw2代表的三段内存。raw0被ptr0，ptr1，ptr2使用raw1被ptr3，ptr4，ptr6使用raw2被ptr5，ptr7使用而list0是由ptr0-5组成的，list1是由ptr6和ptr7组成的。</code></pre><p>从这张图上我们就可以看出bufferlist的设计思路： </p><ul><li>对于bufferlist来说，仅关心一个个ptr。bufferlist将ptr连在一起，当做是一段连续的内存使用。因此，可以通过bufferlist::iterator一个字节一个字节的迭代整个bufferlist中的所有内容，而不需要关心到底有几个ptr，更不用关心这些ptr到底和系统内存是怎么对应的；也可以通过bufferlist::write_file方法直接将bufferlist中的内容出到一个文件中；或者通过bufferlist::write_fd方法将bufferlist中的内容写入到某个fd中。</li><li>bufferraw负责管理系统内存的，bufferraw只关心一件事：维护其所管理的系统内存的引用计数，并且在引用计数减为0时——即没有ptr再使用这块内存时，释放这块内存。</li><li>bufferptr负责连接bufferlist和bufferraw。bufferptr关心的是如何使用内存。每一个bufferptr一定有一个bufferraw为其提供系统内存，然后ptr决定使用这块内存的哪一部分。bufferlist只用通过ptr才能对应到系统内存中，而bufferptr而可以独立存在，只是大部分ptr还是为bufferlist服务的，独立的ptr使用的场景并不是很多。<br><img src="/images/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8_2.png"><br>通过引入ptr这样一个中间层次，bufferlist使用内存的方式可以非常灵活。</li></ul><ol><li>快速encode&#x2F;decode。在Ceph中经常需要将一个bufferlist编码（encode）到另一个bufferlist中，例如在msg发送消息的时候，通常msg拿到的osd等逻辑层传递给它的bufferlist，然后msg还需要给这个bufferlist加上消息头和消息尾，而消息头和消息尾也是用bufferlist表示的。这时候，msg通常会构造一个空的bufferlist，然后将消息头、消息尾、内容都encode到这个空的bufferlist。而bufferlist之间的encode实际只需要做ptr的copy，而不涉及到系统内存的申请和Copy，效率较高。</li><li>一次分配，多次使用。调用malloc之类的函数申请内存是非常重量级的操作。利用ptr这个中间层可以缓解这个问题，可以一次性申请一块较大的内存，也就是一个较大的bufferraw，然后每次需要内存的时候，构造一个bufferptr，指向这个bufferraw的不同部分。这样就不再需要向系统申请内存了。最后将这些ptr都加入到一个bufferlist中，就可以形成一个虚拟的连续内存。</li><li>减少内存分配次数和碎片。利用bufferptr这个中间层进行内存的多次使用，多个bufferptr可以引用同一段bufferraw的不同区域，这个bufferraw可以预先一次性申请较大一段连续内存，从而避免了多次申请内存以及内存碎片的产生。</li></ol><h4 id="buffer-raw"><a href="#buffer-raw" class="headerlink" title="buffer::raw"></a>buffer::raw</h4><p>raw的数据成员部分代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">buffer</span>::raw</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">char</span> *data;    <span class="comment">//数据指针</span></span><br><span class="line">    <span class="type">unsigned</span> len;     <span class="comment">//数据长度</span></span><br><span class="line">    std::atomic&lt;<span class="type">unsigned</span>&gt; nref&#123;<span class="number">0</span>&#125;;      <span class="comment">//引用计数</span></span><br><span class="line">    <span class="type">int</span> mempool; </span><br><span class="line">    <span class="keyword">mutable</span> ceph::spinlock crc_spinlock;     <span class="comment">//读写锁</span></span><br><span class="line">    map&lt;pair&lt;<span class="type">size_t</span>, <span class="type">size_t</span>&gt;, pair&lt;<span class="type">uint32_t</span>, <span class="type">uint32_t</span>&gt;&gt; crc_map;    <span class="comment">//crc校验信息</span></span><br><span class="line">    ......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>最基本的成员：data是指向具体数据的指针，len是数据的长度，nref是引用计数。而mempool是其对应的内存池的index，这个和data空间的分配有关，暂时不去管它。</p><p>data指向的数据有很多来源，直接通过malloc从内存分配只是最基础的一种，可能还来自mmap内存映射的空间，甚至可以通过pipe管道＋splice实现零拷贝获取空间。有些时候，分配的空间时，会提出对齐的要求，比如按页对齐等等。对于每一种数据来源，需要不同逻辑的数据分配和释放函数，所以raw对应了很多子类，分别表示不同的数据。</p><p>下列类都继承了buffer::raw，实现了对data对应内存空间的申请</p><ol><li>类raw_malloc实现了用malloc函数分配内存空间的功能</li><li>类class buffer::raw_mmap_pages实现了通过mmap来把内存匿名映射到进程的地址空间</li><li>类class buffer::raw_posix_aligned调用了函数posix_memalign来申请内存地址对齐的内存空间。</li><li>类class buffer::raw_hack_aligned是在系统不支持内存对齐申请的情况下自己实现了内存地址的对齐</li><li>类class buffer::raw_pipe实现了pipe做为Buffer的内存空间</li><li>类class buffer::raw_char使用了C++的new操作符来申请空间</li></ol><p>这是因为这些来源不同，要求不同，buffer::raw也就有了一些变体，举个例子，对应于malloc的raw子类为buffer::raw_malloc，构造和析构函数中实现了使用malloc进行数据分配和释放的逻辑：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">buffer</span>::raw_malloc : <span class="keyword">public</span> buffer::raw</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MEMPOOL_CLASS_HELPERS</span>();</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">raw_malloc</span><span class="params">(<span class="type">unsigned</span> l)</span> : raw(l)</span></span><br><span class="line"><span class="function">    &#123;</span></span><br><span class="line">    <span class="keyword">if</span> (len)</span><br><span class="line">    &#123;</span><br><span class="line">        data = (<span class="type">char</span> *)<span class="built_in">malloc</span>(len);</span><br><span class="line">        <span class="keyword">if</span> (!data)</span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">bad_alloc</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        data = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">inc_total_alloc</span>(len);</span><br><span class="line">    <span class="built_in">inc_history_alloc</span>(len);</span><br><span class="line">    bdout &lt;&lt; <span class="string">&quot;raw_malloc &quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="string">&quot; alloc &quot;</span> &lt;&lt; (<span class="type">void</span> *)data &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; l &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; buffer::<span class="built_in">get_total_alloc</span>() &lt;&lt; bendl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">raw_malloc</span>(<span class="type">unsigned</span> l, <span class="type">char</span> *b) : <span class="built_in">raw</span>(b, l)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="built_in">inc_total_alloc</span>(len);</span><br><span class="line">    bdout &lt;&lt; <span class="string">&quot;raw_malloc &quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="string">&quot; alloc &quot;</span> &lt;&lt; (<span class="type">void</span> *)data &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; l &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; buffer::<span class="built_in">get_total_alloc</span>() &lt;&lt; bendl;</span><br><span class="line">    &#125;</span><br><span class="line">    ~<span class="built_in">raw_malloc</span>() <span class="keyword">override</span></span><br><span class="line">    &#123;</span><br><span class="line">    <span class="built_in">free</span>(data);</span><br><span class="line">    <span class="built_in">dec_total_alloc</span>(len);</span><br><span class="line">    bdout &lt;&lt; <span class="string">&quot;raw_malloc &quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="string">&quot; free &quot;</span> &lt;&lt; (<span class="type">void</span> *)data &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; buffer::<span class="built_in">get_total_alloc</span>() &lt;&lt; bendl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">raw *<span class="title">clone_empty</span><span class="params">()</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">raw_malloc</span>(len);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>对应于malloc的raw子类为buffer::raw_mmap_pages，顾名思义，也能够猜到，这个数据的来源是通过mmap分配的匿名内存映射。因此析构的时候，毫不意外，掉用munmap解除映射，归还空间给系统：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">buffer</span>::raw_mmap_pages : <span class="keyword">public</span> buffer::raw &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">raw_mmap_pages</span><span class="params">(<span class="type">unsigned</span> l)</span> : raw(l) &#123;</span></span><br><span class="line">        data = (<span class="type">char</span>*)::<span class="built_in">mmap</span>(<span class="literal">NULL</span>, len, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANON, <span class="number">-1</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (!data)</span><br><span class="line">            <span class="keyword">throw</span> <span class="built_in">bad_alloc</span>();</span><br><span class="line">        <span class="built_in">inc_total_alloc</span>(len);</span><br><span class="line">        <span class="built_in">inc_history_alloc</span>(len);</span><br><span class="line">        bdout &lt;&lt; <span class="string">&quot;raw_mmap &quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="string">&quot; alloc &quot;</span> &lt;&lt; (<span class="type">void</span> *)data &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; l &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; buffer::<span class="built_in">get_total_alloc</span>() &lt;&lt; bendl;</span><br><span class="line">    &#125;</span><br><span class="line">    ~<span class="built_in">raw_mmap_pages</span>() &#123;</span><br><span class="line">        ::<span class="built_in">munmap</span>(data, len);</span><br><span class="line">        <span class="built_in">dec_total_alloc</span>(len);</span><br><span class="line">        bdout &lt;&lt; <span class="string">&quot;raw_mmap &quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="string">&quot; free &quot;</span> &lt;&lt; (<span class="type">void</span> *)data &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; buffer::<span class="built_in">get_total_alloc</span>() &lt;&lt; bendl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">raw* <span class="title">clone_empty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">raw_mmap_pages</span>(len);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="buffer-ptr"><a href="#buffer-ptr" class="headerlink" title="buffer::ptr"></a>buffer::ptr</h3><p>buffer::ptr是在buffer::raw系列的基础上，这个类也别名bufferptr， 这个类是raw这个类的包装升级版本，它的_raw就是指向buffer::raw类型的变量。成员部分如下（include&#x2F;buffer.h）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CEPH_BUFFER_API</span> ptr</span><br><span class="line">&#123;</span><br><span class="line">    raw *_raw;</span><br><span class="line">    <span class="type">unsigned</span> _off, _len;</span><br><span class="line">    ......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>类buffer::ptr就是对于buffer::raw的一部分数据段，ptr是raw里的一个任意的数据段，_off是在_raw里的偏移量，_len是在ptr的长度。<br><img src="/images/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8_3.png"><br>raw是真正存储数据的地方，而ptr只是指向某个raw中的一段的指针。其数据成员 _raw为指向raw的指针，_off表示数据起始偏移，_len表示数据长度。这边还有提一下ptr的append函数，直观上ptr不应该提供append函数，事实上ptr的append确实很局限，只有当ptr对应的raw区域后方有空闲空间的时候，才能append成功，至于空间不够的情况，应该是交给list等高层类来处理。代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> buffer::ptr::<span class="built_in">append</span>(<span class="type">const</span> <span class="type">char</span> *p, <span class="type">unsigned</span> l)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">assert</span>(_raw);</span><br><span class="line">    <span class="built_in">assert</span>(l &lt;= <span class="built_in">unused_tail_length</span>());</span><br><span class="line">    <span class="type">char</span> *c = _raw-&gt;data + _off + _len;</span><br><span class="line">    <span class="built_in">maybe_inline_memcpy</span>(c, p, l, <span class="number">32</span>);</span><br><span class="line">    _len += l;</span><br><span class="line">    <span class="keyword">return</span> _len + _off;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>buffer::ptr其他常见操作</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">buffer::ptr&amp; buffer::ptr::<span class="keyword">operator</span>= (<span class="type">const</span> ptr&amp; p)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (p._raw) &#123;</span><br><span class="line">        p._raw-&gt;nref.<span class="built_in">inc</span>();</span><br><span class="line">        bdout &lt;&lt; <span class="string">&quot;ptr &quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="string">&quot; get &quot;</span> &lt;&lt; _raw &lt;&lt; bendl;</span><br><span class="line">    &#125;</span><br><span class="line">    buffer::raw *raw = p._raw; </span><br><span class="line">    <span class="built_in">release</span>();</span><br><span class="line">    <span class="keyword">if</span> (raw) &#123;</span><br><span class="line">        _raw = raw;</span><br><span class="line">        _off = p._off;</span><br><span class="line">        _len = p._len;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        _off = _len = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">buffer::raw *buffer::ptr::<span class="built_in">clone</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> _raw-&gt;<span class="built_in">clone</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> buffer::ptr::<span class="built_in">swap</span>(ptr&amp; other)</span><br><span class="line">&#123;</span><br><span class="line">    raw *r = _raw;</span><br><span class="line">    <span class="type">unsigned</span> o = _off;</span><br><span class="line">    <span class="type">unsigned</span> l = _len;</span><br><span class="line">    _raw = other._raw;</span><br><span class="line">    _off = other._off;</span><br><span class="line">    _len = other._len;</span><br><span class="line">    other._raw = r;</span><br><span class="line">    other._off = o;</span><br><span class="line">    other._len = l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">char</span>&amp; buffer::ptr::<span class="keyword">operator</span>[](<span class="type">unsigned</span> n) <span class="type">const</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">assert</span>(_raw);</span><br><span class="line">    <span class="built_in">assert</span>(n &lt; _len);</span><br><span class="line">    <span class="keyword">return</span> _raw-&gt;<span class="built_in">get_data</span>()[_off + n];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">char</span>&amp; buffer::ptr::<span class="keyword">operator</span>[](<span class="type">unsigned</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">assert</span>(_raw);</span><br><span class="line">    <span class="built_in">assert</span>(n &lt; _len);</span><br><span class="line">    <span class="keyword">return</span> _raw-&gt;<span class="built_in">get_data</span>()[_off + n];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> buffer::ptr::<span class="built_in">cmp</span>(<span class="type">const</span> ptr&amp; o) <span class="type">const</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> l = _len &lt; o._len ? _len : o._len;</span><br><span class="line">    <span class="keyword">if</span> (l) &#123;</span><br><span class="line">        <span class="type">int</span> r = <span class="built_in">memcmp</span>(<span class="built_in">c_str</span>(), o.<span class="built_in">c_str</span>(), l);</span><br><span class="line">        <span class="keyword">if</span> (r)</span><br><span class="line">            <span class="keyword">return</span> r;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (_len &lt; o._len)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">if</span> (_len &gt; o._len)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;                 </span><br></pre></td></tr></table></figure><h3 id="buffer-list"><a href="#buffer-list" class="headerlink" title="buffer::list"></a>buffer::list</h3><p>类buffer::list是一个使用广泛的类，它是多个buffer::ptr的列表，也就是多个内存数据段的列表。多个bufferptr形成一个list，这就是bufferlist。简单来说，list就是一个ptr组成的链表：（include&#x2F;buffer.h）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CEPH_BUFFER_API</span> list</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// my private bits</span></span><br><span class="line">std::list&lt;ptr&gt; _buffers;    <span class="comment">//所有的ptr</span></span><br><span class="line"><span class="type">unsigned</span> _len;       <span class="comment">//所有的ptr的数据总长度</span></span><br><span class="line"><span class="type">unsigned</span> _memcopy_count; <span class="comment">//当调用函数rebuild用来内存对齐时，需要内存拷贝的数据量</span></span><br><span class="line">ptr append_buffer;       <span class="comment">// 当有小的数据就添加到这个buffer里</span></span><br><span class="line">    <span class="keyword">mutable</span> iterator last_p;       <span class="comment">//访问list的迭代器</span></span><br><span class="line">......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><img src="/images/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8_4.png"><br>buffers是一个ptr的链表，_len是整个_buffers中所有的ptr的数据的总长度，_memcopy_count用于统计memcopy的字节数，append_buffer是用于优化append操作的缓冲区，可以看出bufferlist将数据以不连续链表的方式存储。</p><h3 id="bufferlist的迭代器"><a href="#bufferlist的迭代器" class="headerlink" title="bufferlist的迭代器"></a>bufferlist的迭代器</h3><p>迭代器中提供的seek(unsigned o)和advance(int o)等函数中的o都是指bufferlist的偏移，而不是单个ptr内的偏移。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">bool</span> is_const&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CEPH_BUFFER_API</span> iterator_impl</span><br><span class="line">    : <span class="keyword">public</span> std::iterator&lt;std::forward_iterator_tag, <span class="type">char</span>&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">bl_t</span> *bl;</span><br><span class="line">    <span class="type">list_t</span> *ls;   <span class="comment">// meh.. just here to avoid an extra pointer dereference..</span></span><br><span class="line">    <span class="type">unsigned</span> off; <span class="comment">// in bl</span></span><br><span class="line">    <span class="type">list_iter_t</span> p;</span><br><span class="line">    <span class="type">unsigned</span> p_off; <span class="comment">// in *p</span></span><br><span class="line">    ......</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其数据成员的含义如下：</p><ul><li>bl：指针，指向bufferlist</li><li>ls：指针，指向bufferlist的成员 _buffers</li><li>p: 类型是std::list::iterator，用来迭代遍历bufferlist中的bufferptr</li><li>p_off：当前位置在对应的bufferptr中的偏移量</li><li>off：当前位置在整个bufferlist中的偏移量</li></ul><h3 id="bufferlist常用函数"><a href="#bufferlist常用函数" class="headerlink" title="bufferlist常用函数"></a>bufferlist常用函数</h3><p>librados只给出bufferlist API</p><ol><li>clear()<br> 清空bufferlist中的内容</li><li>push_front(raw* &#x2F; ptr &amp;)<br>push_back(raw* &#x2F; ptr &amp;)<br> 在_buffers的前面或后面增加新的ptr</li><li>rebuild()<br>rebuild(ptr &amp;nb)<br> 将bufferlist中buffers链表中所有的ptr中的数据存到一个ptr中，并将_buffers原有数据clear，然后将新的单个ptr push到_buffers中。<br> 带参数时使用参数传入的ptr作为目标ptr，不带参数时自己创建一个ptr。</li><li>claim(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT);<br> 将bl的数据拿过来，替换原有的数据。调用后bl数据被清空。</li><li>claim_append(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT);<br>claim_prepend(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT);<br> 将bl的数据拿过来，splice到_buffers的尾部&#x2F;头部。</li><li>append(…)<br> 将数据追加到_buffers尾部，已有ptr空间不够时，会自动分配新的ptr。</li><li>splice(unsigned off, unsigned len, list *claim_by &#x3D; 0)            bl.splice(10,10,&amp;bl2);<br> 将_buffers中总偏移off处长度为len的数据，move到claim_by对应的bufferlist的尾部。注意是move不是copy。</li><li>write(int off, int len, std::ostream &amp;out)<br> 将_buffers中总偏移量off处长度为len的数据，写入到ostream。注意是copy，不是move。</li><li>push_front(ptr&amp; pb)<br> 添加一个ptr到list头部</li><li>push_front(raw *r)<br>添加一个raw到list头部中，先构造一个ptr，后添加list中</li><li>is_aligned(align)<br>判断内存是否以参数align对齐，每一个ptr都必须以align对齐</li><li>read_fd()&#x2F;write_fd()<br>把数据写入文件描述符或者从文件描述符读取数据</li><li>read_file()&#x2F;write_file()<br>把数据写入文件或从文件读取数据的功能</li><li>write_stream()</li></ol><p>内存对齐：有些情况下，需要内存地址对齐，例如当以directIO方式写入数据至磁盘时，需要内存地址按照内存页面大小（page）对齐，也即buffer::list的内存地址都需按照page对齐。函数rebuild用来完成对齐的功能。其实现的方法也比较简单，检查没有对齐的ptr，申请一块新对齐的内存，把数据拷贝过去，释放内存空间就可以了。</p><p>相关链接：<br>    <a href="http://bean-li.github.io/bufferlist-in-ceph/">http://bean-li.github.io/bufferlist-in-ceph/</a><br>    <a href="https://www.jianshu.com/p/01e1f4e398df">https://www.jianshu.com/p/01e1f4e398df</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Ceph-Bufferlist的设计与使用&quot;&gt;&lt;a href=&quot;#Ceph-Bufferlist的设计与使用&quot; class=&quot;headerlink&quot; title=&quot;Ceph Bufferlist的设计与使用&quot;&gt;&lt;/a&gt;Ceph Bufferlist的设计与使用&lt;/</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph序列化</title>
    <link href="https://watsonlu6.github.io/Ceph%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>https://watsonlu6.github.io/Ceph%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96/</id>
    <published>2021-07-10T04:43:01.000Z</published>
    <updated>2024-07-27T14:36:56.441Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ceph-数据序列化"><a href="#Ceph-数据序列化" class="headerlink" title="Ceph 数据序列化"></a>Ceph 数据序列化</h2><p>Ceph 作为主要处理磁盘和网络的分布式存储系统，数据序列化是其最基本的功能之一。当一个结构通过网络发送或写入磁盘时，它会被编码为一串字节。可序列化的结构体具有 encode 和 decode 方法，用于将结构体序列化后存入 bufferlist，或从 bufferlist 读取字节串并反序列化为结构体。</p><p>在 Ceph 中，经常需要将一个 bufferlist 编码（encode）到另一个 bufferlist 中。例如，在 msg 发送消息时，msg 通常会接收到由 OSD 等逻辑层传递给它的 bufferlist，然后 msg 需要给这个 bufferlist 添加消息头和消息尾，而消息头和消息尾也是用 bufferlist 表示的。在这种情况下，msg 通常会构造一个空的 bufferlist，然后将消息头、消息尾和内容都编码到这个空的 bufferlist 中。</p><p>在 bufferlist 之间进行编码实际上只需要进行指针的复制，而不涉及系统内存的申请和复制，因此效率较高。encode 和 decode 方法的主要作用是方便 Ceph 不同模块之间的参数传输。</p><p>在Ceph代码中有很多例子，这里有一个例子。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AcmeClass</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> member1;</span><br><span class="line">    std::string member2;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">encode</span><span class="params">(bufferlist &amp;bl)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">ENCODE_START</span>(<span class="number">1</span>, <span class="number">1</span>, bl);</span><br><span class="line">        ::<span class="built_in">encode</span>(member1, bl);</span><br><span class="line">        ::<span class="built_in">encode</span>(member2, bl);</span><br><span class="line">        <span class="built_in">ENCODE_FINISH</span>(bl);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">decode</span><span class="params">(bufferlist::iterator &amp;bl)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">DECODE_START</span>(<span class="number">1</span>, bl);</span><br><span class="line">        ::<span class="built_in">decode</span>(member1, bl);</span><br><span class="line">        ::<span class="built_in">decode</span>(member2, bl);</span><br><span class="line">        <span class="built_in">DECODE_FINISH</span>(bl);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><code>ENCODE_START</code>宏写入标头 说明version和 compat_version（初值均为 1）。每当对encode进行更改时，version就会增加。仅当更改会影响decode时compat_version才会增加  - 比如新结构体只在尾部添加字段，不会影响旧结构体的解析，因此在结构末尾添加字段的更改不需要增加 compat_version。<br><code>DECODE_START</code>宏采用一个参数，指定encode代码可以处理的最新消息版本。这与消息中编码的 compat_version 进行比较，如果消息太新，则会抛出异常。因为对 compat_verison 的更改很少，所以在添加字段时通常不需要担心。</p><h2 id="Ceph序列化的方式"><a href="#Ceph序列化的方式" class="headerlink" title="Ceph序列化的方式"></a>Ceph序列化的方式</h2><p>序列化（在 Ceph 中称为 encode）的目的是将数据结构表示为二进制流，以便通过网络传输或保存在磁盘等存储介质上。其逆过程称为反序列化（在 Ceph 中称为 decode）。例如，对于字符串“abc”，其序列化结果为7个字节（bytes）：03 00 00 00 61 62 63，其中前四个字节（03 00 00 00）表示字符串的长度为3个字符，后三个字节（61 62 63）分别是字符“abc”的 ASCII 码的十六进制表示。Ceph 采用 little-endian 的序列化方式，即低地址存放最低有效字节，因此32位整数0x12345678的序列化结果为78 56 34 12。</p><p>由于序列化在整个 Ceph 系统中是非常基础且常用的功能，Ceph 将其序列化方式设计为统一的结构，即任何支持序列化的数据结构都必须提供一对定义在全局命名空间中的序列化&#x2F;反序列化（encode&#x2F;decode）函数。例如，如果我们定义了一个结构体 inode，就必须在全局命名空间中定义以下两个方法：</p><ol><li><code>encode(struct inode, bufferlist bl);</code></li><li><code>decode(struct inode, bufferlist::iterator bl);</code></li></ol><p>在此基础上，序列化的使用变得非常简单。对于任意可序列化的类型 T 的实例 instance_T，可以通过如下语句将 instance_T 序列化并保存到 bufferlist 类的实例 instance_bufferlist 中。</p><p>bufferlist类（定义于include&#x2F;buffer.h）是ceph核心的缓存类，用于保存序列化结果、数据缓存、网络通信等，能够将bufferlist理解为一个可变长度的char数组。</p><p>如下代码演示了将一个时间戳以及一个inode序列化到一个bufferlist中。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">utime_t</span> timestamp;</span><br><span class="line"><span class="type">inode_t</span> inode;</span><br><span class="line">bufferlist bl;</span><br><span class="line">::<span class="built_in">encode</span>(timetamp, bl)</span><br><span class="line">::<span class="built_in">encode</span>(inode, bl);</span><br></pre></td></tr></table></figure><p>序列化后的数据能够经过反序列化方法读取，例如如下代码片断从一个bufferlist中反序列化一个时间戳和一个inode（前提是该bl中已经被序列化了一个utime_t和一个inode，不然会报错）。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bufferlist::iterator bl;</span><br><span class="line">::<span class="built_in">decode</span>(timetamp, bl)</span><br><span class="line">::<span class="built_in">decode</span>(inode, bl);</span><br></pre></td></tr></table></figure><h2 id="各种数据类型的序列化"><a href="#各种数据类型的序列化" class="headerlink" title="各种数据类型的序列化"></a>各种数据类型的序列化</h2><p>Ceph为其全部用到数据类型提供了序列化方法或反序列化方法，这些数据类型包括了绝大部分<code>基础数据类型（int、bool等）</code>、<code>结构体类型的序列化（ceph_mds_request_head等）</code>、<code>集合类型（vector、list、set、map等）</code>、以及<code>自定义的复杂数据类型（例如表示inode的inode_t等）</code>，如下分别介绍不一样数据类型的序列化实现方式。</p><h4 id="1、基本数据类型的序列化"><a href="#1、基本数据类型的序列化" class="headerlink" title="1、基本数据类型的序列化"></a>1、基本数据类型的序列化</h4><p>基本数据类型的序列化结果基本就是该类型在内存中的表示形式。基本数据类型的序列化方法使用手工编写，定义在include&#x2F;encoding.h中，包括如下类型：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__u8, __s8, <span class="type">char</span>, <span class="type">bool</span></span><br><span class="line">ceph_le64, ceph_le32, ceph_le16,</span><br><span class="line"><span class="type">float</span>, <span class="type">double</span>,</span><br><span class="line"><span class="type">uint64_t</span>, <span class="type">int64_t</span>, <span class="type">uint32_t</span>, <span class="type">int32_t</span>, <span class="type">uint16_t</span>, <span class="type">int16_t</span>,</span><br><span class="line">string, <span class="type">char</span>*</span><br></pre></td></tr></table></figure><p>在手工编写encode方法过程当中，为了不重复代码，借助了WRITE_RAW_ENCODER和WRITE_INTTYPE_ENCODER两个宏。</p><h4 id="2、结构体类型的序列化"><a href="#2、结构体类型的序列化" class="headerlink" title="2、结构体类型的序列化"></a>2、结构体类型的序列化</h4><p>结构体类型的序列化方法与基本数据类型的序列化方法一致，即便用结构体的内存布局做为序列化的形式。在结构体定义完成后，经过调用WRITE_RAW_ENCODER宏函数生成结构体的全局encode方法，例如结构体ceph_mds_request_head相关结构实现以下。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">ceph_mds_request_head</span> &#123;</span><br><span class="line">    __le64 oldest_client_tid;</span><br><span class="line">    __le32 mdsmap_epoch;</span><br><span class="line">    __le32 flags;</span><br><span class="line">    __u8 num_retry, num_fwd;</span><br><span class="line">    __le16 num_releases;</span><br><span class="line">    __le32 op;</span><br><span class="line">    __le32 caller_uid, caller_gid;</span><br><span class="line">    __le64 ino;</span><br><span class="line">&#125; __attribute__ ((packed));</span><br><span class="line"><span class="built_in">WRITE_RAW_ENCODER</span>(ceph_mds_request_head)</span><br></pre></td></tr></table></figure><p>其中：<br>    ceph_mds_request_head结构体定义在include&#x2F;ceph_fs.h . WRITE_RAW_ENCODER(ceph_mds_request_head)语句位于include&#x2F;types.h WRITE_RAW_ENCODER宏函数定义在include&#x2F;encoding.h WRITE_RAW_ENCODER宏函数其实是经过调用encode_raw实现的，而encode_raw调用bufferlist的append的方法，经过内存拷贝，将数据结构放入到bufferlist中。相关代码为：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">encode_raw</span><span class="params">(<span class="type">const</span> T&amp; t, bufferlist&amp; bl)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    bl.<span class="built_in">append</span>((<span class="type">char</span>*)&amp;t, <span class="built_in">sizeof</span>(t));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">decode_raw</span><span class="params">(T&amp; t, bufferlist::iterator &amp;p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    p.<span class="built_in">copy</span>(<span class="built_in">sizeof</span>(t), (<span class="type">char</span>*)&amp;t);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3、集合数据类型的序列化"><a href="#3、集合数据类型的序列化" class="headerlink" title="3、集合数据类型的序列化"></a>3、集合数据类型的序列化</h4><p>集合数据类型序列化的基本思路包括两步：</p><ul><li>序列化集合大小，</li><li>序列化集合内的全部元素</li></ul><p>例如vector<T>&amp; v的序列化方法：其中元素的序列化经过调用该元素的encode方法实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">encode</span><span class="params">(<span class="type">const</span> std::vector&lt;T&gt;&amp; v, bufferlist&amp; bl)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __u32 n = v.<span class="built_in">size</span>();</span><br><span class="line">    <span class="built_in">encode</span>(n, bl);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">typename</span> std::vector&lt;T&gt;::const_iterator p = v.<span class="built_in">begin</span>(); p != v.<span class="built_in">end</span>(); ++p)</span><br><span class="line">    <span class="built_in">encode</span>(*p, bl);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>经常使用集合数据类型的序列化已经由Ceph实现，位于include&#x2F;encoding.h中，包括如下集合类型：pair, triple, list, set, vector, map, multimap, hash_map, hash_set, deque。集合类型的序列化方法皆为基于泛型（模板类）的实现方式，适用于全部泛型派生类。</p><h4 id="4、复杂数据类型的序列化"><a href="#4、复杂数据类型的序列化" class="headerlink" title="4、复杂数据类型的序列化"></a>4、复杂数据类型的序列化</h4><p>除以上两种业务无关的数据类型外，其它数据类型的序列化实现包括两部分： 在类型内部现实encode方法，将类型内部的encode方法重定义为全局方法。如下以utime_t类为例：utime_t内部实现了encode和decode两个方法，WRITE_CLASS_ENCODER宏函数将这两个方法转化为全局方法。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">utime_t</span> &#123;</span><br><span class="line">    <span class="keyword">struct</span> &#123;</span><br><span class="line">    __u32 tv_sec, tv_nsec;</span><br><span class="line">    &#125; tv;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">encode</span><span class="params">(bufferlist &amp;bl)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">    ::<span class="built_in">encode</span>(tv.tv_sec, bl);</span><br><span class="line">    ::<span class="built_in">encode</span>(tv.tv_nsec, bl);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">decode</span><span class="params">(bufferlist::iterator &amp;p)</span> </span>&#123;</span><br><span class="line">    ::<span class="built_in">decode</span>(tv.tv_sec, p);</span><br><span class="line">    ::<span class="built_in">decode</span>(tv.tv_nsec, p);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">WRITE_CLASS_ENCODER</span>(<span class="type">utime_t</span>)</span><br></pre></td></tr></table></figure><p>复杂数据结构内部的encode方法的实现方式一般是调用其内部主要数据结构的encode方法，例如utime_t类的encode方法其实是序列化内部的tv.tv_sec和tv.tv_nsec两个成员。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Ceph-数据序列化&quot;&gt;&lt;a href=&quot;#Ceph-数据序列化&quot; class=&quot;headerlink&quot; title=&quot;Ceph 数据序列化&quot;&gt;&lt;/a&gt;Ceph 数据序列化&lt;/h2&gt;&lt;p&gt;Ceph 作为主要处理磁盘和网络的分布式存储系统，数据序列化是其最基本的功能</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph相关数据结构</title>
    <link href="https://watsonlu6.github.io/Ceph%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>https://watsonlu6.github.io/Ceph%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</id>
    <published>2021-07-02T15:33:02.000Z</published>
    <updated>2024-07-27T14:36:43.411Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ceph-相关数据结构"><a href="#Ceph-相关数据结构" class="headerlink" title="Ceph 相关数据结构"></a>Ceph 相关数据结构</h2><p>要想深入到Ceph的源码底层，就必须对代码通用库里的一些关键，常见的数据结构进行学习，这样才能更好的理解源代码。从最高的逻辑层次为<code>Pool</code>的概念，然后是<code>PG</code>的概念。其次是<code>OSDＭap</code>记录了集群的所有的配置信息。数据结构<code>OSDOp</code>是一个操作上下文的封装。结构<code>object_info_t</code>保存了一个元数据信息和访问信息。对象<code>ObjectState</code>是在<code>object_info_t</code>基础上添加了一些内存的状态信息。<code>SnapSetContext</code>和<code>ObjectContext</code>分别保存了快照和对象上下文相关的信息。<code>Session</code>保存了一个端到端的链接相关的上下文。</p><h3 id="Pool"><a href="#Pool" class="headerlink" title="Pool"></a>Pool</h3><p><code>Pool</code>是整个集群层面定义的一个逻辑的存储池。对一个Pool可以设置相应的数据冗余类型，目前有副本和纠删码两种实现。数据结构pg_pool_t用于保存Pool的相关信息。<br>Pool的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">pg_pool_t</span> &#123;</span><br><span class="line">  <span class="type">static</span> <span class="type">const</span> <span class="type">char</span> *APPLICATION_NAME_CEPHFS;</span><br><span class="line">  <span class="type">static</span> <span class="type">const</span> <span class="type">char</span> *APPLICATION_NAME_RBD;</span><br><span class="line">  <span class="type">static</span> <span class="type">const</span> <span class="type">char</span> *APPLICATION_NAME_RGW;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">enum</span> &#123;</span><br><span class="line">    TYPE_REPLICATED = <span class="number">1</span>,     <span class="comment">// replication  副本   </span></span><br><span class="line">    <span class="comment">//TYPE_RAID4 = 2,   // raid4 (never implemented)   从来没实现的raid4</span></span><br><span class="line">    TYPE_ERASURE = <span class="number">3</span>,      <span class="comment">// erasure-coded   纠删码</span></span><br><span class="line">  &#125;;</span><br><span class="line">  <span class="keyword">enum</span> &#123;</span><br><span class="line">    FLAG_HASHPSPOOL = <span class="number">1</span>&lt;&lt;<span class="number">0</span>, <span class="comment">// hash pg seed and pool together (instead of adding)</span></span><br><span class="line">    FLAG_FULL       = <span class="number">1</span>&lt;&lt;<span class="number">1</span>, <span class="comment">// pool is full</span></span><br><span class="line">    FLAG_EC_OVERWRITES = <span class="number">1</span>&lt;&lt;<span class="number">2</span>, <span class="comment">// enables overwrites, once enabled, cannot be disabled</span></span><br><span class="line">    FLAG_INCOMPLETE_CLONES = <span class="number">1</span>&lt;&lt;<span class="number">3</span>, <span class="comment">// may have incomplete clones (bc we are/were an overlay)</span></span><br><span class="line">    FLAG_NODELETE = <span class="number">1</span>&lt;&lt;<span class="number">4</span>, <span class="comment">// pool can&#x27;t be deleted</span></span><br><span class="line">    FLAG_NOPGCHANGE = <span class="number">1</span>&lt;&lt;<span class="number">5</span>, <span class="comment">// pool&#x27;s pg and pgp num can&#x27;t be changed</span></span><br><span class="line">    FLAG_NOSIZECHANGE = <span class="number">1</span>&lt;&lt;<span class="number">6</span>, <span class="comment">// pool&#x27;s size and min size can&#x27;t be changed</span></span><br><span class="line">    FLAG_WRITE_FADVISE_DONTNEED = <span class="number">1</span>&lt;&lt;<span class="number">7</span>, <span class="comment">// write mode with LIBRADOS_OP_FLAG_FADVISE_DONTNEED</span></span><br><span class="line">    FLAG_NOSCRUB = <span class="number">1</span>&lt;&lt;<span class="number">8</span>, <span class="comment">// block periodic scrub</span></span><br><span class="line">    FLAG_NODEEP_SCRUB = <span class="number">1</span>&lt;&lt;<span class="number">9</span>, <span class="comment">// block periodic deep-scrub</span></span><br><span class="line">    FLAG_FULL_QUOTA = <span class="number">1</span>&lt;&lt;<span class="number">10</span>, <span class="comment">// pool is currently running out of quota, will set FLAG_FULL too</span></span><br><span class="line">    FLAG_NEARFULL = <span class="number">1</span>&lt;&lt;<span class="number">11</span>, <span class="comment">// pool is nearfull</span></span><br><span class="line">    FLAG_BACKFILLFULL = <span class="number">1</span>&lt;&lt;<span class="number">12</span>, <span class="comment">// pool is backfillfull</span></span><br><span class="line">    FLAG_SELFMANAGED_SNAPS = <span class="number">1</span>&lt;&lt;<span class="number">13</span>, <span class="comment">// pool uses selfmanaged snaps</span></span><br><span class="line">    FLAG_POOL_SNAPS = <span class="number">1</span>&lt;&lt;<span class="number">14</span>,        <span class="comment">// pool has pool snaps</span></span><br><span class="line">    FLAG_CREATING = <span class="number">1</span>&lt;&lt;<span class="number">15</span>,          <span class="comment">// initial pool PGs are being created</span></span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="type">utime_t</span> create_time;      <span class="comment">//Pool创建时间</span></span><br><span class="line">  <span class="type">uint64_t</span> flags;           <span class="comment">///&lt; FLAG_*   Pool的相关标志</span></span><br><span class="line">  __u8 type;                <span class="comment">///&lt; TYPE_*   类型</span></span><br><span class="line">  __u8 size, min_size;     <span class="comment">///&lt;Pool的size和min_size，即副本数和至少保证的副本数</span></span><br><span class="line">  __u8 crush_rule;          <span class="comment">///&lt; crush placement rule    rule的编号</span></span><br><span class="line">  __u8 object_hash;         <span class="comment">///&lt; hash mapping object name to ps   对象映射的hash函数</span></span><br><span class="line">  __u8 pg_autoscale_mode;   <span class="comment">///&lt; PG_AUTOSCALE_MODE_        PG数自动增减模式</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  __u32 pg_num = <span class="number">0</span>, pgp_num = <span class="number">0</span>;  <span class="comment">///&lt; pg、pgp的数量</span></span><br><span class="line">  __u32 pg_num_pending = <span class="number">0</span>;       <span class="comment">///&lt; pg_num we are about to merge down to</span></span><br><span class="line">  __u32 pg_num_target = <span class="number">0</span>;        <span class="comment">///&lt; pg_num we should converge toward</span></span><br><span class="line">  __u32 pgp_num_target = <span class="number">0</span>;       <span class="comment">///&lt; pgp_num we should converge toward</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  map&lt;string,string&gt; properties;  <span class="comment">///&lt; OBSOLETE</span></span><br><span class="line">  string erasure_code_profile; <span class="comment">///&lt; name of the erasure code profile in OSDMap</span></span><br><span class="line">  <span class="type">epoch_t</span> last_change;      <span class="comment">///&lt; most recent epoch changed, exclusing snapshot changes</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/// last epoch that forced clients to resend</span></span><br><span class="line">  <span class="type">epoch_t</span> last_force_op_resend = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">/// last epoch that forced clients to resend (pre-nautilus clients only)</span></span><br><span class="line">  <span class="type">epoch_t</span> last_force_op_resend_prenautilus = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">/// last epoch that forced clients to resend (pre-luminous clients only)</span></span><br><span class="line">  <span class="type">epoch_t</span> last_force_op_resend_preluminous = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/// metadata for the most recent PG merge</span></span><br><span class="line">  <span class="type">pg_merge_meta_t</span> last_pg_merge_meta;</span><br><span class="line">  </span><br><span class="line">  <span class="type">snapid_t</span> snap_seq;        <span class="comment">///&lt; seq for per-pool snapshot</span></span><br><span class="line">  <span class="type">epoch_t</span> snap_epoch;       <span class="comment">///&lt; osdmap epoch of last snap</span></span><br><span class="line">  <span class="type">uint64_t</span> auid;            <span class="comment">///&lt; who owns the pg</span></span><br><span class="line"></span><br><span class="line">  <span class="type">uint64_t</span> quota_max_bytes; <span class="comment">///&lt; maximum number of bytes for this pool</span></span><br><span class="line">  <span class="type">uint64_t</span> quota_max_objects; <span class="comment">///&lt; maximum number of objects for this pool</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * Pool snaps (global to this pool).  These define a SnapContext for</span></span><br><span class="line"><span class="comment">   * the pool, unless the client manually specifies an alternate</span></span><br><span class="line"><span class="comment">   * context.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  map&lt;<span class="type">snapid_t</span>, <span class="type">pool_snap_info_t</span>&gt; snaps;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * Alternatively, if we are defining non-pool snaps (e.g. via the</span></span><br><span class="line"><span class="comment">   * Ceph MDS), we must track @removed_snaps (since @snaps is not</span></span><br><span class="line"><span class="comment">   * used).  Snaps and removed_snaps are to be used exclusive of each</span></span><br><span class="line"><span class="comment">   * other!</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  interval_set&lt;<span class="type">snapid_t</span>&gt; removed_snaps;</span><br><span class="line"></span><br><span class="line">  <span class="type">unsigned</span> pg_num_mask, pgp_num_mask;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Tier cache : Base Storage = N : 1</span></span><br><span class="line">  <span class="comment">// ceph osd tier add &#123;data_pool&#125; &#123;cache pool&#125;</span></span><br><span class="line">  set&lt;<span class="type">uint64_t</span>&gt; tiers;      <span class="comment">///&lt; pools that are tiers of us</span></span><br><span class="line">  <span class="type">int64_t</span> tier_of;         <span class="comment">///&lt; pool for which we are a tier</span></span><br><span class="line">  <span class="comment">// Note that write wins for read+write ops</span></span><br><span class="line">  <span class="comment">// WriteBack mode, read_tier is same as write_tier. Both are cache pool.</span></span><br><span class="line">  <span class="comment">// Diret mode. cache pool is read_tier, not write_tier. </span></span><br><span class="line">  <span class="comment">// ceph osd tier set-overlay &#123;data_pool&#125; &#123;cache_pool&#125;</span></span><br><span class="line">  <span class="type">int64_t</span> read_tier;       <span class="comment">///&lt; pool/tier for objecter to direct reads to</span></span><br><span class="line">  <span class="type">int64_t</span> write_tier;      <span class="comment">///&lt; pool/tier for objecter to direct writes to</span></span><br><span class="line">  <span class="comment">// Set cache mode</span></span><br><span class="line">  <span class="comment">// ceph osd tier cache-mode &#123;cache-pool&#125; &#123;cache-mode&#125;</span></span><br><span class="line">  <span class="type">cache_mode_t</span> cache_mode;  <span class="comment">///&lt; cache pool mode</span></span><br><span class="line"></span><br><span class="line">  <span class="type">uint64_t</span> target_max_bytes;   <span class="comment">///&lt; tiering: target max pool size</span></span><br><span class="line">  <span class="type">uint64_t</span> target_max_objects; <span class="comment">///&lt; tiering: target max pool size</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 目标脏数据率：当脏数据比例达到这个值，后台 agent 开始 flush 数据</span></span><br><span class="line">  <span class="type">uint32_t</span> cache_target_dirty_ratio_micro; <span class="comment">///&lt; cache: fraction of target to leave dirty</span></span><br><span class="line">  <span class="comment">// 高目标脏数据率：当脏数据比例达到这个值，后台 agent 开始高速 flush 数据</span></span><br><span class="line">  <span class="type">uint32_t</span> cache_target_dirty_high_ratio_micro; <span class="comment">///&lt; cache: fraction of  target to flush with high speed</span></span><br><span class="line">  <span class="comment">// 数据满的比率：当数据达到这个比例时，认为数据已满，需要进行缓存淘汰</span></span><br><span class="line">  <span class="type">uint32_t</span> cache_target_full_ratio_micro;  <span class="comment">///&lt; cache: fraction of target to fill before we evict in earnest</span></span><br><span class="line">  <span class="comment">// 对象在 cache 中被刷入到 storage 层的最小时间</span></span><br><span class="line">  <span class="type">uint32_t</span> cache_min_flush_age;  <span class="comment">///&lt; minimum age (seconds) before we can flush</span></span><br><span class="line">  <span class="comment">// 对象在 cache 中被淘汰的最小时间</span></span><br><span class="line">  <span class="type">uint32_t</span> cache_min_evict_age;  <span class="comment">///&lt; minimum age (seconds) before we can evict</span></span><br><span class="line">  <span class="comment">// HitSet 相关参数</span></span><br><span class="line">  HitSet::Params hit_set_params; <span class="comment">///&lt; The HitSet params to use on this pool</span></span><br><span class="line">  <span class="comment">// 每间隔 hit_set_period 一段时间，系统重新产生一个新的 hit_set 对象来记录对象的h缓存统计信息</span></span><br><span class="line">  <span class="type">uint32_t</span> hit_set_period;      <span class="comment">///&lt; periodicity of HitSet segments (seconds)</span></span><br><span class="line">  <span class="comment">// 记录系统保存最近的多少个 hit_set 记录</span></span><br><span class="line">  <span class="type">uint32_t</span> hit_set_count;       <span class="comment">///&lt; number of periods to retain</span></span><br><span class="line">  <span class="comment">// hitset archive 对象的命名规则 </span></span><br><span class="line">  <span class="type">bool</span> use_gmt_hitset;        <span class="comment">///&lt; use gmt to name the hitset archive object</span></span><br><span class="line">  <span class="type">uint32_t</span> min_read_recency_for_promote;   <span class="comment">///&lt; minimum number of HitSet to check before promote on read</span></span><br><span class="line">  <span class="type">uint32_t</span> min_write_recency_for_promote;  <span class="comment">///&lt; minimum number of HitSet to check before promote on write</span></span><br><span class="line">  <span class="type">uint32_t</span> hit_set_grade_decay_rate;   <span class="comment">///&lt; current hit_set has highest priority on objects</span></span><br><span class="line">                                       <span class="comment">///&lt; temperature count,the follow hit_set&#x27;s priority decay </span></span><br><span class="line">                                       <span class="comment">///&lt; by this params than pre hit_set</span></span><br><span class="line">                                       <span class="comment">//当前hit_set在对象温度计数上具有最高优先级，后续hit_set的优先级比预hit_set衰减此参数</span></span><br><span class="line">  <span class="type">uint32_t</span> hit_set_search_last_n;   <span class="comment">///&lt; accumulate atmost N hit_sets for temperature  为温度累积最多N次hit_sets</span></span><br><span class="line"></span><br><span class="line">  <span class="type">uint32_t</span> stripe_width;        <span class="comment">///&lt; erasure coded stripe size in bytes</span></span><br><span class="line"></span><br><span class="line">  <span class="type">uint64_t</span> expected_num_objects; <span class="comment">///&lt; expected number of objects on this pool, a value of 0 indicates</span></span><br><span class="line">                                 <span class="comment">///&lt; user does not specify any expected value</span></span><br><span class="line">  <span class="type">bool</span> fast_read;            <span class="comment">///&lt; whether turn on fast read on the pool or not</span></span><br><span class="line">  <span class="type">pool_opts_t</span> opts; <span class="comment">///&lt; options</span></span><br><span class="line">  <span class="comment">/// application -&gt; key/value metadata</span></span><br><span class="line">  map&lt;string, std::map&lt;string, string&gt;&gt; application_metadata;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  vector&lt;<span class="type">uint32_t</span>&gt; grade_table;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="type">uint32_t</span> <span class="title">get_grade</span><span class="params">(<span class="type">unsigned</span> i)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (grade_table.<span class="built_in">size</span>() &lt;= i)</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> grade_table[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">calc_grade_table</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> v = <span class="number">1000000</span>;</span><br><span class="line">    grade_table.<span class="built_in">resize</span>(hit_set_count);        <span class="comment">// hit_set_count记录系统保存最近的多少个 hit_set 记录</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> i = <span class="number">0</span>; i &lt; hit_set_count; i++) &#123;</span><br><span class="line">      v = v * (<span class="number">1</span> - (hit_set_grade_decay_rate / <span class="number">100.0</span>));</span><br><span class="line">      grade_table[i] = v;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>数据结构pg_pool_t的成员变量和方法较多，不一一介绍了。</p><h2 id="PG"><a href="#PG" class="headerlink" title="PG"></a>PG</h2><p><code>PG</code>可以认为是一组对象的集合，该集合里的对象有共同特征：副本都分布在相同的OSD列表中。结构体pg_t只是一个PG的静态描述信息（只有三个成员变量），类PG及其子类ReplicatedPG都是和PG相关的处理。<br>pg_t的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">pg_t</span> &#123;</span><br><span class="line">  <span class="type">uint64_t</span> m_pool;    <span class="comment">//pg所在的pool</span></span><br><span class="line">  <span class="type">uint32_t</span> m_seed;    <span class="comment">//pg的序号</span></span><br><span class="line">  <span class="type">static</span> <span class="type">const</span> <span class="type">uint8_t</span> calc_name_buf_size = <span class="number">36</span>;  <span class="comment">// max length for max values len(&quot;18446744073709551615.ffffffff&quot;) + future suffix len(&quot;_head&quot;) + &#x27;\0&#x27;</span></span><br><span class="line">  <span class="function"><span class="type">hobject_t</span> <span class="title">get_hobj_start</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">  <span class="function"><span class="type">hobject_t</span> <span class="title">get_hobj_end</span><span class="params">(<span class="type">unsigned</span> pg_num)</span> <span class="type">const</span></span>;</span><br><span class="line">  <span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">generate_test_instances</span><span class="params">(list&lt;<span class="type">pg_t</span>*&gt;&amp; o)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="OSDMap"><a href="#OSDMap" class="headerlink" title="OSDMap"></a>OSDMap</h2><p><code>OSDMap类</code>定义了Ceph整个集群的全局信息。它由Monitor实现管理，并以全量或者增量的方式向整个集群扩散。每一个epoch对应的OSDMap都需要持久化保存在meta下对应对象的omap属性中。内部类Incremental以增量的形式保存了OSDMap新增的信息。OSDMap包含了四类信息：首先是集群的信息，其次是pool的信息，然后是临时PG相关信息，最后就是所有OSD的状态信息。<br>OSDMap类的数据结构如下：（src&#x2F;osd&#x2F;OSDMap.h）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OSDMap</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">MEMPOOL_CLASS_HELPERS</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">typedef</span> interval_set&lt;</span><br><span class="line">    <span class="type">snapid_t</span>,</span><br><span class="line">    mempool::osdmap::flat_map&lt;<span class="type">snapid_t</span>,<span class="type">snapid_t</span>&gt;&gt; <span class="type">snap_interval_set_t</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">Incremental</span> &#123;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MEMPOOL_CLASS_HELPERS</span>();</span><br><span class="line"><span class="comment">//系统相关的信息</span></span><br><span class="line">    <span class="comment">/// feature bits we were encoded with.  the subsequent OSDMap</span></span><br><span class="line">    <span class="comment">/// encoding should match.</span></span><br><span class="line">    <span class="type">uint64_t</span> encode_features;</span><br><span class="line">    uuid_d fsid;    <span class="comment">//当前集群的fsid值</span></span><br><span class="line">    <span class="type">epoch_t</span> epoch; <span class="comment">//当前集群的epoch值 new epoch; we are a diff from epoch-1 to epoch</span></span><br><span class="line">    <span class="type">utime_t</span> modified;   <span class="comment">//创建修改的时间戳</span></span><br><span class="line">    <span class="type">int64_t</span> new_pool_max; <span class="comment">//incremented by the OSDMonitor on each pool create</span></span><br><span class="line">    <span class="type">int32_t</span> new_flags;</span><br><span class="line">    <span class="type">int8_t</span> new_require_osd_release = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// full (rare)</span></span><br><span class="line">    bufferlist fullmap;  <span class="comment">// in lieu of below.</span></span><br><span class="line">    bufferlist crush;</span><br><span class="line">......</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">//集群相关的信息</span></span><br><span class="line">  uuid_d fsid;     <span class="comment">//当前集群的fsid值</span></span><br><span class="line">  <span class="type">epoch_t</span> epoch;        <span class="comment">//当前集群的epoch值 what epoch of the osd cluster descriptor is this</span></span><br><span class="line">  <span class="type">utime_t</span> created, modified; <span class="comment">//创建、修改的时间戳 epoch start time   </span></span><br><span class="line">  <span class="type">int32_t</span> pool_max;     <span class="comment">//最大的pool数量 the largest pool num, ever</span></span><br><span class="line">  <span class="type">uint32_t</span> flags;       <span class="comment">//一些标志信息</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">//OSD相关的信息</span></span><br><span class="line">  <span class="type">int</span> num_osd;         <span class="comment">//OSD的总数量 not saved; see calc_num_osds</span></span><br><span class="line">  <span class="type">int</span> num_up_osd;      <span class="comment">//处于up状态的OSD的数量 not saved; see calc_num_osds</span></span><br><span class="line">  <span class="type">int</span> num_in_osd;      <span class="comment">//处于in状态的OSD的数量 not saved; see calc_num_osds</span></span><br><span class="line">  <span class="type">int32_t</span> max_osd;     <span class="comment">//OSD的最大数目</span></span><br><span class="line">  vector&lt;<span class="type">uint32_t</span>&gt; osd_state;      <span class="comment">//OSD的状态</span></span><br><span class="line">  mempool::osdmap::map&lt;<span class="type">int32_t</span>,<span class="type">uint32_t</span>&gt; crush_node_flags; <span class="comment">// crush node -&gt; CEPH_OSD_* flags</span></span><br><span class="line">  mempool::osdmap::map&lt;<span class="type">int32_t</span>,<span class="type">uint32_t</span>&gt; device_class_flags; <span class="comment">// device class -&gt; CEPH_OSD_* flags</span></span><br><span class="line"></span><br><span class="line">  <span class="type">utime_t</span> last_up_change, last_in_change;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// These features affect OSDMap[::Incremental] encoding, or the</span></span><br><span class="line">  <span class="comment">// encoding of some type embedded therein (CrushWrapper, something</span></span><br><span class="line">  <span class="comment">// from osd_types, etc.).</span></span><br><span class="line">  <span class="type">static</span> <span class="keyword">constexpr</span> <span class="type">uint64_t</span> SIGNIFICANT_FEATURES =</span><br><span class="line">    CEPH_FEATUREMASK_PGID64 |</span><br><span class="line">    CEPH_FEATUREMASK_PGPOOL3 |</span><br><span class="line">    CEPH_FEATUREMASK_OSDENC |</span><br><span class="line">    CEPH_FEATUREMASK_OSDMAP_ENC |</span><br><span class="line">    CEPH_FEATUREMASK_OSD_POOLRESEND |</span><br><span class="line">    CEPH_FEATUREMASK_NEW_OSDOP_ENCODING |</span><br><span class="line">    CEPH_FEATUREMASK_MSG_ADDR2 |</span><br><span class="line">    CEPH_FEATUREMASK_CRUSH_TUNABLES5 |</span><br><span class="line">    CEPH_FEATUREMASK_CRUSH_CHOOSE_ARGS |</span><br><span class="line">    CEPH_FEATUREMASK_SERVER_LUMINOUS |</span><br><span class="line">    CEPH_FEATUREMASK_SERVER_MIMIC |</span><br><span class="line">    CEPH_FEATUREMASK_SERVER_NAUTILUS;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">addrs_s</span> &#123;</span><br><span class="line">    mempool::osdmap::vector&lt;std::shared_ptr&lt;<span class="type">entity_addrvec_t</span>&gt; &gt; client_addrs;</span><br><span class="line">    mempool::osdmap::vector&lt;std::shared_ptr&lt;<span class="type">entity_addrvec_t</span>&gt; &gt; cluster_addrs;</span><br><span class="line">    mempool::osdmap::vector&lt;std::shared_ptr&lt;<span class="type">entity_addrvec_t</span>&gt; &gt; hb_back_addrs;</span><br><span class="line">    mempool::osdmap::vector&lt;std::shared_ptr&lt;<span class="type">entity_addrvec_t</span>&gt; &gt; hb_front_addrs;</span><br><span class="line">  &#125;;</span><br><span class="line">  </span><br><span class="line">  std::shared_ptr&lt;addrs_s&gt; osd_addrs;    <span class="comment">//OSD的地址</span></span><br><span class="line">  <span class="type">entity_addrvec_t</span> _blank_addrvec;</span><br><span class="line">  mempool::osdmap::vector&lt;__u32&gt;   osd_weight;   <span class="comment">//OSD的权重 16.16 fixed point, 0x10000 = &quot;in&quot;, 0 = &quot;out&quot;</span></span><br><span class="line">  mempool::osdmap::vector&lt;<span class="type">osd_info_t</span>&gt; osd_info;    <span class="comment">//OSD 的基本信息</span></span><br><span class="line">  std::shared_ptr&lt; mempool::osdmap::vector&lt;uuid_d&gt; &gt; osd_uuid;  <span class="comment">//OSD对应的uuid</span></span><br><span class="line">  mempool::osdmap::vector&lt;<span class="type">osd_xinfo_t</span>&gt; osd_xinfo;   <span class="comment">//OSD一些扩展信息</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//PG相关的信息</span></span><br><span class="line">  std::shared_ptr&lt;PGTempMap&gt; pg_temp;  <span class="comment">// temp pg mapping (e.g. while we rebuild)</span></span><br><span class="line">  std::shared_ptr&lt; mempool::osdmap::map&lt;<span class="type">pg_t</span>,<span class="type">int32_t</span> &gt; &gt; primary_temp;  <span class="comment">// temp primary mapping (e.g. while we rebuild)</span></span><br><span class="line">  std::shared_ptr&lt; mempool::osdmap::vector&lt;__u32&gt; &gt; osd_primary_affinity; <span class="comment">///&lt; 16.16 fixed point, 0x10000 = baseline</span></span><br><span class="line">  <span class="comment">// remap (post-CRUSH, pre-up)</span></span><br><span class="line">  mempool::osdmap::map&lt;<span class="type">pg_t</span>,mempool::osdmap::vector&lt;<span class="type">int32_t</span>&gt;&gt; pg_upmap; <span class="comment">///&lt; remap pg</span></span><br><span class="line">  mempool::osdmap::map&lt;<span class="type">pg_t</span>,mempool::osdmap::vector&lt;pair&lt;<span class="type">int32_t</span>,<span class="type">int32_t</span>&gt;&gt;&gt; pg_upmap_items; <span class="comment">///&lt; remap osds in up set</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//pool的相关信息</span></span><br><span class="line">  mempool::osdmap::map&lt;<span class="type">int64_t</span>,<span class="type">pg_pool_t</span>&gt; pools;   <span class="comment">//pool的id到pg_pool_t的映射</span></span><br><span class="line">  mempool::osdmap::map&lt;<span class="type">int64_t</span>,string&gt; pool_name;  <span class="comment">//pool的id到pool的名字的映射</span></span><br><span class="line">  mempool::osdmap::map&lt;string,map&lt;string,string&gt; &gt; erasure_code_profiles;    <span class="comment">//pool的EC相关信息</span></span><br><span class="line">  mempool::osdmap::map&lt;string,<span class="type">int64_t</span>&gt; name_pool;  <span class="comment">//pool的名字到pool的id的映射</span></span><br></pre></td></tr></table></figure><h2 id="Op"><a href="#Op" class="headerlink" title="Op"></a>Op</h2><p><code>结构体Op</code>封装了完成一个操作的相关上下文信息，包括target地址信息(op_target_t)、链接信息(session)等</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。</span></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">Op</span> : <span class="keyword">public</span> RefCountedObject &#123;</span><br><span class="line">    OSDSession *session;   <span class="comment">//OSD相关的Session信息 </span></span><br><span class="line">    <span class="type">int</span> incarnation;    <span class="comment">//引用次数</span></span><br><span class="line">    <span class="type">op_target_t</span> target;   <span class="comment">//地址信息</span></span><br><span class="line">    ConnectionRef con;  <span class="comment">// for rx buffer only</span></span><br><span class="line">    <span class="type">uint64_t</span> features;  <span class="comment">// explicitly specified op features</span></span><br><span class="line">    vector&lt;OSDOp&gt; ops;   <span class="comment">// 对应多个操作的封装</span></span><br><span class="line">    <span class="type">snapid_t</span> snapid;     <span class="comment">//快照的ID</span></span><br><span class="line">    SnapContext snapc;   <span class="comment">//pool层级的快照信息</span></span><br><span class="line">    ceph::real_time mtime;</span><br><span class="line">    bufferlist *outbl;    <span class="comment">//输出的bufferlist</span></span><br><span class="line">    vector&lt;bufferlist*&gt; out_bl;     <span class="comment">//每个操作对应的bufferlist</span></span><br><span class="line">    vector&lt;Context*&gt; out_handler;    <span class="comment">//每个操作对应的回调函数</span></span><br><span class="line">    vector&lt;<span class="type">int</span>*&gt; out_rval;     <span class="comment">//每个操作对应的输出结果</span></span><br><span class="line">    <span class="type">int</span> priority;</span><br><span class="line">    Context *onfinish;</span><br><span class="line">    <span class="type">uint64_t</span> ontimeout;</span><br><span class="line">    <span class="type">ceph_tid_t</span> tid;</span><br><span class="line">    <span class="type">int</span> attempts;</span><br><span class="line">    <span class="type">version_t</span> *objver;</span><br><span class="line">    <span class="type">epoch_t</span> *reply_epoch;</span><br><span class="line">    ceph::coarse_mono_time stamp;</span><br><span class="line">    <span class="type">epoch_t</span> map_dne_bound;</span><br><span class="line">    <span class="type">int</span> budget;</span><br><span class="line">    <span class="comment">/// true if we should resend this message on failure</span></span><br><span class="line">    <span class="type">bool</span> should_resend;</span><br><span class="line">    <span class="comment">/// true if the throttle budget is get/put on a series of OPs,</span></span><br><span class="line">    <span class="comment">/// instead of per OP basis, when this flag is set, the budget is</span></span><br><span class="line">    <span class="comment">/// acquired before sending the very first OP of the series and</span></span><br><span class="line">    <span class="comment">/// released upon receiving the last OP reply.</span></span><br><span class="line">    <span class="type">bool</span> ctx_budgeted;</span><br><span class="line">    <span class="type">int</span> *data_offset;</span><br><span class="line"></span><br><span class="line">    <span class="type">osd_reqid_t</span> reqid; <span class="comment">// explicitly setting reqid</span></span><br><span class="line">    ZTracer::Trace trace;</span><br></pre></td></tr></table></figure><h2 id="op-target-t"><a href="#op-target-t" class="headerlink" title="op_target_t"></a>op_target_t</h2><p>数据结构op_target_t封装了对象所在的PG，以及PG对应的OSD列表等地址信息。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//封装了对象所在的PG，以及PG对应的OSD列表等地址信息</span></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">op_target_t</span> &#123;</span><br><span class="line">    <span class="type">int</span> flags = <span class="number">0</span>;    <span class="comment">//标志</span></span><br><span class="line">    <span class="type">epoch_t</span> epoch = <span class="number">0</span>;  <span class="comment">///&lt; latest epoch we calculated the mapping</span></span><br><span class="line">    <span class="type">object_t</span> base_oid;   <span class="comment">//读取的对象</span></span><br><span class="line">    <span class="type">object_locator_t</span> base_oloc;   <span class="comment">//对象的pool信息</span></span><br><span class="line">    <span class="type">object_t</span> target_oid;     <span class="comment">//最终读取的目标对象</span></span><br><span class="line">    <span class="type">object_locator_t</span> target_oloc;   <span class="comment">//最终目标对象的pool信息</span></span><br><span class="line">    <span class="comment">///&lt; true if we are directed at base_pgid, not base_oid</span></span><br><span class="line">    <span class="type">bool</span> precalc_pgid = <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">///&lt; true if we have ever mapped to a valid pool</span></span><br><span class="line">    <span class="type">bool</span> pool_ever_existed = <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">///&lt; explcit pg target, if any</span></span><br><span class="line">    <span class="type">pg_t</span> base_pgid;</span><br><span class="line">    <span class="type">pg_t</span> pgid; <span class="comment">///&lt; last (raw) pg we mapped to</span></span><br><span class="line">    <span class="type">spg_t</span> actual_pgid; <span class="comment">///&lt; last (actual) spg_t we mapped to</span></span><br><span class="line">    <span class="type">unsigned</span> pg_num = <span class="number">0</span>; <span class="comment">///&lt; last pg_num we mapped to</span></span><br><span class="line">    <span class="type">unsigned</span> pg_num_mask = <span class="number">0</span>; <span class="comment">///&lt; last pg_num_mask we mapped to</span></span><br><span class="line">    <span class="type">unsigned</span> pg_num_pending = <span class="number">0</span>; <span class="comment">///&lt; last pg_num we mapped to</span></span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; up; <span class="comment">///&lt; set of up osds for last pg we mapped to</span></span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; acting; <span class="comment">///&lt; set of acting osds for last pg we mapped to</span></span><br><span class="line">    <span class="type">int</span> up_primary = <span class="number">-1</span>; <span class="comment">///&lt; last up_primary we mapped to</span></span><br><span class="line">    <span class="type">int</span> acting_primary = <span class="number">-1</span>;  <span class="comment">///&lt; last acting_primary we mapped to</span></span><br><span class="line">    <span class="type">int</span> size = <span class="number">-1</span>; <span class="comment">///&lt; the size of the pool when were were last mapped</span></span><br><span class="line">    <span class="type">int</span> min_size = <span class="number">-1</span>; <span class="comment">///&lt; the min size of the pool when were were last mapped</span></span><br><span class="line">    <span class="type">bool</span> sort_bitwise = <span class="literal">false</span>; <span class="comment">///&lt; whether the hobject_t sort order is bitwise</span></span><br><span class="line">    <span class="type">bool</span> recovery_deletes = <span class="literal">false</span>; <span class="comment">///&lt; whether the deletes are performed during recovery instead of peering</span></span><br><span class="line">    <span class="type">bool</span> used_replica = <span class="literal">false</span>;</span><br><span class="line">    <span class="type">bool</span> paused = <span class="literal">false</span>;</span><br><span class="line">    <span class="type">int</span> osd = <span class="number">-1</span>;      <span class="comment">///&lt; the final target osd, or -1</span></span><br><span class="line">    <span class="type">epoch_t</span> last_force_resend = <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h2 id="CRUSH-Map"><a href="#CRUSH-Map" class="headerlink" title="CRUSH Map"></a>CRUSH Map</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_rule_step</span> &#123;</span><br><span class="line">__u32 op;    <span class="comment">//操作类型</span></span><br><span class="line">__s32 arg1;   <span class="comment">//操作数1</span></span><br><span class="line">__s32 arg2;    <span class="comment">//操作数2</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">enum</span> <span class="title class_">crush_opcodes</span> &#123;</span><br><span class="line">CRUSH_RULE_NOOP = <span class="number">0</span>,</span><br><span class="line">CRUSH_RULE_TAKE = <span class="number">1</span>,          <span class="comment">/* arg1 = value to start with */</span></span><br><span class="line">CRUSH_RULE_CHOOSE_FIRSTN = <span class="number">2</span>, <span class="comment">/* arg1 = num items to pick */</span> <span class="comment">/* arg2 = type */</span>      </span><br><span class="line">CRUSH_RULE_CHOOSE_INDEP = <span class="number">3</span>,  <span class="comment">/* same */</span></span><br><span class="line">CRUSH_RULE_EMIT = <span class="number">4</span>,          <span class="comment">/* no args */</span></span><br><span class="line">CRUSH_RULE_CHOOSELEAF_FIRSTN = <span class="number">6</span>,</span><br><span class="line">CRUSH_RULE_CHOOSELEAF_INDEP = <span class="number">7</span>,</span><br><span class="line">CRUSH_RULE_SET_CHOOSE_TRIES = <span class="number">8</span>, <span class="comment">/* override choose_total_tries */</span></span><br><span class="line">CRUSH_RULE_SET_CHOOSELEAF_TRIES = <span class="number">9</span>, <span class="comment">/* override chooseleaf_descend_once */</span></span><br><span class="line">CRUSH_RULE_SET_CHOOSE_LOCAL_TRIES = <span class="number">10</span>,</span><br><span class="line">CRUSH_RULE_SET_CHOOSE_LOCAL_FALLBACK_TRIES = <span class="number">11</span>,</span><br><span class="line">CRUSH_RULE_SET_CHOOSELEAF_VARY_R = <span class="number">12</span>,</span><br><span class="line">CRUSH_RULE_SET_CHOOSELEAF_STABLE = <span class="number">13</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 用于指定相对于传递给 do_rule 的 max 参数的选择 num (arg1)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CRUSH_CHOOSE_N            0</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CRUSH_CHOOSE_N_MINUS(x)   (-(x))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 规则掩码用于描述规则的用途。</span></span><br><span class="line"><span class="comment"> * 给定规则集和输出集的大小，我们在规则列表中搜索匹配的 rule_mask。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_rule_mask</span> &#123;</span><br><span class="line">__u8 ruleset;   <span class="comment">//ruleId</span></span><br><span class="line">__u8 type;   <span class="comment">//多副本还是纠删码</span></span><br><span class="line">__u8 min_size;   <span class="comment">//副本数大于等于时适用</span></span><br><span class="line">__u8 max_size;   <span class="comment">//副本数小于等于时适用</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_rule</span> &#123;</span><br><span class="line">__u32 len;   <span class="comment">//steps数组的长度</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_rule_mask</span> mask;   <span class="comment">//releset相关的配置参数</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_rule_step</span> steps[<span class="number">0</span>];   <span class="comment">//step集合</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> crush_rule_size(len) (sizeof(struct crush_rule) + \</span></span><br><span class="line"><span class="meta">      (len)*sizeof(struct crush_rule_step))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * A bucket is a named container of other items (either devices or</span></span><br><span class="line"><span class="comment"> * other buckets).</span></span><br><span class="line"><span class="comment"> * 桶是其他item（设备或其他存储桶）的命名容器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * 使用三种算法中的一种来选择的，这些算法代表了性能和重组效率之间的权衡。 </span></span><br><span class="line"><span class="comment"> * 如果您不确定要使用哪种存储桶类型，我们建议您使用 ::CRUSH_BUCKET_STRAW2。</span></span><br><span class="line"><span class="comment"> * 该表总结了在添加或删除item时每个选项的速度如何与映射稳定性相比较。</span></span><br><span class="line"><span class="comment"> * Bucket Alg     Speed       Additions    Removals</span></span><br><span class="line"><span class="comment"> * ------------------------------------------------</span></span><br><span class="line"><span class="comment"> * uniform         O(1)       poor         poor</span></span><br><span class="line"><span class="comment"> * list            O(n)       optimal      poor</span></span><br><span class="line"><span class="comment"> * straw2          O(n)       optimal      optimal</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">enum</span> <span class="title class_">crush_algorithm</span> &#123;</span><br><span class="line">CRUSH_BUCKET_UNIFORM = <span class="number">1</span>,</span><br><span class="line">CRUSH_BUCKET_LIST = <span class="number">2</span>,</span><br><span class="line">CRUSH_BUCKET_TREE = <span class="number">3</span>,</span><br><span class="line">CRUSH_BUCKET_STRAW = <span class="number">4</span>,</span><br><span class="line">CRUSH_BUCKET_STRAW2 = <span class="number">5</span>,</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">extern</span> <span class="type">const</span> <span class="type">char</span> *<span class="title">crush_bucket_alg_name</span><span class="params">(<span class="type">int</span> alg)</span></span>;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CRUSH_LEGACY_ALLOWED_BUCKET_ALGS (\</span></span><br><span class="line"><span class="meta">(1 &lt;&lt; CRUSH_BUCKET_UNIFORM) |\</span></span><br><span class="line"><span class="meta">(1 &lt;&lt; CRUSH_BUCKET_LIST) |\</span></span><br><span class="line"><span class="meta">(1 &lt;&lt; CRUSH_BUCKET_STRAW))</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket</span> &#123;</span><br><span class="line">__s32 id;       <span class="comment">//bucket的编号。小于0 /*!&lt; bucket identifier, &lt; 0 and unique within a crush_map */</span></span><br><span class="line">__u16 type;      <span class="comment">//bucket的类型/*!&lt; &gt; 0 bucket type, defined by the caller */</span></span><br><span class="line">__u8 alg;        <span class="comment">//使用的crush算法/*!&lt; the item selection ::crush_algorithm */</span></span><br><span class="line">__u8 hash;       <span class="comment">//使用的hash算法/* which hash function to use, CRUSH_HASH_* */</span></span><br><span class="line">__u32 weight;   <span class="comment">//权重 /*!&lt; 16.16 fixed point cumulated children weight */</span></span><br><span class="line">__u32 size;      <span class="comment">//items的数量/*!&lt; size of the __items__ array */</span></span><br><span class="line">    __s32 *items;    <span class="comment">//子bucket/*!&lt; array of children: &lt; 0 are buckets, &gt;= 0 items */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_weight_set</span> &#123;</span><br><span class="line">  __u32 *weights; <span class="comment">/*!&lt; 16.16 fixed point weights in the same order as items */</span></span><br><span class="line">  __u32 size;     <span class="comment">/*!&lt; size of the __weights__ array */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_choose_arg</span> &#123;</span><br><span class="line">  __s32 *ids;                           <span class="comment">/*!&lt; values to use instead of items */</span></span><br><span class="line">  __u32 ids_size;                       <span class="comment">/*!&lt; size of the __ids__ array */</span></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">crush_weight_set</span> *weight_set;  <span class="comment">/*!&lt; weight replacements for a given position */</span></span><br><span class="line">  __u32 weight_set_positions;           <span class="comment">/*!&lt; size of the __weight_set__ array */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_choose_arg_map</span> &#123;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">crush_choose_arg</span> *args; <span class="comment">/*!&lt; replacement for each bucket in the crushmap */</span></span><br><span class="line">  __u32 size;                    <span class="comment">/*!&lt; size of the __args__ array */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket_uniform</span> &#123;</span><br><span class="line">       <span class="keyword">struct</span> <span class="title class_">crush_bucket</span> h; <span class="comment">/*!&lt; generic bucket information */</span></span><br><span class="line">__u32 item_weight;  <span class="comment">/*!&lt; 16.16 fixed point weight for each item */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket_list</span> &#123;</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">crush_bucket</span> h; <span class="comment">/*!&lt; generic bucket information */</span></span><br><span class="line">__u32 *item_weights;  <span class="comment">/*!&lt; 16.16 fixed point weight for each item */</span></span><br><span class="line">__u32 *sum_weights;   <span class="comment">/*!&lt; 16.16 fixed point sum of the weights */</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket_tree</span> &#123;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket</span> h;  <span class="comment">/* note: h.size is _tree_ size, not number of</span></span><br><span class="line"><span class="comment">   actual items */</span></span><br><span class="line">__u8 num_nodes;</span><br><span class="line">__u32 *node_weights;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket_straw</span> &#123;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket</span> h;</span><br><span class="line">__u32 *item_weights;   <span class="comment">/* 16-bit fixed point */</span></span><br><span class="line">__u32 *straws;         <span class="comment">/* 16-bit fixed point */</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket_straw2</span> &#123;</span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">crush_bucket</span> h; <span class="comment">/*!&lt; generic bucket information */</span></span><br><span class="line">__.  <span class="comment">/*!&lt; 16.16 fixed point weight for each item */</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_map</span> &#123;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_bucket</span> **buckets;  **类型，所有的bucket都存在这里</span><br><span class="line">        <span class="comment">/*! 一个大小为__max_rules__ 的crush_rule 指针数组。</span></span><br><span class="line"><span class="comment">         * 如果规则被删除，数组的一个元素可能为NULL（没有API 可以这样做，但将来可能会有一个）。 </span></span><br><span class="line"><span class="comment">         * 规则必须使用crunch_add_rule() 添加。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">crush_rule</span> **rules;   <span class="comment">//**类型，多层嵌套的rules</span></span><br><span class="line">    __s32 max_buckets; <span class="comment">/*!&lt; the size of __buckets__ */</span>  <span class="comment">// bucket的总数</span></span><br><span class="line">__u32 max_rules; <span class="comment">/*!&lt; the size of __rules__ */</span>      <span class="comment">// rule的总数</span></span><br><span class="line">__s32 max_devices;   <span class="comment">// osd的总数</span></span><br><span class="line">__u32 choose_local_tries;   <span class="comment">//选择的总次数</span></span><br><span class="line">__u32 choose_local_fallback_tries;  </span><br><span class="line">__u32 choose_total_tries;</span><br><span class="line">__u32 chooseleaf_descend_once;</span><br><span class="line">__u8 chooseleaf_vary_r;</span><br><span class="line">__u8 chooseleaf_stable;</span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">该值是在构建器解码或构建后计算的。 它在此处公开（而不是具有“构建 CRUSH 工作空间”功能），以便调用者可以保留静态缓冲区、在堆栈上分配空间，或者在需要时避免调用堆分配器。 </span></span><br><span class="line"><span class="comment">        工作空间的大小取决于映射，而传递给映射器的临时向量的大小取决于所需结果集的大小。尽管如此，没有什么能阻止调用者在一个膨胀 foop 中分配两个点并传递两个点。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">size_t</span> working_size;</span><br></pre></td></tr></table></figure><p><img src="/images/Ceph%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Ceph-相关数据结构&quot;&gt;&lt;a href=&quot;#Ceph-相关数据结构&quot; class=&quot;headerlink&quot; title=&quot;Ceph 相关数据结构&quot;&gt;&lt;/a&gt;Ceph 相关数据结构&lt;/h2&gt;&lt;p&gt;要想深入到Ceph的源码底层，就必须对代码通用库里的一些关键，常见的</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph源码编译调试</title>
    <link href="https://watsonlu6.github.io/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95/"/>
    <id>https://watsonlu6.github.io/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95/</id>
    <published>2021-06-20T07:24:13.000Z</published>
    <updated>2024-07-27T14:38:39.225Z</updated>
    
    <content type="html"><![CDATA[<p>对于一个ceph开发人员来说编译源码以及打rpm是其必备技能。无论是fix bug还是向社区提交pull request都离不开编译源码。</p><h2 id="编译环境"><a href="#编译环境" class="headerlink" title="编译环境"></a>编译环境</h2><p><strong>环境介绍</strong></p><ul><li>ceph version: N版 14.2.16</li><li>硬件环境：Centos7虚拟机</li></ul><p><strong>网络环境与源加速</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">额外软件源、生成新的缓存</span></span><br><span class="line">yum -y install centos-release-scl</span><br><span class="line">yum -y install epel-release        </span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum list</span><br><span class="line">yum update</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">更换pip源，创建 .pip 目录</span></span><br><span class="line">mkdir ~/.pip                      </span><br><span class="line">cd ~/.pip                                      </span><br><span class="line">vi pip.conf</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">写入以下配置</span></span><br><span class="line">[global]</span><br><span class="line">index-url = https://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">[install]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">配置yum源</span></span><br><span class="line">vim /etc/yum.repos.d/ceph.repo</span><br><span class="line"></span><br><span class="line">[norch]</span><br><span class="line">name=norch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[x86_64]</span><br><span class="line">name=x86 64</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[SRPMS]</span><br><span class="line">name=SRPMS</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[aarch64]</span><br><span class="line">name=aarch64</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/aarch64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br></pre></td></tr></table></figure><p><strong>安装编译环境及依赖包</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">yum -y install rdma-core-devel systemd-devel keyutils-libs-devel openldap-devel leveldb-devel snappy-devel lz4-devel curl-devel nss-devel</span><br><span class="line">yum -y install libzstd zstd gcc cmake make git wget</span><br><span class="line">yum -y install devtoolset-7-gcc devtoolset-7-gcc-c++ devtoolset-7-binutils       # 安装gcc 7.2</span><br><span class="line">scl enable devtoolset-7 bash      #临时生效</span><br><span class="line">source /opt/rh/devtoolset-7/enable</span><br><span class="line">echo &quot;source /opt/rh/devtoolset-7/enable&quot; &gt;&gt;/etc/profile  #长期生效</span><br><span class="line">gcc -v                         #查看环境gcc版本</span><br><span class="line">wget https://github.com/Kitware/CMake/releases/download/v3.18.2/cmake-3.18.2.tar.gz      #安装cmake3</span><br><span class="line">tar -zxvf cmake-3.18.2.tar.gz</span><br><span class="line">cd cmake-3.18.2 </span><br><span class="line">yum -y install ncurses-devel openssl-devel</span><br><span class="line">./bootstrap</span><br><span class="line">gmake &amp;&amp; gmake install</span><br><span class="line">ln -s /usr/local/share/cmake /usr/bin/</span><br><span class="line">cmake -version</span><br></pre></td></tr></table></figure><p><strong>安装 ccache 加速编译</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下载安装包并解压</span></span><br><span class="line">mkdir /home/ccache        </span><br><span class="line">cd /home/ccache</span><br><span class="line">wget https://github.com/ccache/ccache/releases/download/v4.0/ccache-4.0.tar.gz</span><br><span class="line">tar -zxvf ccache-4.0.tar.gz</span><br><span class="line">cd ccache-4.0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编译安装</span></span><br><span class="line">mkdir build     </span><br><span class="line">cd build</span><br><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DZSTD_FROM_INTERNET=ON ..</span><br><span class="line">make -j12</span><br><span class="line">make install</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改配置</span></span><br><span class="line">mkdir -p /root/.config/ccache/          </span><br><span class="line">vi /root/.config/ccache/ccache.conf</span><br><span class="line">max_size = 16G</span><br><span class="line">sloppiness = time_macros</span><br><span class="line">run_second_cpp = true</span><br></pre></td></tr></table></figure><h2 id="编译ceph代码"><a href="#编译ceph代码" class="headerlink" title="编译ceph代码"></a>编译ceph代码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 下载Ceph源码一</span></span></span><br><span class="line">mkdir /home/ceph</span><br><span class="line">cd /home/ceph</span><br><span class="line">git clone git://github.com/ceph/ceph.git       #(git clone https://github.com/ceph/ceph.git)</span><br><span class="line">cd ceph</span><br><span class="line">git checkout nautilus                            #切换分支，这里以 N 版本为例</span><br><span class="line">git submodule update --init --recursive          #进入ceph目录，下载ceph代码依赖</span><br><span class="line">   </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 下载Ceph源码二</span></span></span><br><span class="line">wget https://mirrors.aliyun.com/ceph/debian-nautilus/pool/main/c/ceph/ceph_14.2.22.orig.tar.gz</span><br><span class="line">tar -zxvf ceph_14.2.22.orig.tar.gz</span><br><span class="line">cd ceph_14.2.2</span><br><span class="line"></span><br><span class="line">./install-deps.sh                                #执行依赖安装脚本，ceph 自带的解决依赖的脚本</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 修改cmake参数，因为后面需要使用gdb debug客户端程序，客户端程序会依赖librados库，所以我们必须以debug的模式去编译ceph，否则编译器会优化掉很多参数，导致很多信息缺失，需要修改一下ceph cmake的参数。如图所示</span></span></span><br><span class="line">vim do_cmake.sh    </span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;CMAKE&#125; -DCMAKE_C_FLAGS=<span class="string">&quot;-O0 -g3 -gdwarf-4&quot;</span> -DCMAKE_CXX_FLAGS=<span class="string">&quot;-O0 -g3 -gdwarf-4&quot;</span> -DBOOST_J=$(<span class="built_in">nproc</span>) <span class="variable">$ARGS</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span> ..</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可以看到这里修改了cmake的参数，增加了两个配置项，稍微解释一下</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">CMAKE_C_FLAGS=“-O0 -g3 -gdwarf-4” ： c 语言编译配置</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">CMAKE_CXX_FLAGS=“-O0 -g3 -gdwarf-4” ：c++ 编译配置</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-O0 : 关闭编译器的优化，如果没有，使用GDB追踪程序时，大多数变量被优化,无法显示, 生产环境必须关掉</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-g3 : 意味着会产生大量的调试信息</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-gdwarf-4 : dwarf 是一种调试格式，dwarf-4 版本为4</span></span><br><span class="line">     </span><br><span class="line">./do_cmake.sh -DWITH_MANPAGE=OFF -DWITH_BABELTRACE=OFF -DWITH_MGR_DASHBOARD_FRONTEND=OFF -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_CCACHE=ON --DWITH_PYTHON3=ON --DMGR_PYTHON_VERSION=3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行 cmake，解释一下，DWITH_MGR_DASHBOARD_FRONTEND=OFF 主要是因为 ceph dashboard 用到了一些国外的 nodejs源，国内无法下载，会导致编译失败超时。-DWITH_CCACHE=ON 如果你没有安装 步骤 2-2 的 ccache 的话，可以去掉这个参数。</span></span><br><span class="line">    </span><br><span class="line">cd build</span><br><span class="line">make -j20 #（线程数等于cpu core的2倍，可以提高编译的速度，20核CPU、32G内存的服务器）</span><br></pre></td></tr></table></figure><p>修改do_cmake.sh<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_1.png"><br>编译进度<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_2.png"></p><p>自此已经编译完ceph源代码！</p><h2 id="运行测试集群"><a href="#运行测试集群" class="headerlink" title="运行测试集群"></a>运行测试集群</h2><p>发行版的 ceph 安装包安装的集群默认是没有办法debug调试。这里推荐 ceph 内置的debug调试——vstart，非常方便模仿特殊场景进行debug调试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /home/watson/ceph/build        # 进入build目录</span><br><span class="line">make vstart                       # 编译模拟启动环境（make help 查看有哪些target可以单独编译）</span><br><span class="line">MDS=0 RGW=1 ../src/vstart.sh -d -l -n --bluestore         # (模拟启动，指令前半部分的MDS=0 RGW=1之类的就是设定你想要模拟的集群结构（集群的配置文件在ceph/build/ceph.conf）)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动完成后，可以在模拟集群环境下执行各种 ceph 指令(模拟集群所有的指令都在 build/bin 目录)</span></span><br><span class="line">bin/ceph -s                       # 查看 ceph 集群状态</span><br><span class="line">bin/radosgw-admin user list       # 查看用户</span><br><span class="line">../src/stop.sh                    # 关闭测试集群</span><br></pre></td></tr></table></figure><p>编译vstasrt环境<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_3.png"><br>启动vstart环境<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_4.png"><br>查看 ceph 集群状态<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_5.png"><br>查看Ceph用户<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_6.png"></p><h2 id="运行单元测试用例"><a href="#运行单元测试用例" class="headerlink" title="运行单元测试用例"></a>运行单元测试用例</h2><p>更改了代码准备提交到公司内部repo或者社区repo都需要先执行一下最小测试集，看看自己修改的代码有没有影响到别的模块(社区也会进行同样的测试)。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd build</span><br><span class="line">make                       #修改代码后先编译，可以模块编译</span><br><span class="line">man ctest                  #查看ctest的功能</span><br><span class="line">ctest -j20                 #运行所有测试（使用所有处理器并行）</span><br><span class="line">ctest -R [regex matching test name(s)]                  #运行部分模块测试，使用 -R（正则表达式匹配）</span><br><span class="line">ctest -V -R [regex matching test name(s)]               #使用 -V（详细）标志运行</span><br><span class="line">ctest -j20 -V -R [regex matching test name(s)]          #运行正则表达式匹配的模块测试，显示详细信息，并发进行</span><br></pre></td></tr></table></figure><p>注意：许多从 src&#x2F;test 构建的目标不是使用ctest运行的。以 “unittest” 开头的目标在其中运行make check，因此可以使用运行ctest。以 “ceph_test” 开头的目标不能，应该手动运行。发生故障时，请在 build&#x2F;Testing&#x2F;Temporary 中查找日志。</p><p><strong>开发编译测试过程</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 编写保存源代码</span><br><span class="line">2. make -j20 unittest_crush               #模块编译</span><br><span class="line">3. ctest -j20 -V -R unittest_crush         #模块测试</span><br></pre></td></tr></table></figure><p><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_7.png"></p><h2 id="通过librados客户端调试CRUSH算法"><a href="#通过librados客户端调试CRUSH算法" class="headerlink" title="通过librados客户端调试CRUSH算法"></a>通过librados客户端调试CRUSH算法</h2><p><strong>编写客户端代码</strong><br>调用librados 库写入数据<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_8.png"></p><p><strong>运行librados代码</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install librados2-devel  libradospp   libradosstriper-devel -y  #安装相关开发包（C/C++开发包）</span><br><span class="line">gcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib                         #编译客户端程序 rados_write.c</span><br></pre></td></tr></table></figure><p>这里解释一下gcc 几个参数，首先需要理解的是c程序在编译时依赖的库和运行时依赖库是分开指定的，也就是说，编译的时候使用的库，不一定就是运行时使用的库</p><ul><li>g : 允许gdb调试</li><li>lrados : -l 指定依赖库的名字为rados</li><li>L : 指定编译时依赖库的的路径， 如果不指定将在系统目录下寻找</li><li>o : 编译的二进制文件名</li><li>Wl : 指定编译时参数</li><li>rpath : 指定运行时依赖库的路径， 如果不指定将在系统目录下寻找</li></ul><p><strong>运行客户端程序</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./rados_write</span><br><span class="line">bin/rados ls -p default.rgw.meta                 #在集群中确认一下是否写入数据</span><br></pre></td></tr></table></figure><p>运行rados_write程序<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_9.png"><br>确认写入数据<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_10.png"></p><p>ceph的开发者模式是测试ceph功能和调试代码非常方便的途径，因为集群默认开启了debug模式，所有的日志都会详细的输出，并且为了调试的方便，在正式环境中的多线程多队列，在这都会简化。</p><h2 id="使用GDB调试分析Object至OSD映射"><a href="#使用GDB调试分析Object至OSD映射" class="headerlink" title="使用GDB调试分析Object至OSD映射"></a>使用GDB调试分析Object至OSD映射</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install -y gdb                   #安装gdb</span><br><span class="line">gcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib   #编译客户端程序 rados_write.c</span><br><span class="line">gdb ./rados_write                    #使用gdb 调试 rados_write 程序</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动程序后，需要设置断点，这里选择的是 crush_do_rule 函数，因为这个函数是 object–&gt;到PG 流程的终点</span></span><br><span class="line">b crush_do_rule                      #在crush_do_rule 函数设置断点</span><br><span class="line">bt                                   #查看当前的函数堆栈</span><br></pre></td></tr></table></figure><p>gdb调试raodos_wirte程序<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_11.png"><br>设置调试断点<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_12.png"><br>查看当前函数栈<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_13.png"></p><p>得到的函数流程如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#0   crush_do_rule at /home/watson/ceph/src/crush/mapper.c:904</span><br><span class="line">#1   do_rule at /home/watson/ceph/src/crush/CrushWrapper.h:1570</span><br><span class="line">#2   OSDMap::_pg_to_raw_osds at /home/watson/ceph/src/osd/OSDMap.cc:2340</span><br><span class="line">#3   OSDMap::_pg_to_up_acting_osds at /home/watson/ceph/src/osd/OSDMap.cc:2586</span><br><span class="line">#4   pg_to_up_acting_osds  at /home/watson/ceph/src/osd/OSDMap.h:1209</span><br><span class="line">#5   Objecter::_calc_target at /home/watson/ceph/src/osdc/Objecter.cc:2846</span><br><span class="line">#6   Objecter::_op_submit  at /home/watson/ceph/src/osdc/Objecter.cc:2367</span><br><span class="line">#7   Objecter::_op_submit_with_budget at /home/watson/ceph/src/osdc/Objecter.cc:2284</span><br><span class="line">#8   Objecter::op_submit at /home/watson/ceph/src/osdc/Objecter.cc:2251</span><br><span class="line">#9   librados::IoCtxImpl::operate  at /home/watson/ceph/src/librados/IoCtxImpl.cc:690</span><br><span class="line">#10  librados::IoCtxImpl::write at /home/watson/ceph/src/librados/IoCtxImpl.cc:623</span><br><span class="line">#11  rados_write at /home/watson/ceph/src/librados/librados_c.cc:1133</span><br><span class="line">#12  main at rados_write.c:73</span><br></pre></td></tr></table></figure><p>不关心librados是如何封装请求，只关心object到pg的计算过程，所以这里决定从 Objecter::_calc_target 函数开始debug 整个过程，重新开始，然后再次设置断点。重新开始，计算 object的hash值 ps</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b Objecter::_calc_target        #断点</span><br></pre></td></tr></table></figure><p><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_14.png"></p><p>卡住在断点处，现在我们打开tui模式跟踪代码， <code>crtl + x + a</code> 可以切换到tui界面<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_15.png"><br>这里按 n 逐行debug代码， 这里我想显示打印 pg_pool_t *p 和 op_target_t *t 的信息<br>其中 pg_pool_t 是pool的结构体，包含pool相关的所有信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p *pi                   #查看pi的数据结构</span><br></pre></td></tr></table></figure><p><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_17.png"><br>而 op_target_t 则是整个写入操作封装的结构信息，包含对象的名字，写入pool的id<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_18.png"><br>继续 n 单步调试，这里我们会进去 osdmap-&gt;object_locator_to_pg 函数。然后一步一步调试……<br>object到PG的函数流程图<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_19.jpg"><br>PG映射到OSD函数流程图<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_20.jpg"><br>crush_choose_firstn选择的过程<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_21.png"></p><h2 id="使用VScode远程调试Ceph"><a href="#使用VScode远程调试Ceph" class="headerlink" title="使用VScode远程调试Ceph"></a>使用VScode远程调试Ceph</h2><p>以ceph osd部分为例，为您演示通过第三方社区提供的vscode 编辑软件，对ceph osd进行进行图形化单步调试以及配置操作。vscode是微软公司一个开源的编译器具备轻量的特点，通过插件安装方式提供了丰富的调试功能。通常 Linux环境的c&#x2F;c++软件开发使用GDB进行命令行调试，命令行操方式极其不方便。使用vscode 的图形化界面可替代gdb 命令行 ，整个开发调试过程更加便捷。Ceph源码路径在&#x2F;home&#x2F;watson&#x2F;ceph目录下，其编译运行文件在&#x2F;home&#x2F;watson&#x2F;ceph&#x2F;build&#x2F;bin当中。启动调试前需要停止本地的osd运行服务。<br><strong>下载安装windows的vscode和ssh</strong><br>在以下地址下载vscode:  <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a><br>安装openssh (一般情况不用自己手动安装)如果需要远程开发，Windows机器也需要支持openssh，如果本机没有，会报错。可以到微软官网上下载ssh。<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_22.png"><br>在vscode安装Remote Development和Remote-SSH<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_23.png"><br>在安装完成之后，点击左侧的Remote-SSH选项卡，再将鼠标移向CONNECTIONS栏，点击出现的configure：<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_25.png"><br>填写linux服务器的ssh端口和用户名（如果是默认的22端口可不用填写）<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_26.png"><br>按下ctrl + s 保存 然后连接（&#x2F;home&#x2F;watson&#x2F;ceph&#x2F;）<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_27.png"><br>输入密码，总共有多次输入密码的流程留意窗口变化<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_28.png"><br>打开远程服务器的文件夹<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_29.png"><br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_30.png"></p><p><strong>远程连接遇到的问题以及技巧</strong></p><p>因为ceph工程文件数量众多会出现无法在这个大型工作区中监视文件更改。请按照说明链接来解决此问题的问题。原因：工作区很大并且文件很多，导致VS Code文件观察程序的句柄达到上限。<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_31.png"><br>解决方法：编辑linux服务器中的 &#x2F;etc&#x2F;sysctl.conf；将以下一行添加到文件末尾，可以将限制增加到最大值<br>    <code>fs.inotify.max_user_watches=524288</code></p><p>保存之后终端窗口 输入sysctl -p可解决。<br>远程调试<br>首先前提Linux服务器已经安装了GDB，否则会提示出错。在ceph工程目录下添加launch.json文件。在最左上栏运行(R) -&gt; 添加配置 ，注意一定要在ceph当前工程目录。修改配置launch.json中的program、args选项。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">launch.json</span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="comment">// 使用 IntelliSense 了解相关属性。 </span></span><br><span class="line">    <span class="comment">// 悬停以查看现有属性的描述。</span></span><br><span class="line">    <span class="comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ceph-debug&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/build/bin/unittest_crush&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;-d&quot;</span><span class="punctuation">,</span> <span class="string">&quot;--cluster&quot;</span><span class="punctuation">,</span> <span class="string">&quot;ceph&quot;</span><span class="punctuation">,</span><span class="string">&quot;--id&quot;</span><span class="punctuation">,</span> <span class="string">&quot;0&quot;</span><span class="punctuation">,</span> <span class="string">&quot;--setuser&quot;</span><span class="punctuation">,</span> <span class="string">&quot;root&quot;</span><span class="punctuation">,</span> <span class="string">&quot;--setgroup&quot;</span><span class="punctuation">,</span> <span class="string">&quot;root&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stopAtEntry&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;externalConsole&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;MIMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;setupCommands&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Enable pretty-printing for gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-enable-pretty-printing&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>按照下图点击就可以开始调试之路<br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_31.png"><br><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_32.png"></p><h2 id="报错记录"><a href="#报错记录" class="headerlink" title="报错记录"></a>报错记录</h2><p>报错1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RPC failed; result=35, HTTP code = 0 fatal: The remote end hung up unexpectedly无法克隆 &#x27;https://github.com/xxxx/xxxxxxxx.git&#x27; 到子模组路径 &#x27;xxxxxxxxx&#x27;</span><br><span class="line">解决：</span><br><span class="line">    通过设置Git的http缓存大小，解决了这个问题，在当前工程目录下运行如下命令：</span><br><span class="line">        git config --global http.postBuffer 20M     (如果20M不行就50M)</span><br></pre></td></tr></table></figure><p>报错2</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">编译出现了一个问题，卡在5%Built target rocksdb_ext这里 </span><br><span class="line">原因：国外网络太慢，下载boost_1_72_0.tar.bz2太慢了，换网络或者在先用本地下载再传到服务器上（ceph/build/boost/src目录下）</span><br></pre></td></tr></table></figure><p><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_34.png"></p><p>报错3</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">No Package found for python-scipy</span><br><span class="line">vim ceph.spec.in</span><br></pre></td></tr></table></figure><p><img src="/images/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95_35.png"></p><p>报错4</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&quot;Error: Package: golang-github-prometheus-2.26.1-2.el7.x86_64 (epel) Requires: /usr/bin/systemd-sysusers&quot;, 去掉该需求</span><br><span class="line">vim ~/ceph-14.2.16/ceph.spec.in</span><br><span class="line"># 内容</span><br><span class="line">#BuildRequires:   golang-github-prometheus</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;对于一个ceph开发人员来说编译源码以及打rpm是其必备技能。无论是fix bug还是向社区提交pull request都离不开编译源码。&lt;/p&gt;
&lt;h2 id=&quot;编译环境&quot;&gt;&lt;a href=&quot;#编译环境&quot; class=&quot;headerlink&quot; title=&quot;编译环境&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph_librados_api使用</title>
    <link href="https://watsonlu6.github.io/Ceph_librados_api%E4%BD%BF%E7%94%A8/"/>
    <id>https://watsonlu6.github.io/Ceph_librados_api%E4%BD%BF%E7%94%A8/</id>
    <published>2021-06-18T06:28:31.000Z</published>
    <updated>2024-07-27T14:37:33.694Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Librados-API概述"><a href="#Librados-API概述" class="headerlink" title="Librados API概述"></a>Librados API概述</h1><p>Ceph存储集群提供基本的存储服务，Ceph以独特的方式将对象、块和文件存储集成到一个存储系统中。基于RADOS，可以不限于RESTful或POSIX接口，使用librados API能够创建自定义的Ceph存储集群接口（除了块存储、对象存储和文件系统存储外）。<br>librados API能够与Ceph存储集群中的两种类型的守护进程进行交互：</p><ul><li>Ceph Mon守护进程，维护集群映射的主副本</li><li>Ceph OSD守护进程，它将数据作为对象存储在存储节点上<br><img src="/images/Ceph_Librados_api%E4%BD%BF%E7%94%A8_1.png"><br>要使用 API，您需要一个正在运行的 Ceph 存储集群。（本教程教程使用ceph编译的vstart启动的开发编程环境）<br>编译模拟启动环境<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">make vstart  #模拟启动</span><br><span class="line">MDS=0 RGW=1 ../src/vstart.sh -d -l -n --bluestore  #模拟集群所有的指令都在 build/bin 目录</span><br><span class="line">bin/ceph -s  #查看 ceph 集群状态</span><br><span class="line">../src/stop.sh #停止模拟集群</span><br></pre></td></tr></table></figure></li></ul><h3 id="第-1-步：获取librados"><a href="#第-1-步：获取librados" class="headerlink" title="第 1 步：获取librados"></a>第 1 步：获取librados</h3><p>Ceph客户端应用必须绑定librados才能连接Ceph存储集群。在写使用librados的ceph客户端应用前，要安装librados及其依赖包。librados API本身是用C++实现，也有C、Python、Java和PHP的API。（本教程仅限于librados C&#x2F;C++API）<br>获取C&#x2F;C++的librados</p><ul><li>要在 Debian&#x2F;Ubuntu 发行版上安装C&#x2F;C++ 的librados开发支持文件，执行以下命令：<br>  <code>sudo apt-get install librados-dev</code></li><li>要在 RHEL&#x2F;CentOS 发行版上安装C&#x2F;C++ 的librados开发支持文件，执行以下命令：<br>  <code>sudo yum install librados2-devel</code></li><li>安装librados 后，可以在&#x2F;usr&#x2F;include&#x2F;rados 下找到 C&#x2F;C++所需的头文件<br>  <code>ls /usr/include/rados</code></li></ul><h2 id="第-2-步：配置集群句柄"><a href="#第-2-步：配置集群句柄" class="headerlink" title="第 2 步：配置集群句柄"></a>第 2 步：配置集群句柄</h2><p>一个Ceph客户端，通过librados直接与OSD交互，来存储和取出数据。为了与OSD交互，客户端应用必须直接调用libradosAPI连接一个Ceph Monitor。一旦连接好以后，librados会从Monitor处取回一个Cluster map。当客户端的应用想读或者取数据的时候，它会创建一个I&#x2F;O上下文并且与一个pool绑定。通过这个I&#x2F;O上下文，客户端将Object的名字提供给librados，然后librados会根据Object的名字和Cluster map计算出相应的PG和OSD的位置。然后客户端就可以读或者写数据。客户端的应用无需知道这个集群的拓扑结构。<br><img src="/images/Ceph_Librados_api%E4%BD%BF%E7%94%A8_2.png"><br>Ceph存储集群手柄封装客户端配置，包括：</p><ul><li>基于用户ID的rados_create() 或者基于用户名的rados_create2()(首选) </li><li>cephx认证密钥</li><li>Mon ID和IP地址</li><li>日志记录级别</li><li>调试级别</li></ul><p>因此，Ceph客户端应用程序使用Ceph群集的步骤：</p><ol><li>创建一个集群句柄，客户端应用将使用该句柄连接到存储集群中；</li><li>使用该手柄进行连接。要连接到集群的客户端应用必须提供Mon地址，用户名和认证密钥（默认启用cephx）。<br>提示：与不同的 Ceph 存储集群或与具有不同用户的同一个集群通信需要不同的集群句柄。RADOS 提供了多种设置所需值的方法。对于Mon和加密密钥设置，处理它们的一种简单方法是确保 Ceph 配置文件包含密钥环文件的密钥环路径和至少一个Mon地址（例如mon host）。例如:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">mon host = 192.168.1.1</span><br><span class="line">keyring = /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure></li><li>创建句柄后，读取 Ceph 配置文件来配置句柄。可以将参数传递给客户端应用程序并使用解析命令行参数的函数（例如rados_conf_parse_argv()）或解析 Ceph 环境变量（例如rados_conf_parse_env()）来解析它们。</li><li>连接后，客户端应用程序可以调用仅使用集群句柄影响整个集群的函数。例如，一旦有了集群句柄，就可以：<br> • 获取集群统计信息<br> • 使用池操作（存在、创建、列出、删除）<br> • 获取和设置配置</li></ol><p>Ceph 的强大功能之一是能够绑定到不同的池。每个池可能有不同数量的归置组、对象副本和复制策略。例如，可以将池设置为使用 SSD 存储常用对象的“热”池或使用纠删码的“冷”池。各种语言的librados 绑定的主要区别在于 C 与C++、Java 和 Python 的面向对象绑定之间。面向对象的绑定使用对象来表示集群句柄、IO 上下文、迭代器、异常等。</p><p><strong>C调用librados 示例</strong><br>对于 C，使用管理员用户创建一个简单的集群句柄，配置它并连接到集群如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;rados/librados.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">const</span> <span class="type">char</span>* argv[])</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">rados_t</span> cluster;</span><br><span class="line">        <span class="type">char</span> cluster_name[] = <span class="string">&quot;ceph&quot;</span>;</span><br><span class="line">        <span class="type">char</span> user_name[] = <span class="string">&quot;client.admin&quot;</span>;</span><br><span class="line">        <span class="type">char</span> conf_flie[] = <span class="string">&quot;/home/watson/ceph/build/ceph.conf&quot;</span>;</span><br><span class="line">        <span class="type">uint64_t</span> flags;</span><br><span class="line">        <span class="type">int</span> err;</span><br><span class="line">        err = rados_create2(&amp;cluster,cluster_name,user_name,flags);</span><br><span class="line">        <span class="keyword">if</span>(err &lt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;%s: Couldn&#x27;t create the cluster handle!%s\n&quot;</span>,argv[<span class="number">0</span>],strerror(-err));</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Create a cluster handle!!!\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        err = rados_conf_read_file(cluster,conf_flie);</span><br><span class="line">        <span class="keyword">if</span>(err &lt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;%s: cannot read config file: %s\n&quot;</span>,argv[<span class="number">0</span>],strerror(-err));</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Read the config flie\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        err = rados_conf_parse_argv(cluster,argc,argv);</span><br><span class="line">        <span class="keyword">if</span>(err &lt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;%s: cannot parse command line arguments: %s\n&quot;</span>,argv[<span class="number">0</span>],strerror(-err));</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Read the command line arguments\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        err = rados_connect(cluster);</span><br><span class="line">        <span class="keyword">if</span>(err &lt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">                <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;%s: cannot connect to cluster: %s\n&quot;</span>,argv[<span class="number">0</span>],strerror(-err));</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Connected to the cluster\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用-lrados编译客户端应用代码并链接到librados，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc ceph-client.c -lrados -o ceph-client</span><br></pre></td></tr></table></figure><p>ceph源码开发vstart环境下的编译，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib</span><br></pre></td></tr></table></figure><p><strong>C++调用librados示例</strong><br>Ceph项目在ceph&#x2F;examples&#x2F;librados目录中提供了一个 C++ 示例。对于 C++，使用管理员用户的简单集群句柄需要初始化librados::Rados集群句柄对象</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;rados/librados.hpp&gt;</span></span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    *通过librados::Rados句柄处理整个RADOS系统层面以及pool层面的管理。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">const</span> <span class="type">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    librados::Rados cluster;    <span class="comment">//定义一个操控集群的句柄对象</span></span><br><span class="line">    <span class="type">char</span> cluster_name[] = <span class="string">&quot;ceph&quot;</span>;     <span class="comment">//集群名字</span></span><br><span class="line">    <span class="type">char</span> user_name[] = <span class="string">&quot;client.admin&quot;</span>;   <span class="comment">//集群用户名</span></span><br><span class="line">    <span class="type">char</span> conf_flie[] = <span class="string">&quot;/home/watson/ceph/build/ceph.conf&quot;</span>;   <span class="comment">//集群配置文件</span></span><br><span class="line">    <span class="type">uint64_t</span> flags;</span><br><span class="line">    ret = cluster.<span class="built_in">init2</span>(user_name,cluster_name,flags);      <span class="comment">//初始化句柄对象</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t initialize the cluster handle! error: &quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;Create a cluster handle.&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret = cluster.<span class="built_in">conf_read_file</span>(conf_flie);     <span class="comment">//读配置文件获取Mon的信息</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t read the ceph configuration file! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;Read the ceph configuration file.&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret = cluster.<span class="built_in">conf_parse_argv</span>(argc,argv);   <span class="comment">//解析命令行输入的参数</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t parsed command line options!error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;Parsed command line options.&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret = cluster.<span class="built_in">connect</span>();   <span class="comment">//连接集群</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t connect to cluster! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;Connected to the cluster.&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    cluster.<span class="built_in">pool_create</span>(<span class="string">&quot;testpool&quot;</span>); <span class="comment">//创建存储池</span></span><br><span class="line">    std::list&lt;std::string&gt; poolList; </span><br><span class="line">    cluster.<span class="built_in">pool_list</span>(poolList);   <span class="comment">//获取存储池列表</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> iter : poolList)&#123;</span><br><span class="line">        std::cout&lt;&lt;iter&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译源码，然后，使用-lrados链接librados，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">g++ -g -c ceph-client.cc -o ceph-client.o </span><br><span class="line">g++ -g ceph-client.o -lrados -o ceph-client</span><br></pre></td></tr></table></figure><p>ceph源码开发vstart环境下的编译，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ -g librados_rados.cpp -lrados -L/home/watson/ceph/build/lib -o librados_rados -Wl,-rpath,/home/watson/ceph/build/lib</span><br></pre></td></tr></table></figure><h2 id="第-3-步：创建-I-O-上下文"><a href="#第-3-步：创建-I-O-上下文" class="headerlink" title="第 3 步：创建 I&#x2F;O 上下文"></a>第 3 步：创建 I&#x2F;O 上下文</h2><p>一旦客户端应用程序拥有集群句柄并连接到 Ceph 存储集群，就可以创建 I&#x2F;O 上下文并开始读取和写入数据。I&#x2F;O 上下文将连接绑定到特定池。用户必须具有适当的CAPS权限才能访问指定的池。例如，具有读取权限但没有写入权限的用户将只能读取数据。I&#x2F;O 上下文功能包括：</p><ul><li>写入&#x2F;读取数据和扩展属性</li><li>列出并迭代对象和扩展属性</li><li>快照池、列表快照等<br><img src="/images/Ceph_Librados_api%E4%BD%BF%E7%94%A8_3.png"><br>RADOS 使客户端应用程序能够进行同步和异步交互。一旦应用程序具有 I&#x2F;O 上下文，读&#x2F;写操作只需要知道对象&#x2F;xattr 名称。librados中封装的 CRUSH 算法使用Cluster map来选择合适的 OSD。OSD 守护进程自动处理副本。librados库将对象映射到归置组。以下示例使用默认数据池。但是，也可以使用 API 列出池、确保它们存在或创建和删除池。对于写操作，示例说明了如何使用同步模式。对于读取操作，示例说明了如何使用异步模式。<code>(提示：使用此 API 删除池时要小心。如果删除池，则该池和池中的所有数据都将丢失。)</code><br><strong>C创建Ceph IO上下文示例</strong><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;rados/librados.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">const</span> <span class="type">char</span>* argv[])</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">rados_t</span> cluster;      <span class="comment">//集群句柄</span></span><br><span class="line">    <span class="type">rados_ioctx_t</span> io;     <span class="comment">//io上下文</span></span><br><span class="line">    <span class="type">char</span> cluster_name[] = <span class="string">&quot;ceph&quot;</span>;</span><br><span class="line">    <span class="type">char</span> user_name[] = <span class="string">&quot;client.admin&quot;</span>;</span><br><span class="line">    <span class="type">char</span> conf_flie[] = <span class="string">&quot;/home/watson/ceph/build/ceph.conf&quot;</span>;</span><br><span class="line">    <span class="type">char</span> poolname[] = <span class="string">&quot;testpool&quot;</span>;</span><br><span class="line">    <span class="type">uint64_t</span> flags;</span><br><span class="line">    <span class="type">int</span> err;</span><br><span class="line">    <span class="comment">/*  为了使示例代码更可观性，不对返回值判错，实际应用中需要进行判错，请养成良好习惯！  */</span></span><br><span class="line">    err = rados_create2(&amp;cluster,cluster_name,user_name,flags);</span><br><span class="line">    err = rados_conf_read_file(cluster,conf_flie);</span><br><span class="line">    err = rados_conf_parse_argv(cluster,argc,argv);</span><br><span class="line">    err = rados_connect(cluster);</span><br><span class="line">    <span class="keyword">if</span>(err &lt; <span class="number">0</span>)                     <span class="comment">//检查是否连接到集群上</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;%s: Cannot connect to cluster: %s\n&quot;</span>,argv[<span class="number">0</span>],strerror(-err));</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Connected to the cluster......\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//err = rados_pool_delete(cluster,poolname);</span></span><br><span class="line">    <span class="type">int</span> poolID = rados_pool_lookup(cluster,poolname);  <span class="comment">//通过poolname获取pool的ID，若池不存在返回-ENOENT</span></span><br><span class="line">    <span class="keyword">if</span>(poolID == -ENOENT)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;this pool does not exist,and create the pool...... \n&quot;</span>);</span><br><span class="line">        rados_pool_create(cluster,poolname);</span><br><span class="line">    &#125;</span><br><span class="line">    err = rados_ioctx_create(cluster,poolname,&amp;io);    <span class="comment">//初始化io上下文</span></span><br><span class="line">    <span class="type">char</span> obj_name[] = <span class="string">&quot;obj&quot;</span>;</span><br><span class="line">    <span class="type">char</span> obj_content[] = <span class="string">&quot;Hello librados&quot;</span>;</span><br><span class="line">    err = rados_write(io,obj_name,obj_content,<span class="built_in">strlen</span>(obj_content),<span class="number">0</span>);    <span class="comment">//往集群写入对象</span></span><br><span class="line">    <span class="keyword">if</span>(err == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;rados_write success......\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">char</span> xattr[] = <span class="string">&quot;en_US&quot;</span>;</span><br><span class="line">    err = rados_setxattr(io,obj_name,<span class="string">&quot;lang&quot;</span>,xattr,<span class="number">5</span>);     <span class="comment">//给对象设置属性</span></span><br><span class="line">    <span class="keyword">if</span>(err == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Set object xattr success......\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">rados_completion_t</span> comp;</span><br><span class="line">    err = rados_aio_create_completion(<span class="literal">NULL</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>,&amp;comp);      <span class="comment">//异步读</span></span><br><span class="line">    <span class="type">char</span> read_ret[<span class="number">1024</span>];</span><br><span class="line">    err = rados_aio_read(io,obj_name,comp,read_ret,<span class="keyword">sizeof</span>(read_ret),<span class="number">0</span>);</span><br><span class="line">    rados_aio_wait_for_complete(comp);</span><br><span class="line">    <span class="keyword">if</span>( err == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%s\&#x27;s content is %s\n&quot;</span>,obj_name,read_ret);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;read_aio_read: err\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    rados_aio_release(comp);</span><br><span class="line">    err = rados_read(io,obj_name,read_ret,<span class="keyword">sizeof</span>(read_ret),<span class="number">0</span>);        <span class="comment">//同步读</span></span><br><span class="line">    <span class="keyword">if</span>( err &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%s\&#x27;s content is %s\n&quot;</span>,obj_name,read_ret);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;read_read: err\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">char</span> xattr_ret[<span class="number">100</span>];</span><br><span class="line">    err = rados_getxattr(io,obj_name,<span class="string">&quot;lang&quot;</span>,xattr_ret,<span class="number">6</span>);     <span class="comment">//获取对象属性</span></span><br><span class="line">    <span class="keyword">if</span>( err &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Read %s\&#x27;s xattr \&quot;lang\&quot; is %s\n&quot;</span>,obj_name,xattr_ret);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;rados_getxattr: err\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    err = rados_rmxattr(io,obj_name,<span class="string">&quot;lang&quot;</span>);     <span class="comment">//删除对象属性</span></span><br><span class="line">    err = rados_remove(io,obj_name);     <span class="comment">//删除对象</span></span><br><span class="line">    rados_ioctx_destroy(io);   <span class="comment">//释放io上下文</span></span><br><span class="line">    rados_shutdown(cluster);    <span class="comment">//关闭集群句柄</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><strong>C++创建Ceph IO上下文示例</strong><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;rados/librados.hpp&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">const</span> <span class="type">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    librados::Rados cluster;</span><br><span class="line">    librados::IoCtx io_ctx;</span><br><span class="line">    <span class="type">char</span> cluster_name[] = <span class="string">&quot;ceph&quot;</span>;</span><br><span class="line">    <span class="type">char</span> user_name[] = <span class="string">&quot;client.admin&quot;</span>;</span><br><span class="line">    <span class="type">char</span> conf_flie[] = <span class="string">&quot;/home/watson/ceph/build/ceph.conf&quot;</span>;</span><br><span class="line">    <span class="type">char</span> poolname[] = <span class="string">&quot;testpool&quot;</span>;</span><br><span class="line">    <span class="type">uint64_t</span> flags;</span><br><span class="line">    <span class="type">int</span> ret;</span><br><span class="line">    <span class="comment">/*  为了使示例代码更可观性，不对返回值判错，实际应用中需要进行判错，请养成良好习惯！  */</span></span><br><span class="line">    ret = cluster.<span class="built_in">init2</span>(user_name,cluster_name,flags);</span><br><span class="line">    ret = cluster.<span class="built_in">conf_read_file</span>(conf_flie);</span><br><span class="line">    ret = cluster.<span class="built_in">conf_parse_argv</span>(argc,argv);</span><br><span class="line">    ret = cluster.<span class="built_in">connect</span>();       </span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span> )           <span class="comment">//测试集群连接情况</span></span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t connect to cluster! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;Connected to the cluster.&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> poolID = cluster.<span class="built_in">pool_lookup</span>(poolname);     <span class="comment">//通过pool名检测是否存在pool</span></span><br><span class="line">    <span class="keyword">if</span>(poolID == -ENOENT)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;this pool does not exist,and create the pool...... \n&quot;</span>);</span><br><span class="line">        cluster.<span class="built_in">pool_create</span>(poolname);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;pool &quot;</span>&lt;&lt;poolID&lt;&lt;<span class="string">&quot;  is using......&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret = cluster.<span class="built_in">ioctx_create</span>(poolname,io_ctx);    <span class="comment">//初始化io_ctx</span></span><br><span class="line">    <span class="type">char</span> obj_name[] = <span class="string">&quot;obj&quot;</span>;</span><br><span class="line">    librados::bufferlist bl;</span><br><span class="line">    bl.<span class="built_in">append</span>(<span class="string">&quot;Hello Librados!&quot;</span>);</span><br><span class="line">    ret = io_ctx.<span class="built_in">write_full</span>(obj_name,bl);         <span class="comment">//往集群写入数据</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t write object! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;Write success......&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    librados::bufferlist lang_bl;</span><br><span class="line">    lang_bl.<span class="built_in">append</span>(<span class="string">&quot;en_US&quot;</span>);</span><br><span class="line">    ret = io_ctx.<span class="built_in">setxattr</span>(obj_name,<span class="string">&quot;lang&quot;</span>,lang_bl);          <span class="comment">//给对象设置属性</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t write object xattr! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;<span class="string">&quot;Set xattr success......&quot;</span>&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    librados::bufferlist read_bl;                 <span class="comment">//异步读</span></span><br><span class="line">    <span class="type">int</span> read_len = <span class="number">1024</span>;</span><br><span class="line">    librados::AioCompletion *read_completion = librados::Rados::<span class="built_in">aio_create_completion</span>();</span><br><span class="line">    ret = io_ctx.<span class="built_in">aio_read</span>(obj_name,read_completion,&amp;read_bl,read_len,<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t read object! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    read_completion-&gt;<span class="built_in">wait_for_complete</span>();    <span class="comment">//等待异步完成</span></span><br><span class="line">    ret = read_completion-&gt;<span class="built_in">get_return_value</span>();       <span class="comment">//获取返回值</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t read object! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;read_bl.<span class="built_in">c_str</span>()&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    librados::bufferlist lang_res;</span><br><span class="line">    ret = io_ctx.<span class="built_in">getxattr</span>(obj_name,<span class="string">&quot;lang&quot;</span>,lang_res);       <span class="comment">//获取属性</span></span><br><span class="line">    <span class="keyword">if</span>(ret &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr&lt;&lt;<span class="string">&quot;Couldn&#x27;t read object xattr! error&quot;</span>&lt;&lt;ret&lt;&lt;std::endl;</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        std::cout&lt;&lt;lang_res.<span class="built_in">c_str</span>()&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    ret = io_ctx.<span class="built_in">rmxattr</span>(obj_name,<span class="string">&quot;lang&quot;</span>);     <span class="comment">//删除对象属性</span></span><br><span class="line">    ret = io_ctx.<span class="built_in">remove</span>(obj_name);           <span class="comment">//删除对象</span></span><br><span class="line">    io_ctx.<span class="built_in">close</span>();       <span class="comment">//关闭io</span></span><br><span class="line">    cluster.<span class="built_in">shutdown</span>();      <span class="comment">//关闭集群句柄</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="第-4-步：结束会话"><a href="#第-4-步：结束会话" class="headerlink" title="第 4 步：结束会话"></a>第 4 步：结束会话</h2><p>一旦客户端应用程序完成了 I&#x2F;O 上下文和集群句柄，应用程序应该关闭连接并关闭句柄。对于异步 I&#x2F;O，应用程序还应确保挂起的异步操作已完成。<br><strong>C结束会话示例</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rados_ioctx_destroy(io); </span><br><span class="line">rados_shutdown(cluster);</span><br></pre></td></tr></table></figure><p><strong>C++结束会话示例</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">io_ctx.<span class="built_in">close</span>(); </span><br><span class="line">cluster.<span class="built_in">shutdown</span>();</span><br></pre></td></tr></table></figure><p>补充：查看pool下的object对象 –all 显示所有namespace的object</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rados ls -p pool --all</span><br></pre></td></tr></table></figure><h2 id="LIBRADOS常用接口"><a href="#LIBRADOS常用接口" class="headerlink" title="LIBRADOS常用接口"></a>LIBRADOS常用接口</h2><ol><li><p>集群配置：提供了获取和设置配置值的方法，读取Ceph配置文件，并解析参数。<br> Rados.conf_get(option)<br> Rados.conf_set(option, val)<br> Rados.conf_read_file(path)<br> Rados.conf_parse_argv(args)<br> Rados.version()</p></li><li><p>连接管理：连接到集群、检查集群、检索集群的统计数据，并从集群断开连接。也可以断言集群句柄处于一个特定的状态（例如，”配置”，”连接”等等）。<br> Rados.connect(timeout)<br> Rados.shutdown()<br> Rados.get_fsid()<br> Rados.get_cluster_stats()</p></li><li><p>池操作：列出可用的池，创建一个池，检查一个池是否存在，并删除一个池。<br> Rados.list_pools()<br> Rados.create_pool(pool_name, crush_rule, auid)<br> Rados.pool_exists(pool_name)<br> Rados.delete_pool(pool_name)</p></li><li><p>CLI 命令：Ceph CLI命令在内部使用以下librados Python绑定方法。<br> Rados.mon_command(cmd, inbuf, timeout, target)<br> Rados.osd_command(osdid, cmd, inbuf, timeout)<br> Rados.mgr_command(cmd, inbuf, timeout, target)<br> Rados.pg_command(pgid, cmd, inbuf, timeout)</p></li><li><p>I&#x2F;O上下文：为了将数据写入Ceph对象存储和从Ceph对象存储读取数据，必须创建一个输入&#x2F;输出上下文（ioctx）。Rados类提供了open_ioctx()和open_ioctx2()方法。其余的操作涉及调用Ioctx和其他类的方法。<br> Rados.open_ioctx(ioctx_name)<br> Ioctx.require_ioctx_open()<br> Ioctx.get_stats()<br> Ioctx.get_last_version()<br> Ioctx.close()</p></li><li><p>对象操作：同步或异步地读和写对象。一个对象有一个名称（或键）和数据。<br> Ioctx.aio_write(object_name, to_write, offset, oncomplete, onsafe)<br> Ioctx.aio_write_full(object_name, to_write, oncomplete, onsafe)<br> Ioctx.aio_append(object_name, to_append, oncomplete, onsafe)<br> Ioctx.write(key, data, offset)<br> Ioctx.write_full(key, data)<br> Ioctx.aio_flush()<br> Ioctx.set_locator_key(loc_key)<br> Ioctx.aio_read(object_name, length, offset, oncomplete)<br> Ioctx.read(key, length, offset)<br> Ioctx.stat(key)<br> Ioctx.trunc(key, size)<br> Ioctx.remove_object(key)</p></li><li><p>对象扩展属性：在一个对象上设置扩展属性(XATTRs)。<br> Ioctx.set_xattr(key, xattr_name, xattr_value)<br> Ioctx.get_xattrs(oid)<br> XattrIterator.<strong>next</strong>()<br> Ioctx.get_xattr(key, xattr_name)<br> Ioctx.rm_xattr(key, xattr_name)</p></li><li><p>对象接口：从一个池中检索一个对象的列表，并对它们进行迭代。提供的对象接口使每个对象看起来像一个文件，可以对对象进行同步操作。对于异步操作，应该使用I&#x2F;O上下文的方法。<br> Ioctx.list_objects()<br> ObjectIterator.<strong>next</strong>()<br> Object.read(length&#x3D;1024 * 1024)<br> Object.write(string_to_write)<br> Object.get_xattrs()<br> Object.get_xattr(xattr_name)<br> Object.set_xattr(xattr_name, xattr_value)<br> Object.rm_xattr(xattr_name)<br> Object.stat()<br> Object.remove()</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Librados-API概述&quot;&gt;&lt;a href=&quot;#Librados-API概述&quot; class=&quot;headerlink&quot; title=&quot;Librados API概述&quot;&gt;&lt;/a&gt;Librados API概述&lt;/h1&gt;&lt;p&gt;Ceph存储集群提供基本的存储服务，Ceph</summary>
      
    
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"/>
    
    
    <category term="云存储" scheme="https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"/>
    
    <category term="Ceph" scheme="https://watsonlu6.github.io/tags/Ceph/"/>
    
  </entry>
  
</feed>
