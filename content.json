{"meta":{"title":"watson'blogs","subtitle":"","description":"","author":"John Doe","url":"https://watsonlu6.github.io","root":"/"},"pages":[],"posts":[{"title":"13 virsh使用","slug":"13-virsh使用","date":"2024-04-01T07:50:26.000Z","updated":"2025-04-29T13:42:41.956Z","comments":true,"path":"13-virsh使用/","permalink":"https://watsonlu6.github.io/13-virsh%E4%BD%BF%E7%94%A8/","excerpt":"","text":"1 概述virsh [选项]… [命令字符串] virsh [选项]… 命令 [参数]… 2 描述virsh 程序是用于管理 virsh 客户域的主要接口。该程序可用于创建、暂停和关闭域，还可以用于列出当前域。Libvirt 是一个 C 工具包，用于与 Linux（及其他操作系统）最新版本的虚拟化功能交互。它是在 GNU 宽通用公共许可证下可用的免费软件。Linux 操作系统的虚拟化意味着能够在单个硬件系统上同时运行多个操作系统实例，其中基本资源由 Linux 实例驱动。该库旨在提供长期稳定的 C API。目前支持 Xen、QEMU、KVM、LXC、OpenVZ、VirtualBox 和 VMware ESX。 大多数 virsh 使用的基本结构如下： virsh [选项]… &lt;*命令*&gt; &lt;*域*&gt; [参数]… 其中，命令 是下面列出的命令之一；域 是数字域 ID、域名或域 UUID；参数 是特定于命令的选项。少数例外情况是命令作用于所有域、整个机器或直接作用于 Xen 虚拟机管理程序时。这些例外情况将在相关命令中明确说明。注意：可以为域指定数字名称，但这样做会导致域只能通过域 ID 识别。换句话说，如果提供数字值，它将被解释为域 ID，而不是名称。任何以 # 开头的 命令 将被视为注释并静默忽略，所有其他无法识别的 命令 将被诊断。 virsh 程序可以通过在 shell 命令行中提供命令及其参数来运行单个 命令，也可以通过 命令字符串 运行，命令字符串 是一个由多个 命令 操作及其参数组成的单个 shell 参数，命令之间用空格分隔，并用分号或换行符分隔，其中未引用的反斜杠换行对将被省略。在 命令字符串 中，virsh 理解与 shell 相同的单引号、双引号和反斜杠转义，但在创建单个 shell 参数时必须添加另一层 shell 转义，任何以未引用的 # 开头的单词将开始一个注释，直到换行符结束。如果在命令行中未给出命令，virsh 将启动一个最小的解释器等待输入命令，然后使用 quit 命令退出程序。 virsh 程序支持以下 选项。 -c, --connect URI连接到指定的 URI，类似于 connect 命令，而不是默认连接。 -d, --debug 级别启用整数 级别 及更高级别的调试消息。级别 范围为 0 到 4（默认值）。有关每个 级别 的描述，请参阅下面 VIRSH_DEBUG 环境变量的文档。 -e, --escape 字符串为 console 命令设置替代转义序列。默认使用 telnet 的 ^]。使用脱字符表示法时允许的字符为：字母字符、@、[、]、\\、^、_。 -h, --help忽略所有其他参数，并表现得像给出了 help 命令一样。 -k, --keepalive-interval 间隔设置发送保活消息的 间隔（以秒为单位），以检查与服务器的连接是否仍然存活。将间隔设置为 0 会禁用客户端保活机制。 -K, --keepalive-count 计数设置在服务器未响应的情况下可以发送保活消息的次数，而不会将连接标记为死亡。如果 间隔 设置为 0，则此设置无效。 -l, --log 文件将日志详细信息输出到 文件。 -q, --quiet避免额外的信息性消息。 -r, --readonly使初始连接为只读，类似于 connect 命令的 --readonly 选项。 -t, --timing输出每个命令的耗时信息。 -v, --version[&#x3D;short]忽略所有其他参数，并打印 virsh 所使用的 libvirt 库的版本。 -V, --version&#x3D;long忽略所有其他参数，并打印 virsh 所使用的 libvirt 库的版本以及编译的选项和驱动程序。 3 注意事项大多数 virsh 操作依赖于 libvirt 库能够连接到已运行的 libvirtd 服务。通常可以使用命令 service libvirtd start 完成此操作。 由于与虚拟机管理程序通信所使用的通道，大多数 virsh 命令需要 root 权限才能运行。以非 root 用户身份运行将返回错误。 大多数 virsh 命令是同步执行的，除了 shutdown、setvcpus 和 setmem 等少数命令。在这些情况下，virsh 程序返回并不意味着操作已完成，必须定期轮询以检测客户机是否已完成操作。 virsh 致力于向后兼容。尽管 help 命令仅列出命令的首选用法，但如果旧版本的 virsh 支持命令或选项的替代拼写（例如 --tunnelled 而不是 --tunneled），则使用旧拼写的脚本将继续有效。 一些 virsh 命令接受可选的带单位的整数；如果未提供单位，则命令中会列出默认单位（出于历史原因，某些命令默认为字节，而其他命令默认为千字节）。以下不区分大小写的后缀可用于选择特定单位： b, byte 字节 1KB 千字节 1,000k, KiB 千二进制字节 1,024MB 兆字节 1,000,000M, MiB 兆二进制字节 1,048,576GB 吉字节 1,000,000,000G, GiB 吉二进制字节 1,073,741,824TB 太字节 1,000,000,000,000T, TiB 太二进制字节 1,099,511,627,776PB 拍字节 1,000,000,000,000,000P, PiB 拍二进制字节 1,125,899,906,842,624EB 艾字节 1,000,000,000,000,000,000E, EiB 艾二进制字节 1,152,921,504,606,846,976 4 通用命令以下命令是通用的，即不特定于某个域。 help语法：help [命令或分组]此命令列出每个 virsh 命令。不带选项使用时，所有命令将按相关类别分组显示，每行一个命令，并显示每个分组的关键字。要仅显示特定分组的命令，请将该分组的关键字作为选项。例如： 示例 1： virsh # help host 主机和虚拟机管理程序（帮助关键字 ‘host’）: capabilities 显示功能cpu-models 显示架构的 CPU 型号connect 连接到虚拟机管理程序freecell NUMA 空闲内存hostname 打印虚拟机管理程序主机名qemu-attach 附加到现有 QEMU 进程qemu-monitor-command QEMU 监视器命令qemu-agent-command QEMU 客户机代理命令sysinfo 打印虚拟机管理程序系统信息uri 打印虚拟机管理程序规范 URI 要显示特定命令的详细信息，请将其名称作为选项。例如： 示例 2：virsh # help list名称list - 列出域概要list [–inactive] [–all] 描述返回域列表。 选项–inactive 列出非活动域–all 列出非活动和活动域 quit, exit语法：quitexit退出交互式终端。 version语法：version [–daemon]打印此构建的主要版本信息。如果指定 –daemon，则输出中会包含 libvirt 守护程序的版本。 示例： 123456789101112$ virsh version Compiled against library: libvirt 1.2.3 Using library: libvirt 1.2.3 Using API: QEMU 1.2.3 Running hypervisor: QEMU 2.0.50 $ virsh version --daemon Compiled against library: libvirt 1.2.3 Using library: libvirt 1.2.3 Using API: QEMU 1.2.3 Running hypervisor: QEMU 2.0.50 Running against daemon: 1.2.6 cd语法：cd [目录]将当前目录更改为 目录。cd 命令的默认目录是主目录，如果环境中没有 HOME 变量，则为根目录。 pwd语法：pwd打印当前目录。 connect语法：connect [URI] [–readonly]连接到虚拟机管理程序。当 shell 首次启动时，此命令会自动运行，并使用命令行中 -c 选项请求的 URI 参数。URI 参数指定如何连接到虚拟机管理程序。URI 文档 https://libvirt.org/uri.html 列出了支持的值，但最常见的有： xen:&#x2F;&#x2F;&#x2F;system用于连接到本地 Xen 虚拟机管理程序。 qemu:&#x2F;&#x2F;&#x2F;system以 root 身份连接到本地监督 QEMU 和 KVM 域的守护程序。 qemu:&#x2F;&#x2F;&#x2F;session以普通用户身份连接到其自己的 QEMU 和 KVM 域集。 lxc:&#x2F;&#x2F;&#x2F;system连接到本地 Linux 容器。要查找当前使用的 URI，请查看下面记录的 uri 命令。 对于远程访问，请参阅 URI 文档 https://libvirt.org/uri.html 了解如何创建 URI。*–readonly* 选项允许只读连接。 uri语法：uri打印虚拟机管理程序的规范 URI，在 shell 模式下可能有用。 hostname语法：hostname打印虚拟机管理程序的主机名。 sysinfo语法：sysinfo打印虚拟机管理程序的 XML 表示的系统信息（如果可用）。 nodeinfo语法：nodeinfo返回有关节点的基本信息，例如 CPU 的数量和类型，以及物理内存的大小。强烈不建议使用此命令，因为提供的信息不能保证在所有硬件平台上都准确。CPU 频率 值仅反映机器中第一个 CPU 当前的运行速度。此速度可能因 CPU 而异，并且会随着主机操作系统调整而变化。用于获取数据的数据结构不可扩展，因此仅支持全局节点&#x2F;插槽&#x2F;核心&#x2F;线程（插槽&#x2F;核心&#x2F;线程是每个 NUMA 节点）拓扑信息。如果主机 CPU 有任何进一步的分组（例如 die、集群等）或 NUMA 拓扑不对称，则数据结构无法真实表示系统。在这种情况下，将报告一个伪造的拓扑（节点 &#x3D; 1，插槽 &#x3D; 1，核心 &#x3D; 主机 CPU 数量，线程 &#x3D; 1），仅正确表示主机 CPU 总数。建议使用 capabilities 命令替代，该命令在 &#x2F;capabilities&#x2F;host&#x2F;topology XPath 下报告数据（频率除外）。 nodecpumap语法：nodecpumap [–pretty]显示节点的 CPU 总数、在线 CPU 数量以及在线 CPU 列表。使用 –pretty 时，在线 CPU 将以范围形式打印，而不是列表形式。 nodecpustats语法：nodecpustats [CPU] [–percent]返回节点的 CPU 统计信息。如果指定 CPU，则仅打印指定 CPU 的统计信息。如果指定 –percent，则打印每种 CPU 统计信息在 1 秒内的百分比。 nodememstats语法：nodememstats [单元]返回节点的内存统计信息。如果指定 单元，则仅打印指定单元的统计信息。 nodesevinfo语法：nodesevinfo报告有关节点的 AMD SEV 启动安全功能的信息（如果有）。其中一些信息也会在域功能 XML 文档中报告。 nodesuspend语法：nodesuspend [目标] [持续时间]将节点（主机）置于系统范围的睡眠状态，并安排节点的实时时钟中断在指定的 持续时间 后恢复节点。目标 指定主机将挂起到的状态，可以是 “mem”（挂起到 RAM）、”disk”（挂起到磁盘）或 “hybrid”（挂起到 RAM 和磁盘）。持续时间 指定主机挂起的时间（以秒为单位），应至少为 60 秒。 node-memory-tune语法：node-memory-tune [shm-pages-to-scan] [shm-sleep-millisecs] [shm-merge-across-nodes]允许显示或设置节点内存参数。shm-pages-to-scan 可用于设置共享内存服务在休眠前扫描的页数；shm-sleep-millisecs 可用于设置共享内存服务在下一次扫描前休眠的毫秒数；shm-merge-across-nodes 指定是否可以从不同的 NUMA 节点合并页。设置为 0 时，仅可以合并物理位于同一 NUMA 节点内存区域的页。设置为 1 时，可以合并所有节点的页。默认为 1。注意：当前“共享内存服务”仅指 KSM（内核同页合并）。 capabilities语法：capabilities [–xpath 表达式] [–wrap]打印描述当前连接的虚拟机管理程序功能的 XML 文档。这包括主机在 CPU 和功能方面的能力部分，以及每种可以虚拟化的客户机的描述集。更完整的描述请参阅：https://libvirt.org/formatcaps.htmlXML 还显示 NUMA 拓扑信息（如果可用）。如果 –xpath 参数提供 XPath 表达式，则将对输出 XML 求值，并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但为了方便进一步处理，**–wrap** 参数会将匹配节点包装在公共根节点中。 domcapabilities语法：domcapabilities [virttype] [emulatorbin] [arch] [machine][–xpath 表达式] [–wrap][–disable-deprecated-features]打印描述当前连接的虚拟机管理程序域功能的 XML 文档，这些信息可以来自现有域或 virsh capabilities 输出。如果您打算创建新域并想知道它是否可以通过为特定模拟器和架构创建域来使用 VFIO，这可能很有用。每个虚拟机管理程序对哪些选项是必需的、哪些是可选的会有不同的要求。虚拟机管理程序可以支持为任何选项提供默认值。 virttype 选项指定使用的虚拟化类型。该值可以来自域 XML 中 &lt;domain&#x2F;&gt; 顶级元素的 ‘type’ 属性，也可以来自 virsh capabilities 输出中每个 &lt;guest&#x2F;&gt; 元素的 ‘type’ 属性。emulatorbin 选项指定模拟器的路径。该值可以来自域 XML 中的 &lt;emulator&gt; 元素或 virsh capabilities 输出。arch 选项指定用于域的架构。该值可以来自域 XML 中 &lt;os&#x2F;&gt; 元素和 &lt;type&#x2F;&gt; 子元素的 “arch” 属性，也可以来自 virsh capabilities 输出中 &lt;arch&#x2F;&gt; 元素的 “name” 属性。machine 指定模拟器的机器类型。该值可以来自域 XML 中 &lt;os&#x2F;&gt; 元素和 &lt;type&#x2F;&gt; 子元素的 “machine” 属性，也可以来自 virsh capabilities 输出中特定架构和域类型的机器列表。 对于 QEMU 虚拟机管理程序，必须提供 virttype 为 ‘qemu’ 或 ‘kvm’ 以及 emulatorbin 或 arch 才能为默认 machine 生成输出。提供 machine 值将为特定机器生成输出。 如果 –xpath 参数提供 XPath 表达式，则将对输出 XML 求值，并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但为了方便进一步处理，**–wrap** 参数会将匹配节点包装在公共根节点中。–disable-deprecated-features 参数将修改 host-model CPU XML 的内容，更新功能列表，其中包含虚拟机管理程序标记为已弃用的 CPU 模型的任何功能。这些功能将与 “disable” 策略配对。 pool-capabilities语法：pool-capabilities打印描述连接的存储驱动程序存储池功能的 XML 文档。如果您打算创建新存储池并需要了解可用的池类型、支持的存储池源和目标卷格式以及创建池所需的源元素，这可能很有用。 inject-nmi语法：inject-nmi 域向客户机注入 NMI。 list语法：list [–inactive | –all][–managed-save] [–title]{ [–table] | –name | –id } [–uuid][–persistent] [–transient][–with-managed-save] [–without-managed-save][–autostart] [–no-autostart][–with-snapshot] [–without-snapshot][–with-checkpoint] [–without-checkpoint][–state-running] [–state-paused][–state-shutoff] [–state-other]打印有关现有域的信息。如果未指定选项，则打印有关运行域的信息。 示例 1：列表的示例格式如下： 123456virsh list Id 名称 状态 ---------------------------------- 0 Domain-0 running 2 fedora paused 名称是域的名称。ID 是域的数字 ID。状态是运行状态（见下文）。 状态状态字段列出每个域当前的状态。域可以处于以下可能的状态之一： running域当前正在 CPU 上运行。 idle域处于空闲状态，未运行或不可运行。这可能是由于域正在等待 I&#x2F;O（传统等待状态）或因为没有其他事情可做而进入睡眠状态。 paused域已暂停，通常是由于管理员运行 virsh suspend。在暂停状态下，域仍会消耗分配的资源（如内存），但没有资格由虚拟机管理程序调度。 in shutdown域正在关闭过程中，即客户操作系统已收到通知并应正在优雅地停止其操作。 shut off域未运行。通常这表示域已完全关闭或尚未启动。 crashed域已崩溃，这始终是暴力结束。通常只有在域配置为崩溃时不重启时才会出现此状态。 pmsuspended域已由客户电源管理暂停，例如进入 s3 状态。 通常仅列出活动域。要列出非活动域，请指定 –inactive 或 –all 以同时列出活动和非活动域。 过滤要进一步过滤域列表，可以指定 list 命令支持的一个或多个过滤标志。这些标志按功能分组。从组中指定一个或多个标志会启用过滤组。注意，某些标志组合可能不会产生任何结果。支持的过滤标志和组：–persistent标志 –persistent 用于在返回的列表中包括持久客户机。要包括临时客户机，请指定 –transient。–with-managed-save要列出具有托管保存映像的域，请指定标志 –with-managed-save。对于没有托管保存映像的域，请指定 –without-managed-save。–state-running以下过滤标志按状态选择域：*–state-running* 用于运行域，*–state-paused* 用于暂停域，*–state-shutoff* 用于关闭域，*–state-other* 用于所有其他状态作为后备。–autostart要列出自动启动域，请使用标志 –autostart。要列出禁用此功能的域，请使用 –no-autostart。–with-snapshot可以使用标志 –with-snapshot 列出具有快照映像的域，使用 –without-snapshot 列出没有快照的域。–with-checkpoint可以使用标志 –with-checkpoint 列出具有检查点的域，使用 –without-checkpoint 列出没有检查点的域。 在与旧服务器通信时，此命令被迫使用一系列具有固有竞争的 API 调用，如果在收集列表时域状态在调用之间更改，则可能不会列出域或域可能多次出现。新服务器没有此问题。 如果指定 –managed-save，则具有托管保存状态（仅在域处于关闭状态时可能，因此需要指定 –inactive 或 –all 才能实际列出它们）的域将在列表中显示为已保存。此标志仅可与默认 –table 输出一起使用。注意，此标志不会过滤域列表。 如果指定 –name，则打印域名而不是表格格式，每行一个。如果指定 –uuid，则打印域的 UUID 而不是名称。如果指定 –id，则打印域的 ID 而不是名称。但是，可以组合 –name、*–uuid* 和 –id 以仅选择要打印的所需字段。标志 –table 指定应使用传统的表格格式输出，但它与 –name 和 –id 互斥。这是默认值，如果未指定 –name、*–uuid* 或 –id，则将使用它。如果未指定 –name 或 –uuid，但指定了 –id，则仅列出活动域，即使使用 –all 参数，否则输出将仅包含一堆仅包含 -1 的行。如果 –table 与 –uuid 组合，则域 UUID 将作为额外列打印。 如果指定 –title，则会在额外列中打印域的简短描述（标题）。此标志仅可与默认 –table 输出一起使用。 示例 2： 123456virsh list --title Id 名称 状态 标题 ---------------------------------- 0 Domain-0 running Mailserver 1 2 fedora paused freecell语法：freecell [{ [–cellno] 单元号 | –all }]打印机器或 NUMA 单元中的可用内存量。freecell 命令可以根据指定的选项提供三种不同的机器可用内存显示之一。不带选项时，显示机器上的总空闲内存。使用 –all 选项时，显示每个单元的空闲内存和机器上的总空闲内存。最后，使用数字参数或 –cellno 加单元号时，将仅显示指定单元的空闲内存。 freepages语法：freepages [{ [–cellno] 单元号 [–pagesize] 页大小 | –all }]打印 NUMA 单元中的可用页数。单元号 指您感兴趣的 NUMA 单元。页大小 是一个带单位的整数（参见上面的 NOTES）。或者，如果使用 –all，则打印每个可能的 NUMA 单元和页大小组合的信息。 allocpages语法：allocpages [–pagesize] 页大小 [–pagecount] 页数 [ [–cellno] 单元号] [–add] [–all]更改主机上 页大小 的页池大小。如果指定 –add，则将 页数 页添加到池中。但是，如果未指定 –add，则 页数 将作为池的新绝对大小（这可用于释放一些页并缩小池）。单元号 修饰符可用于将修改范围缩小到单个主机 NUMA 单元。在范围的另一端是 –all，它将在所有 NUMA 单元上执行修改。 cpu-baseline语法：cpu-baseline 文件 [–features] [–migratable]计算基线 CPU，该 CPU 将由 &lt;文件&gt; 中给出的所有主机 CPU 支持。（参见 hypervisor-cpu-baseline 命令以获取可由特定虚拟机管理程序提供的 CPU。）主机 CPU 列表是通过从 &lt;文件&gt; 中提取所有 &lt;cpu&gt; 元素构建的。因此，&lt;文件&gt; 可以包含由换行符分隔的一组 &lt;cpu&gt; 元素，甚至可以包含 capabilities 命令打印的一组完整的 &lt;capabilities&gt; 元素。如果指定 –features，则生成的 XML 描述将显式包括构成 CPU 的所有功能，没有此选项时，属于 CPU 模型的功能将不会在 XML 描述中列出。如果指定 –migratable，则阻止迁移的功能将不会包含在生成的 CPU 中。 cpu-compare语法：cpu-compare 文件 [–error] [–validate]将 XML &lt;文件&gt; 中的 CPU 定义与主机 CPU 进行比较。（参见 hypervisor-cpu-compare 命令以将 CPU 定义与特定虚拟机管理程序在主机上能够提供的 CPU 进行比较。）XML &lt;文件&gt; 可以包含主机或客户机 CPU 定义。主机 CPU 定义是 capabilities 命令打印的 &lt;cpu&gt; 元素及其内容。客户机 CPU 定义是域 XML 定义中的 &lt;cpu&gt; 元素及其内容，或从域功能 XML（由 domcapabilities 命令打印）中找到的主机 CPU 模型创建的 CPU 定义。除了 &lt;cpu&gt; 元素本身外，此命令还接受包含 CPU 定义的完整域 XML、capabilities XML 或域功能 XML。有关客户机 CPU 定义的更多信息，请参阅：https://libvirt.org/formatdomain.html#elementsCPU如果指定 –error，则当给定 CPU 与主机 CPU 不兼容时，命令将返回错误，并打印提供有关不兼容性更多详细信息的消息。如果指定 –validate，则根据内部 RNG 模式验证 XML 文档的格式。 cpu-models语法：cpu-models 架构打印 libvirt 为指定架构已知的 CPU 模型列表。特定虚拟机管理程序是否能够创建使用任何打印的 CPU 模型的域是一个单独的问题，可以通过查看 domcapabilities 命令返回的域功能 XML 来回答。此外，对于某些架构，libvirt 不知道任何 CPU 模型，可用的 CPU 模型仅受虚拟机管理程序的限制。对于这些架构，此命令将打印所有 CPU 模型都被接受，实际支持的 CPU 模型列表可以在域功能 XML 中检查。 hypervisor-cpu-compare语法：hypervisor-cpu-compare 文件 [virttype] [emulator] [arch] [machine] [–error] [–validate]将 XML &lt;文件&gt; 中的 CPU 定义与虚拟机管理程序在主机上能够提供的 CPU 进行比较。（这与 cpu-compare 不同，后者在不考虑任何特定虚拟机管理程序及其能力的情况下将 CPU 定义与主机 CPU 进行比较。） XML 文件 应包含客户机 CPU 定义：域 XML 定义中的 &lt;cpu&gt; 元素及其内容，或从域功能 XML（由 domcapabilities 命令打印）中的 &lt;mode name&#x3D;”host-model”&gt; 元素创建的主机 CPU 模型创建的 CPU 定义。域功能 XML 中的 &lt;mode name&#x3D;”host-model”&gt; 元素本身或其 &lt;cpu&gt; 父元素不被接受。必须将元素转换为实际的 CPU 定义。有关客户机 CPU 定义的更多信息，请参阅： https://libvirt.org/formatdomain.html#elementsCPU或者，此命令在提供完整域或域功能 XML 时将自动提取 CPU 定义。 出于历史原因，XML 文件 也可以包含主机 CPU 定义，但强烈不建议这种用法，因为它很可能会提供不正确的结果。 virttype 选项指定虚拟化类型（可在域 XML 中 &lt;domain&gt; 顶级元素的 ‘type’ 属性中使用）。emulator 指定模拟器的路径，arch 指定 CPU 架构，machine 指定机器类型。如果指定 –error，则当给定 CPU 与主机 CPU 不兼容时，命令将返回错误，并打印提供有关不兼容性更多详细信息的消息。如果指定 –validate，则根据内部 RNG 模式验证 XML 文档的格式。 hypervisor-cpu-baseline语法：hypervisor-cpu-baseline [文件] [virttype] [emulator] [arch] [machine][–features] [–migratable] [模型] 计算基线 CPU，该 CPU 将与 XML 文件 中定义的所有 CPU 以及虚拟机管理程序在主机上能够提供的 CPU 兼容。（这与 cpu-baseline 不同，后者在计算基线 CPU 时不考虑任何虚拟机管理程序的能力。）作为 文件 的替代方案，如果 XML 仅包含没有附加功能的 CPU 模型，则可以将 CPU 模型名称本身作为 模型 传递。必须使用 文件 和 模型 中的一个。 XML 文件 应包含从域功能 XML（在每个主机上由 domcapabilities 命令打印）中的 &lt;mode name&#x3D;”host-model”&gt; 元素创建的主机 CPU 模型创建的客户机 CPU 定义。域功能 XML 中的 &lt;mode name&#x3D;”host-model”&gt; 元素本身或其 &lt;cpu&gt; 父元素不被接受。必须将元素转换为实际的 CPU 定义。 或者，此命令在提供域功能 XML 时将自动提取 CPU 定义。 出于历史原因，XML 文件 也可以包含主机 CPU 定义，但强烈不建议这种用法，因为它很可能会提供不正确的结果。 当 文件 仅包含单个 CPU 定义时，命令将打印相同的 CPU，并附加虚拟机管理程序能力施加的限制。具体来说，在没有其他选项的情况下运行 virsh hypervisor-cpu-baseline 命令对 virsh domcapabilities 的结果进行操作，将域功能 XML 中的主机 CPU 模型转换为可直接在域 XML 中使用的形式。运行带有 模型（或仅包含模型且没有功能元素的单个 CPU 定义的 文件）的命令，该模型在 virsh domcapabilities 中被标记为不可用，将提供阻止此 CPU 模型可用的功能列表。 virttype 选项指定虚拟化类型（可在域 XML 中 &lt;domain&gt; 顶级元素的 ‘type’ 属性中使用）。emulator 指定模拟器的路径，arch 指定 CPU 架构，machine 指定机器类型。如果指定 –features，则生成的 XML 描述将显式包括构成 CPU 的所有功能，没有此选项时，属于 CPU 模型的功能将不会在 XML 描述中列出。如果指定 –migratable，则阻止迁移的功能将不会包含在生成的 CPU 中。 hypervisor-cpu-models语法：hypervisor-cpu-models [–virttype virttype] [–emulator emulator][–arch arch] [–machine machine] [–all] 打印虚拟机管理程序为指定架构已知的 CPU 模型列表。不能保证列出的 CPU 将在主机上运行。要确定 CPU 模型与主机的兼容性，请参阅 virsh hypervisor-cpu-baseline 和 virsh hypervisor-cpu-compare。 virttype 选项指定虚拟化类型（可在域 XML 中 &lt;domain&gt; 顶级元素的 ‘type’ 属性中使用）。emulator 指定模拟器的路径，arch 指定 CPU 架构，machine 指定机器类型。 默认情况下，仅报告虚拟机管理程序在主机上声称“可用”的模型。选项 –all 将报告虚拟机管理程序已知的每个 CPU 模型，包括不受支持的模型（例如新一代模型）。 5 域命令以下命令直接操作域，如前所述，大多数命令将域作为第一个参数。域可以指定为短整数、名称或完整的UUID。 autostart语法：autostart [–disable] 域配置域在启动时自动启动。选项 –disable 禁用自动启动。 blkdeviotune语法：blkdeviotune 域 设备 [ [–config] [–live] | [–current]][[total-bytes-sec] | [read-bytes-sec] [write-bytes-sec]][[total-iops-sec] | [read-iops-sec] [write-iops-sec]][[total-bytes-sec-max] | [read-bytes-sec-max] [write-bytes-sec-max]][[total-iops-sec-max] | [read-iops-sec-max] [write-iops-sec-max]][[total-bytes-sec-max-length] | [read-bytes-sec-max-length] [write-bytes-sec-max-length]][[total-iops-sec-max-length] | [read-iops-sec-max-length] [write-iops-sec-max-length]][size-iops-sec] [group-name] 设置或查询域块设备的IO参数。设备指定域中附加的磁盘设备的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或源文件（&lt;source file&#x3D;’name’&#x2F;&gt;）（参见domblklist列出这些名称）。 如果未指定限制，则查询当前IO限制设置。否则，使用以下标志更改限制：*–total-bytes-sec* 指定总吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–read-bytes-sec* 指定读取吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–write-bytes-sec* 指定写入吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–total-iops-sec* 指定每秒总IO操作限制。*–read-iops-sec* 指定每秒读取IO操作限制。*–write-iops-sec* 指定每秒写入IO操作限制。*–total-bytes-sec-max* 指定最大总吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–read-bytes-sec-max* 指定最大读取吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–write-bytes-sec-max* 指定最大写入吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–total-iops-sec-max* 指定每秒最大总IO操作限制。*–read-iops-sec-max* 指定每秒最大读取IO操作限制。*–write-iops-sec-max* 指定每秒最大写入IO操作限制。*–total-bytes-sec-max-length* 指定允许最大总吞吐量限制的持续时间（秒）。*–read-bytes-sec-max-length* 指定允许最大读取吞吐量限制的持续时间（秒）。*–write-bytes-sec-max-length* 指定允许最大写入吞吐量限制的持续时间（秒）。*–total-iops-sec-max-length* 指定允许最大总IO操作限制的持续时间（秒）。*–read-iops-sec-max-length* 指定允许最大读取IO操作限制的持续时间（秒）。*–write-iops-sec-max-length* 指定允许最大写入IO操作限制的持续时间（秒）。*–size-iops-sec* 指定每秒大小IO操作限制。*–group-name* 指定在多个驱动器之间共享IO配额的组名。对于QEMU域，如果未提供名称，则默认为每个设备有一个单独的组。 旧版本的virsh仅接受下划线而非连字符的选项，例如*–total_bytes_sec*。 字节和IOPS值是独立的，但仅设置一个值（例如–read-bytes-sec）会将该类别中的其他两个值重置为无限制。显式的0也会清除任何限制。对于给定的总值的非零值不能与读取或写入的非零值混合使用。 长度值的处理方式由管理程序决定。对于QEMU管理程序，如果设置了IO限制值或最大值，则默认显示1秒的值。提供0将值重置回默认值。 如果指定*–live，则影响正在运行的客户机。如果指定–config，则影响持久客户机的下一次启动。如果指定–current，则根据客户机的当前状态等效于–live或–config。设置磁盘IO参数时，可以同时指定–live和–config标志，但–current是互斥的。查询时只能指定–live、–config或–current*中的一个。如果未指定标志，行为因管理程序而异。 domthrottlegroupset语法：domthrottlegroupset 域 组名 [ [–config] [–live] | [–current]][[total-bytes-sec] | [read-bytes-sec] [write-bytes-sec]][[total-iops-sec] | [read-iops-sec] [write-iops-sec]][[total-bytes-sec-max] | [read-bytes-sec-max] [write-bytes-sec-max]][[total-iops-sec-max] | [read-iops-sec-max] [write-iops-sec-max]][[total-bytes-sec-max-length] | [read-bytes-sec-max-length] [write-bytes-sec-max-length]][[total-iops-sec-max-length] | [read-iops-sec-max-length] [write-iops-sec-max-length]][size-iops-sec] 添加或更新特定域的节流组。组名指定唯一的节流组名称，定义限制，并将被驱动器引用。 如果未指定限制，则默认为全零，这将失败。否则，使用以下标志设置限制：*–total-bytes-sec* 指定总吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–read-bytes-sec* 指定读取吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–write-bytes-sec* 指定写入吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–total-iops-sec* 指定每秒总IO操作限制。*–read-iops-sec* 指定每秒读取IO操作限制。*–write-iops-sec* 指定每秒写入IO操作限制。*–total-bytes-sec-max* 指定最大总吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–read-bytes-sec-max* 指定最大读取吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–write-bytes-sec-max* 指定最大写入吞吐量限制为缩放整数，默认单位为字节&#x2F;秒（如果未指定后缀）。*–total-iops-sec-max* 指定每秒最大总IO操作限制。*–read-iops-sec-max* 指定每秒最大读取IO操作限制。*–write-iops-sec-max* 指定每秒最大写入IO操作限制。*–total-bytes-sec-max-length* 指定允许最大总吞吐量限制的持续时间（秒）。*–read-bytes-sec-max-length* 指定允许最大读取吞吐量限制的持续时间（秒）。*–write-bytes-sec-max-length* 指定允许最大写入吞吐量限制的持续时间（秒）。*–total-iops-sec-max-length* 指定允许最大总IO操作限制的持续时间（秒）。*–read-iops-sec-max-length* 指定允许最大读取IO操作限制的持续时间（秒）。*–write-iops-sec-max-length* 指定允许最大写入IO操作限制的持续时间（秒）。*–size-iops-sec* 指定每秒大小IO操作限制。 字节和IOPS值是独立的，但仅设置一个值（例如–read-bytes-sec）会将该类别中的其他两个值重置为无限制。显式的0也会清除任何限制。对于给定的总值的非零值不能与读取或写入的非零值混合使用。 长度值的处理方式由管理程序决定。对于QEMU管理程序，如果设置了IO限制值或最大值，则默认显示1秒的值。提供0将值重置回默认值。 如果指定*–live，则影响正在运行的客户机。如果指定–config，则影响持久客户机的下一次启动。如果指定–current，则根据客户机的当前状态等效于–live或–config。设置磁盘IO参数时，可以同时指定–live和–config标志，但–current*是互斥的。如果未指定标志，行为因管理程序而异。 domthrottlegroupdel语法：domthrottlegroupdel 域 组名 [ [–config] [–live] | [–current]] 使用指定的组名从域中删除节流组。如果节流组当前被磁盘资源引用，则尝试删除节流组将失败。如果组名不存在，将发生错误。 如果指定*–live，则影响正在运行的客户机。如果客户机未运行，则返回错误。如果指定–config，则影响持久客户机的下一次启动。如果指定–current，则根据客户机的当前状态等效于–live或–config*。 domthrottlegroupinfo语法：domthrottlegroupinfo 域 组名 [ [–config] [–live] | [–current]] 显示域节流组信息，包括IO限制设置。 如果指定*–live，则从正在运行的客户机获取节流组数据。如果客户机未运行，则返回错误。如果指定–config，则从持久客户机的下一次启动获取节流组数据。如果指定–current或未指定–live和–config，则根据客户机的当前状态获取节流组数据，可以是活动或离线状态。如果同时指定–live和–config，则–config*选项优先获取当前描述。 domthrottlegrouplist语法：domthrottlegrouplist 域 [–inactive]打印显示与域关联的所有节流组名称的表格。如果指定*–inactive*，则查询将在下次启动时使用的节流组数据，而不是当前正在使用的域。 blkiotune语法：blkiotune 域 [–weight 权重] [–device-weights 设备权重][–device-read-iops-sec 设备读取IOPS][–device-write-iops-sec 设备写入IOPS][–device-read-bytes-sec 设备读取字节][–device-write-bytes-sec 设备写入字节][ [–config] [–live] | [–current]] 显示或设置块IO参数。QEMU&#x2F;KVM支持*–weight。–weight*范围为[100, 1000]。在内核2.6.39之后，该值可以在[10, 1000]范围内。 设备权重是一个字符串，列出一个或多个设备&#x2F;权重对，格式为&#x2F;路径&#x2F;到&#x2F;设备,权重,&#x2F;路径&#x2F;到&#x2F;设备,权重。每个权重在[100, 1000]范围内，内核2.6.39之后为[10, 1000]，或值为0以从每设备列表中删除该设备。仅修改字符串中列出的设备；其他设备的现有每设备权重保持不变。 设备读取IOPS是一个字符串，列出一个或多个设备&#x2F;读取IOPS对，格式为&#x2F;路径&#x2F;到&#x2F;设备,读取IOPS,&#x2F;路径&#x2F;到&#x2F;设备,读取IOPS。每个读取IOPS是一个无符号整数，值为0以从每设备列表中删除该设备。仅修改字符串中列出的设备；其他设备的现有每设备读取IOPS保持不变。 设备写入IOPS是一个字符串，列出一个或多个设备&#x2F;写入IOPS对，格式为&#x2F;路径&#x2F;到&#x2F;设备,写入IOPS,&#x2F;路径&#x2F;到&#x2F;设备,写入IOPS。每个写入IOPS是一个无符号整数，值为0以从每设备列表中删除该设备。仅修改字符串中列出的设备；其他设备的现有每设备写入IOPS保持不变。 设备读取字节是一个字符串，列出一个或多个设备&#x2F;读取字节对，格式为&#x2F;路径&#x2F;到&#x2F;设备,读取字节,&#x2F;路径&#x2F;到&#x2F;设备,读取字节。每个读取字节是一个无符号长整型，值为0以从每设备列表中删除该设备。仅修改字符串中列出的设备；其他设备的现有每设备读取字节保持不变。 设备写入字节是一个字符串，列出一个或多个设备&#x2F;写入字节对，格式为&#x2F;路径&#x2F;到&#x2F;设备,写入字节,&#x2F;路径&#x2F;到&#x2F;设备,写入字节。每个写入字节是一个无符号长整型，值为0以从每设备列表中删除该设备。仅修改字符串中列出的设备；其他设备的现有每设备写入字节保持不变。 如果指定*–live，则影响正在运行的客户机。如果指定–config，则影响持久客户机的下一次启动。如果指定–current，则根据客户机的当前状态等效于–live或–config。可以同时指定–live和–config标志，但–current*是互斥的。如果未指定标志，行为因管理程序而异。 blockcommit语法：blockcommit 域 路径 [带宽] [–bytes] [基础][–shallow] [顶部] [–delete] [–keep-relative][–wait [–async] [–verbose]] [–timeout 秒][–active] [{–pivot | –keep-overlay}] 通过将链顶部的更改（快照或增量文件）提交到基础映像中，减少基础映像链的长度。默认情况下，此命令尝试展平整个链。如果基础和&#x2F;或顶部指定为链中的文件，则操作仅限于提交该部分链；*–shallow可以代替基础指定要提交的顶部映像的直接基础文件。正在提交的文件将变为无效，可能在操作开始时即如此；使用–delete标志将在成功完成提交操作后尝试删除这些无效文件。使用–keep-relative*标志时，基础文件路径将保持相对路径。 当省略顶部或将其指定为活动映像时，还可以指定*–active以触发两阶段活动提交。在第一阶段，顶部被复制到基础中，作业只能取消，顶部仍包含尚未在基础中的数据。在第二阶段，顶部和基础保持相同，直到调用带有–abort标志的blockjob（保持顶部作为跟踪从该时间点更改的活动映像）或–pivot*标志（使基础成为新的活动映像并使顶部无效）。 默认情况下，此命令尽快返回，整个磁盘的数据在后台提交；可以使用blockjob检查操作进度。但是，如果指定*–wait，则此命令将阻塞，直到操作完成（或对于–active，进入第二阶段），或由于可选的timeout秒超时或发送SIGINT（通常使用Ctrl-C）而取消操作。使用–verbose与–wait将产生定期状态更新。如果触发作业取消，–async将尽快将控制权返回给用户，否则命令可能会继续阻塞一段时间，直到作业完成清理。使用–pivot是结合–active* –wait与自动blockjob –pivot的简写；使用*–keep-overlay是结合–active* –wait与自动blockjob –abort的简写。 路径指定磁盘的完全限定路径；它对应于域中附加的磁盘设备的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或源文件（&lt;source file&#x3D;’name’&#x2F;&gt;）（参见domblklist列出这些名称）。带宽指定复制带宽限制（MiB&#x2F;s），但对于QEMU，仅在线域可能为非零值。有关带宽参数的更多信息，请参见blockjob命令的相应部分。 blockcopy语法：blockcopy 域 路径 { 目标 [格式] [–blockdev] | –xml 文件 }[–shallow] [–reuse-external] [带宽][–wait [–async] [–verbose]] [{–pivot | –finish}][–timeout 秒] [粒度] [缓冲区大小] [–bytes][–transient-job] [–synchronous-writes] [–print-xml]将磁盘基础映像链复制到目标。必须存在目标作为目标文件名，或*–xml包含描述目标的顶级&lt;disk&gt;元素的XML文件名。此外，如果给出目标，则应指定格式以声明目标的格式（如果省略格式，则libvirt将重用源的格式，或使用–reuse-external强制探测目标格式，这可能是潜在的安全漏洞）。该命令支持–raw作为–format&#x3D;raw的布尔标志同义词。使用目标时，目标被视为常规文件，除非使用–blockdev表示它是块设备。默认情况下，此命令展平整个链；但如果指定–shallow*，则副本共享基础链。 如果指定*–reuse-external，则目标必须存在并具有足够的空间来保存副本。如果–shallow与–reuse-external*一起使用，则预创建的映像必须具有与原始映像的基础文件相同的客户可见内容。这可以用于修改目标上的基础文件名。 默认情况下，复制作业在后台运行，并分为两个阶段。最初，作业必须从源复制所有数据，在此阶段，作业只能取消以恢复到源磁盘，不保证目标的状态。在此阶段完成后，源和目标保持镜像，直到调用带有*–abort和–pivot标志的blockjob切换到副本，或不带–pivot的调用将目标保留为该时间点的忠实副本。但是，如果指定–wait，则此命令将阻塞，直到镜像阶段开始，或如果可选的timeout秒超时或发送SIGINT（通常使用Ctrl-C）则取消操作。使用–verbose与–wait将产生定期状态更新。使用–pivot（类似于blockjob –pivot）或–finish（类似于blockjob –abort）隐含–wait，并将干净地结束作业，而不是保持在镜像阶段。如果超时或–finish触发作业取消，–async*将尽快将控制权返回给用户，否则命令可能会继续阻塞一段时间，直到作业实际取消。 路径指定磁盘的完全限定路径。带宽指定复制带宽限制（MiB&#x2F;s）。指定负值被视为无符号长整型值，可能基本上无限制，但更可能溢出；使用0更安全。有关带宽参数的更多信息，请参见blockjob命令的相应部分。指定粒度允许微调检测到脏区域时要复制的粒度；较大的值触发较少的IO开销，但总体上可能复制更多的数据（默认值通常正确）；管理程序可能限制为2的幂或落在特定范围内。指定缓冲区大小将控制在复制期间可以同时传输的数据量；较大的值使用更多内存，但可能允许更快完成（默认值通常正确）。 –transient-job允许指定如果VM在作业完成前崩溃或关闭，用户不需要恢复作业。如果管理程序应用了复制作业对临时域的限制，此标志将删除该限制。 如果指定*–synchronous-writes*，则块作业将等待客户写入传播到原始映像和复制目标，以确保如果目标存储较慢，作业会收敛。这可能会影响块作业运行时的写入性能。 如果指定*–print-xml*，则打印用于启动块复制作业的XML，而不是启动作业。 blockjob语法：blockjob 域 路径 { [–abort] [–async] [–pivot] |[–info] [–raw] [–bytes] | [带宽] }管理活动块操作。有三种互斥模式：*–info、带宽和–abort。–async和–pivot隐含中止模式；–raw隐含信息模式；如果未给出模式，则假定为–info*模式。 路径指定磁盘的完全限定路径；它对应于域中附加的磁盘设备的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或源文件（&lt;source file&#x3D;’name’&#x2F;&gt;）（参见domblklist列出这些名称）。 在*–abort模式下，将中止指定磁盘上的活动作业。如果还指定–async，则此命令将立即返回，而不是等待取消完成。如果指定–pivot*，则请求将活动复制或活动提交作业切换到新映像。 在*–info模式下，将打印指定磁盘上的活动作业信息。默认情况下，输出为单个人类可读的摘要行；此格式可能在将来的版本中更改。添加–raw以稳定格式列出结构的每个字段。如果设置–bytes*标志，则如果服务器无法提供字节&#x2F;秒分辨率，则命令将出错；省略标志时，原始输出以MiB&#x2F;s列出，人类可读输出自动选择服务器支持的最佳分辨率。 带宽可用于设置活动作业的带宽限制（MiB&#x2F;s）。如果指定*–bytes，则带宽值解释为字节&#x2F;秒。指定负值被视为无符号长整型值或基本上无限制。管理程序可以选择是否拒绝该值或将其转换为允许的最大值。可选地，可以使用缩放正数作为带宽（参见上面的NOTES）。使用–bytes与缩放值允许选择更细的粒度。不带–bytes的缩放值将向下舍入到MiB&#x2F;s。请注意，–bytes*可能不受管理程序支持。 请注意，对应于拉模式备份的块作业报告的进度不是备份的进度，而是备份所需的临时空间的使用情况。 blockpull语法：blockpull 域 路径 [带宽] [–bytes] [基础][–wait [–verbose] [–timeout 秒] [–async]][–keep-relative]从基础映像链中填充磁盘。默认情况下，此命令展平整个链；但如果指定基础，包含链中基础文件之一的名称，则该文件成为新的基础文件，仅拉取链的中间部分。一旦从基础映像链中拉取了所有请求的数据，磁盘就不再依赖于该部分基础链。 默认情况下，此命令尽快返回，整个磁盘的数据在后台拉取；可以使用blockjob检查操作进度。但是，如果指定*–wait，则此命令将阻塞，直到操作完成，或如果可选的timeout秒超时或发送SIGINT（通常使用Ctrl-C）则取消操作。使用–verbose与–wait将产生定期状态更新。如果触发作业取消，–async*将尽快将控制权返回给用户，否则命令可能会继续阻塞一段时间，直到作业完成清理。 使用*–keep-relative*标志将保持基础链名称的相对性。 路径指定磁盘的完全限定路径；它对应于域中附加的磁盘设备的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或源文件（&lt;source file&#x3D;’name’&#x2F;&gt;）（参见domblklist列出这些名称）。带宽指定复制带宽限制（MiB&#x2F;s）。有关带宽参数的更多信息，请参见blockjob命令的相应部分。 blockresize语法：blockresize 域 路径 ([大小] | [–capacity])在域运行时调整块设备的大小。路径指定块设备的绝对路径；它对应于域中附加的磁盘设备的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或源文件（&lt;source file&#x3D;’name’&#x2F;&gt;）（参见domblklist列出这些名称）。 对于没有元数据的映像格式（原始格式）存储在固定大小的存储（例如块设备）中，可以使用*–capacity*标志将设备调整为基础设备的完整大小。 大小是一个缩放整数（参见上面的NOTES），如果没有后缀，则默认为KiB（1024字节的块）。必须使用“B”后缀以获取字节（请注意，由于历史原因，这与vol-resize不同，后者默认不带后缀的字节）。 console语法：console 域 [设备名] [–safe] [–force] [–resume]连接到客户机的虚拟串行控制台。可选的设备名参数引用为客户机配置的备用控制台、串行或并行设备的设备别名。如果省略，将打开主控制台。 如果指定*–safe标志，则仅在驱动程序支持安全控制台处理时尝试连接。此标志指定服务器必须确保对控制台设备的独占访问。可选地，可以指定–force*标志，请求断开任何现有会话，例如在连接断开的情况下。 如果指定*–resume*标志，则在连接到控制台后恢复客户机。 cpu-stats语法：cpu-stats 域 [–total] [起始] [数量]提供域的CPU统计信息。域应正在运行。默认显示所有CPU的统计信息和总计。使用*–total仅显示总计统计信息，起始仅显示从起始开始的CPU的统计信息，数量仅显示数量*个CPU的统计信息。 create语法：create 文件 [–console] [–paused] [–autodestroy][–pass-fds N,M,…] [–validate] [–reset-nvram] 从XML &lt;文件&gt;创建域。可选地，可以传递*–validate*选项以根据内部RNG模式验证输入XML文件的格式（与使用virt-xml-validate(1)工具相同）。使用此命令创建的域将是临时的（一旦销毁即消失）或现有的持久客户机，将运行一次性使用的配置，保持持久XML不变（这在基于原始XML的各种配置的自动化测试中可能很方便）。请参见下面的示例以了解用法演示。 如果使用*–paused选项且驱动程序支持，则域将被暂停；否则将运行。如果请求–console，则在创建后附加到控制台。如果请求–autodestroy*，则当virsh关闭其与libvirt的连接或以其他方式退出时，客户机将自动销毁。 如果指定*–pass-fds*，则参数是逗号分隔的打开文件描述符列表，这些文件描述符应传递到客户机中。文件描述符将在客户机中重新编号，从3开始。这仅支持基于容器的虚拟化。 如果指定*–reset-nvram*，则任何现有的NVRAM文件将被删除并从其原始模板重新初始化。 示例： 从现有域准备模板（如果从头开始编写，则直接跳到3a）# virsh dumpxml &lt;域&gt; &gt; 域.xml 使用您选择的编辑器编辑模板： a. 必须更改！&lt;name&gt;和&lt;uuid&gt;（&lt;uuid&gt;也可以删除），或 b. 不要更改！&lt;name&gt;或&lt;uuid&gt;# $EDITOR 域.xml 从domain.xml创建域，取决于是否遵循2a或2b： a. 域将是临时的 b. 现有的持久客户机将使用修改后的一次性配置运行# virsh create domain.xml define语法：define 文件 [–validate]从XML &lt;文件&gt;定义域。可选地，可以使用*–validate*（与使用virt-xml-validate(1)工具相同）根据内部RNG模式验证输入XML文件的格式。域定义已注册但未启动。如果域已在运行，则更改将在下次启动时生效。 desc语法：desc 域 [ [–live] [–config] | [–current]] [–title] [–edit] [–new-desc 新描述或标题消息] 显示或修改域的描述和标题。这些值是允许存储任意文本数据的用户字段，以便轻松识别域。标题应简短，尽管未强制执行。（另请参见适用于基于XML的域元数据的元数据。） 标志*–live或–config选择此命令是作用于域的实时定义还是持久定义。如果同时指定–live和–config，则–config选项优先获取当前描述，并在设置描述时同时更新实时配置和配置。–current*是互斥的，如果未指定这些标志，则隐含。 标志*–edit*指定应打开包含当前描述或标题内容的编辑器，并在之后保存内容。 标志*–title*选择操作标题字段而不是描述。 如果既不指定*–edit也不指定–new-desc*，则显示注释或描述而不是修改。 destroy语法：destroy 域 [–graceful] [–remove-logs]立即终止域域。这不会给域操作系统任何反应的机会，相当于物理机器上拔掉电源线。在大多数情况下，您会希望改用shutdown命令。但是，这不会删除客户机使用的任何存储卷，如果域是持久的，则可以稍后重新启动。 如果域是临时的，则一旦客户机停止运行，任何快照的元数据将丢失，但快照内容仍然存在，具有相同名称和UUID的新域可以使用snapshot-create恢复快照元数据。类似地，任何检查点的元数据将丢失，但可以使用checkpoint-create恢复。 如果指定*–graceful*，则在客户机在合理超时后未停止时，不采取极端措施（例如SIGKILL）；而是返回错误。 如果指定*–remove-logs，则删除每个域*的日志文件。并非所有部署配置都受支持。 对于QEMU，仅当使用virlogd处理QEMU进程输出时才支持该标志。否则忽略该标志。 domblkerror语法：domblkerror 域显示块设备上的错误。当domstate命令说域由于I&#x2F;O错误而暂停时，此命令通常很有用。domblkerror命令列出所有处于错误状态的块设备以及每个设备上看到的错误。 domblkinfo语法：domblkinfo 域 [块设备 –all] [–human]获取域的块设备大小信息。块设备对应于域中附加的磁盘设备的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或源文件（&lt;source file&#x3D;’name’&#x2F;&gt;）（参见domblklist列出这些名称）。如果指定*–human，则输出将具有人类可读的输出。如果指定–all，则输出将是一个表格，显示与域关联的所有块设备大小信息。–all*选项优先于其他选项。 domblklist语法：domblklist 域 [–inactive] [–details]打印一个表格，显示与域关联的所有块设备的简要信息。如果指定*–inactive，则查询将在下次启动时使用的块设备，而不是当前正在使用的域。如果指定–details，还将打印磁盘类型和设备值。其他需要块设备名称的上下文（例如domblkinfo或用于磁盘快照的snapshot-create*）将接受此命令打印的目标或唯一源名称。 domblkstat语法：domblkstat 域 [块设备] [–human]获取正在运行的域的块设备统计信息。块设备对应于域中附加的磁盘设备的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或源文件（&lt;source file&#x3D;’name’&#x2F;&gt;）（参见domblklist列出这些名称）。在LXC或QEMU域上，省略块设备将汇总整个域的块设备统计信息。 使用*–human*以获得更人类可读的输出。 这些字段的可用性取决于管理程序。不支持的字段将从输出中缺失。如果与较新版本的libvirtd通信，可能会出现其他字段。 字段说明（字段按以下顺序出现）： rd_req - 读取操作计数 rd_bytes - 读取字节计数 wr_req - 写入操作计数 wr_bytes - 写入字节计数 errs - 错误计数 flush_operations - 刷新操作计数 rd_total_times - 读取操作花费的总时间（纳秒） wr_total_times - 写入操作花费的总时间（纳秒） flush_total_times - 刷新操作花费的总时间（纳秒） &lt;– 管理程序提供的其他字段 –&gt; domblkthreshold语法：domblkthreshold 域 设备 阈值设置传递块阈值事件的阈值值。设备指定磁盘设备目标或使用’target[1]’语法给定设备的支持链元素。阈值是偏移的缩放值。如果块设备应写入超过该偏移，则将传递事件。 domcontrol语法：domcontrol 域返回用于控制域的VMM接口状态。对于除“ok”或“error”之外的状态，该命令还打印自控制接口进入其当前状态以来经过的秒数。 domdirtyrate-calc语法：domdirtyrate-calc &lt;域&gt; [–seconds &lt;秒&gt;]–mode&#x3D;[page-sampling | dirty-bitmap | dirty-ring] 计算活动域的内存脏页率，用户可能期望以此决定是否适合迁移出去。seconds参数可用于在特定时间计算脏页率，现在最多允许60s，如果缺失则默认为1s。这三种page-sampling、dirty-bitmap、dirty-ring模式在指定计算模式时是互斥且可选的，如果缺失则page-sampling是默认模式。通过调用’domstats –dirtyrate’可以获取计算的脏页率信息。 domdisplay语法：domdisplay 域 [–include-password] [ [–type] 类型] [–all]输出可用于通过VNC、SPICE或RDP连接到域的图形显示的URI。可以使用type参数选择特定的图形显示类型（例如“vnc”、“spice”、“rdp”）。如果指定*–include-password，则SPICE通道密码将包含在URI中。如果指定–all*，则显示所有可能的图形显示，因为VM可能有多个图形显示。 domdisplay-reload语法：domdisplay-reload &lt;域&gt; [–type &lt;类型&gt;]重新加载域的图形显示。这会重新加载其TLS证书而不重新启动域。type可以是virDomainGraphicsReloadType枚举中的任何常量。默认情况下，重新加载任何支持的类型（目前仅VNC）。 domfsfreeze语法：domfsfreeze 域 [ [–mountpoint] 挂载点…]冻结正在运行的域中的已挂载文件系统，以准备一致的快照。 –mountpoint选项接受参数挂载点，这是要冻结的文件系统的挂载点路径。此选项可以多次出现。如果未指定此选项，则冻结每个已挂载的文件系统。 注意：snapshot-create命令具有*–quiesce*选项，可以自动冻结和解冻文件系统以保持快照一致。domfsfreeze命令仅在用户希望利用libvirt不支持的存储设备的本地快照功能时才需要。 domfsinfo语法：domfsinfo 域显示正在运行的域中已挂载文件系统的列表。该列表包含挂载点、客户机中已挂载设备的名称、文件系统类型和域XML中使用的唯一目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）。 注意：此命令需要在域的客户机操作系统中配置并运行客户机代理。 domfsthaw语法：domfsthaw 域 [ [–mountpoint] 挂载点…]解冻由domfsfreeze命令冻结的正在运行的域中的已挂载文件系统。–mountpoint选项接受参数挂载点，这是要解冻的文件系统的挂载点路径。此选项可以多次出现。如果未指定此选项，则解冻每个已挂载的文件系统。 domfstrim语法：domfstrim 域 [–minimum 字节] [–mountpoint 挂载点]在正在运行的域中的所有已挂载文件系统上发出fstrim命令。它丢弃文件系统未使用的块。如果指定*–minimum字节，则告诉客户内核连续空闲范围的长度。小于此值的可能会被忽略（这是一个提示，客户可能不会遵守）。通过增加此值，fstrim操作将更快完成，对于具有严重碎片化空闲空间的文件系统，尽管并非所有块都会被丢弃。默认值为零，表示“丢弃每个空闲块”。此外，如果用户只想修剪一个挂载点，可以通过可选的–mountpoint*参数指定。 domhostname语法：domhostname 域 [–source lease|agent]如果管理程序可用，则返回域的主机名。–source参数指定用于主机名的数据源，当前为“lease”读取DHCP租约或“agent”通过代理查询客户操作系统。如果未指定，驱动程序返回可用的默认方法（某些驱动程序仅支持一种类型的源）。 domid语法：domid 域名或UUID将域名（或UUID）转换为域ID domif-getlink语法：domif-getlink 域 接口设备 [–config]查询域虚拟接口的链接状态。如果指定*–config，则查询持久配置，为了兼容性，–persistent是–config*的别名。接口设备可以是接口的目标名称或MAC地址。 domif-setlink语法：domif-setlink 域 接口设备 状态 [–config] [–print-xml]修改域虚拟接口的链接状态。状态的可能值为“up”和“down”。如果指定*–config，则仅修改域的持久配置，为了兼容性，–persistent是–config的别名。接口设备可以是接口的目标名称或MAC地址。如果指定–print-xml*，则打印用于更新接口的XML。 domifaddr语法：domifaddr 域 [接口] [–full] [–source lease|agent|arp]获取正在运行的域的接口列表及其IP和MAC地址，或如果指定接口，则仅显示一个接口的有限输出。请注意，接口可能依赖于驱动程序，可以是客户操作系统中的名称或域XML中看到的名称。此外，在某些管理程序（特别是QEMU）下，整个命令可能要求为查询的域配置客户代理。 如果指定*–full*，则当接口有多个IP地址或别名时，始终显示接口名称和MAC地址；否则，仅对第一个名称和MAC地址显示接口名称和MAC地址，其他使用相同名称和MAC地址的显示为“-”。 –source参数指定用于地址的数据源，当前为“lease”读取DHCP租约，“agent”通过代理查询客户操作系统，或“arp”从主机的arp表中获取IP。如果未指定，“lease”是默认值。 backup-begin语法：backup-begin 域 [backupxml] [checkpointxml] [–reuse-external]开始新的备份作业。如果省略backupxml，则默认为使用libvirt生成的文件名的完整备份；提供XML允许微调，例如请求相对于早期检查点的增量备份，控制哪些磁盘参与或涉及哪些文件名，或请求使用拉模型备份。backup-dumpxml命令显示libvirt分配的任何结果值。有关备份XML的更多信息，请参见：https://libvirt.org/formatbackup.html 如果使用*–reuse-external，则指示libvirt重用用户在backupxml*中提供的临时和输出文件。 如果指定checkpointxml，则使用包含domaincheckpoint顶级元素的第二个文件创建同时检查点，以便在创建备份时进行稍后的增量备份。有关检查点的更多详细信息，请参见checkpoint-create。 此命令尽快返回，备份作业在后台运行；可以使用domjobinfo检查推送模型备份的进度，或使用event等待事件（拉模型备份的进度由连接到NBD导出的任何第三方控制）。使用domjobabort结束作业。 backup-dumpxml语法：backup-dumpxml [–xpath 表达式] [–wrap] 域输出描述当前备份作业的XML。如果**–xpath参数提供XPath表达式，则将对输出XML求值，并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但是为了便于进一步处理，–wrap**参数将使匹配节点包装在公共根节点中。 domiflist语法：domiflist 域 [–inactive]打印显示与域关联的所有虚拟接口的简要信息的表格。如果指定*–inactive，则查询将在下次启动时使用的虚拟接口，而不是当前正在使用的域。其他需要虚拟接口MAC地址的上下文（例如detach-interface或domif-setlink*）将接受此命令打印的MAC地址。 domifstat语法：domifstat 域 接口设备获取正在运行的域的网络接口统计信息。网络接口统计信息仅适用于具有物理源接口的接口。这不包括例如“user”接口类型，因为它是具有NAT到外部世界的虚拟LAN。接口设备可以是接口的目标名称或MAC地址。请注意，对于非托管ethernet类型，返回的统计信息可能交换了RX&#x2F;TX。 domiftune语法：domiftune 域 接口设备 [ [–config] [–live] | [–current]][–inbound 平均,峰值,突发,下限][–outbound 平均,峰值,突发] 设置或查询域的网络接口的带宽参数。接口设备可以是接口的目标名称（&lt;target dev&#x3D;’name’&#x2F;&gt;）或MAC地址。 如果未指定*–inbound或–outbound，则此命令将查询并显示带宽设置。否则，它将设置入站或出站带宽。平均,峰值,突发,下限与命令attach-interface中的相同。平均、峰值和下限的值以千字节&#x2F;秒表示，而突发以峰值*速度的单次突发中的千字节表示，如网络XML文档中所述：https://libvirt.org/formatnetwork.html#quality-of-service。 要清除入站或出站设置，分别使用*–inbound或–outbound*，平均值为零。 如果指定*–live，则影响正在运行的客户机。如果指定–config，则影响持久客户机的下一次启动。如果指定–current，则根据客户机的当前状态等效于–live或–config。可以同时指定–live和–config标志，但–current*是互斥的。如果未指定标志，行为因管理程序而异。 dominfo语法：dominfo 域返回有关域的基本信息。 domjobabort语法：domjobabort 域 [–postcopy]中止当前正在运行的域作业。当中止的作业是进入后复制模式的迁移时，由于迁移中涉及的没有一个主机具有域的完整状态，因此无法中止。可选的*–postcopy可用于中断此类迁移，尽管这样做可能会有效地暂停域，直到迁移恢复（另请参见迁移的–postcopy-resume*选项）。 domjobinfo语法：domjobinfo 域 [–completed [–keep-completed]] [–anystats] [–rawstats]返回在域上运行的作业的信息。*–completed告诉virsh返回最近完成的作业的信息。完成的作业的统计信息在读取后自动销毁（除非使用–keep-completed*）或重新启动libvirtd时。 通常仅打印正在运行和成功完成的作业的统计信息。*–anystats*也可用于显示失败作业的统计信息。 如果使用*–rawstats*，则所有字段按从服务器接收的方式打印，不尝试解释数据。“Job type:”字段是特殊的，因为它是通过API报告的，不是统计信息的一部分。 请注意，对于已完成的迁移返回的时间信息可能完全不相关，除非源主机和目标主机具有同步时间（即，两者都运行NTP守护程序）。 domlaunchsecinfo语法：domlaunchsecinfo 域返回与正在运行的域关联的启动安全参数的信息。报告的参数集将根据活动的启动安全保护类型而变化。如果没有活动，则不报告任何参数。 domsetlaunchsecstate语法：domsetlaunchsecstate 域 –secrethdr hdr-filename–secret secret-filename [–set-address 地址]在客户内存中设置启动安全密钥。客户必须在其配置中启用launchSecurity类型并处于暂停状态。成功后，客户可以转换为运行状态。失败时，应销毁客户。–secrethdr指定包含base64编码的密钥头的文件名。头包括管理程序固件恢复启动密钥明文所需的工件。*–secret*指定包含base64编码的加密启动密钥的文件名。–set-address选项可用于指定客户内存中设置密钥的物理地址。如果未指定，地址将由管理程序确定。 dommemstat语法：dommemstat 域 [–period 秒] [ [–config] [–live] | [–current]]获取正在运行的域的内存统计信息。这些字段的可用性取决于管理程序。不支持的字段将从输出中缺失。如果与较新版本的libvirtd通信，可能会出现其他字段。字段说明： swap_in - 从交换空间读取的数据量（KiB） swap_out - 写入交换空间的内存量（KiB） major_fault - 需要磁盘IO的页面错误数 minor_fault - 其他页面错误数 unused - 系统未使用的内存量（KiB） available - 域可见的可用内存量（KiB） actual - 当前气球值（KiB） rss - 运行域进程的驻留集大小（KiB） usable - 可以通过气球回收而不会导致主机交换的内存量（KiB） last-update - 上次统计信息更新的时间戳（秒） disk_caches - 无需额外IO即可回收的内存量，通常是磁盘缓存（KiB） hugetlb_pgalloc - 从域内启动的成功大页分配数 hugetlb_pgfail - 从域内启动的失败大页分配数 对于带有内存气球的QEMU&#x2F;KVM，将可选的*–period设置为大于0的值（秒）将允许气球驱动程序返回额外的统计信息，这些统计信息将由后续的dommemstat命令显示。将–period*设置为0将停止气球驱动程序的收集，但不会清除气球驱动程序中的统计信息。至少需要QEMU&#x2F;KVM 1.5在主机上运行。 –live、*–config和–current标志仅在用于设置气球驱动程序的收集周期时有效。如果指定–live，则仅影响正在运行的客户机的收集周期。如果指定–config，则影响持久客户机的下一次启动。如果指定–current，则根据客户机的当前状态等效于–live或–config*。 可以同时指定*–live和–config标志，但–current*是互斥的。如果未指定标志，行为因客户机状态而异。 domname语法：domname 域ID或UUID将域ID（或UUID）转换为域名 dompmsuspend语法：dompmsuspend 域 目标 [–duration 秒]将正在运行的域挂起到以下状态之一（可能的目标值）： mem - 相当于S3 ACPI状态 disk - 相当于S4 ACPI状态 hybrid - RAM保存到磁盘但不关闭电源–duration参数指定域挂起后唤醒前的秒数（另请参见dompmwakeup）。默认为0，表示无限挂起时间。（此功能目前不受任何管理程序驱动程序支持，应使用0。） 注意：此命令需要在域的客户机操作系统中配置并运行客户机代理。 请注意，至少对于QEMU，当使用目标磁盘时，域的进程将终止，并在libvirt请求唤醒域时启动新进程。因此，任何运行时更改（例如设备热插拔或内存设置）将丢失，除非这些更改是使用*–config*标志进行的。 dompmwakeup语法：dompmwakeup 域从pmsuspended状态唤醒域（由dompmsuspend或客户机本身挂起）。向处于pmsuspended状态的客户注入唤醒，而不是等待先前请求的持续时间（如果有）过去。此操作不一定失败，如果域正在运行。 domrename语法：domrename 域 新名称重命名域。此命令将当前域名更改为第二个参数中指定的新名称。注意：域必须处于非活动状态。 domstate语法：domstate 域 [–reason]返回有关域的状态。*–reason*告诉virsh还打印状态的原因。 domstats语法：domstats [–raw] [–enforce] [–backing] [–nowait] [–state][–cpu-total] [–balloon] [–vcpu] [–interface][–block] [–perf] [–iothread] [–memory] [–dirtyrate] [–vm][ [–list-active] [–list-inactive][–list-persistent] [–list-transient] [–list-running][–list-paused] [–list-shutoff] [–list-other]] | [域 …]获取多个或所有域的统计信息。不带任何参数时，此命令打印所有域的所有可用统计信息。要收集统计信息的域列表可以通过将域列为空格分隔的列表来限制，或通过指定其中一个过滤标志*–list-NNN*。（这些方法不能组合使用。） 默认情况下，某些返回的字段可能会通过一组漂亮的打印机转换为更人类友好的值。要抑制此行为，请使用*–raw*标志。 可以通过特定标志选择各个统计信息组。默认情况下，返回所有支持的统计信息组。支持的统计信息组标志包括：*–state、–cpu-total、–balloon、–vcpu、–interface、–block、–perf、–iothread、–memory、–dirtyrate、–vm*。 请注意，根据管理程序类型和版本或域状态，可能不会返回以下所有统计信息。选择*–state*组时，返回以下字段： state.state - VM的状态，作为virDomainState枚举中的数字返回 state.reason - 进入给定状态的原因，作为与给定状态对应的virDomain*Reason枚举中的整数返回–cpu-total返回： cpu.time - 此域的总CPU时间（纳秒） cpu.user - 用户CPU时间（纳秒） cpu.system - 系统CPU时间（纳秒） cpu.haltpoll.success.time - CPU暂停轮询成功时间（纳秒） cpu.haltpoll.fail.time - CPU暂停轮询失败时间（纳秒） cpu.cache.monitor.count - 此域的缓存监视器数量 cpu.cache.monitor.&lt;num&gt;.name - 缓存监视器&lt;num&gt;的名称 cpu.cache.monitor.&lt;num&gt;.vcpus - 缓存监视器&lt;num&gt;的vcpu列表 cpu.cache.monitor.&lt;num&gt;.bank.count - 缓存监视器&lt;num&gt;中的缓存库数量 cpu.cache.monitor.&lt;num&gt;.bank.&lt;index&gt;.id - 主机为缓存监视器&lt;num&gt;中的库&lt;index&gt;分配的缓存ID cpu.cache.monitor.&lt;num&gt;.bank.&lt;index&gt;.bytes - 域在缓存库&lt;index&gt;上使用的最后一级缓存的字节数 –balloon返回： balloon.current - 当前使用的内存（KiB） balloon.maximum - 允许的最大内存（KiB） balloon.swap_in - 从交换空间读取的数据量（KiB） balloon.swap_out - 写入交换空间的内存量（KiB） balloon.major_fault - 需要磁盘IO的页面错误数 balloon.minor_fault - 其他页面错误数 balloon.unused - 系统未使用的内存量（KiB） balloon.available - 域可见的可用内存量（KiB） balloon.rss - 运行域进程的驻留集大小（KiB） balloon.usable - 可以通过气球回收而不会导致主机交换的内存量（KiB） balloon.last-update - 上次统计信息更新的时间戳（秒） balloon.disk_caches - 无需额外IO即可回收的内存量，通常是磁盘（KiB） balloon.hugetlb_pgalloc - 通过virtio气球从域内启动的成功大页分配数 balloon.hugetlb_pgfail - 通过virtio气球从域内启动的失败大页分配数 –vcpu返回： vcpu.current - 当前在线虚拟CPU的数量 vcpu.maximum - 在线虚拟CPU的最大数量 vcpu.&lt;num&gt;.state - 虚拟CPU&lt;num&gt;的状态，作为virVcpuState枚举中的数字 vcpu.&lt;num&gt;.time - 虚拟CPU&lt;num&gt;花费的虚拟CPU时间（纳秒） vcpu.&lt;num&gt;.wait - vCPU&lt;num&gt;线程在调度程序有其他任务在其前面运行时在运行队列中等待的时间（纳秒），在Linux上需要CONFIG_SCHED_INFO vcpu.&lt;num&gt;.halted - 虚拟CPU&lt;num&gt;是否暂停：是或否（可能表示处理器空闲或甚至禁用，取决于架构） vcpu.&lt;num&gt;.delay - vCPU&lt;num&gt;线程在调度程序有其他任务在其前面运行时在运行队列中等待的时间（纳秒）。作为窃取时间暴露给VM。此组统计信息还报告额外的管理程序来源的每vCPU统计信息。此组中的管理程序特定统计信息具有以下命名方案： vcpu.&lt;num&gt;.$NAME.$TYPE$NAME管理程序提供的统计信息字段的名称$TYPE值的类型。返回以下类型： cur当前瞬时值sum聚合值max峰值值返回值可以是无符号长整型或布尔型。含义是管理程序特定的。请参阅--vm组的免责声明，该组也由管理程序特定的统计信息组成。 –interface返回： net.count - 此域上的网络接口数量 net.&lt;num&gt;.name - 接口&lt;num&gt;的名称 net.&lt;num&gt;.rx.bytes - 接收的字节数 net.&lt;num&gt;.rx.pkts - 接收的数据包数 net.&lt;num&gt;.rx.errs - 接收错误数 net.&lt;num&gt;.rx.drop - 丢弃的接收数据包数 net.&lt;num&gt;.tx.bytes - 发送的字节数 net.&lt;num&gt;.tx.pkts - 发送的数据包数 net.&lt;num&gt;.tx.errs - 发送错误数 net.&lt;num&gt;.tx.drop - 丢弃的发送数据包数 –perf返回所有启用的性能事件的统计信息： perf.cmt - 当前使用的缓存使用量（字节） perf.mbmt - 从一级缓存到另一级的总系统带宽 perf.mbml - 内存控制器的内存流量带宽 perf.cpu_cycles - CPU周期计数（总&#x2F;经过） perf.instructions - 指令计数 perf.cache_references - 缓存命中计数 perf.cache_misses - 缓存未命中计数 perf.branch_instructions - 分支指令计数 perf.branch_misses - 分支未命中计数 perf.bus_cycles - 总线周期计数 perf.stalled_cycles_frontend - 前端指令处理器管道中的停滞CPU周期计数 perf.stalled_cycles_backend - 后端指令处理器管道中的停滞CPU周期计数 perf.ref_cpu_cycles - 参考CPU周期计数 perf.cpu_clock - CPU时钟时间 perf.task_clock - 任务时钟时间 perf.page_faults - 页面错误计数 perf.context_switches - 上下文切换计数 perf.cpu_migrations - CPU迁移计数 perf.page_faults_min - 次要页面错误计数 perf.page_faults_maj - 主要页面错误计数 perf.alignment_faults - 对齐错误计数 perf.emulation_faults - 仿真错误计数有关每个事件的更多详细信息，请参阅perf命令。 –block返回有关与每个域关联的磁盘的信息。使用*–backing*标志将此信息扩展到覆盖支持链中的所有资源，而不是默认情况下将信息限制为每个客户磁盘的活动层。列出的信息包括： block.count - 列出的块设备数量 block.&lt;num&gt;.name - 块设备&lt;num&gt;的目标名称（如果存在*–backing*，则多个条目使用相同的名称） block.&lt;num&gt;.backingIndex - 当存在*–backing*时，与域XML中列出的&lt;backingStore&gt;索引匹配 block.&lt;num&gt;.path - 块设备&lt;num&gt;的文件源，如果是本地文件或块设备 block.&lt;num&gt;.rd.reqs - 读取请求数 block.&lt;num&gt;.rd.bytes - 读取字节数 block.&lt;num&gt;.rd.times - 读取花费的总时间（纳秒） block.&lt;num&gt;.wr.reqs - 写入请求数 block.&lt;num&gt;.wr.bytes - 写入字节数 block.&lt;num&gt;.wr.times - 写入花费的总时间（纳秒） block.&lt;num&gt;.fl.reqs - 总刷新请求数 block.&lt;num&gt;.fl.times - 缓存刷新花费的总时间（纳秒） block.&lt;num&gt;.errors - 仅Xen：’oo_req’值 block.&lt;num&gt;.allocation - 最高写入扇区的偏移（字节） block.&lt;num&gt;.capacity - 源文件的逻辑大小（字节） block.&lt;num&gt;.physical - 源文件的物理大小（字节） block.&lt;num&gt;.threshold - 传递VIR_DOMAIN_EVENT_ID_BLOCK_THRESHOLD事件的阈值（字节）。参见domblkthreshold。 –iothread返回有关运行客户机上的IOThread的信息（如果管理程序支持）。每个线程的“poll-max-ns”是允许每个轮询间隔发生的最大纳秒数。轮询间隔是允许线程处理数据的时间段，然后客户将其CPU量子返回给主机。设置得太小的值不允许IOThread在CPU上运行足够长的时间来处理数据。设置得太高的值会消耗过多的CPU时间，每个IOThread无法允许CPU上运行的其他线程获得时间。轮询间隔不可用于统计目的。 iothread.count - 后续列表中的IOThread的最大数量作为无符号整数。列表中的每个IOThread将使用其iothread_id值作为&lt;id&gt;。如果轮询值不受支持，则&lt;id&gt;条目可能少于iothread.count值。 iothread.&lt;id&gt;.poll-max-ns - &lt;id&gt; IOThread使用的最大轮询时间（纳秒）。值为0（零）表示轮询被禁用。 iothread.&lt;id&gt;.poll-grow - 轮询时间增长值。值为0（零）表示增长由管理程序管理。 iothread.&lt;id&gt;.poll-shrink - 轮询时间缩小值。值为（零）表示缩小由管理程序管理。 –memory返回： memory.bandwidth.monitor.count - 此域的内存带宽监视器数量 memory.bandwidth.monitor.&lt;num&gt;.name - 监视器&lt;num&gt;的名称 memory.bandwidth.monitor.&lt;num&gt;.vcpus - 监视器&lt;num&gt;的vcpu列表 memory.bandwidth.monitor.&lt;num&gt;.node.count - 内存 控制器在监视器&lt;num&gt;中的数量 memory.bandwidth.monitor.&lt;num&gt;.node.&lt;index&gt;.id - 主机为监视器&lt;num&gt;中的控制器&lt;index&gt;分配的内存控制器ID memory.bandwidth.monitor.&lt;num&gt;.node.&lt;index&gt;.bytes.local - 通过@vcpus消耗的字节数，这些vcpu通过属于调度主机CPU的同一处理器中的内存控制器。 memory.bandwidth.monitor.&lt;num&gt;.node.&lt;index&gt;.bytes.total - 通过@vcpus消耗的总字节数，这些vcpu通过所有内存控制器，无论是本地还是远程控制器。 –dirtyrate返回： dirtyrate.calc_status - 上次内存脏页率计算的状态，作为virDomainDirtyRateStatus枚举中的数字返回。 dirtyrate.calc_start_time - 上次内存脏页率计算的开始时间。 dirtyrate.calc_period - 上次内存脏页率计算的周期。 dirtyrate.megabytes_per_second - 计算的内存脏页率（MiB&#x2F;s）。 dirtyrate.calc_mode - 上次测量使用的计算模式（page-sampling&#x2F;dirty-bitmap&#x2F;dirty-ring） dirtyrate.vcpu.&lt;num&gt;.megabytes_per_second - 虚拟CPU的内存脏页率（MiB&#x2F;s） –vm返回：--vm选项启用管理程序特定统计信息的报告。字段的命名和含义完全取决于管理程序。此组中的统计信息具有以下命名方案：vm.$NAME.$TYPE$NAME管理程序提供的统计信息字段的名称$TYPE值的类型。返回以下类型： cur当前瞬时值sum聚合值max峰值值返回值可以是无符号长整型或布尔型。警告：此组中报告的统计信息是运行时收集的，源自管理程序，因此不属于libvirt的常规稳定API策略。 Libvirt无法保证从外部源报告的统计信息将在管理程序的未来版本中存在，或命名或含义保持一致。但是，对现有字段的更改预计很少。 选择特定的统计信息组不保证守护程序支持所选统计信息组。标志--enforce强制命令在守护程序不支持所选组时失败。 在收集统计信息时，libvirtd可能会等待一段时间，如果给定域上已有另一个作业正在运行以完成。这可能导致不必要的延迟传递统计信息。使用--nowait抑制此行为。另一方面，此类域可能会缺少某些统计信息。 domtime语法：domtime 域 { [–now] [–pretty] [–sync] [–time 时间] }获取或设置域的系统时间。当不带任何参数（但域）运行时，打印域的当前系统时间。--pretty修饰符可用于以更人类可读的形式打印时间。 当指定--time时间时，不获取域的时间，而是设置它。--now修饰符的作用类似于--time $now的别名，这意味着它设置virsh运行的主机上当前的时间。在这两种情况下（设置和获取），时间是以1970-01-01纪元以来的秒数为单位的UTC时间。--sync修改设置行为：忽略传递的时间，而是从域的RTC读取要设置的时间。请注意，某些管理程序可能需要配置客户代理才能获取或设置客户时间。 domuuid语法：domuuid 域名或ID将域名或ID转换为域UUID domxml-from-native语法：domxml-from-native 格式 配置将原生客户配置格式格式的文件配置转换为域XML格式。对于QEMU&#x2F;KVM管理程序，格式参数必须为qemu-argv。对于Xen管理程序，格式参数可以是xen-xm、xen-xl或xen-sxpr。对于LXC管理程序，格式参数必须为lxc-tools。对于VMware&#x2F;ESX管理程序，格式参数必须为vmware-vmx。对于Bhyve管理程序，格式参数必须为bhyve-argv。 domxml-to-native语法：domxml-to-native 格式 { [–xml] xml | –domain 域名或ID或UUID }将文件xml转换为域XML格式，或将现有的--domain转换为名为格式的原生客户配置格式。xml和--domain参数是互斥的。有关格式参数的类型，请参阅domxml-from-native。 dump语法：dump 域 核心文件路径 [–bypass-cache]{ [–live] | [–crash] | [–reset] }[–verbose] [–memory-only] [–format 字符串]将域的核心转储到文件以进行分析。如果指定--live，域在核心转储完成之前继续运行，而不是提前暂停。如果指定--crash，域以崩溃状态停止，而不仅仅是保持在暂停状态。如果指定--reset，域在成功转储后重置。注意，这三个开关是互斥的。如果指定--bypass-cache，保存将避免文件系统缓存，尽管这可能会减慢操作速度。如果指定--memory-only，文件是elf文件，仅包含域的内存和CPU通用寄存器值。如果域直接使用主机设备，这将非常有用。--format 字符串用于指定“memory-only”转储的格式，字符串可以是以下之一：elf、kdump-zlib（zlib压缩的kdump压缩格式）、kdump-lzo（lzo压缩的kdump压缩格式）、kdump-snappy（snappy压缩的kdump压缩格式）、win-dmp（Windows完整崩溃转储格式）。 可以使用domjobinfo virsh命令监视进度，并使用domjobabort命令（由另一个virsh实例发送）取消。另一个选项是向运行dump命令的virsh进程发送SIGINT（通常使用Ctrl-C）。--verbose显示转储的进度。 注意：某些管理程序可能需要用户手动确保核心文件路径参数指定的文件和路径具有适当的权限。 注意：旧kvmdump格式的崩溃转储正在过时，自其版本6.1.0以来无法由崩溃实用程序加载和处理。需要--memory-only选项以生成可由崩溃实用程序稍后处理的有效ELF文件。 dumpxml语法：dumpxml [–inactive] [–security-info] [–update-cpu] [–migratable][–xpath 表达式] [–wrap] 域 将域信息作为XML转储输出到stdout，此格式可由create命令使用。可以使用影响XML转储的其他选项。--inactive告诉virsh转储将在域下次启动时使用的域配置，而不是当前域配置。使用--security-info还将包括XML转储中的安全敏感信息。--update-cpu根据主机CPU更新域CPU要求。使用--migratable可以请求适合迁移的XML，即与较旧版本的libvirt兼容，并可能使用内部运行时选项进行修改。此选项可能会自动启用其他选项（--update-cpu、--security-info，…）根据需要。 如果 --xpath 参数提供XPath表达式，则将对输出XML求值，并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但是为了便于进一步处理，--wrap参数将使匹配节点包装在公共根节点中。 edit语法：edit 域编辑域的XML配置文件，这将影响客户机的下次启动。这等效于：virsh dumpxml –inactive –security-info 域 &gt; 域.xmlvi 域.xml（或使用其他文本编辑器进行更改）virsh define 域.xml不同之处在于它进行了一些错误检查。使用的编辑器可以由$VISUAL或$EDITOR环境变量提供，默认为vi。 emulatorpin语法：emulatorpin 域 [CPU列表] [ [–live] [–config] | [–current]]查询或更改域的模拟器线程到主机物理CPU的绑定。有关CPU列表的信息，请参见vcpupin。如果指定--live，则影响正在运行的客户机。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等效于--live或--config。可以同时指定--live和--config标志（如果存在CPU列表），但--current是互斥的。如果未指定标志，行为因管理程序而异。 event语法：event {[域] { 事件 | –all } [–loop] [–timeout 秒] [–timestamp] | –list}等待一类域事件发生，并在事件发生时打印适当的详细信息。事件可以选择由域过滤。仅使用--list作为参数将提供此客户端已知的可能事件值的列表，尽管连接可能不允许注册所有这些事件。也可以使用--all代替事件一次注册所有可能的事件类型。 默认情况下，此命令是一次性的，一旦事件发生即返回成功；您可以发送SIGINT（通常通过Ctrl-C）立即退出。如果指定--timeout，则命令在秒后放弃等待事件。使用--loop，命令打印所有事件，直到超时或中断键。 当使用--timestamp时，将在事件之前打印人类可读的时间戳。 get-user-sshkeys语法：get-user-sshkeys 域 用户打印给定用户在客户域中的SSH授权密钥。请注意，文件中的条目具有*sshd(8)*定义的内部结构，virsh&#x2F;libvirt将密钥视为不透明字符串，即不解释它们。 guest-agent-timeout语法：guest-agent-timeout 域 [–timeout 值]设置等待客户代理命令响应的时长。默认情况下，代理命令无限期阻塞等待响应。值必须是正值（等待给定的秒数）或以下值之一： -2 - 无限期阻塞等待结果（当省略--timeout时使用）， -1 - 将超时重置为默认值（当前在libvirt守护程序中定义为5秒）， 0 - 完全不等待，在所有基于客户代理的API中，当发生超时时，如果实际命令已发送到客户代理，则返回的错误代码将为VIR_ERR_AGENT_COMMAND_TIMEOUT。 guestinfo语法：guestinfo 域 [–user] [–os] [–timezone] [–hostname] [–filesystem][–disk] [–interface]从客户代理的角度打印有关客户的信息。请注意，此命令需要在域的客户操作系统中配置并运行客户代理。当不带任何参数运行时，此命令打印客户代理当时支持的所有信息类型，省略不可用的类型。在这种情况下总是报告成功。 您可以通过指定一个或多个标志来限制返回的信息类型。可用的信息类型标志包括--user、--os、--timezone、--hostname、--filesystem、--disk、--interface和--load。如果明确请求的信息类型当时不受客户代理支持，则进程将提供退出代码1。 请注意，根据管理程序类型和在域内运行的客户代理的版本，可能不会返回以下所有信息。 --user信息类型时，可能会返回以下字段： user.count - 此域上的活动用户数 user.&lt;num&gt;.name - 用户&lt;num&gt;的用户名 user.&lt;num&gt;.domain - 用户&lt;num&gt;的域（可能仅在某些客户类型上存在） user.&lt;num&gt;.login-time - 用户&lt;num&gt;的登录时间（自纪元以来的毫秒数） --os返回： os.id - 标识操作系统的字符串 os.name - 操作系统的名称 os.pretty-name - 操作系统的漂亮名称 os.version - 操作系统的版本 os.version-id - 操作系统的版本ID os.kernel-release - 操作系统内核的发布 os.kernel-version - 操作系统内核的版本 os.machine - 机器硬件名称 os.variant - 操作系统的特定变体或版本 os.variant-id - 操作系统的特定变体或版本的ID --timezone返回： timezone.name - 时区的名称 timezone.offset - 与UTC的偏移（秒） --hostname返回： hostname - 域的主机名 --filesystem返回： fs.count - 此域上定义的文件系统数量 fs.&lt;num&gt;.mountpoint - 文件系统&lt;num&gt;的挂载点路径 fs.&lt;num&gt;.name - 客户中的设备名称（例如sda1）用于文件系统&lt;num&gt; fs.&lt;num&gt;.fstype - 文件系统&lt;num&gt;的类型 fs.&lt;num&gt;.total-bytes - 文件系统&lt;num&gt;的总大小 fs.&lt;num&gt;.used-bytes - 文件系统&lt;num&gt;中使用的字节数 fs.&lt;num&gt;.disk.count - 文件系统&lt;num&gt;目标的磁盘数量 fs.&lt;num&gt;.disk.&lt;num&gt;.alias - 磁盘&lt;num&gt;的设备别名（例如sda） fs.&lt;num&gt;.disk.&lt;num&gt;.serial - 磁盘&lt;num&gt;的序列号 fs.&lt;num&gt;.disk.&lt;num&gt;.device - 磁盘&lt;num&gt;的设备节点 --disk返回： disk.count - 此域上定义的磁盘数量 disk.&lt;num&gt;.name - 设备节点（Linux）或设备UNC（Windows） disk.count - 该域上定义的磁盘数量 disk..name - 设备节点(Linux)或设备UNC路径(Windows) disk..partition - 指示这是分区还是整块磁盘 disk..dependency.count - 设备依赖项的数量 disk..dependency..name - 依赖项名称 disk..serial - 可选的磁盘序列号 disk..alias - 磁盘的设备别名(例如sda) disk..guest_alias - 客户机分配给磁盘的可选别名 disk..guest_bus - 客户机报告的总线类型 --interface 返回信息 if.count - 该域上定义的接口数量 if..name - 客户机中的接口名称(例如eth0) if..hwaddr - 客户机中的硬件地址 if..addr.count - 接口的IP地址数量 if..addr..type - 地址的IP地址类型(例如ipv4) if..addr..addr - 地址的IP地址 if..addr..prefix - 地址的IP前缀 --load 返回信息 load.1m - 客户机最近1分钟的平均负载 load.5m - 客户机最近5分钟的平均负载 load.15m - 客户机最近15分钟的平均负载 guestvcpus语法： 1guestvcpus domain [ [--enable] | [--disable]] [cpulist] 从客户机角度查询或更改vCPU状态。当不带cpulist参数调用时，查询客户机可用的vCPU、它们的状态以及是否可以离线。 如果提供了cpulist，则必须同时提供*–enable或–disable*之一。然后将在域上执行所需操作。 有关cpulist的信息，请参阅vcpupin。 iothreadadd语法： 1iothreadadd domain iothread_id [ [--config] [--live] | [--current]] 使用指定的iothread_id向域添加新的IOThread。如果iothread_id已存在，命令将失败。iothread_id必须大于零。 如果指定*–live，则影响运行中的客户机。如果客户机未运行，则返回错误。如果指定–config，则影响持久客户机的下次启动。如果指定–current，则根据客户机当前状态等同于–live或–config*。 iothreaddel语法： 1iothreaddel domain iothread_id [ [--config] [--live] | [--current]] 使用指定的iothread_id从域中删除IOThread。如果IOThread当前已分配给磁盘资源(例如通过attach-disk命令)，则删除IOThread的尝试将失败。如果iothread_id不存在，将发生错误。 如果指定*–live，则影响运行中的客户机。如果客户机未运行，则返回错误。如果指定–config，则影响持久客户机的下次启动。如果指定–current，则根据客户机当前状态等同于–live或–config*。 iothreadinfo语法： 1iothreadinfo domain [ [--live] [--config] | [--current]] 显示域的基本IOThread信息，包括每个IOThread的IOThread ID和CPU亲和性。 如果指定*–live，则从运行中的客户机获取IOThread数据。如果客户机未运行，则返回错误。如果指定–config，则从持久客户机的下次启动获取IOThread数据。如果指定–current或未指定–live和–config*，则根据客户机当前状态(可以是运行中或离线)获取IOThread数据。 iothreadpin语法： 1iothreadpin domain iothread cpulist [ [--live] [--config] | [--current]] 更改域IOThread与主机物理CPU的绑定关系。要检索所有IOThread的列表，请使用iothreadinfo。要绑定iothread，请为IOThread ID指定所需的cpulist，如iothreadinfo输出中所列。 cpulist是物理CPU编号的列表。其语法是逗号分隔的列表，也可以使用’-‘和’^’的特殊标记(例如’0-4’，’0-3,^2’)。’-‘表示范围，’^’表示排除。如果要重置iothreadpin设置，即将iothread绑定到所有物理CPU，只需将cpulist指定为’r’。 如果指定*–live，则影响运行中的客户机。如果客户机未运行，则返回错误。如果指定–config，则影响持久客户机的下次启动。如果指定–current，则根据客户机当前状态等同于–live或–config。如果存在cpulist，则可以同时给出–live和–config标志，但–current*是互斥的。如果未指定标志，则行为因hypervisor而异。 注意：表达式是按顺序计算的，因此”0-15,^8”与”9-14,0-7,15”相同，但与”^8,0-15”不同。 iothreadset语法： 1234iothreadset domain iothread_id [ [--poll-max-ns ns] [--poll-grow factor][--poll-shrink divisor] [--thread-pool-min value][--thread-pool-max value]][ [--config] [--live] | [--current]] 使用指定的iothread_id修改域的现有iothread。*–poll-max-ns提供IOThread允许的最大轮询间隔(纳秒)。如果提供0(零)，则禁用该IOThread的轮询。–poll-grow是调整当前轮询时间以达到最大轮询时间的因子。如果提供0(零)，则将使用默认因子。–poll-shrink*是减少当前轮询时间以低于最大轮询间隔的商数。如果提供0(零)，则将使用默认商数。对于运行中的客户机，轮询值是纯动态的。保存、销毁、停止等操作将导致轮询值在下一次启动、恢复等时返回hypervisor默认值。 –thread-pool-min和*–thread-pool-max选项分别设置给定iothread工作池中线程数的下限和上限。对于非活动配置的更改，可以指定-1以从域配置中删除相应的边界。对于运行中客户机的更改，建议首先设置上限(–thread-pool-max*)，然后才设置下限(–thread-pool-min)。允许下限与上限相同，但不允许上限为零值。 如果指定*–live，则影响运行中的客户机。如果客户机未运行，则返回错误。如果指定–current或未指定–live，则视为指定了–live*。(这里的”current”表示客户机当前的状态：运行中或离线。) managedsave语法： 1managedsave domain [--bypass-cache] [&#123;--running | --paused&#125;] [--verbose] 保存并销毁(停止)运行中的域，以便以后可以从相同的状态重新启动。当下次为该域运行virsh start命令时，它将自动从此保存状态启动。如果指定*–bypass-cache*，保存将绕过文件系统缓存，尽管这可能会减慢操作速度。 可以使用domjobinfo virsh命令监视进度，并使用domjobabort命令(由另一个virsh实例发送)取消。另一个选项是向运行managedsave命令的virsh进程发送SIGINT(通常使用Ctrl-C)。*–verbose*显示保存进度。 通常，启动托管保存将根据保存时域的状态决定是运行还是暂停；传递*–running或–paused*标志将允许覆盖启动应使用的状态。 可以使用dominfo命令查询域当前是否有任何托管保存映像。 managedsave-define语法： 1managedsave-define domain xml [&#123;--running | --paused&#125;] 更新稍后启动domain时将使用的域XML。xml参数必须是包含替代XML的文件名，仅更改域XML中主机特定的部分。例如，它可以用于更改磁盘文件路径。 托管保存映像记录域应启动到运行状态还是暂停状态。通常，此命令不会更改记录的状态；传递*–running或–paused*标志将允许覆盖启动应使用的状态。 managedsave-dumpxml语法： 1managedsave-dumpxml [--security-info] [--xpath EXPRESSION] [--wrap] domain 提取使用managedsave命令创建保存状态文件file时生效的域XML。使用*–security-info*还将包括安全敏感信息。 如果 –xpath 参数提供XPath表达式，则将对输出XML求值并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但是为了便于进一步处理，**–wrap**参数将使匹配节点包装在公共根节点中。 managedsave-edit语法： 1managedsave-edit domain [&#123;--running | --paused&#125;] 编辑与managedsave命令创建的domain保存状态文件关联的XML配置。 托管保存映像记录域应启动到运行状态还是暂停状态。通常，此命令不会更改记录的状态；传递*–running或–paused*标志将允许覆盖恢复应使用的状态。 这相当于： 123virsh managedsave-dumpxml domain-name &gt; state-file.xmlvi state-file.xml (或使用其他文本编辑器进行更改)virsh managedsave-define domain-name state-file-xml 只是它进行了一些错误检查。 使用的编辑器可以由$VISUAL或$EDITOR环境变量提供，默认为vi。 managedsave-remove语法： 1managedsave-remove domain 删除域的managedsave状态文件(如果存在)。这确保域下次启动时将执行完整引导。 maxvcpus语法： 1maxvcpus [type] 提供此连接上客户机VM支持的虚拟CPU最大数量。如果提供，type参数必须是XML中元素的有效类型属性。 memtune语法： 123memtune domain [--hard-limit size] [--soft-limit size][--swap-hard-limit size][--min-guarantee size] [ [--config] [--live] | [--current]] 允许您显示或设置域内存参数。不带标志时，显示当前设置；带标志时，如果hypervisor支持，则调整相应的限制。LXC和QEMU&#x2F;KVM支持*–hard-limit、–soft-limit和–swap-hard-limit。–min-guarantee*仅由ESX hypervisor支持。这些限制都是带单位的整数(参见上面的NOTES)，如果没有后缀，则默认为kibibytes(1024字节的块)。Libvirt向上舍入到最接近的kibibyte。一些hypervisor需要比KiB更大的粒度，不是偶数倍的请求将被向上舍入。例如，vSphere&#x2F;ESX将参数向上舍入到mebibytes(1024 kibibytes)。 如果指定*–live，则影响运行中的客户机。如果指定–config，则影响持久客户机的下次启动。如果指定–current，则根据客户机当前状态等同于–live或–config。可以同时给出–live和–config标志，但–current*是互斥的。如果未指定标志，则行为因hypervisor而异。 对于QEMU&#x2F;KVM，参数应用于整个QEMU进程。因此，在计算它们时，需要将客户机RAM、客户机视频RAM和QEMU本身的一些内存开销相加。最后一部分很难确定，因此需要猜测和尝试。 对于LXC，显示的hard_limit值是来自XML的当前内存设置或virsh setmem命令的结果。 –hard-limit 客户机可以使用的最大内存。 –soft-limit 在内存争用期间强制执行的内存限制。 –swap-hard-limit 客户机可以使用的内存加交换空间的最大值。这必须大于提供的hard-limit值。 –min-guarantee 客户机的保证最小内存分配。 将这些限制的值指定为-1表示无限制。 metadata语法： 12metadata domain [ [--live] [--config] | [--current]][--edit] [uri] [key] [set] [--remove] 显示或修改域的自定义XML元数据。元数据是用户定义的XML，允许在域定义中存储任意XML数据。可以在域XML中存储多个单独的自定义元数据片段。这些片段由通过uri参数提供的私有XML命名空间标识。(另请参阅处理域文本元数据的desc。) 标志*–live或–config选择此命令是处理域的实时定义还是持久定义。如果同时指定–live和–config，则在获取当前描述时–config选项优先，而在设置描述时同时更新实时配置和配置。–current*是互斥的，如果未指定这些标志，则隐含。 标志*–remove指定应删除由uri*参数指定的元数据元素，而不是更新。 标志*–edit指定应打开由uri参数标识的元数据的编辑器，并在之后保存内容。否则，可以通过set*参数提供新内容。 当通过*–edit或set设置元数据时，必须指定key*参数，并用于前缀自定义元素以将它们绑定到私有命名空间。 如果未指定*–edit和set，则显示与uri*命名空间对应的XML元数据，而不是修改它。 migrate语法： 12345678910111213141516171819202122migrate [--live] [--offline] [--direct] [--p2p [--tunnelled]][--persistent] [--undefinesource] [--suspend][--copy-storage-all][--copy-storage-inc] [--change-protection] [--unsafe][--verbose][--rdma-pin-all] [--abort-on-error] [--postcopy][--postcopy-after-precopy] [--postcopy-resume] [--zerocopy]domain desturi [migrateuri] [graphicsuri] [listen-address] [dname][--timeout seconds [--timeout-suspend | --timeout-postcopy]][--xml file][--migrate-disks disk-list] [--migrate-disks-detect-zeroes disk-list][--disks-port port][--compressed] [--comp-methods method-list][--comp-mt-level] [--comp-mt-threads] [--comp-mt-dthreads][--comp-xbzrle-cache] [--comp-zlib-level] [--comp-zstd-level][--auto-converge] [auto-converge-initial][auto-converge-increment] [--persistent-xml file] [--tls][--postcopy-bandwidth bandwidth][--parallel [--parallel-connections connections]][--bandwidth bandwidth] [--tls-destination hostname][--disks-uri URI] [--copy-storage-synchronous-writes][--available-switchover-bandwidth bandwidth] 将域迁移到另一主机。添加*–live进行实时迁移；&lt;–p2p&gt;进行点对点迁移；–direct进行直接迁移；或–tunnelled进行隧道迁移。–offline迁移域定义而不在目标主机上启动域，也不在源主机上停止它。离线迁移可用于非活动域，并且必须与–persistent*选项一起使用。 –persistent使域在目标主机上持久化(参见下面与*–xml一起使用时的特殊情况)，–undefinesource在源主机上取消定义域，–suspend*使域在目标主机上暂停。 –copy-storage-all表示使用完整磁盘复制的非共享存储迁移，*–copy-storage-inc表示使用增量复制的非共享存储迁移(源和目标共享相同的基础映像)。在这两种情况下，磁盘映像必须存在于目标主机上，–copy-storage-…选项仅告诉libvirt将数据从源主机上的映像传输到目标主机上相同位置的映像。默认情况下，仅传输非共享非只读映像。使用–migrate-disks显式指定要通过逗号分隔的disk-list参数传输的磁盘目标列表。–migrate-disks-detect-zeroes选项接受逗号分隔的磁盘目标名称列表，为列出的迁移磁盘启用零块检测。这些块不会被传输或在目标上分配，有效地稀疏化磁盘，但以CPU开销为代价。用户必须确保在使用此选项之前，任何预先创建的存储源都被清除，从而读取全零，否则目标映像可能会损坏。使用–copy-storage-synchronous-writes*标志时，磁盘数据迁移将同步处理客户机磁盘写入到原始源和目标，以确保磁盘迁移收敛，但可能会降低突发性能。 –change-protection强制在迁移过程中不对域进行不兼容的配置更改；如果hypervisor支持，此标志会隐式启用，但如果hypervisor缺乏更改保护支持，可以显式使用此标志来拒绝迁移。 –verbose显示迁移进度。 –abort-on-error如果在迁移过程中发生软错误(例如I&#x2F;O错误)，则取消迁移。 –postcopy在迁移中启用后复制逻辑，但实际上不启动后复制，即迁移以预复制模式启动。一旦迁移运行，用户可以从另一个virsh实例发送migrate-postcopy命令切换到后复制，或使用*–postcopy-after-precopy与–postcopy一起让libvirt在预复制第一遍完成后自动切换到后复制。可以使用–postcopy-bandwidth限制后复制阶段消耗的最大带宽。可以使用–bandwidth限制预复制阶段消耗的最大带宽。如果在迁移处于后复制模式时主机之间的连接中断，则域无法在源主机或目标主机上恢复，migrate命令将报告错误，使域在两个主机上都处于活动状态。要从这种情况中恢复，请使用额外的–postcopy-resume*标志重复原始migrate命令。 –auto-converge强制在实时迁移期间收敛。可以使用auto-converge-initial设置初始客户机CPU限制率。如果初始限制率不足以确保收敛，则定期增加auto-converge-increment。 –rdma-pin-all可以与RDMA迁移(即当migrateuri以rdma:&#x2F;&#x2F;开头时)一起使用，告诉hypervisor在迁移开始前一次性固定域的所有内存，而不是根据需要固定内存页。对于QEMU&#x2F;KVM，这需要使用hard_limit内存调优元素(在域XML中)并设置为域配置的最大内存加上QEMU进程本身消耗的任何内存。注意不要将内存限制设置得过高(从而允许域锁定主机的大部分内存)。这样做可能对域和主机本身都危险，因为主机的内核可能会耗尽内存。 –zerocopy请求使用零复制机制迁移内存页。对于QEMU&#x2F;KVM，这意味着QEMU将被临时允许锁定主机内存中的所有客户机页，尽管同时只会锁定那些排队传输的页。 注意：各个hypervisor通常不支持所有可能的迁移类型。例如，QEMU不支持直接迁移。 在某些情况下，libvirt可能会拒绝迁移域，因为这样做可能会导致潜在问题，如数据损坏，因此迁移被认为是不安全的。对于QEMU域，如果域使用磁盘而没有显式将缓存模式设置为”none”，则可能会发生这种情况。除非磁盘映像存储在一致的集群文件系统(如GFS2或GPFS)上，否则迁移此类域是不安全的。如果您确定迁移是安全的，或者您只是不在乎，请使用*–unsafe*强制迁移。 dname用于在迁移期间将域重命名为新名称，通常也可以省略。 –xml file虽然通常不需要，但可以用于在目标上提供替代XML文件，以对域XML的任何主机特定部分提供更大范围的更改，例如考虑源和目标在访问底层存储时的命名差异。如果*–xml与–persistent一起使用，通常需要通过–persistent-xml*(见下文)提供持久XML定义，该定义以与传递给*–file*的XML相同的方式修复。 如果启用*–persistent，可以使用–persistent-xml* file提供替代XML文件，该文件将用作目标主机上的持久客户机定义。 如果启用*–live，–timeout* seconds告诉virsh在实时迁移超过该秒数时运行指定的操作。可以指定*–timeout-suspend，域将在超时后暂停，迁移将离线完成；如果命令行上未指定–timeout-``选项，则这是默认值。当使用**–timeout-postcopy时，virsh将在超时后从预复制切换到后复制；迁移必须使用–postcopy*选项启动才能工作。 –compressed激活压缩，使用*–comp-methods选择压缩方法。支持的方法有”mt”、”xbzrle”、”zlib”和”zstd”。支持的方法集及其组合取决于hypervisor和迁移选项。QEMU仅在使用–parallel时支持”zlib”和”zstd”方法，并且不能同时使用。当未指定方法时，将使用hypervisor默认方法。只要不使用–parallel，QEMU默认为”xbzrle”。对于–parallel迁移，QEMU不提供任何默认压缩方法，因此必须使用–comp-method显式指定。可以进一步调整压缩方法。–comp-mt-level设置”mt”方法的压缩级别。值范围为0到9，其中1是最大速度，9是最大压缩。–comp-mt-threads和–comp-mt-dthreads分别设置源上的压缩线程数和目标上的解压缩线程数。–comp-xbzrle-cache设置页面缓存大小(字节)。–comp-zlib-level设置使用”zlib”方法时的压缩级别。值范围为0到9，默认为1，其中0是无压缩，1是最大速度，9是最大压缩。–comp-zstd-level*设置使用”zstd”方法时的压缩级别。值范围为0到20，默认为1，其中0是无压缩，1是最大速度，20是最大压缩。 提供*–tls使迁移使用主机配置的TLS设置(参见&#x2F;etc&#x2F;libvirt&#x2F;qemu.conf中的migrate_tls_x509_cert_dir)以执行域的迁移。使用需要在源和目标上正确设置TLS。通常，目标主机的TLS证书必须与主机名匹配才能使TLS验证成功。当证书与目标主机名不匹配且已知预期证书的主机名时，可以在启动迁移时使用–tls-destination传递预期的hostname*。 –parallel选项将使迁移数据通过多个并行连接发送。可以使用*–parallel-connections*设置此类连接的数量。并行连接可能有助于饱和源和目标之间的网络链接，从而加快迁移速度。 可以通过中断virsh(通常使用Ctrl-C)或从另一个virsh实例发送domjobabort命令来取消正在运行的迁移。 desturi和migrateuri参数可用于控制迁移使用的目标。desturi对于托管迁移很重要，但对于直接迁移未使用；migrateuri对于直接迁移是必需的，但对于托管迁移通常可以自动确定。 注意：普通迁移和点对点迁移的desturi参数有不同的语义： 普通迁移：desturi是从客户端机器看到的目标主机的地址。 点对点迁移：desturi是从源机器看到的目标主机的地址。 在特殊情况下，当您需要完全控制连接和&#x2F;或libvirt无法远程访问网络时，可以在URI中使用UNIX传输并指定查询中的套接字路径，例如对于qemu驱动程序，您可以使用： 1qemu+unix:///system?socket=/path/to/socket 当未指定migrateuri时，libvirt将自动确定hypervisor特定的URI。一些hypervisor(包括QEMU)具有可选的”migration_host”配置参数(当主机有多个网络接口时有用)。如果未指定此参数，libvirt通过查找目标主机的配置主机名来确定名称。 在以下几种情况下，指定migrateuri可能有帮助： 配置的主机名不正确，或DNS有问题。如果主机的主机名无法解析以匹配其公共IP地址之一，则libvirt将生成不正确的URI。在这种情况下，应显式指定migrateuri，使用IP地址或正确的主机名。 主机有多个网络接口。如果主机有多个网络接口，可能希望出于安全或性能原因通过特定接口发送迁移数据流。在这种情况下，应显式指定migrateuri，使用与要使用的网络关联的IP地址。 防火墙限制了可用端口。当libvirt生成迁移URI时，它将使用hypervisor特定规则选择端口号。一些hypervisor只需要在防火墙中打开单个端口，而其他hypervisor则需要整个端口号范围。在后一种情况下，可以指定migrateuri以选择默认范围之外的特定端口号，以符合本地防火墙策略。 desturi使用UNIX传输方法。在这种高级情况下，libvirt不应猜测migrateuri，而应使用UNIX套接字路径URI指定： 1unix:///path/to/socket 有关迁移URI的更多详细信息，请参见https://libvirt.org/migration.html#uris。 可选的graphicsuri覆盖用于在迁移结束时自动重新连接图形客户端的连接参数。如果省略，libvirt将基于目标主机IP地址计算参数。如果客户端无法直接访问虚拟化主机连接的网络，需要通过代理连接，则可以使用graphicsuri指定客户端应连接的地址。URI格式如下： 1protocol://hostname[:port]/[?parameters] 其中protocol是”spice”或”vnc”，parameters是由’&amp;’分隔的协议特定参数列表。当前识别的参数是”tlsPort”和”tlsSubject”。例如， 1spice://target.host.com:1234/?tlsPort=4567 可选的listen-address设置目标端hypervisor应绑定以进行传入迁移的监听地址。接受IPv4和IPv6地址以及主机名(解析在目标端完成)。一些hypervisor不支持指定监听地址，如果使用此参数将返回错误。如果desturi使用UNIX传输方法，则不能使用此参数。 可选的disks-port设置目标端hypervisor应绑定以进行磁盘传入通信的端口。目前仅QEMU支持。 也可以指定可选的disks-uri(与disks-port互斥)以指定迁移磁盘时远程hypervisor应绑定&#x2F;连接的内容。这可以是tcp:&#x2F;&#x2F;address:port以指定监听地址(覆盖磁盘迁移的--migrate-uri和--listen-address)和端口，或者如果您需要磁盘迁移通过具有指定路径的UNIX套接字进行，则可以是unix:&#x2F;&#x2F;&#x2F;path&#x2F;to&#x2F;socket。在这种情况下，您需要确保相同的套接字路径对源和目标hypervisor都可访问，并且连接到源上的套接字(在hypervisor在目标上创建后)实际上将连接到目标。如果您使用SELinux(至少在源主机上)，您需要确保源上的套接字对libvirtd&#x2F;QEMU可访问以进行连接。Libvirt无法更改现有套接字的上下文，因为它与套接字的文件表示不同，并且上下文由其创建者选择(通常使用*setsockcreatecon{,_raw}()*函数)。 可选的--available-switchover-bandwidth覆盖自动计算的带宽(MiB&#x2F;s)，该带宽用于(预复制)迁移的最后阶段，在此期间CPU停止，所有剩余内存和设备状态被传输。了解此带宽对于准确估计域停机时间和决定切换的正确时刻很重要。通常这将基于迁移使用的带宽进行估计，但这可能低于实际可用带宽。当迁移会反复迭代认为没有足够的带宽来遵守配置的最大停机时间时，使用此选项可能有助于迁移收敛。 migrate-compcache语法： 1migrate-compcache domain [--size bytes] 设置和&#x2F;或获取用于在实时迁移期间压缩重复传输内存页的缓存大小(字节)。不带size调用时，该命令仅打印压缩缓存的当前大小。指定size时，请求hypervisor将压缩缓存更改为size字节，然后打印当前大小(由于hypervisor的舍入，结果可能与请求的大小不同)。size选项应在域实时迁移时使用，作为对迁移进度和从domjobinfo获得的压缩缓存未命中数增加的反应。 migrate-getmaxdowntime语法： 1migrate-getmaxdowntime domain 获取正在实时迁移到另一主机的域的最大可容忍停机时间。这是实时迁移结束时允许客户机停机的毫秒数。 migrate-getspeed语法：migrate-getspeed 域名 [–postcopy]获取域名的最大迁移带宽（以MiB&#x2F;s为单位）。如果指定了--postcopy选项，则命令将获取在后复制迁移阶段允许的最大带宽。 migrate-postcopy语法：migrate-postcopy 域名将当前迁移从预复制切换到后复制。此操作仅支持以--postcopy选项启动的迁移。 migrate-setmaxdowntime语法：migrate-setmaxdowntime 域名 停机时间为正在实时迁移到另一主机的域名设置最大可容忍停机时间。停机时间是允许客户机在实时迁移结束时停止运行的毫秒数。 migrate-setspeed语法：migrate-setspeed 域名 带宽 [–postcopy]为正在迁移到另一主机的域名设置最大迁移带宽（以MiB&#x2F;s为单位）。带宽被解释为无符号长整型值。指定负值将提供一个基本无限制的值给虚拟机监控程序，虚拟机监控程序可以选择拒绝该值或将其转换为允许的最大值。如果指定了--postcopy选项，则命令将设置在后复制迁移阶段允许的最大带宽。 numatune语法：numatune 域名 [–mode 模式] [–nodeset 节点集][ [–config] [–live] | [–current]]设置或获取域名的NUMA参数，对应于域XML的&lt;numatune&gt;元素。不带标志时，显示当前设置。 模式可以是strict、interleave、preferred或restrictive之一，或者是虚拟机监控程序支持的virDomainNumatuneMemMode枚举中的任何有效数字。对于运行中的域名，模式无法更改，节点集只能在域名以restrictive模式启动时更改。 节点集是主机用于运行域名的NUMA节点列表。其语法为逗号分隔的列表，使用-表示范围，^表示排除节点。 如果指定--live，则设置运行中客户机的调度信息。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等同于--live或--config。 对于Linux主机上的运行中客户机，更改域名的NUMA参数并不意味着客户机内存会立即移动到不同的节点集。内存迁移取决于客户机活动，空闲客户机的内存将在其之前的节点集中保留更长时间。VFIO设备的存在也会将部分客户机内存锁定在启动客户机时使用的节点集中，无论节点集如何更改。 perf语法：perf 域名 [–enable 事件规范] [–disable 事件规范][ [–config] [–live] | [–current]]获取当前性能事件设置或为客机域名启用&#x2F;禁用特定性能事件。 Perf是Linux中的性能分析工具，可以检测CPU性能计数器、跟踪点、kprobes和uprobes（动态跟踪）。Perf支持一系列可测量事件，可以测量来自不同来源的事件。例如，一些事件是纯内核计数器，称为软件事件，包括上下文切换、小故障等。目前仅QEMU&#x2F;KVM支持此命令。 事件规范是一个或多个事件的逗号分隔列表。有效事件名称包括： 有效性能事件名称 cmt - 监控平台上应用程序对缓存的使用情况。 mbmt - 监控一级缓存和另一级之间的总系统内存带宽。 mbml - 限制通过套接字上内存控制器的数据量（字节&#x2F;秒）。 cache_misses - 监控平台上应用程序的缓存未命中次数。 cache_references - 监控平台上应用程序的缓存命中次数。 instructions - 监控平台上应用程序执行的指令数。 cpu_cycles - 监控CPU周期数（总&#x2F;经过）。可与指令一起使用以获取每条指令的周期数。 branch_instructions - 监控平台上应用程序执行的分支指令数。 branch_misses - 监控平台上应用程序执行的分支未命中数。 bus_cycles - 监控平台上应用程序执行的总线周期数。 stalled_cycles_frontend - 监控平台上应用程序在指令处理器流水线前端停滞的CPU周期数。 stalled_cycles_backend - 监控平台上应用程序在指令处理器流水线后端停滞的CPU周期数。 ref_cpu_cycles - 监控不受CPU频率缩放影响的总CPU周期数。 cpu_clock - 监控平台上应用程序消耗的CPU时钟时间。 task_clock - 监控平台上应用程序消耗的任务时钟时间。 page_faults - 监控平台上应用程序的页面错误数。 context_switches - 监控平台上应用程序的上下文切换数。 cpu_migrations - 监控平台上应用程序的CPU迁移数。 page_faults_min - 监控平台上应用程序的小页面错误数。 page_faults_maj - 监控平台上应用程序的大页面错误数。 alignment_faults - 监控平台上应用程序的对齐错误数。 emulation_faults - 监控平台上应用程序的模拟错误数。 注意：可以使用domstats命令通过--perf标志检索统计信息。 如果指定--live，则影响运行中的客户机。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等同于--live或--config。 reboot语法：reboot 域名 [–mode 模式列表]重启域名。此操作类似于从控制台运行重启命令。命令在执行重启操作后立即返回，可能远早于域名实际重启。 域名重启的确切行为由其XML定义中的on_reboot参数设置。 默认情况下，虚拟机监控程序会尝试选择适当的关闭方法。要指定替代方法，--mode参数可以指定一个逗号分隔的列表，包括acpi、agent、initctl、signal和paravirt。驱动程序尝试每种模式的顺序未定义，与virsh指定的顺序无关。要严格控制顺序，请一次使用单一模式并重复命令。 reset语法：reset 域名立即重置域名，无需任何客户机关闭。重置模拟机器上的电源重置按钮，所有客户硬件看到RST线设置并重新初始化内部状态。注意：无客户操作系统关闭的重置可能导致数据丢失。 restore语法：restore 状态文件 [–bypass-cache] [–xml 文件][–running | –paused] [–reset-nvram][–parallel-channels]从virsh保存的状态文件恢复域名。有关更多信息，请参阅save。 如果指定--bypass-cache，则恢复将绕过文件系统缓存。根据具体场景，这可能会减慢或加快操作。 --xml文件通常省略，但可用于提供替代XML文件，仅更改域XML中主机特定部分。例如，可用于考虑在客户机保存后由于磁盘快照导致的底层存储文件命名差异。 通常，恢复保存的镜像将使用保存镜像中记录的状态来决定是运行还是暂停；传递--running或--paused标志可以覆盖恢复时应使用的状态。 如果指定--reset-nvram，则任何现有的NVRAM文件将被删除并从原始模板重新初始化。 --parallel-channels选项可以指定从文件加载内存时使用的并行IO通道数。并行保存可以显著减少保存大型内存域所需的时间。 注意：为避免损坏域内文件系统内容，除非已将存储卷还原到创建状态文件时的内容，否则不应重复使用保存的状态文件进行第二次恢复。 resume语法：resume 域名将域名从暂停状态移出。这将允许先前暂停的域名现在有资格由底层虚拟机监控程序调度。 save语法：save 域名 状态文件 [–bypass-cache] [–xml 文件][–image-format 格式] [–parallel-channels 通道][–running | –paused] [–verbose]将运行中的域名（RAM，而非磁盘状态）保存到状态文件中，以便稍后恢复。保存后，域名将不再在系统上运行，因此为域名分配的内存将可供其他域名使用。virsh restore从此状态文件恢复。 如果指定--bypass-cache，则保存将绕过文件系统缓存。根据具体场景，这可能会减慢或加快操作。 可以使用domjobinfo virsh命令监视进度，并使用domjobabort命令（由另一个virsh实例发送）取消。另一种选择是向运行保存命令的virsh进程发送SIGINT（通常使用Ctrl-C）。--verbose显示保存的进度。 这大致相当于在运行中的计算机上执行休眠，具有所有相同的限制。恢复时，开放的网络连接可能会中断，因为TCP超时可能已过期。 --xml文件通常省略，但可用于提供替代XML文件，仅更改域XML中主机特定部分。例如，可用于考虑在客户机保存后由于磁盘快照导致的底层存储文件命名差异。 通常，恢复保存的镜像将决定是运行还是暂停；传递--running或--paused标志可以覆盖恢复时应使用的状态。 --image-format选项可以更改用于将数据保存到文件的默认图像格式。有关详细信息，请参阅qemu.conf配置文件。 --parallel-channels选项可以指定将内存保存到文件时使用的并行IO通道数。使用并行IO通道需要使用稀疏图像保存格式。并行保存可以显著减少保存大型内存域所需的时间。 域保存状态文件假定磁盘映像在创建和恢复点之间保持不变。对于更完整的系统恢复点（磁盘状态与内存状态一起保存），请参阅快照系列命令。 save-image-define语法：save-image-define 文件 xml [–running | –paused]更新稍后在restore命令中使用文件时将使用的域XML。xml参数必须是包含替代XML的文件名，仅更改域XML中主机特定部分。例如，可用于考虑在客户机保存后由于磁盘快照导致的底层存储文件命名差异。 保存镜像记录域名应恢复到运行还是暂停状态。通常，此命令不会更改记录的状态；传递--running或--paused标志可以覆盖恢复时应使用的状态。 save-image-dumpxml语法：save-image-dumpxml [–security-info] [–xpath 表达式][–wrap] 文件提取在通过save命令创建保存状态文件文件时生效的域XML。使用--security-info还将包括安全敏感信息。 如果--xpath参数提供XPath表达式，则将对输出XML求值，并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但为了便于进一步处理，--wrap参数将导致匹配节点包装在公共根节点中。 save-image-edit语法：save-image-edit 文件 [–running | –paused]编辑由save命令创建的保存状态文件文件关联的XML配置。 保存镜像记录域名应恢复到运行还是暂停状态。通常，此命令不会更改记录的状态；传递--running或--paused标志可以覆盖恢复时应使用的状态。 此命令等效于： virsh save-image-dumpxml 状态文件 &gt; 状态文件.xmlvi 状态文件.xml（或使用其他文本编辑器进行更改）virsh save-image-define 状态文件 状态文件.xml 但会进行一些错误检查。 使用的编辑器可以由$VISUAL或$EDITOR环境变量提供，默认为vi。 schedinfo语法：schedinfo 域名 [ [–config] [–live] | [–current]][ [–set] 参数&#x3D;值]…schedinfo [–weight 数字] [–cap 数字] 域名显示（和设置）域调度器参数。每个虚拟机监控程序的可用参数如下： LXC（posix调度器）：cpu_shares、vcpu_period、vcpu_quotaQEMU&#x2F;KVM（posix调度器）：cpu_shares、vcpu_period、vcpu_quota、emulator_period、emulator_quota、global_period、global_quota、iothread_period、iothread_quotaXen（credit调度器）：weight、capESX（分配调度器）：reservation、limit、shares 如果指定--live，则设置运行中客户机的调度信息。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等同于--live或--config。 注意：cpu_shares参数在cgroups v1中的有效值范围为2-262144，在cgroups v2中为1-10000。 注意：weight和cap参数仅针对XEN_CREDIT调度器定义。 注意：vcpu_period、emulator_period和iothread_period参数的有效值范围为1000-1000000或0，vcpu_quota、emulator_quota和iothread_quota参数的有效值范围为1000-17592186044415或小于0。值为0等同于不指定该参数。 screenshot语法：screenshot 域名 [图像文件路径] [–screen 屏幕ID]捕获当前域名控制台的屏幕截图并存储到文件中。如果虚拟机监控程序支持域名的多个显示器，则屏幕ID允许指定要捕获的屏幕。它是屏幕的序号。对于多个显卡，头设备先枚举，例如两个显卡各有四个头设备，屏幕ID 5表示第二个显卡的第二个头设备。 send-key语法：send-key 域名 [–codeset 代码集] [–holdtime 保持时间]键码…将键码序列解析为发送到域名的按键。每个键码可以是数值或对应代码集中的符号名称。如果指定--holdtime，则每个按键将保持指定的毫秒数。默认代码集为linux，但使用--codeset选项可以选择其他代码集。 如果指定多个键码，则它们将同时发送到客户机，并且可能以随机顺序接收。如果需要不同的按键，必须使用多个send-key调用。 支持的代码集包括： linux：数值由Linux通用输入事件子系统定义。符号名称与相应的Linux键常量宏名称匹配。 xt：数值由原始XT键盘控制器定义。不提供符号名称。 atset1：数值由AT键盘控制器集1（即XT兼容集）定义。不提供符号名称。 atset2：数值由AT键盘控制器集2定义。不提供符号名称。 atset3：数值由AT键盘控制器集3（即PS&#x2F;2兼容集）定义。不提供符号名称。 os_x：数值由macOS键盘输入子系统定义。符号名称与相应的macOS键常量宏名称匹配。 xt_kbd：数值由Linux KBD设备定义。这些是原始XT代码集的变体，但扩展键码的编码通常不同。不提供符号名称。 win32：数值由Win32键盘输入子系统定义。符号名称与相应的Win32键常量宏名称匹配。 usb：数值由USB HID规范定义。不提供符号名称。 qnum：数值由QNUM扩展定义，用于发送原始键码。这些是XT代码集的变体，但扩展键码的第二字节低位设置为1，而不是第一字节高位。不提供符号名称。 示例： # 使用xt代码集发送三个按键’k’、’e’、’y’。这些按键同时按下，客户机可能以随机顺序接收。virsh send-key dom –codeset xt 37 18 21 # 发送一个按键’right-ctrl+C’virsh send-key dom KEY_RIGHTCTRL KEY_C # 发送一个Tab键，保持1秒virsh send-key –holdtime 1000 0xf send-process-signal语法：send-process-signal 域名ID 进程ID 信号名向虚拟域域名ID中运行的进程进程ID发送信号信号名。进程ID是虚拟域命名空间中的进程ID。 信号名可以是整数信号常量号，或以下符号名称之一： “nop”, “hup”, “int”, “quit”, “ill”,“trap”, “abrt”, “bus”, “fpe”, “kill”,“usr1”, “segv”, “usr2”, “pipe”, “alrm”,“term”, “stkflt”, “chld”, “cont”, “stop”,“tstp”, “ttin”, “ttou”, “urg”, “xcpu”,“xfsz”, “vtalrm”, “prof”, “winch”, “poll”,“pwr”, “sys”, “rt0”, “rt1”, “rt2”, “rt3”,“rt4”, “rt5”, “rt6”, “rt7”, “rt8”, “rt9”,“rt10”, “rt11”, “rt12”, “rt13”, “rt14”, “rt15”,“rt16”, “rt17”, “rt18”, “rt19”, “rt20”, “rt21”,“rt22”, “rt23”, “rt24”, “rt25”, “rt26”, “rt27”,“rt28”, “rt29”, “rt30”, “rt31”, “rt32” 符号名称可以可选地以sig或sig_为前缀，并且可以是大写或小写。 示例：1234virsh send-process-signal myguest 1 15 virsh send-process-signal myguest 1 term virsh send-process-signal myguest 1 sigterm virsh send-process-signal myguest 1 SIG_HUP set-lifecycle-action语法：set-lifecycle-action 域名 类型 动作[ [–config] [–live] | [–current]]为指定的生命周期类型设置生命周期动作。有效类型为”poweroff”、”reboot”和”crash”，对于每种类型，有效动作为”destroy”、”restart”、”rename-restart”、”preserve”。对于类型”crash”，还支持附加动作”coredump-destroy”和”coredump-restart”。 set-user-password语法：set-user-password 域名 用户 密码 [–encrypted]设置客户域中用户帐户的密码。 如果指定--encrypted，则假定密码已按客户操作系统要求的方法加密。 对于QEMU&#x2F;KVM，这需要配置并运行客户代理。 set-user-sshkeys语法：set-user-sshkeys 域名 用户 [–file 文件] [–reset | –remove]从文件读取密钥并追加到客户域名中用户的SSH授权密钥文件中。文件中的密钥必须位于单独的行上，并且每行必须遵循sshd(8)定义的授权密钥格式。 如果指定--reset，则在追加新密钥之前清除客户授权密钥文件内容。作为一种特殊情况，如果提供--reset但未提供文件，则不添加新密钥，并清除授权密钥文件。 如果指定--remove，则从授权密钥文件中删除从文件读取的密钥，而不是添加新密钥。如果密钥不存在于文件中，不会视为错误。 setmaxmem语法：setmaxmem 域名 大小 [ [–config] [–live] | [–current]]更改客户域的最大内存分配限制。如果指定--live，则影响运行中的客户机。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等同于--live或--config。 大小是一个缩放整数（参见NOTES）；默认单位为KiB（1024字节块），除非提供后缀（旧选项名称--kilobytes作为已弃用的同义词）。Libvirt向上舍入到最接近的KiB。某些虚拟机监控程序需要比KiB更大的粒度，不是偶数的请求将向上舍入。例如，vSphere&#x2F;ESX将参数向上舍入到MiB（1024 KiB）。 setmem语法：setmem 域名 大小 [ [–config] [–live] | [–current]]更改客户域的内存分配。如果指定--live，则对运行中的客户机执行内存气球操作。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等同于--live或--config。 大小是一个缩放整数（参见NOTES）；默认单位为KiB（1024字节块），除非提供后缀（旧选项名称--kilobytes作为已弃用的同义词）。Libvirt向上舍入到最接近的KiB。某些虚拟机监控程序需要比KiB更大的粒度，不是偶数的请求将向上舍入。例如，vSphere&#x2F;ESX将参数向上舍入到MiB（1024 KiB）。 对于Xen，仅当域是半虚拟化或运行PV气球驱动程序时，才能调整运行中域的内存。 对于LXC，设置的值是cgroups的limit_in_bytes值或用户内存（包括文件缓存）的最大量。在容器内查看内存时，这是&#x2F;proc&#x2F;meminfo中的”MemTotal”值。从主机查看值时，使用virsh memtune命令。要查看当前使用的内存和允许设置的最大值，请使用virsh dominfo命令。 setvcpus语法：setvcpus 域名 数量 [–maximum] [ [–config] [–live] | [–current] ][–guest] [–hotpluggable]更改客户域中活动的虚拟CPU数量。默认情况下，此命令适用于活动客户域。要更改非活动客户域的设置，请使用--config标志。 数量值可能受主机、虚拟机监控程序或客户域原始描述中的限制。对于Xen，仅当域是半虚拟化时才能调整运行中域的虚拟CPU。 如果指定--config标志，则更改将保存到客户域的存储XML配置中，并在下次启动客户域时生效。 如果指定--live，则客户域必须处于活动状态，并且更改立即生效。如果虚拟机监控程序支持，可以同时指定--config和--live标志。如果在客户机完成启动之前运行此命令，客户机可能无法处理更改。 如果指定--current，则根据客户机的当前状态等同于--live或--config。 未指定标志时，假定为--live，且客户域必须处于活动状态。在这种情况下，由虚拟机监控程序决定是否也假定--config标志，从而调整XML配置以使更改持久。 如果指定--guest，则CPU计数在客户机中修改，而不是在虚拟机监控程序中。此标志仅适用于活动域，并且可能需要在客户机中配置客户代理。 要允许将vcpu添加到持久定义中，以便在域启动后可以热拔插，必须指定--hotpluggable标志。支持vcpu拔插的活动域中添加到域的vcpu会自动标记为可热拔插。 --maximum标志控制下次启动域时可以热插拔的虚拟CPU的最大数量。因此，它必须仅与--config标志一起使用，而不能与--live或--current标志一起使用。注意，如果为客机指定了处理器拓扑，则可能无法更改最大vcpu计数。 setvcpu语法：setvcpu 域名 vcpu列表 [–enable] | [–disable][ [–live] [–config] | [–current] ] 使用热插拔机制更改单个vCPU的状态。 有关vcpu列表格式的信息，请参阅vcpupin。虚拟机监控程序驱动程序可能要求vcpu列表包含完全属于一个可热插拔实体的vCPU。这通常只是一个vCPU，但某些架构（如ppc64）要求一次指定完整核心。 注意：虚拟机监控程序可能拒绝禁用某些vcpu，例如vcpu 0或其他。 如果指定--live，则影响运行中的域。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等同于--live或--config。这是默认行为。可以同时指定--live和--config标志，但--current是互斥的。 shutdown语法：shutdown 域名 [–mode 模式列表]优雅地关闭域名。此操作与域操作系统协调以执行优雅关闭，因此不能保证成功，并且可能需要可变长度的时间，具体取决于域中必须关闭的服务。 域名关闭的确切行为由其XML定义中的on_poweroff参数设置。 如果域名是临时的，则一旦客户机停止运行，任何快照和检查点的元数据将丢失，但底层内容仍然存在，并且具有相同名称和UUID的新域可以使用snapshot-create恢复快照元数据，使用checkpoint-create恢复检查点元数据。 默认情况下，虚拟机监控程序会尝试选择适当的关闭方法。要指定替代方法，--mode参数可以指定一个逗号分隔的列表，包括acpi、agent、initctl、signal和paravirt。驱动程序尝试每种模式的顺序未定义，与virsh指定的顺序无关。要严格控制顺序，请一次使用单一模式并重复命令。 start语法：start 域名或UUID [–console] [–paused][–autodestroy] [–bypass-cache] [–force-boot][–pass-fds N,M,…] [–reset-nvram] 启动（先前定义的）非活动域，从最后一个managedsave状态恢复，或者如果没有managedsave状态，则通过全新启动。如果使用--paused选项且驱动程序支持，则域将暂停；否则将运行。如果请求--console，则在创建后附加到控制台。如果请求--autodestroy，则当virsh关闭其与libvirt的连接或以其他方式退出时，客户机将自动销毁。如果指定--bypass-cache且存在managedsave状态，则恢复将绕过文件系统缓存，尽管这可能会减慢操作。如果指定--force-boot，则丢弃任何managedsave状态并执行全新启动。 如果指定--pass-fds，则参数是逗号分隔的打开文件描述符列表，应传递到客户机中。文件描述符将在客户机中重新编号，从3开始。这仅支持基于容器的虚拟化。 如果指定--reset-nvram，则任何现有的NVRAM文件将被删除并从原始模板重新初始化。 suspend语法：suspend 域名暂停运行中的域。它保留在内存中，但不再被调度。 ttyconsole语法：ttyconsole 域名输出用于域TTY控制台的设备。如果信息不可用，进程将提供退出码1。 undefine语法：undefine 域名 [–managed-save] [–snapshots-metadata][–checkpoints-metadata] [–nvram] [–keep-nvram][–storage 卷 | –remove-all-storage[–delete-storage-volume-snapshots] –wipe-storage][–tpm] [–keep-tpm]取消定义域。如果域正在运行，则将其转换为临时域，而不停止它。如果域处于非活动状态，则删除域配置。 --managed-save标志确保任何托管保存映像（参见managedsave命令）也被清理。没有此标志，尝试取消定义具有托管保存映像的域将失败。 --snapshots-metadata标志确保在取消定义非活动域时清理任何快照（参见snapshot-list命令）。没有此标志，尝试取消定义具有快照元数据的非活动域将失败。如果域处于活动状态，则忽略此标志。 --checkpoints-metadata标志确保在取消定义非活动域时清理任何检查点（参见checkpoint-list命令）。没有此标志，尝试取消定义具有检查点元数据的非活动域将失败。如果域处于活动状态，则忽略此标志。 --nvram和--keep-nvram分别指定删除或保留nvram（&#x2F;domain&#x2F;os&#x2F;nvram&#x2F;）文件。如果域具有nvram文件且省略标志，则取消定义将失败。 --storage标志接受参数卷，这是逗号分隔的卷目标名称或存储卷源路径列表，将与未定义的域一起删除。卷只能在非活动域上取消定义和删除。仅在域未定义后尝试删除卷；如果无法删除所有请求的卷，则错误消息指示仍保留的内容。如果卷路径未在域定义中找到，则视为卷已成功删除。只有由libvirt在存储池中管理的卷可以这种方式删除。请注意，这仅删除支持链的顶层映像，任何支持的存储将保留，因为它们可能共享。（参见domblklist以获取与域关联的目标名称列表）。示例：–storage vda,&#x2F;path&#x2F;to&#x2F;storage.img --remove-all-storage标志指定应删除域的所有存储卷，就像通过--storage指定一样。 --delete-storage-volume-snapshots（以前为--delete-snapshots）标志指定也应删除与存储卷关联的任何快照。需要提供--remove-all-storage标志。并非所有存储驱动程序都支持此选项，目前仅rbd。在同时删除由不支持此标志的存储驱动程序处理的卷时使用此选项将导致失败。 --wipe-storage标志指定在删除之前应擦除存储卷。 --tpm和--keep-tpm分别指定删除或保留TPM的持久状态目录结构和文件。如果省略标志，则域XML中TPM模拟器定义中的persistent_state属性确定是否保留TPM状态。 注意：对于非活动域，必须使用域名或UUID作为域名。 vcpucount语法：vcpucount 域名 [–maximum | –active] [–config | –live | –current] [–guest]打印有关给定域名的虚拟CPU计数的信息。如果未指定标志，则以表格形式列出所有可能的计数；否则，输出仅限于请求的数值。由于历史原因，表格在可以通过--active标志单独查询的行上标记为”current”，而不是与--current标志相关。 --maximum请求有关域可以通过setvcpus添加的vcpu最大上限的信息，而--active显示当前使用情况；这两个标志不能同时指定。--config需要持久客户机，并请求有关下次启动域时的信息，--live需要运行中的域并列出当前值，--current根据域的当前状态查询（如果运行中则为--live，如果非活动则为--config）；这三个标志互斥。 如果指定--guest，则从客户机的角度报告CPU计数。此标志仅适用于活动域，并且可能需要在客户机中配置客户代理。 vcpuinfo语法：vcpuinfo 域名 [–pretty]返回有关域虚拟CPU的基本信息，如vCPU数量、运行时间、与物理处理器的关联性。 使用--pretty时，CPU关联性显示为范围。 示例： 1234567891011virsh vcpuinfo fedora VCPU: 0 CPU: 0 State: running CPU time: 7,0s CPU Affinity: yyyy VCPU: 1 CPU: 1 State: running CPU time: 0,7s CPU Affinity: yyyy 状态 State字段显示虚拟CPU的当前操作状态： offline：虚拟CPU离线且不被域使用。并非所有虚拟机监控程序都支持此状态。 running：虚拟CPU可供域使用且正在运行。 blocked：虚拟CPU可供域使用但正在等待资源。并非所有虚拟机监控程序都支持此状态，在这种情况下可能报告为running。 no state：无法确定虚拟CPU状态。如果虚拟机监控程序比virsh新，则可能发生这种情况。 N&#x2F;A：没有关于虚拟CPU状态的信息可用。如果域未运行或虚拟机监控程序未报告虚拟CPU状态，则可能是这种情况。 vcpupin语法：vcpupin 域名 [vcpu] [cpulist] [ [–live] [–config] | [–current]] 查询或更改域VCPU与主机物理CPU的固定。要固定单个vcpu，指定cpulist；否则，可以查询一个vcpu或省略vcpu以一次列出所有。 cpulist是物理CPU编号列表。其语法为逗号分隔的列表，也可以使用-和^的特殊标记（例如0-4、0-3,^2）。-表示范围，^表示排除。要将vcpu固定到所有物理CPU，指定r作为cpulist。如果指定--live，则影响运行中的客户机。如果指定--config，则影响持久客户机的下一次启动。如果指定--current，则根据客户机的当前状态等同于--live或--config。如果存在cpulist，则可以同时指定--live和--config标志，但--current是互斥的。如果未指定标志，则行为因虚拟机监控程序而异。 注意：表达式按顺序求值，因此”0-15,^8”与”9-14,0-7,15”相同，但与”^8,0-15”不同。 vncdisplay语法：vncdisplay 域名 输出VNC显示的IP地址和端口号。如果信息不可用，进程将提供退出码1。 6 设备命令以下命令用于操作与域关联的设备。domain可以指定为短整数、名称或完整的UUID。要更好地理解命令选项允许的值，请阅读文档 https://libvirt.org/formatdomain.html 中关于设备部分的格式，以获取最准确的取值范围。 attach-device语法：attach-device domain FILE [ [ [–live] [–config] | [–current]] | [–persistent]]将一个设备附加到域，使用XML文件中的设备定义（如&lt;disk&gt;或&lt;interface&gt;作为顶级元素）。有关libvirt设备XML格式的详细信息，请参阅 https://libvirt.org/formatdomain.html#elementsDevices 。如果指定--config，则命令会修改持久化客户机配置，设备附加操作将在下次libvirt启动域时生效。对于cdrom和floppy设备，此命令仅替换现有设备中的介质；建议使用update-device命令完成此操作。对于透传主机设备，如果PCI设备未使用托管模式，还需使用nodedev-detach命令。 如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，则使用遗留API，其行为取决于管理程序驱动。 出于兼容性考虑，--persistent对于离线域的行为类似于--config，对于运行中的域则类似于--live --config。 注意：使用部分设备定义XML文件可能导致意外结果，因为某些字段可能自动生成，从而匹配到非预期的设备。 attach-disk语法：attach-disk domain source target [ [ [–live] [–config] | [–current]] | [–persistent]] [–targetbus bus] [–driver driver] [–subdriver subdriver] [–iothread iothread] [–cache cache] [–io io] [–type type] [–alias alias] [–mode mode] [–sourcetype sourcetype] [–source-protocol protocol] [–source-host-name hostname:port] [–source-host-transport transport] [–source-host-socket socket] [–serial serial] [–wwn wwn] [–rawio] [–address address] [–multifunction] [–print-xml] [–throttle-groups groups] 将新磁盘设备附加到域。source是文件或设备的路径，除非指定--source-protocol，此时source是网络磁盘的名称。target控制磁盘暴露给客户机操作系统的总线或设备，表示“逻辑”设备名称；可选的targetbus属性指定模拟的磁盘设备类型，可能的值取决于驱动，典型值包括ide、scsi、virtio、xen、usb、sata或sd。如果省略，总线类型将从设备名称的样式推断（例如，名为’sda’的设备通常通过SCSI总线导出）。driver可以是Xen管理程序的file、tap或phy，具体取决于访问类型；或QEMU模拟器的qemu。可以通过subdriver传递更多驱动细节。对于Xen，subdriver可以是aio；对于QEMU，subdriver应匹配磁盘源的格式，如raw或qcow2。如果未指定subdriver，则使用管理程序默认值。但默认值可能不正确，尤其是QEMU出于安全原因未配置为检测磁盘格式。type可以指定lun、cdrom或floppy作为磁盘默认值的替代，但此用法仅替换现有虚拟cdrom或floppy设备中的介质；建议改用update-device命令。alias可设置用户提供的别名。mode可指定两种特定模式readonly或shareable。sourcetype可指定源的类型（block|file|network）。cache可以是“default”、“none”、“writethrough”、“writeback”、“directsync”或“unsafe”之一。io控制I&#x2F;O的特定策略；QEMU客户机支持“threads”、“native”和“io_uring”。iothread是域IOThreads范围内可附加此磁盘的编号（仅限QEMU）。serial是磁盘设备的序列号。wwn是磁盘设备的WWN。rawio表示磁盘需要rawio能力。address是磁盘设备的地址，格式为pci:domain.bus.slot.function、scsi:controller.bus.unit、ide:controller.bus.unit、usb:bus.port、sata:controller.bus.unit或ccw:cssid.ssid.devno。Virtio-ccw设备的cssid必须设置为0xfe。multifunction表示指定的PCI地址是多功能PCI设备地址。throttle-groups是应用的节流组的逗号分隔列表。 还支持使用网络磁盘。如前所述，用户可以提供--source-protocol，此时source参数将被解释为源名称。如果用户打算提供网络磁盘或主机信息，则必须提供--source-protocol。主机信息可以使用标签--source-host-name、--source-host-transport和--source-host-socket提供，分别表示主机的名称、主机的传输方法和主机使用的套接字。--source-host-socket和--source-host-name不能同时提供，如果用户想提供--source-host-socket，则必须提供--source-host-transport。--source-host-name参数支持host:port语法，如果用户还想提供端口。 如果指定--print-xml，则打印将要附加的磁盘的XML，而不是实际附加磁盘。 如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，则使用遗留API，其行为取决于管理程序驱动。 出于兼容性考虑，--persistent对于离线域的行为类似于--config，对于运行中的域则类似于--live --config。同样，--shareable是--mode shareable的别名。 attach-interface语法：attach-interface domain type source [ [ [–live] [–config] | [–current]] | [–persistent]] [–target target] [–mac mac] [–script script] [–model model] [–inbound average,peak,burst,floor] [–outbound average,peak,burst] [–alias alias] [–managed] [–print-xml] [–source-mode mode] 将新网络接口附加到域。type可以是以下之一：network表示通过libvirt虚拟网络连接，bridge表示通过主机上的桥接设备连接，direct表示直接连接到主机的网络接口或桥接，hostdev表示使用主机上的PCI设备透传连接，vhostuser表示使用virtio传输协议连接。 source表示连接的源。源取决于接口类型：network虚拟网络的名称，bridge桥接设备的名称，direct主机接口或桥接的名称，hostdev主机接口的PCI地址，格式为domain:bus:slot.function，vhostuserUNIX套接字的路径（控制平面）。--target用于指定用于将域连接到源的tap&#x2F;macvtap设备。以’vnet’开头的名称被视为自动生成，并在每次附加接口时被清除&#x2F;重新生成。--mac指定网络接口的MAC地址；如果未提供MAC地址，将自动生成一个新地址（如果命令行中给出“--config”，则存储在持久化配置中）。--script用于指定附加到桥接时调用的自定义脚本路径——这将替代默认脚本，而不是附加到默认脚本。此选项仅适用于bridge类型的接口和Xen域。--model指定呈现给域的网络设备模型。alias可设置用户提供的别名。--inbound和--outbound控制接口的带宽。至少必须指定average、floor对中的一个。其他两个peak和burst是可选的，因此“average,peak”、“average,,burst”、“average,,,floor”、“average”和“,,,floor”也是合法的。average、floor和peak的值以千字节每秒表示，而burst以单次突发中的千字节数表示，如https://libvirt.org/formatnetwork.html#quality-of-service 中所述。--managed仅适用于hostdev类型，并告诉libvirt接口应被管理，这意味着libvirt会将其从主机分离并重新附加。--source-mode对于vhostuser接口是必需的，接受值server和client，分别控制管理程序是等待其他进程连接还是发起连接。如果指定--print-xml，则打印将要附加的接口的XML，而不是实际附加接口。如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则影响域的当前状态（可以是运行中或离线）。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，则使用遗留API，其行为取决于管理程序驱动。出于兼容性考虑，--persistent对于离线域的行为类似于--config，对于运行中的域则类似于--live --config。注意：可选的target值是节点上创建的作为后端设备的名称。如果未提供，将自动创建名为“vnetN”或“vifN”的设备。 detach-device语法：detach-device domain FILE [ [ [–live] [–config] | [–current]] | [–persistent]] 从域中分离设备，使用与attach-device命令相同的XML描述。对于透传主机设备，如果设备未使用托管模式，还需使用nodedev-reattach命令。 注意：提供的设备XML描述应与其在域XML中的定义一样具体。用于匹配设备的属性集是驱动内部的。使用部分定义或尝试分离域XML中不存在但与现有设备共享某些特定属性的设备可能导致意外结果。 特性：设备拔出在大多数情况下是异步的，并且需要客户机配合。这意味着客户机可以任意拒绝或延迟拔出。由于此命令中使用的libvirt API设计为同步的，即使设备尚未拔出，它也会在超时后返回成功，以允许与域的进一步交互（例如，如果客户机无响应）。需要确保设备已拔出的调用者可以使用libvirt事件（参见virsh event）在设备移除时收到通知。注意，事件可能在命令返回之前到达。 如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，则使用遗留API，其行为取决于管理程序驱动。 出于兼容性考虑，--persistent对于离线域的行为类似于--config，对于运行中的域则类似于--live --config。 注意，旧版本的virsh使用--config作为--persistent的别名。 detach-device-alias语法：detach-device-alias domain alias [ [ [–live] [–config] | [–current]]] 从domain中分离具有给定alias的设备。此命令在拔出请求发送到管理程序后成功返回。设备的实际移除通过libvirt事件异步通知（参见virsh event）。 如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。 detach-disk语法：detach-disk domain target [ [ [–live] [–config] | [–current]] | [–persistent]] [–print-xml]从域中分离磁盘设备。target是域中看到的设备。如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，则使用遗留API，其行为取决于管理程序驱动。 出于兼容性考虑，--persistent对于离线域的行为类似于--config，对于运行中的域则类似于--live --config。 注意，旧版本的virsh使用--config作为--persistent的别名。 如果指定--print-xml，则打印用于分离磁盘的XML，而不是实际分离磁盘。 有关已知特性，请参阅detach-device的文档。 detach-interface语法：detach-interface domain type [–mac mac] [ [ [–live] [–config] | [–current]] | [–persistent]] [–print-xml] 从域中分离网络接口。type可以是network表示物理网络设备，或bridge表示桥接到设备。如果域上有多个接口，建议使用mac选项区分它们。 如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，则使用遗留API，其行为取决于管理程序驱动。 出于兼容性考虑，--persistent对于离线域的行为类似于--config，对于运行中的域则类似于--live --config。 注意，旧版本的virsh使用--config作为--persistent的别名。 如果指定--print-xml，则打印用于分离接口的XML，而不是实际分离接口。 有关已知特性，请参阅detach-device的文档。 update-device语法：update-device domain file [–force] [ [ [–live] [–config] | [–current]] | [–persistent] ]根据XML file中的设备定义更新与domain关联的设备的特性。--force选项可用于强制更新设备，例如，即使CD-ROM在域中被锁定&#x2F;挂载，也可以弹出它。有关libvirt设备XML格式的详细信息，请参阅 https://libvirt.org/formatdomain.html#elementsDevices。 如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。未指定任何标志等同于指定--current。 出于兼容性考虑，--persistent对于离线域的行为类似于--config，对于运行中的域则类似于--live --config。 注意，旧版本的virsh使用--config作为--persistent的别名。 注意：使用部分设备定义XML文件可能导致意外结果，因为某些字段可能自动生成，从而匹配到非预期的设备。 update-memory-device语法：update-memory-device domain [–print-xml] [ [–alias alias] | [–node node] ] [ [–live] [–config] | [–current] ] [–requested-size size]此命令查找给定domain中的&lt;memory&#x2F;&gt;设备，更改请求的值，并将更新的设备XML传递给守护进程。如果指定--print-xml，则不更改设备，而是将更新的设备XML打印到stdout。如果domain中有多个&lt;memory&#x2F;&gt;设备，请使用--alias或--node选择所需的设备。 如果指定--live，则影响正在运行的域。如果指定--config，则影响持久化客户机的下次启动。如果指定--current，则等同于--live或--config，具体取决于客户机的当前状态。--live和--config可以同时指定，但--current是互斥的。未指定任何标志等同于指定--current。 如果指定--requested-size，则将内存目标下的&lt;requested&#x2F;&gt;更改为请求的size（作为比例整数，参见上面的NOTES）。如果没有提供后缀，则默认为kibibytes。此选项仅适用于virtio-mem内存设备模型。 change-media语法：change-media domain path [–eject] [–insert] [–update] [source] [–force] [ [–live] [–config] | [–current] ] [–print-xml] [–block] 更改CDROM或软盘驱动器的介质。path可以是完全限定的路径或磁盘设备的唯一目标名称（&lt;target dev&#x3D;’hdc’&gt;）。source指定要插入或更新的介质的路径。--block标志允许在CDROM或软盘驱动器使用块设备作为介质（而不是文件）时设置后备类型。 --eject表示将弹出介质。--insert表示将插入介质。必须指定source。如果设备有源（例如&lt;source file&#x3D;’media’&gt;），且未指定source，则--update等同于--eject。如果设备没有源，且指定了source，则--update等同于--insert。如果设备有源，且指定了source，则--update的行为类似于--eject和--insert的组合。如果未指定--eject、--insert和--update，则默认使用--update。--force选项可用于强制更改介质。如果指定--live，则修改运行中客户机的实时配置。如果指定--config，则修改持久化配置，效果在客户机下次启动时生效。--current可以是live或config，具体取决于管理程序的实现。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，行为因管理程序而异。如果指定--print-xml，则打印用于更改介质的XML，而不是实际更改介质。 dom-fd-associate语法：dom-fd-associate domain –name FDGROUPNAME –pass-fds M,N,…. [–seclabel-writable] [–seclabel-restore] 将--pass-fds参数描述的一个或多个fd作为--name关联到domain。传递的fd组的生命周期与连接相同，因此退出virsh后会取消注册它们。 默认情况下，如果需要，会应用安全标签，但为了避免不必要地保持打开状态，使用后不会恢复它们。可以通过使用--seclabel-restore标志请求尽力恢复安全标签。 7 节点设备命令以下命令操作主机设备，这些设备旨在通过域&lt;devices&gt;部分中的&lt;hostdev&gt;元素传递给客户机域。节点设备键通常由总线名称后跟其地址指定，所有组件之间使用下划线分隔，例如pci_0000_00_02_1、usb_1_5_3或net_eth1_00_27_13_6a_fe_00。nodedev-list提供了libvirt已知的所有主机设备的完整列表，尽管这包括无法分配给客户机的设备（例如，尝试分离控制主机硬盘控制器的PCI设备，客户机的磁盘映像位于该硬盘上，可能导致主机系统锁定或重启）。 有关节点设备定义的更多信息，请参阅：https://libvirt.org/formatnode.html。 透传设备不能同时被主机及其客户机域使用，也不能被多个活动客户机同时使用。如果PCI设备的&lt;hostdev&gt;描述包含属性managed&#x3D;’yes’，并且管理程序驱动支持它，则设备处于托管模式，尝试在活动客户机中使用该透传设备将自动表现为在适当的时间调用nodedev-detach（客户机启动、设备热插拔）和nodedev-reattach（客户机停止、设备热拔出）。如果PCI设备未标记为托管，则必须手动分离才能供客户机使用，并手动重新附加才能返回主机。此外，如果设备是手动分离的，则即使客户机在托管模式下使用该设备，主机也无法重新获得控制权，除非有匹配的reattach操作。 nodedev-create语法：nodedev-create FILE [–validate]在主机节点上创建一个设备，然后可以将其分配给虚拟机。通常，libvirt能够自动确定哪些主机节点可用，但此命令允许注册libvirt未自动检测到的主机硬件。file包含节点设备的顶级&lt;device&gt;描述的XML。如果指定--validate，则根据内部RNG模式验证XML文档的格式。 nodedev-update语法：nodedev-update device FILE [ [–live] [–config] | [–current] ]更新主机上的设备。device可以是设备名称或“wwnn,wwpn”格式的wwn对（目前仅适用于vHBA）。file包含节点设备的顶级&lt;device&gt;描述的XML。--current可以是live或config，具体取决于管理程序的实现。--live和--config可以同时指定，但--current是互斥的。如果未指定任何标志，行为因管理程序而异。 nodedev-destroy语法：nodedev-destroy device销毁（停止）主机上的设备。device可以是设备名称或“wwnn,wwpn”格式的wwn对（目前仅适用于vHBA）。注意，这会使libvirt停止管理主机设备，甚至可能使该设备在物理主机上不可用，直到重新启动。 nodedev-define语法：nodedev-define FILE [–validate]从XML FILE定义非活动持久化设备或修改现有持久化设备。如果指定--validate，则根据内部RNG模式验证XML文档的格式。 nodedev-undefine语法：nodedev-undefine device取消持久化设备的配置。如果设备处于活动状态，则使其变为临时设备。 nodedev-start语法：nodedev-start device启动（先前定义的）非活动设备。 nodedev-detach语法：nodedev-detach nodedev [–driver backend_driver]将nodedev从主机驱动分离，并绑定到一个特殊驱动，该驱动提供管理程序将设备分配给虚拟机（使用域XML定义中的&lt;hostdev&gt;）所需的API。此操作通过nodedev-reattach反转，并由管理程序驱动对托管设备（其XML定义中具有“managed&#x3D;’yes’”的设备）自动完成。 不同的管理程序期望被分配的设备绑定到不同的驱动。例如，QEMU的“vfio”后端要求设备绑定到驱动“vfio-pci”或“VFIO变体”驱动（这是一个支持vfio-pci提供的完整API以及其他API以支持实时迁移等功能的驱动）。--driver参数可用于指定设备应绑定到的特定驱动（例如，设备特定的VFIO变体驱动）。当省略--driver时，使用管理程序的默认驱动（QEMU为“vfio-pci”，Xen为“pciback”）。 nodedev-dumpxml语法：nodedev-dumpxml [–inactive] [–xpath EXPRESSION] [–wrap] device转储给定节点设备的&lt;device&gt; XML表示，包括设备名称、拥有设备的总线、供应商和产品ID以及libvirt可用的任何设备功能（例如是否支持设备重置）。device可以是设备名称或“wwnn,wwpn”格式的wwn对（仅适用于HBA）。可以使用影响XML转储的附加选项。--inactive告诉virsh转储节点设备下次启动时将使用的配置，而不是当前节点设备配置。 如果 --xpath 参数提供了XPath表达式，则将对输出XML求值，并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但为了方便进一步处理，--wrap参数将使匹配节点包装在公共根节点中。 nodedev-info语法：nodedev-info device返回关于device对象的基本信息。 nodedev-list语法：nodedev-list [–cap capability] [–tree] [–inactive | –all] [–persistent | –transient]列出节点上libvirt已知的所有可用设备。cap用于按功能类型过滤列表，类型必须用逗号分隔，例如--cap pci,scsi。有效的功能类型包括’system’、’pci’、’usb_device’、’usb’、’net’、’scsi_host’、’scsi_target’、’scsi’、’storage’、’fc_host’、’vports’、’scsi_generic’、’drm’、’mdev’、’mdev_types’、’ccw’、’ccwgroup’、’ccwgroup_member’、’css’、’ap_card’、’ap_queue’、’ap_matrix’。默认情况下，仅列出活动设备。--inactive用于仅列出非活动设备，--all用于列出活动和非活动设备。--persistent用于仅列出持久化设备，--transient用于仅列出临时设备。不提供--persistent或--transient将列出所有设备，除非另有过滤。--transient与--persistent和--inactive互斥。如果使用--tree，则输出格式化为表示每个节点父节点的树。--tree与除--all之外的所有其他选项互斥。 nodedev-reattach语法：nodedev-reattach nodedev声明nodedev不再被任何客户机使用，主机可以恢复设备的正常使用。对于托管模式的PCI设备和USB设备，此操作是自动完成的，但必须显式调用以匹配任何显式的nodedev-detach。 nodedev-reset语法：nodedev-reset nodedev触发nodedev的设备重置，在将节点设备在客户机透传和主机之间传输之前非常有用。Libvirt通常会在需要时隐式执行此操作，但此命令允许在需要时显式重置。 nodedev-event语法：nodedev-event { [nodedev ] event [–loop] [–timeout seconds] [–timestamp] | –list}等待一类节点设备事件发生，并在事件发生时打印适当的详细信息。事件可以选择由nodedev过滤。仅使用--list作为参数将提供此客户端已知的可能event值列表，尽管连接可能不允许注册所有这些事件。 默认情况下，此命令是一次性的，一旦事件发生即返回成功；您可以发送SIGINT（通常通过Ctrl-C）立即退出。如果指定--timeout，则命令在seconds后放弃等待事件。使用--loop，命令将打印所有事件，直到超时或中断键。 当使用--timestamp时，将在事件之前打印人类可读的时间戳。 nodedev-autostart语法：nodedev-autostart [–disable ] device配置设备在主机启动或父设备可用时自动启动。使用--disable，设备将设置为手动模式，不再由主机自动启动。此命令仅支持持久化定义的介导设备。 8 虚拟网络命令以下命令用于操作网络。Libvirt具备定义虚拟网络的能力，这些虚拟网络可被域（domain）使用并与实际网络设备连接。关于此功能的详细信息，请参阅文档：https://libvirt.org/formatnetwork.html 。许多虚拟网络命令与域命令类似，但虚拟网络的命名方式是通过其名称或UUID。 net-autostart语法：net-autostart 网络 [--disable]配置虚拟网络在启动时自动启动。使用--disable选项可禁用自动启动功能。 net-create语法：net-create 文件 [--validate]根据XML文件创建一个临时（transient）虚拟网络并实例化（启动）该网络。关于libvirt使用的XML网络格式描述，请参阅文档：https://libvirt.org/formatnetwork.html。 可选地，可以通过--validate验证输入XML文件的格式是否符合内部RNG模式。 net-define语法：net-define 文件 [--validate]根据XML文件定义一个非活动的持久虚拟网络或修改现有的持久网络。可选地，可以通过--validate验证输入XML文件的格式是否符合内部RNG模式。 net-desc语法：net-desc 网络 [ [–live] [–config] | [–current] ] [–title] [–edit] [–new-desc 新描述或标题消息] 显示或修改网络的描述和标题。这些值是用户字段，允许存储任意文本数据以便轻松识别网络。标题应简短，但未强制要求。（另请参阅net-metadata，该命令用于操作基于XML的网络元数据。） 标志--live或--config选择此命令是作用于网络的实时定义还是持久定义。如果同时指定--live和--config，则在获取当前描述时，--config选项优先；而在设置描述时，实时配置和持久配置均会更新。--current是独占选项，如果未指定任何标志，则默认为此选项。 标志--edit指定打开包含当前描述或标题的编辑器，并在编辑后保存内容。 标志--title选择操作标题字段而非描述字段。 如果未指定--edit和--new-desc，则显示备注或描述而非修改。 net-destroy语法：net-destroy 网络立即销毁（停止）指定的临时或持久虚拟网络，网络通过名称或UUID指定。 net-dumpxml语法：net-dumpxml [–inactive] [–xpath 表达式] [–wrap] 网络将虚拟网络信息以XML格式输出到标准输出。如果指定--inactive，则物理功能不会扩展为其关联的虚拟功能。 如果提供 --xpath 参数和XPath表达式，则会对输出XML进行评估，并仅打印匹配的节点。默认行为是将每个匹配节点作为独立文档打印，但为了方便进一步处理，--wrap参数会将匹配节点包装在公共根节点中。 net-edit语法：net-edit 网络编辑网络的XML配置文件。 此命令等效于： virsh net-dumpxml --inactive 网络 &gt; 网络.xmlvi 网络.xml（或使用其他文本编辑器修改）virsh net-define 网络.xml 不同之处在于它会进行一些错误检查。 使用的编辑器可通过环境变量$VISUAL或$EDITOR指定，默认为vi。 net-event语法：net-event {[网络] 事件 [–loop] [–timeout 秒数] [–timestamp] | –list}等待一类网络事件发生，并在事件发生时打印相关详细信息。事件可以通过网络进行筛选。仅使用--list作为参数时，将列出此客户端已知的可能事件值，但连接可能不允许注册所有这些事件。 默认情况下，此命令是一次性的，事件发生后即返回成功；可通过发送SIGINT（通常为Ctrl-C）立即退出。如果指定--timeout，则命令在秒数后放弃等待事件。使用--loop时，命令会打印所有事件，直到超时或中断。 当使用--timestamp时，会在事件前打印人类可读的时间戳。 net-info语法：net-info 网络返回关于网络对象的基本信息。 net-list语法：net-list [–inactive | –all]{ [–table] | –name | –uuid }[–persistent] [–transient][–autostart] [–no-autostart][–title] 返回活动网络的列表。如果指定--all，则还包括已定义但非活动的网络；如果指定--inactive，则仅列出非活动网络。还可以通过--persistent筛选持久网络，--transient筛选临时网络，--autostart筛选启用自动启动的网络，--no-autostart筛选禁用自动启动的网络。 如果指定--name，则打印网络名称，每行一个。如果指定--uuid，则打印网络UUID而非名称。标志--table指定使用传统的表格格式输出，这是默认行为。这些选项互斥。 如果指定--title，则会在额外列中打印简短的网络描述（标题）。此标志仅适用于默认的--table输出。 注意：在与旧服务器通信时，此命令被迫使用一系列API调用，存在竞争条件，可能导致池未被列出或多次出现，如果在调用期间池状态发生变化。新服务器无此问题。 net-metadata语法：net-metadata 网络 [ [–live] [–config] | [–current] ][–edit] [uri] [key] [set] [–remove] 显示或修改网络的自定义XML元数据。元数据是用户定义的XML，允许在网络定义中存储任意XML数据。可以在网络XML中存储多个独立的自定义元数据片段，这些片段通过uri参数提供的私有XML命名空间标识。（另请参阅net-desc，该命令用于操作网络的文本元数据，如标题和描述。） 标志--live或--config选择此命令是作用于网络的实时定义还是持久定义。如果同时指定--live和--config，则在获取当前描述时，--config选项优先；而在设置描述时，实时配置和持久配置均会更新。--current是独占选项，如果未指定任何标志，则默认为此选项。 标志--remove指定通过uri参数标识的元数据元素应被删除而非更新。 标志--edit指定打开包含通过uri参数标识的元数据的编辑器，并在编辑后保存内容。否则，可以通过set参数提供新内容。 通过--edit或set设置元数据时，必须指定key参数，用于将自定义元素前缀绑定到私有命名空间。 如果未指定--edit和set，则显示与uri命名空间对应的XML元数据而非修改。 net-name语法：net-name 网络-UUID将网络UUID转换为网络名称。 net-start语法：net-start 网络启动一个（先前定义的）非活动网络。 net-undefine语法：net-undefine 网络取消持久网络的配置定义。如果网络处于活动状态，则将其转为临时网络。 net-uuid语法：net-uuid 网络名称将网络名称转换为网络UUID。 net-update语法：net-update 网络 命令 部分 xml[–parent-index 索引 ] [ [–live] [ –config ] | [ –current ] ] 更新现有网络定义的指定部分，更改可立即生效，无需销毁和重新启动网络。 命令可以是“add-first”、“add-last”、“add”（add-last的同义词）、“delete”或“modify”。 部分可以是“bridge”、“domain”、“ip”、“ip-dhcp-host”、“ip-dhcp-range”、“forward”、“forward-interface”、“forward-pf”、“portgroup”、“dns-host”、“dns-txt”或“dns-srv”，每个部分通过XML元素层次结构的连接命名。例如，“ip-dhcp-host”会更改网络XML中&lt;ip&gt;元素内&lt;dhcp&gt;元素内的&lt;host&gt;元素。 xml可以是完整XML元素的文本（例如“&lt;host mac&#x3D;’00:11:22:33:44:55’ ip&#x3D;’1.2.3.4’&#x2F;&gt;”），或包含完整XML元素的文件名。通过查看提供的文本的第一个字符来区分：如果是“&lt;”，则为XML文本；否则为包含XML文本的文件名。 --parent-index选项用于指定父元素的索引（从0开始）。例如，dhcp &lt;host&gt;元素可能位于网络中的多个&lt;ip&gt;元素之一；如果未提供父索引，则会选择“最合适”的&lt;ip&gt;元素（通常是已包含&lt;dhcp&gt;元素的唯一一个），但如果指定--parent-index，则会修改该特定&lt;ip&gt;实例。 如果指定--live，则影响运行中的网络；如果指定--config，则影响持久网络的下次启动；如果指定--current，则等同于--live或--config，具体取决于网络的当前状态。可以同时指定--live和--config，但--current是独占的。未指定任何标志等同于指定--current。 net-dhcp-leases语法：net-dhcp-leases 网络 [mac]获取连接到指定虚拟网络的所有网络接口的DHCP租约列表，如果指定mac，则仅输出该接口的信息。 9 网络端口命令以下命令用于操作网络端口。当虚拟机添加虚拟网络接口时，Libvirt虚拟网络会创建端口。通常无需使用这些命令，因为虚拟机生命周期中的相关操作由管理程序驱动自动完成。这些命令主要用于调试问题或从错误&#x2F;残留状态中恢复。 net-port-list语法：net-port-list { [--table] | --uuid } 网络列出网络中记录的所有端口。如果指定--uuid，则打印端口的UUID而非表格。标志--table指定使用传统的表格格式输出，此为默认行为。这些选项互斥。 net-port-create语法：net-port-create 网络 文件 [--validate]根据端口描述分配新网络端口并预留资源。可选地，可通过--validate验证输入XML文件的格式是否符合内部RNG模式。 net-port-dumpxml语法：net-port-dumpxml [--xpath 表达式] [--wrap] 网络 端口将端口信息以XML格式输出到标准输出。如果提供 --xpath 参数和XPath表达式，则会对输出XML进行评估，仅打印匹配的节点。默认行为是将每个匹配节点作为独立文档打印，但--wrap参数会将匹配节点包装在公共根节点中以便后续处理。 net-port-delete语法：net-port-delete 网络 端口删除端口记录并释放其资源。 10 接口命令以下命令用于操作主机接口。这些接口通常可被域（domain）的&lt;interface&gt;元素按名称引用（如系统创建的桥接接口），但主机接口无需与任何特定虚拟机配置绑定。 许多主机接口命令与域命令类似，接口可通过名称或MAC地址指定。但仅当MAC地址唯一时才能作为iface参数使用（若接口与桥接设备MAC地址相同，则会导致歧义错误，此时必须改用名称）。 iface-bridge语法：iface-bridge 接口 桥接 [--no-stp] [延迟] [--no-start]创建名为桥接的桥接设备，并将现有网络设备接口附加到该桥接。新桥接默认立即启动，启用STP且延迟为0秒；可通过--no-stp、--no-start和延迟（整数秒）调整设置。接口的所有IP配置将迁移至新桥接设备。 （撤销操作请参阅iface-unbridge。） iface-define语法：iface-define 文件 [--validate]根据XML文件定义非活动的持久主机接口或修改现有持久接口。可选地，可通过--validate验证输入XML文件的格式是否符合内部RNG模式。 iface-destroy语法：iface-destroy 接口立即销毁（停止）主机接口（如通过”if-down”禁用接口）。 iface-dumpxml语法：iface-dumpxml [--inactive] [--xpath 表达式] [--wrap] 接口将接口信息以XML格式输出到标准输出。若指定--inactive，则输出反映接口下次启动时将使用的持久状态。 若提供 --xpath参数和XPath表达式，则会对输出XML进行评估，仅打印匹配的节点。--wrap参数会将匹配节点包装在公共根节点中。 iface-edit语法：iface-edit 接口编辑主机接口的XML配置文件。此命令等效于：virsh iface-dumpxml 接口 &gt; 接口.xmlvi 接口.xml（或使用其他编辑器修改）virsh iface-define 接口.xml 不同之处在于它会进行错误检查。使用的编辑器由$VISUAL或$EDITOR环境变量指定，默认为vi。 iface-list语法：iface-list [--inactive | --all]返回活动接口列表。若指定--all，则包括已定义但非活动的接口；若指定--inactive，则仅列出非活动接口。 iface-name语法：iface-name 接口将主机接口的MAC地址转换为接口名称（仅当MAC地址唯一时有效）。 接口参数为MAC地址。 iface-mac语法：iface-mac 接口将主机接口名称转换为MAC地址。 接口参数为接口名称。 iface-start语法：iface-start 接口启动已定义的主机接口（如通过”if-up”）。 iface-unbridge语法：iface-unbridge 桥接 [--no-start]拆除名为桥接的桥接设备，释放底层接口并恢复其正常用途，同时将所有IP配置从桥接设备移回底层接口。除非指定--no-start，否则将重启底层接口（此选项仅为对称设计，通常不建议使用）。 （创建桥接请参阅iface-bridge。） iface-undefine语法：iface-undefine 接口取消非活动主机接口的配置定义。 iface-begin语法：iface-begin创建当前主机接口设置的快照，后续可提交（iface-commit）或回滚（iface-rollback）。若快照已存在，则命令将失败直至前一个快照被提交或回滚。若在快照创建后、提交&#x2F;回滚前通过非libvirt API修改接口设置，将导致未定义行为。 iface-commit语法：iface-commit确认自上次iface-begin后的所有更改有效，并删除回滚点。若无快照则命令失败。 iface-rollback语法：iface-rollback将主机接口设置回滚至上一次iface-begin的状态。若无快照则命令失败。主机重启也会隐式触发回滚。 11 存储池命令以下命令用于操作存储池。Libvirt 能够管理多种存储解决方案，包括文件、原始分区和特定域格式的存储，这些存储为虚拟机内的设备提供存储卷。有关此功能的更多详细信息，请参阅文档：https://libvirt.org/formatstorage.html 。许多存储池命令与用于域的类似。 find-storage-pool-sources语法：find-storage-pool-sources 类型 [srcSpec]返回描述所有可能可用存储池源的 XML，这些源可用于创建或定义指定类型的存储池。如果提供了 srcSpec，则它是一个包含 XML 的文件，用于进一步限制对存储池的查询。 并非所有存储池都支持这种发现方式。此外，对于支持发现的存储池，仅需要特定的 XML 元素来返回有效数据，而其他元素甚至某些元素的属性会被忽略，因为它们不是基于搜索条件查找存储池所必需的。以下列出了支持的类型选项以及用于执行搜索的最小 XML 元素。 对于 netfs 或 gluster 存储池，最小需要的 XML 是带有描述 IP 地址或主机名的 name 属性的 &lt;host&gt; 元素，用于查找存储池。port 属性将被忽略，srcSpec 中提供的任何其他 XML 元素也会被忽略。 对于 logical 存储池，srcSpec 文件的内容会被忽略，但如果提供了该文件，则文件必须至少存在。 对于 iscsi 或 iscsi-direct 存储池，最小需要的 XML 是带有描述 IP 地址或主机名的 name 属性的 &lt;host&gt; 元素（iSCSI 服务器地址）。可选地，可以提供 port 属性（默认为 3260）。此外，可以提供一个带有 name 属性的 &lt;initiator&gt; XML 元素，以进一步将 iSCSI 目标搜索限制为特定发起者（用于多 IQN iSCSI 存储池）。 find-pool-sources-as语法：find-storage-pool-sources-as 类型 [主机] [端口] [发起者]此命令选项用于替代 find-storage-pool-sources 的 srcSpec XML 文件，通过可选参数生成查询 XML 文件。该命令将返回与 find-storage-pool-sources 相同的输出 XML。 使用 主机 指定用于网络存储（如 netfs、gluster 和 iscsi 类型存储池）的特定主机。 使用 端口 进一步限制网络端口（如果特定存储后端需要，如 iscsi）。 使用 发起者 进一步将 iscsi 类型存储池的搜索限制为特定目标发起者。 pool-autostart语法：pool-autostart 池名或 UUID [–disable]配置池是否应在启动时自动启动。 pool-build语法：pool-build 池名或 UUID [–overwrite] [–no-overwrite]构建指定的存储池。选项 --overwrite 和 --no-overwrite 仅适用于文件系统、磁盘或逻辑存储池的 pool-build 操作。 对于文件系统存储池，如果未指定任何标志，则 pool-build 仅创建目标路径目录，不会尝试在目标卷设备上运行 mkfs。如果指定 --no-overwrite，则会探测目标设备上是否已存在文件系统，如果存在则返回错误，否则使用 mkfs 格式化目标设备。如果指定 --overwrite，则始终执行 mkfs，并无条件覆盖目标设备上的任何现有数据。 对于磁盘存储池，如果未指定任何标志或指定 --no-overwrite，pool-build 将在尝试在目标卷设备上写入新标签之前检查目标卷设备上是否存在文件系统或分区。如果目标卷设备已有标签，则命令将失败。如果指定 --overwrite，则在写入新标签之前不会检查目标卷设备。标签写入使用存储池源格式类型，如果未指定则使用 dos。 对于逻辑存储池，如果未指定任何标志或指定 --no-overwrite，pool-build 将在尝试初始化和格式化每个设备以供逻辑存储池使用之前检查目标卷设备上是否存在文件系统或分区。如果任何目标卷设备已有标签，则命令将失败。如果指定 --overwrite，则在初始化和格式化每个设备之前不会进行检查。所有目标卷设备通过 pvcreate 正确格式化后，将使用所有设备创建卷组。 pool-create语法：pool-create 文件 [–build] [ [–overwrite] | [–no-overwrite]]从 XML 文件创建并启动存储池对象。 [--build] [ [--overwrite] | [--no-overwrite]] 在创建后执行 pool-build，以避免后续需要单独运行构建命令。--overwrite 和 --no-overwrite 标志的规则与 pool-build 相同。如果仅提供 --build，则调用 pool-build 时不带任何标志。 pool-create-as语法：pool-create-as 名称 类型[–source-host 主机名] [–source-path 路径] [–source-dev 路径][–source-name 名称] [–target 路径] [–source-format 格式][–source-initiator 发起者-IQN][–auth-type 认证类型 –auth-username 用户名[–secret-usage 用途 | –secret-uuid UUID]][–source-protocol-ver 版本][ [–adapter-name 名称] | [–adapter-wwnn WWNN –adapter-wwpn WWPN[–adapter-parent 父设备 |–adapter-parent-wwnn 父设备-WWNN –adapter-parent-wwpn 父设备-WWPN |–adapter-parent-fabric-wwn 父设备-光纤-WWN] ] ][–build] [ [–overwrite] | [–no-overwrite] ] [–print-xml] 从原始参数创建并启动存储池对象名称。如果指定 --print-xml，则打印存储池对象的 XML 而不创建存储池。否则，存储池具有指定的类型。当使用 pool-create-as 创建 disk 类型的存储池时，将在 --source-dev 路径上找到的现有分区用于填充磁盘存储池。因此，建议使用 pool-define-as 和带有 --overwrite 的 pool-build 来正确初始化磁盘存储池。 [--source-host 主机名] 为从远程服务器支持的存储池（类型为 netfs、iscsi、rbd、sheepdog、gluster）提供源主机名。 [--source-path 路径] 为目录支持的存储池（类型为 dir）提供源目录路径。 [--source-dev 路径] 为物理设备支持的存储池（类型为 fs、logical、disk、iscsi、zfs）提供源路径。 [--source-name 名称] 为从命名元素支持的存储池（类型为 logical、rbd、sheepdog、gluster）提供源名称。 [--target 路径] 是存储池映射到主机文件系统的路径。 [--source-format 格式] 提供有关存储池格式的信息（类型为 fs、netfs、disk、logical）。 [--source-initiator 发起者-IQN] 为存储池的 iSCSI 连接提供发起者 IQN（类型为 iscsi-direct）。 [--auth-type 认证类型 --auth-username 用户名 [--secret-usage 用途 | --secret-uuid UUID]] 提供生成存储池认证凭据所需的元素。authtype 为 iscsi 类型存储池的 chap 或 rbd 类型存储池的 ceph。可以提供 usage 或 uuid 值，但不能同时提供。 [--source-protocol-ver 版本] 提供用于通过 NFS 挂载选项 nfsvers=n 联系服务器 NFS 服务的 NFS 协议版本号。ver 值应为无符号整数。 [--adapter-name 名称] 定义用于 scsi_host 适配器类型存储池的 scsi_hostN 适配器名称。 [--adapter-wwnn WWNN --adapter-wwpn WWPN [--adapter-parent 父设备 | --adapter-parent-wwnn 父设备-WWNN --adapter-parent-wwpn 父设备-WWPN | --adapter-parent-fabric-wwn 父设备-光纤-WWN]]] 定义用于 fc_host 适配器类型存储池的 WWNN 和 WWPN。可选地提供父 scsi_hostN 节点设备以用于 vHBA，可以通过父设备名称、父设备-WWNN 和父设备-WWPN 或父设备-光纤-WWN 指定。父设备名称可能在重新启动后因硬件环境变化而更改，因此提供父设备-WWNN 和父设备-WWPN 可确保即使 scsi_hostN 节点设备更改，也能使用相同的物理 HBA。使用父设备-光纤-WWN 允许更灵活地选择同一存储光纤上的 HBA 以定义存储池。 [--build] [ [--overwrite] | [--no-overwrite]] 在创建后执行 pool-build，以确保存储池环境处于预期状态，而无需在启动前运行构建命令。--overwrite 和 --no-overwrite 标志的规则与 pool-build 相同。如果仅提供 --build，则调用 pool-build 时不带任何标志。 对于 logical 存储池，仅需要提供 [--name]。如果提供了 [--source-name]，则必须与卷组名称匹配。如果未提供，则将从 [--name] 生成一个名称。如果提供了 [--target]，则会被忽略，并使用 [--source-name]（或从 [--name] 生成）生成目标源。 pool-define语法：pool-define 文件 [–validate]从 XML 文件定义非活动的持久存储池或修改现有的持久存储池。可选地，可以使用 --validate 验证输入 XML 文件的格式是否符合内部 RNG 模式。 pool-define-as语法：pool-define-as 名称 类型[–source-host 主机名] [–source-path 路径] [–source-dev 路径][–source-name 名称] [–target 路径] [–source-format 格式][–source-initiator 发起者-IQN][–auth-type 认证类型 –auth-username 用户名[–secret-usage 用途 | –secret-uuid UUID]][–source-protocol-ver 版本][ [–adapter-name 名称] | [–adapter-wwnn WWNN –adapter-wwpn WWPN[–adapter-parent 父设备] ] ] [–print-xml]从原始参数创建但不启动存储池对象名称。如果指定 --print-xml，则打印存储池对象的 XML 而不定义存储池。否则，存储池具有指定的类型。 使用与 pool-create-as 相同的参数，但不包括 --build、--overwrite 和 --no-overwrite 选项。 pool-destroy语法：pool-destroy 池名或 UUID销毁（停止）指定的存储池对象。Libvirt 将不再管理存储池对象描述的存储，但存储池中的原始数据不会被更改，稍后可以通过 pool-create 恢复。 pool-delete语法：pool-delete 池名或 UUID销毁指定存储池对象使用的资源。此操作不可恢复。执行此命令后，存储池对象仍将存在，可用于创建新的存储卷。 pool-dumpxml语法：pool-dumpxml [–inactive] [–xpath 表达式] [–wrap] 池名或 UUID返回存储池对象的 XML 信息。--inactive 指示 virsh 转储将在下次启动存储池时使用的配置，而不是当前存储池配置。 如果提供了 --xpath 参数（XPath 表达式），则将对输出 XML 求值并仅打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但为了便于进一步处理，--wrap 参数将导致匹配节点包装在公共根节点中。 pool-edit语法：pool-edit 池名或 UUID编辑存储池的 XML 配置文件。这相当于： 123virsh pool-dumpxml 池名 &gt; 池名.xml vi 池名.xml（或使用其他文本编辑器进行更改） virsh pool-define 池名.xml 不同之处在于它会进行一些错误检查。 使用的编辑器可以通过环境变量 $VISUAL 或 $EDITOR 指定，默认为 vi。 pool-info语法：pool-info [–bytes] 池名或 UUID返回存储池对象的基本信息。如果指定 --bytes，则基本信息的大小不会转换为人类友好的单位。 pool-list语法：pool-list [–inactive] [–all][–persistent] [–transient][–autostart] [–no-autostart][ [–details] [–uuid][–name] [类型] ]列出 libvirt 已知的存储池对象。默认情况下，仅列出活动存储池；--inactive 仅列出非活动存储池，--all 列出所有存储池。 此外，还有几组过滤标志。--persistent 用于列出持久存储池，--transient 用于列出临时存储池。--autostart 列出自动启动的存储池，--no-autostart 列出禁用自动启动的存储池。如果指定 --uuid，则仅打印存储池的 UUID。如果指定 --name，则仅打印存储池的名称。如果同时指定 --name 和 --uuid，则存储池的 UUID 和名称将并排打印，不带任何标题。选项 --details 与 --uuid 和 --name 互斥。 还可以使用 类型 列出指定类型的存储池，存储池类型必须用逗号分隔，例如 --type dir,disk。有效的存储池类型包括 dir、fs、netfs、logical、disk、iscsi、scsi、mpath、rbd、sheepdog、gluster、zfs、vstorage 和 iscsi-direct。 --details 选项指示 virsh 额外显示存储池持久性和容量相关信息（如果可用）。 注意：在与旧服务器通信时，此命令被迫使用一系列具有固有竞争的 API 调用，如果在调用之间存储池状态更改，则存储池可能不会被列出或可能多次出现。新服务器没有此问题。 pool-name语法：pool-name UUID将 UUID 转换为存储池名称。 pool-refresh语法：pool-refresh 池名或 UUID刷新存储池中包含的卷列表。 pool-start语法：pool-start 池名或 UUID [–build] [ [–overwrite] | [–no-overwrite]]启动之前定义但未激活的存储池。 [--build] [ [--overwrite] | [--no-overwrite]] 在 pool-start 之前执行 pool-build，以确保存储池环境处于预期状态，而无需在启动前运行构建命令。--overwrite 和 --no-overwrite 标志的规则与 pool-build 相同。如果仅提供 --build，则调用 pool-build 时不带任何标志。 注意：依赖于远程资源（如 iscsi 或（v）HBA 支持的 scsi 存储池）的存储池可能需要多次刷新才能检测到所有卷（参见 pool-refresh）。这是因为相应的卷设备可能在初始存储池启动或当前刷新尝试期间不在主机的文件系统中。刷新重试的次数取决于网络连接和主机导出相应设备所需的时间。 pool-undefine语法：pool-undefine 池名或 UUID取消定义非活动存储池的配置。 pool-uuid语法：pool-uuid 池名返回命名存储池的 UUID。 pool-event语法：pool-event {[池名] 事件 [–loop] [–timeout 秒] [–timestamp] | –list}等待一类存储池事件发生，并在事件发生时打印适当的详细信息。事件可以选择通过 池名 过滤。仅使用 --list 作为参数将提供此客户端已知的可能 事件 值列表，尽管连接可能不允许注册所有这些事件。 默认情况下，此命令是一次性的，一旦事件发生即返回成功；可以通过发送 SIGINT（通常通过 Ctrl-C）立即退出。如果指定 --timeout，则命令在 秒 后放弃等待事件。使用 --loop 时，命令将打印所有事件，直到超时或中断。 当使用 --timestamp 时，将在事件之前打印人类可读的时间戳。 14 快照命令以下命令用于操作域快照。快照会保存域在某个时间点的磁盘、内存和设备状态，供未来使用。快照有许多用途，从保存操作系统镜像的”干净”副本，到在执行可能具有破坏性的操作前保存域的状态。快照通过唯一名称标识。关于用于表示快照属性的XML格式文档，请参阅https://libvirt.org/formatsnapshot.html。 snapshot-create语法：snapshot-create 域 [xml文件] {[–redefine [–current]] | [–no-metadata] [–halt] [–disk-only] [–reuse-external] [–quiesce] [–atomic] [–live]} [–validate]为域域创建一个快照，属性由xml文件指定。可选地，可以传递*–validate选项来验证输入XML文件的格式是否符合内部RNG模式（与使用virt-xml-validate(1)工具相同）。通常，域快照唯一可设置的属性是和元素，如果给出–disk-only，还包括；其余字段会被忽略，由libvirt自动填充。如果完全省略xmlfile*，那么libvirt将为所有字段选择值。新快照将成为当前快照，如snapshot-current所列。 如果指定*–halt*，创建快照后域将处于非活动状态。 如果指定*–disk-only，快照将只包含磁盘内容，而不是通常包含虚拟机状态的完整系统快照。磁盘快照比完整系统快照捕获得更快，但恢复到磁盘快照可能需要fsck或日志重放，因为它就像电源线突然拔出时的磁盘状态；同时使用–halt和–disk-only*会丢失当时未刷新到磁盘的任何数据。 如果指定*–redefine，那么snapshot-dumpxml生成的所有XML元素都有效；这可以用于将快照层次结构从一台机器迁移到另一台机器，为临时域重新创建层次结构（临时域消失后又以相同名称和UUID重新创建），或者对快照元数据进行微小修改（例如快照中嵌入的域XML的主机特定方面）。使用此标志时，xmlfile参数是必需的，除非同时给出–current*标志，否则域的当前快照不会被更改。 如果指定*–no-metadata，那么会创建快照数据，但会立即丢弃任何元数据（即libvirt不会将快照视为当前快照，除非以后使用–redefine*让libvirt再次了解元数据，否则无法恢复到该快照）。 如果指定*–reuse-external*，并且快照XML请求使用现有文件作为外部快照的目标，那么目标必须存在并且预先创建了正确的格式和元数据。然后文件会被重用；否则会拒绝快照以避免丢失现有文件的内容。 如果指定*–quiesce，libvirt将尝试使用客户代理来冻结和解冻域的挂载文件系统。但是，如果域没有客户代理，快照创建将失败。目前，这需要同时传递–disk-only*。 如果指定*–atomic*，libvirt将保证快照要么成功，要么失败且不做任何更改；并非所有虚拟机监控程序都支持这一点。如果不指定此标志，那么一些虚拟机监控程序可能会在部分执行操作后失败，必须使用dumpxml来查看是否发生了任何部分更改。 如果指定*–live*，libvirt会在客户运行时拍摄快照。同时捕获磁盘快照和域内存快照。这会增加外部快照的内存映像大小。目前仅支持完整系统的外部快照。 快照元数据的存在会阻止尝试取消定义持久客户。但是，对于临时域，当域停止运行时（无论是通过destroy等命令还是内部客户操作），快照元数据会静默丢失。 目前，无法在具有检查点的域中创建快照，尽管此限制将在未来版本中取消。 snapshot-create-as语法：snapshot-create-as 域 { [–print-xml] [–no-metadata] [–halt] [–reuse-external]} [名称] [描述] [–disk-only [–quiesce]] [–atomic] [–validate] [ [–live] [–memspec 内存规格]] [–diskspec 磁盘规格]…为域域创建一个具有给定和的快照；如果省略任一值，libvirt将选择一个值。如果指定*–print-xml，则输出适合snapshot-create的XML，而不是实际创建快照。否则，如果指定–halt，创建快照后域将处于非活动状态，如果指定–disk-only*，快照将不包含虚拟机状态。 –memspec选项可用于控制系统快照是内部还是外部。*–memspec标志是必需的，后跟内存规格，格式为[file&#x3D;]名称[,snapshot&#x3D;类型]，其中类型可以是no、internal或external。要在file&#x3D;名称中包含字面逗号，用第二个逗号转义。–memspec不能与–disk-only*一起使用。 –diskspec选项可用于控制*–disk-only和外部完整系统快照如何创建外部文件。此选项可以出现多次，根据域xml中元素的数量。每个的格式为磁盘[,snapshot&#x3D;类型][,driver&#x3D;类型][,stype&#x3D;类型][,file&#x3D;名称]。必须为块设备支持的磁盘提供diskspec*，因为libvirt不会为这些设备自动生成文件名。可选的stype参数允许控制源文件的类型。支持的值为’file’（默认）和’block’。要从外部快照中排除磁盘，使用–diskspec 磁盘,snapshot&#x3D;no。 要在磁盘或file&#x3D;名称中包含字面逗号，用第二个逗号转义。除非同时存在域、名称和描述，否则必须在每个diskspec前加上字面的*–diskspec*。例如，diskspec为”vda,snapshot&#x3D;external,file&#x3D;&#x2F;path&#x2F;to,,new”会生成以下XML： 如果指定*–reuse-external，并且域XML或diskspec*选项请求使用现有文件作为外部快照的目标，那么目标必须存在并且预先创建了正确的格式和元数据。然后文件会被重用；否则会拒绝快照以避免丢失现有文件的内容。 如果指定*–quiesce，libvirt将尝试使用客户代理来冻结和解冻域的挂载文件系统。但是，如果域没有客户代理，快照创建将失败。目前，这需要同时传递–disk-only*。 如果指定*–no-metadata*，那么会创建快照数据，但会立即丢弃任何元数据（即libvirt不会将快照视为当前快照，除非以后使用snapshot-create让libvirt再次了解元数据，否则无法恢复到该快照）。 如果指定*–atomic*，libvirt将保证快照要么成功，要么失败且不做任何更改；并非所有虚拟机监控程序都支持这一点。如果不指定此标志，那么一些虚拟机监控程序可能会在部分执行操作后失败，必须使用dumpxml来查看是否发生了任何部分更改。 如果指定*–live*，libvirt会在客户运行时拍摄快照。这会增加外部快照的内存映像大小。目前仅支持外部完整系统快照。 目前，无法在具有检查点的域中创建快照，尽管此限制将在未来版本中取消。 可选地，可以传递*–validate*选项来验证此命令内部生成的XML文档是否符合内部RNG模式。 snapshot-current语法：snapshot-current 域 {[–name] | [–security-info] | [快照名称]}不带快照名称时，这将输出域当前快照的XML（如果有）。如果指定*–name，则只输出当前快照名称而不是完整的xml。否则，使用–security-info*还会在XML中包含安全敏感信息。 使用快照名称时，这是请求使现有的命名快照成为当前快照，而不恢复域。 snapshot-edit语法：snapshot-edit 域 [快照名称] [–current] {[–rename] | [–clone]}编辑域快照名称的XML配置文件。如果同时指定快照名称和*–current，还会强制将编辑后的快照设为当前快照。如果省略快照名称，那么必须提供–current*来编辑当前快照。 这相当于： virsh snapshot-dumpxml dom name &gt; snapshot.xmlvi snapshot.xml（或用其他文本编辑器进行更改）virsh snapshot-create dom snapshot.xml –redefine [–current] 只是它会进行一些错误检查。 使用的编辑器可以由$VISUAL或$EDITOR环境变量提供，默认为vi。 如果指定*–rename，那么编辑可以更改快照名称。如果指定–clone*，那么更改快照名称将创建快照元数据的克隆。如果都不指定，那么编辑不能更改快照名称。注意更改快照名称必须小心，因为某些快照的内容，如单个qcow2文件中的内部快照，只能通过原始名称访问。 snapshot-info语法：snapshot-info 域 {快照 | –current}输出命名&lt;快照&gt;或使用*–current*的当前快照的基本信息。 snapshot-list语法：snapshot-list 域 [–metadata] [–no-metadata] [{–parent | –roots | [{–tree | –name}]}] [–topological] [{–from 快照 | –current} [–descendants]] [–leaves] [–no-leaves] [–inactive] [–active] [–disk-only] [–internal] [–external]列出给定域的所有可用快照，默认显示快照名称、创建时间和域状态的列。 通常，表格形式的输出按快照名称排序；使用*–topological*则会排序，使得没有子快照在其祖先之前列出（尽管可能有多个满足此属性的排序）。 如果指定*–parent，则在输出表中添加一列，给出每个快照的父快照名称。如果指定–roots，列表将过滤为只有没有父快照的快照。如果指定–tree*，输出将以树形格式显示，只列出快照名称。这三个选项是互斥的。 如果指定*–name，则只打印快照名称，如果同时使用–parent，则附加制表符分隔的父快照名称。此选项与–tree*互斥。 如果提供*–from，则过滤列表为给定快照的子快照；或者如果提供–current，则从当前快照开始。单独使用或与–parent一起使用时，除非同时存在–descendants，否则列表仅限于直接子快照。与–tree一起使用时，隐含–descendants的使用。此选项与–roots不兼容。注意–from或–current的起点不包括在列表中，除非同时存在–tree*选项。 如果指定*–leaves，列表将过滤为只有没有子快照的快照。同样，如果指定–no-leaves，列表将过滤为只有有子快照的快照。（注意同时省略这两个选项不进行过滤，而同时提供这两个选项将根据服务器是否识别标志而产生相同列表或出错）。过滤选项与–tree*不兼容。 如果指定*–metadata，列表将过滤为只有涉及libvirt元数据的快照，从而会阻止取消定义持久客户，或者在临时域销毁时丢失。同样，如果指定–no-metadata*，列表将过滤为只有无需libvirt元数据存在的快照。 如果指定*–inactive，列表将过滤为域关闭时拍摄的快照。如果指定–active，列表将过滤为域运行时拍摄的快照，并且快照包含恢复到该运行状态的内存状态。如果指定–disk-only*，列表将过滤为域运行时拍摄的快照，但快照只包含磁盘状态。 如果指定*–internal，列表将过滤为使用现有磁盘映像内部存储的快照。如果指定–external*，列表将过滤为使用外部文件存储磁盘映像或内存状态的快照。 snapshot-dumpxml语法：snapshot-dumpxml [–security-info] [–xpath 表达式] [–wrap] 域 快照输出域的快照快照的XML。使用*–security-info*还会包含安全敏感信息。使用snapshot-current可以轻松访问当前快照的XML。 如果 –xpath参数提供XPath表达式，它将对输出XML求值，并且只打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但是为了便于进一步处理，**–wrap**参数将导致匹配节点包装在公共根节点中。 snapshot-parent语法：snapshot-parent 域 {快照 | –current}输出给定快照的父快照名称（如果有），或使用*–current*的当前快照。 snapshot-revert语法：snapshot-revert 域 {快照 | –current} [{–running | –paused}] [–force] [–reset-nvram]将给定域恢复到快照指定的快照，或使用*–current*的当前快照。请注意，这是一个破坏性操作；自上次快照以来的任何域更改都将丢失。还要注意，snapshot-revert完成后域的状态将是原始快照拍摄时的域状态。 通常，恢复到快照会使域处于创建快照时的状态，除了没有虚拟机状态的磁盘快照会使域处于非活动状态。传递*–running或–paused*标志将执行额外的状态更改（如启动非活动域或暂停运行域）。由于临时域不能处于非活动状态，因此在恢复到临时域的磁盘快照时必须使用这些标志之一。 自libvirt 7.10.0起，VM进程总是会重新启动，因此以下段落不再有效。如果快照元数据缺少完整的VM XML，则不再可能恢复到这样的快照。 有许多情况下，快照恢复涉及额外风险，需要使用*–force*继续： 一种是快照缺少恢复配置的完整域信息的情况（如libvirt 0.9.5之前创建的快照）；由于libvirt无法证明当前配置与快照时使用的配置匹配，提供*–force*向libvirt保证快照与当前配置兼容（如果不兼容，域可能会运行失败）。 另一种是从运行域恢复到活动状态的情况，需要创建新的虚拟机监控程序而不是重用现有的虚拟机监控程序，因为它意味着诸如中断现有VNC或Spice连接等缺点；当活动快照使用明显不兼容的配置时，或者与非活动快照结合使用*–start或–pause*标志时，会发生这种情况。 此外，libvirt会拒绝在有托管保存状态时恢复非活动QEMU域的快照。这是因为这些快照不包含内存状态，因此不会替换现有的内存状态。这最终会在运行系统下切换磁盘，并且由于运行时交换内容不匹配，可能会导致大量文件系统损坏或崩溃。 如果指定*–reset-nvram*，任何现有的NVRAM文件将被删除并从其原始模板重新初始化。 snapshot-delete语法：snapshot-delete 域 {快照 | –current} [–metadata] [{–children | –children-only}]删除域名为快照的快照，或使用*–current的当前快照。如果此快照有子快照，此快照的更改将合并到子快照中。如果传递–children，则删除此快照及其任何子快照。如果传递–children-only*，则删除此快照的任何子快照，但保留此快照完整。这两个标志是互斥的。 如果指定*–metadata*，则只删除libvirt维护的快照元数据，而保留快照内容供外部工具访问；否则删除快照还会删除该时间点的数据内容。 15 检查点命令以下命令用于操作域检查点。检查点作为时间点标识客户机磁盘在该时间点后发生了哪些变化，从而可以执行增量和差异备份。检查点通过唯一名称标识。关于用于表示检查点属性的XML格式文档，请参阅https://libvirt.org/formatcheckpoint.html。 checkpoint-create语法：checkpoint-create 域 [xml文件] { –redefine [–redefine-validate] | [–quiesce] }为域域创建一个检查点，属性由xml文件描述一个顶级元素指定。输入XML文件的格式将根据内部RNG模式进行验证（与使用virt-xml-validate(1)工具相同）。如果完全省略xmlfile，那么libvirt将基于当前时间创建一个检查点名称。 如果指定*–redefine，那么checkpoint-dumpxml生成的所有XML元素都有效；这可以用于将检查点层次结构从一台机器迁移到另一台机器，为临时域重新创建层次结构（临时域消失后又以相同名称和UUID重新创建），或者对检查点元数据进行微小修改（例如检查点中嵌入的域XML的主机特定方面）。使用此标志时，xmlfile*参数是必需的。 如果同时指定*–redefine-validate和–redefine*，虚拟机监控程序将对存储在检查点XML之外的其他位置的检查点相关元数据进行验证。注意某些虚拟机监控程序可能要求域处于运行状态才能执行验证。 如果指定*–quiesce*，libvirt将尝试使用客户代理来冻结和解冻域的挂载文件系统。但是，如果域没有客户代理，检查点创建将失败。 检查点元数据的存在会阻止尝试取消定义持久客户。但是，对于临时域，当域停止运行时（无论是通过destroy等命令还是内部客户操作），检查点元数据会静默丢失。 目前，无法在具有快照的域中创建检查点，尽管此限制将在未来版本中取消。 checkpoint-create-as语法：checkpoint-create-as 域 [–print-xml] [名称] [描述] [–quiesce] [–diskspec 磁盘规格]…为域域创建一个具有给定和的检查点；如果省略任一值，libvirt将选择一个值。如果指定*–print-xml，则输出适合checkpoint-create*的XML，而不是实际创建检查点。 –diskspec选项可用于控制哪些客户磁盘参与检查点。此选项可以出现多次，根据域xml中元素的数量。每个的格式为磁盘[,checkpoint&#x3D;类型][,bitmap&#x3D;名称]。除非同时存在域、名称和描述，否则必须在每个diskspec前加上字面的*–diskspec*。例如，diskspec为”vda,checkpoint&#x3D;bitmap,bitmap&#x3D;map1”会生成以下XML： 如果指定*–quiesce*，libvirt将尝试使用客户代理来冻结和解冻域的挂载文件系统。但是，如果域没有客户代理，检查点创建将失败。 目前，无法在具有快照的域中创建检查点，尽管此限制将在未来版本中取消。 checkpoint-edit语法：checkpoint-edit 域 检查点名称编辑域检查点名称的XML配置文件。 这相当于： virsh checkpoint-dumpxml dom 名称 &gt; checkpoint.xmlvi checkpoint.xml（或用其他文本编辑器进行更改）virsh checkpoint-create dom checkpoint.xml –redefine 只是它会进行一些错误检查，包括编辑不应尝试更改检查点名称。 使用的编辑器可以由$VISUAL或$EDITOR环境变量提供，默认为vi。 checkpoint-info语法：checkpoint-info 域 检查点输出命名&lt;检查点&gt;的基本信息。 checkpoint-list语法：checkpoint-list 域 [{–parent | –roots | [{–tree | –name}]}] [–topological] [[–from] 检查点 | [–descendants]] [–leaves] [–no-leaves]列出给定域的所有可用检查点，默认显示检查点名称和创建时间的列。 通常，表格形式的输出按检查点名称排序；使用*–topological*则会排序，使得没有子检查点在其祖先之前列出（尽管可能有多个满足此属性的排序）。 如果指定*–parent，则在输出表中添加一列，给出每个检查点的父检查点名称。如果指定–roots，列表将过滤为只有没有父检查点的检查点。如果指定–tree*，输出将以树形格式显示，只列出检查点名称。这三个选项是互斥的。 如果指定*–name，则只打印检查点名称，如果同时使用–parent，则附加制表符分隔的父检查点名称。此选项与–tree*互斥。 如果提供*–from，则过滤列表为给定检查点的子检查点。单独使用或与–parent一起使用时，除非同时存在–descendants，否则列表仅限于直接子检查点。与–tree一起使用时，隐含–descendants的使用。此选项与–roots不兼容。注意–from的起点不包括在列表中，除非同时存在–tree*选项。 如果指定*–leaves，列表将过滤为只有没有子检查点的检查点。同样，如果指定–no-leaves，列表将过滤为只有有子检查点的检查点。（注意同时省略这两个选项不进行过滤，而同时提供这两个选项将根据服务器是否识别标志而产生相同列表或出错）。过滤选项与–tree*不兼容。 checkpoint-dumpxml语法：checkpoint-dumpxml [–security-info] [–no-domain] [–size] [–xpath 表达式] [–wrap] 域 检查点输出域名为检查点的检查点XML。使用*–security-info*还会包含安全敏感信息。 使用*–size将添加XML指示自创建检查点以来客户数据已更改的当前字节大小（尽管请记住在检查大小和实际创建备份之间的客户活动可能导致备份需要稍多空间）。注意某些虚拟机监控程序可能要求在使用–size时域*处于运行状态。 使用*–no-domain*将从输出中省略元素以获得更紧凑的视图。 如果 –xpath参数提供XPath表达式，它将对输出XML求值，并且只打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但是为了便于进一步处理，**–wrap**参数将导致匹配节点包装在公共根节点中。 checkpoint-parent语法：checkpoint-parent 域 检查点输出给定检查点的父检查点名称（如果有）。 checkpoint-delete语法：checkpoint-delete 域 检查点 [–metadata] [{–children | –children-only}]删除域名为检查点的检查点。自检查点以来磁盘哪些部分发生更改的记录将合并到父检查点（如果有）。如果传递*–children，则删除此检查点及其任何子检查点。如果传递–children-only*，则删除此检查点的任何子检查点，但保留此检查点完整。这两个标志是互斥的。 如果指定*–metadata*，则只删除libvirt维护的检查点元数据，而保留检查点内容供外部工具访问；否则删除检查点还会移除从该时间点执行增量备份的能力。 16 网络过滤器命令以下命令用于操作网络过滤器。网络过滤器允许过滤来自虚拟机和发往虚拟机的网络流量。单个网络流量过滤器以XML编写，可能包含对其他网络过滤器的引用、描述流量过滤规则或两者兼有。网络过滤器由虚拟机从其接口描述中引用。一个网络过滤器可以被多个虚拟机的接口引用。 nwfilter-define语法：nwfilter-define xml文件 [–validate]使新的网络过滤器为libvirt所知。如果已存在同名网络过滤器，将被新XML替换。任何引用此网络过滤器的运行虚拟机将调整其网络流量规则。如果任何运行虚拟机无法实例化网络流量过滤规则，则新XML将被拒绝。 可选地，可以使用*–validate*验证输入XML文件的格式是否符合内部RNG模式。 nwfilter-undefine语法：nwfilter-undefine 网络过滤器名称删除网络过滤器。如果任何运行虚拟机当前正在使用此网络过滤器，删除将失败。 nwfilter-list语法：nwfilter-list列出所有可用的网络过滤器。 nwfilter-dumpxml语法：nwfilter-dumpxml [–xpath 表达式] [–wrap] 网络过滤器名称输出网络过滤器XML。 如果 –xpath 参数提供XPath表达式，它将对输出XML求值，并且只打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但是为了便于进一步处理，**–wrap**参数将导致匹配节点包装在公共根节点中。 nwfilter-edit语法：nwfilter-edit 网络过滤器名称编辑网络过滤器的XML。 这相当于： virsh nwfilter-dumpxml myfilter &gt; myfilter.xmlvi myfilter.xml（或用其他文本编辑器进行更改）virsh nwfilter-define myfilter.xml只是它会进行一些错误检查。新网络过滤器可能会由于与nwfilter-define中提到的相同原因被拒绝。 使用的编辑器可以由$VISUAL或$EDITOR环境变量提供，默认为vi。 17 网络过滤器绑定命令以下命令用于操作网络过滤器绑定。网络过滤器绑定跟踪网络端口与网络过滤器之间的关联。通常绑定由虚拟机监控程序驱动程序在客户机上添加&#x2F;删除NIC时自动管理。 但是，如果管理员为非客户机使用创建&#x2F;删除TAP设备，网络过滤器绑定命令提供了一种直接使用网络过滤器的方式。 nwfilter-binding-create语法：nwfilter-binding-create xml文件 [–validate]将网络端口与网络过滤器关联。网络过滤器后端将立即尝试在端口上实例化过滤规则。此命令可用于为当前运行但未为特定网络端口定义过滤器的客户机关联过滤器。由于绑定通常由虚拟机监控程序自动管理，使用此命令为网络端口定义过滤器然后启动客户机可能会阻止客户机启动，如果它尝试使用网络端口并发现已定义过滤器。 可选地，可以使用*–validate*验证输入XML文件的格式是否符合内部RNG模式。 nwfilter-binding-delete语法：nwfilter-binding-delete 端口名称解除网络端口与网络过滤器的关联。网络过滤器后端将立即拆除端口上存在的过滤规则。此命令可用于在客户机运行时移除网络端口绑定过滤器而无需重新启动客户机。通过使用nwfilter-binding-create可以恢复运行客户机的网络端口绑定过滤器。 nwfilter-binding-list语法：nwfilter-binding-list列出所有关联了过滤器的网络端口。 nwfilter-binding-dumpxml语法：nwfilter-binding-dumpxml [–xpath 表达式] [–wrap] 端口名称输出名为端口名称的网络设备的网络过滤器绑定XML。 如果 –xpath 参数提供XPath表达式，它将对输出XML求值，并且只打印匹配的节点。默认行为是将每个匹配节点打印为独立文档，但是为了便于进一步处理，**–wrap**参数将导致匹配节点包装在公共根节点中。 18 虚拟机监控程序专用命令注意：强烈建议不要使用以下命令。它们可能导致 libvirt 在后续操作中出现混乱或错误行为。一旦使用了这些命令，请不要向 libvirt 开发者报告问题，相关报告将被忽略。如果您发现这些命令是完成某项任务的唯一方式，建议您请求将该功能作为正式特性添加到常规 libvirt 库中。 qemu-attach语法：qemu-attach pid将外部启动的 QEMU 进程附加到 libvirt QEMU 驱动程序中。QEMU 进程必须已使用 UNIX 驱动程序创建监控连接。理想情况下，该进程还应指定 -name 参数。示例：$ qemu-kvm -cdrom ~&#x2F;demo.iso \\-monitor unix:&#x2F;tmp&#x2F;demo,server,nowait \\-name foo \\-uuid cece4f9f-dff0-575d-0e8e-01fe380f12ea &amp;$ QEMUPID&#x3D;$!$ virsh qemu-attach $QEMUPID 并非所有 libvirt 功能在附加到外部启动的 QEMU 进程后都能可靠工作。迁移时可能会出现客户机 ABI 更改的问题，设备热插拔或热拔可能无法正常工作。附加的环境应被视为主要是只读的。 qemu-monitor-command语法：qemu-monitor-command 域 { [–hmp] | [–pretty] [–return-value] }[–pass-fds N,M,…] 命令…通过 QEMU 监控器向指定的域发送任意监控命令。命令的结果将打印在标准输出上。 如果为命令提供了多个参数，它们会在传递给监控器之前用空格连接成一个命令。 注意，libvirt 使用 QMP 与 QEMU 通信，因此命令必须是有效的 QMP 格式 JSON 才能正常工作。如果命令不是 JSON 对象，libvirt 会尝试将其包装为 JSON 对象，以提供方便的接口，例如处理方式相同的命令组： 简单命令: 12\\$ virsh qemu-monitor-command VM 命令名 \\$ virsh qemu-monitor-command VM &#x27;&#123;&quot;execute&quot;:&quot;命令名&quot;&#125;&#x27; 带参数的命令： 1234\\$ virsh qemu-monitor-command VM 命令名 &#x27;&quot;arg1&quot;:123&#x27; &#x27;&quot;arg2&quot;:&quot;test&quot;&#x27; \\$ virsh qemu-monitor-command VM 命令名 &#x27;&#123;&quot;arg1&quot;:123,&quot;arg2&quot;:&quot;test&quot;&#125;&#x27; \\$ virsh qemu-monitor-command VM &#x27;&#123;&quot;execute&quot;:&quot;命令名&quot;, &quot;arguments&quot;:&#123;&quot;arg1&quot;:123,&quot;arg2&quot;:&quot;test&quot;&#125;&#125;&#x27; 如果指定 --pretty，QMP 回复会以美观格式打印。如果指定 --return-value，将从 QMP 响应对象中提取 &#39;return&#39; 键，而不是传递完整的 QEMU 回复。如果指定 --hmp，命令将被视为人类监控命令，libvirt 会自动将其转换为 QMP 并将结果转换回来。如果指定 --pass-fds，参数是一个逗号分隔的打开文件描述符列表，这些文件描述符将与命令一起传递给 QEMU。 qemu-agent-command语法：qemu-agent-command 域 [–timeout 秒数 | –async | –block] 命令…通过 QEMU 代理向指定的域发送任意客户机代理命令。--timeout、--async 和 --block 选项是互斥的。--timeout 需要指定正数的超时秒数。当使用 --async 时，命令会等待超时，无论成功或失败。当使用 --block 时，命令会无限期阻塞等待。 qemu-monitor-event语法：qemu-monitor-event [域] [–event 事件名][–loop] [–timeout 秒数] [–pretty] [–regex] [–no-case][–timestamp] 等待任意 QEMU 监控事件发生，并在事件发生时打印详细信息。可以通过域或事件名对事件进行筛选。可以通过 qemu-monitor-command 使用 &#39;query-events&#39; QMP 命令了解支持的事件。如果使用 --regex，事件名是基本正则表达式而不是字面字符串。如果使用 --no-case，事件名将不区分大小写匹配。 默认情况下，此命令是一次性的，一旦事件发生即返回成功；可以通过发送 SIGINT（通常为 Ctrl-C）立即退出。如果指定 --timeout，命令会在指定秒数后放弃等待事件。使用 --loop 时，命令会打印所有事件，直到超时或中断。如果指定 --pretty，任何 JSON 事件细节会以美观格式打印以提高可读性。 当使用 --timestamp 时，会在事件前打印人类可读的时间戳，并省略 QEMU 提供的时间信息。 lxc-enter-namespace语法：lxc-enter-namespace 域 [–noseclabel] –&#x2F;path&#x2F;to&#x2F;binary [参数1, [参数2, …]]进入指定域的命名空间并执行命令 /path/to/binary，传递指定的参数。二进制路径是相对于容器根文件系统的，而不是主机根文件系统。二进制文件会继承 virsh 可见的环境变量和控制台。命令将以与容器内进程相同的 sVirt 上下文和 cgroups 位置运行。此命令仅在连接到 LXC 虚拟机监控程序驱动程序时有效。仅当 /path/to/binary 的退出状态为 0 时，此命令才会成功。 默认情况下，新进程将以新父容器的安全标签运行。使用 --noseclabel 选项可以让进程保持与 virsh 相同的安全标签。 19 环境变量以下环境变量可以设置以改变 virsh 的行为： VIRSH_DEBUG&#x3D;&lt;0 到 4&gt;启用 virsh 命令的详细调试。有效级别为： VIRSH_DEBUG&#x3D;0DEBUG - 记录所有级别的消息 VIRSH_DEBUG&#x3D;1INFO - 记录 INFO、NOTICE、WARNING 和 ERROR 级别的消息 VIRSH_DEBUG&#x3D;2NOTICE - 记录 NOTICE、WARNING 和 ERROR 级别的消息 VIRSH_DEBUG&#x3D;3WARNING - 记录 WARNING 和 ERROR 级别的消息 VIRSH_DEBUG&#x3D;4ERROR - 仅记录 ERROR 级别的消息 VIRSH_LOG_FILE&#x3D;LOGFILE 记录 virsh 调试消息的文件。 VIRSH_DEFAULT_CONNECT_URI 默认连接的虚拟机监控程序。设置为 URI，格式与 connect 选项接受的格式相同。此环境变量已弃用，推荐使用全局变量 LIBVIRT_DEFAULT_URI，其功能相同。 LIBVIRT_DEFAULT_URI 默认连接的虚拟机监控程序。设置为 URI，格式与 connect 选项接受的格式相同。这会覆盖任何客户端配置文件中的默认 URI，并阻止 libvirt 探测驱动程序。 VISUALedit 及相关选项使用的编辑器。 EDITOR如果未设置 VISUAL，edit 及相关选项使用的编辑器。 VIRSH_HISTSIZE命令历史记录中记住的命令数量。默认值为 500。 LIBVIRT_DEBUG&#x3D;LEVEL启用所有 libvirt API 调用的详细调试。有效级别为： LIBVIRT_DEBUG&#x3D;1DEBUG 及以上级别的消息 LIBVIRT_DEBUG&#x3D;2INFO 及以上级别的消息 LIBVIRT_DEBUG&#x3D;3WARNING 及以上级别的消息 LIBVIRT_DEBUG&#x3D;4ERROR 及以上级别的消息","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"12 libvirt使用示例","slug":"libvirt文档/12-libvirt使用示例","date":"2024-03-12T07:50:26.000Z","updated":"2025-03-15T08:00:51.913Z","comments":true,"path":"libvirt文档/12-libvirt使用示例/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/12-libvirt%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"使用示例本节将介绍该库的基础知识，旨在提供一个快速教程，让开发人员尽快掌握并运行该库。 零到虚拟设备本节介绍创建虚拟机（或容器）的必要步骤。使用的驱动程序是 QEMU，但这些说明也可用于其他驱动程序。这里给出的命令是针对 python3 的，建议你在使用 ipython3 等交互式提示符时运行这些命令。 示例 12.1：定义新存储池1234567891011121314151617181920212223242526272829import libvirt, sysconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)# 我们不需要定义容量，因为我们希望它是无限的。poolXML = &quot;&quot;&quot;&lt;pool type=&#x27;dir&#x27;&gt; &lt;name&gt;示例池&lt;/name&gt; &lt;uuid/&gt; &lt;source&gt; &lt;/source&gt; &lt;target&gt; &lt;path&gt;/var/lib/libvirt/images&lt;/path&gt; &lt;permissions&gt; &lt;mode&gt;0755&lt;/mode&gt; &lt;owner&gt;-1&lt;/owner&gt; &lt;group&gt;-1&lt;/group&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/pool&gt;&quot;&quot;&quot;pool = conn.storagePoolDefineXML(poolXML, 0)pool.setAutostart(1)pool.create() 示例 12.2：定义新存储卷123456789101112131415161718volumeXML = &quot;&quot;&quot;&lt;volume&gt; &lt;name&gt;Volume.qcow2&lt;/name&gt; &lt;allocation&gt;0&lt;/allocation&gt; &lt;capacity unit=&quot;G&quot;&gt;16&lt;/capacity&gt; &lt;target&gt; &lt;path&gt;/var/lib/libvirt/images/Volume.qcow2&lt;/path&gt; &lt;format type=&#x27;qcow2&#x27;/&gt; &lt;permissions&gt; &lt;owner&gt;107&lt;/owner&gt; &lt;group&gt;107&lt;/group&gt; &lt;mode&gt;0744&lt;/mode&gt; &lt;label&gt;体积示例&lt;/label&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/volume&gt;&quot;&quot;&quot;pool.createXML(volumeXML, 0) 示例 12.3：定义域123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899domainXML = &quot;&quot;&quot;&lt;domain type=&#x27;kvm&#x27;&gt; &lt;name&gt;示例域&lt;/name&gt; &lt;memory unit=&#x27;MiB&#x27;&gt;2048&lt;/memory&gt; &lt;currentMemory unit=&#x27;MiB&#x27;&gt;2048&lt;/currentMemory&gt; &lt;vcpu placement=&#x27;static&#x27;&gt;2&lt;/vcpu&gt; &lt;os&gt; &lt;type arch=&#x27;x86_64&#x27;&gt;hvm&lt;/type&gt; &lt;boot dev=&#x27;hd&#x27;/&gt; &lt;boot dev=&#x27;cdrom&#x27;/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;vmport state=&#x27;off&#x27;/&gt; &lt;/features&gt; &lt;clock offset=&#x27;utc&#x27;&gt; &lt;timer name=&#x27;rtc&#x27; tickpolicy=&#x27;catchup&#x27;/&gt; &lt;timer name=&#x27;pit&#x27; tickpolicy=&#x27;delay&#x27;/&gt; &lt;timer name=&#x27;hpet&#x27; present=&#x27;no&#x27;/&gt; &lt;/clock&gt; &lt;pm&gt; &lt;suspend-to-mem enabled=&#x27;yes&#x27;/&gt; &lt;suspend-to-disk enabled=&#x27;yes&#x27;/&gt; &lt;/pm&gt; &lt;devices&gt; &lt;emulator&gt;/usr/bin/kvm-spice&lt;/emulator&gt; &lt;disk type=&#x27;file&#x27; device=&#x27;disk&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;qcow2&#x27;/&gt; &lt;source file=&#x27;/var/lib/libvirt/images/Volume.qcow2&#x27;/&gt; &lt;target dev=&#x27;hda&#x27; bus=&#x27;ide&#x27;/&gt; &lt;address type=&#x27;drive&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; target=&#x27;0&#x27; unit=&#x27;0&#x27;/&gt; &lt;/disk&gt; &lt;disk type=&#x27;file&#x27; device=&#x27;cdrom&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;raw&#x27;/&gt; &lt;source file=&#x27;&#x27;/&gt; &lt;target dev=&#x27;hdb&#x27; bus=&#x27;ide&#x27;/&gt; &lt;address type=&#x27;drive&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; target=&#x27;0&#x27; unit=&#x27;1&#x27;/&gt; &lt;/disk&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-ehci1&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x7&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci1&#x27;&gt; &lt;master startport=&#x27;0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x0&#x27; multifunction=&#x27;on&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci2&#x27;&gt; &lt;master startport=&#x27;2&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci3&#x27;&gt; &lt;master startport=&#x27;4&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x2&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;0&#x27; model=&#x27;pci-root&#x27;/&gt; &lt;controller type=&#x27;ide&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x01&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;virtio-serial&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x06&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/controller&gt; &lt;serial type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;isa-serial&#x27; port=&#x27;0&#x27;&gt; &lt;model name=&#x27;isa-serial&#x27;/&gt; &lt;/target&gt; &lt;/serial&gt; &lt;console type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt; &lt;/console&gt; &lt;channel type=&#x27;spicevmc&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;com.redhat.spice.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/channel&gt; &lt;input type=&#x27;mouse&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;input type=&#x27;keyboard&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;graphics type=&#x27;spice&#x27; autoport=&#x27;yes&#x27;&gt; &lt;listen type=&#x27;address&#x27;/&gt; &lt;image compression=&#x27;off&#x27;/&gt; &lt;/graphics&gt; &lt;sound model=&#x27;ich6&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x04&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/sound&gt; &lt;video&gt; &lt;model type=&#x27;qxl&#x27; ram=&#x27;65536&#x27; vram=&#x27;65536&#x27; vgamem=&#x27;16384&#x27; heads=&#x27;1&#x27; primary=&#x27;yes&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/video&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/redirdev&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/redirdev&gt; &lt;mballoon model=&#x27;virtio&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x07&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/mballoon&gt; &lt;/devices&gt;&lt;/domain&gt;&quot;&quot;&quot;dom = conn.defineXML(domainXML) 示例 12.4：将新磁盘附加到域123456789101112# 所需 &quot;.iso&quot; 文件的路径diskFile = &quot;/tmp/debian.iso&quot;diskXML = &quot;&quot;&quot;&lt;disk type=&#x27;file&#x27; device=&#x27;cdrom&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;raw&#x27;/&gt; &lt;source file=&#x27;&quot;&quot;&quot; + diskFile + &quot;&quot;&quot;&#x27;/&gt; &lt;target dev=&#x27;hdb&#x27; bus=&#x27;ide&#x27;/&gt; &lt;address type=&#x27;drive&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; target=&#x27;0&#x27; unit=&#x27;1&#x27;/&gt;&lt;/disk&gt;&quot;&quot;&quot;dom.updateDeviceFlags(diskXML, 0) 示例 12.5：启动域1dom.create() 我们可以使用 virt-viewer 与我们的域进行交互。只需在终端运行它，我们就能获得可连接的虚拟机列表，从而启动图形会话。 示例 12.6：拍摄快照1234567snapXML = &quot;&quot;&quot;&lt;domainsnapshot&gt; &lt;name&gt;第一快照&lt;/name&gt; &lt;description&gt;刚刚创建的虚拟机&lt;/description&gt;&lt;/domainsnapshot&gt;&quot;&quot;&quot;dom.snapshotCreateXML(snapXML, 0) 示例 12.7：恢复到快照1234567891011# 列出所有快照snapshotList = dom.listAllSnapshots()# 用我们想要的名称查找快照revertTo = Nonefor snap in snapshotList: if snap.getName() == &#x27;FirstSnapshot&#x27;: revertTo = snapif revertTo is not None: dom.revertToSnapshot(revertTo) 截图要执行涉及从虚拟化驱动程序上运行的软件发送和接收数据的操作，我们需要使用流。 示例 12.8：查找域12345678910import sys, libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)domain = conn.lookupByName(&#x27;TestDomain&#x27;) 示例 12.9：使用数据流截屏12stream = conn.newStream()imageType = domain.screenshot(stream, 0) 示例 12.10：从数据流中读取数据123456789file = &quot;Screenshot of &quot; + dom.name()fileHandler = open(file, &#x27;wb&#x27;)streamBytes = stream.recv(262120)while streamBytes != b&#x27;&#x27;: fileHandler.write(streamBytes) streamBytes = stream.recv(262120)fileHandler.close()print(&#x27;Screenshot saved as type: &#x27; + imageType) 示例 12.11：截屏123456789101112131415161718192021222324252627282930313233343536# Example-11.pyimport sysimport libvirtdomName = &#x27;TestAppliance&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)stream = conn.newStream()imageType = domain.screenshot(stream, 0)file = &quot;Screenshot of &quot; + dom.name()fileHandler = open(file, &#x27;wb&#x27;)streamBytes = stream.recv(262120)while streamBytes != b&#x27;&#x27;: fileHandler.write(streamBytes) streamBytes = stream.recv(262120)fileHandler.close()print(&#x27;Screenshot saved as type: &#x27; + imageType)stream.finish()conn.close()exit(0)","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"11 libvirt调试与日志","slug":"libvirt文档/11-libvirt调试与日志","date":"2024-03-11T07:50:26.000Z","updated":"2025-03-15T08:00:49.288Z","comments":true,"path":"libvirt文档/11-libvirt调试与日志/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/11-libvirt%E8%B0%83%E8%AF%95%E4%B8%8E%E6%97%A5%E5%BF%97/","excerpt":"","text":"调试与日志Libvirt 包含日志设施，以便于跟踪库的执行情况。在试图获得 libvirt 支持时，这些日志会经常被要求记录，因此熟悉这些日志至关重要。 libvirt 中的日志记录设施基于 3 个关键概念： 日志消息 - 由 libvirt 代码在运行时生成，包括时间戳、优先级（DEBUG &#x3D; 1、INFO &#x3D; 2、WARNING &#x3D; 3、ERROR &#x3D; 4）、类别、函数名称和行号（表明消息的来源）以及格式化的消息。 日志过滤器 - 控制是否显示特定信息的模式和优先级。过滤器的格式为 x:名称，其中 x 是匹配应适用的最小优先级，name 是要匹配的字符串。优先级为： 1（或调试）- 记录所有信息 2（或 info）- 记录所有非调试信息 3（或警告） - 只记录警告和错误 - 这是默认设置 4（或错误）- 只记录错误 例如，要将所有调试信息记录到 qemu 驱动程序，可使用以下过滤器： 11:qemu 下例记录了来自 qemu 的所有调试信息，并记录了来自远程驱动程序的所有错误信息： 11:qemu 4:remote 日志输出 - 信息通过过滤器后发送到哪里。日志输出的格式有以下几种： x:stderr - 记录到 stderr x:syslog:name - 将日志记录到以 name 为前缀的 syslog 中 x:file:file_path - 将日志记录到以 file_path 指定的文件中 其中 x 是最小优先级。例如，要将所有警告和错误记录到以 libvirtd 为前缀的 syslog 中，可使用以下输出： 13:syslog:libvirtd 以下示例将所有错误和警告信息记录到系统日志，并将所有调试、信息、警告和错误信息记录到 /tmp/libvirt.log： 13:syslog:libvirtd 1:file:/tmp/libvirt.log 11.1 环境变量通过使用环境变量，libvirt 库可以指定所需的日志优先级、过滤器和输出： LIBVIRT_DEBUG 指定了日志信息的最低优先级。这可以看作是一个 “全局” 优先级；如果某条日志信息与 LIBVIRT_LOG_FILTERS 中的特定过滤器不匹配，就会与这个全局优先级进行比较，并根据情况记录下来。 LIBVIRT_LOG_FILTERS 指定要应用的过滤器。 LIBVIRT_LOG_OUTPUTS 指定了要发送信息的输出端。 示例 11.1：使用环境变量运行 virsh要查看有关 virsh 运行情况的更详细信息，我们可以像下面这样运行它： 1LIBVIRT_DEBUG=error LIBVIRT_LOG_FILTERS=&quot;1:remote&quot; virsh list 此示例将只打印来自 virsh 的错误信息，但远程驱动程序将打印所有调试、信息、警告和错误信息。","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"10 libvirt安全模式","slug":"libvirt文档/10-libvirt安全模式","date":"2024-03-10T07:50:26.000Z","updated":"2025-03-15T08:00:47.056Z","comments":true,"path":"libvirt文档/10-libvirt安全模式/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/10-libvirt%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"安全模式虽然 Python 模块提供了对安全和秘密方法的完全访问权限，但这一主题目前超出了本指南的范围。您可以运行以下命令来查找安全和秘密方法： 1pydoc libvirt 这将为您提供针对您的 Linux 发行版的 Python 类、方法和函数的 manpage。","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"9 libvirt事件和计时器处理","slug":"libvirt文档/9-libvirt事件和计时器处理","date":"2024-03-09T07:50:26.000Z","updated":"2025-03-15T08:00:44.903Z","comments":true,"path":"libvirt文档/9-libvirt事件和计时器处理/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/9-libvirt%E4%BA%8B%E4%BB%B6%E5%92%8C%E8%AE%A1%E6%97%B6%E5%99%A8%E5%A4%84%E7%90%86/","excerpt":"","text":"事件和计时器处理Python libvirt 模块提供了处理事件和计时器的完整接口。事件和定时器处理都是通过函数接口而非类&#x2F;方法接口调用的。这使得将接口集成到图形程序或控制台程序中更加容易。 事件处理Python libvirt 模块提供了一个事件处理框架。虽然它对图形程序最有用，但也可用于控制台程序，以提供一致的用户界面并控制控制台事件的处理。 事件处理通过以下函数完成： virEventAddHandle virEventRegisterDefaultImpl virEventRegisterImpl virEventRemoveHandle virEventRunDefaultImpl virEventUpdateHandle 创建事件时，需要事先在 virEventRegisterImpl 或 virEventRegisterDefaultImpl 中注册。 下面是一个使用了大部分这些功能的示例程序。请注意，要通过终端与客户机交互，需要在客户机系统中启用串行控制台。对于 Linux 主机，可以这样做： 12sudo systemctl enable serial-getty@ttyS0.servicesudo systemctl start serial-getty@ttyS0.service 示例 9.1：提供可在客户机重启后继续使用的持久控制台123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 示例-1.py# consolecallback - 提供一个持久的控制台，在客户机重启后仍可使用#!/usr/bin/env python3import sys, os, logging, libvirt, tty, termios, atexitdef reset_term(): termios.tcsetattr(0, termios.TCSADRAIN, attrs)def error_handler(unused, error): # 控制台流会在虚拟机关闭时出错 if (error[0] == libvirt.VIR_ERR_RPC and error[1] == libvirt.VIR_FROM_STREAMS): return logging.warn(error)class Console(object): def __init__(self, uri, uuid): self.uri = uri self.uuid = uuid self.connection = libvirt.open(uri) self.domain = self.connection.lookupByUUIDString(uuid) self.state = self.domain.state(0) self.connection.domainEventRegister(lifecycle_callback, self) self.stream = None self.run_console = True logging.info(&quot;%s 初始状态 %d，原因 %d&quot;, self.uuid, self.state[0], self.state[1])def check_console(console): if (console.state[0] == libvirt.VIR_DOMAIN_RUNNING or console.state[0] == libvirt.VIR_DOMAIN_PAUSED): if console.stream is None: console.stream = console.connection.newStream(libvirt.VIR_STREAM_NONBLOCK) console.domain.openConsole(None, console.stream, 0) console.stream.eventAddCallback(libvirt.VIR_STREAM_EVENT_READABLE, stream_callback, console) else: if console.stream: console.stream.eventRemoveCallback() console.stream = None return console.run_consoledef stdin_callback(watch, fd, events, console): readbuf = os.read(fd, 1024) if readbuf.startswith(&quot;&quot;): console.run_console = False return if console.stream: console.stream.send(readbuf)def stream_callback(stream, events, console): try: received_data = console.stream.recv(1024) except: return os.write(0, received_data)def lifecycle_callback(connection, domain, event, detail, console): console.state = console.domain.state(0) logging.info(&quot;%s 过渡到状态 %d，原因 %d&quot;, console.uuid, console.state[0], console.state[1])# mainif len(sys.argv) != 3: print(&quot;Usage:&quot;, sys.argv[0], &quot;URI UUID&quot;) print(&quot;for example:&quot;, sys.argv[0], &quot;&#x27;qemu:///system&#x27; &#x27;32ad945f-7e78-c33a-e96d-39f25e025d81&#x27;&quot;) sys.exit(1)uri = sys.argv[1]uuid = sys.argv[2]print(&quot;Escape character is ^]&quot;)logging.basicConfig(filename=&#x27;msg.log&#x27;, level=logging.DEBUG)logging.info(&quot;URI: %s&quot;, uri)logging.info(&quot;UUID: %s&quot;, uuid)libvirt.virEventRegisterDefaultImpl()libvirt.registerErrorHandler(error_handler, None)atexit.register(reset_term)attrs = termios.tcgetattr(0)tty.setraw(0)console = Console(uri, uuid)console.stdin_watch = libvirt.virEventAddHandle(0, libvirt.VIR_EVENT_HANDLE_READABLE, stdin_callback, console)while check_console(console): libvirt.virEventRunDefaultImpl() 计时器处理Python libvirt 模块为定时器处理提供了一个框架。创建定时器需要事先用 virEventRegisterImpl 或 virEventRegisterDefaultImpl 注册一个事件循环。 定时器处理通过以下函数完成： virEventAddTimeout virEventUpdateTimeout virEventRemoveTimeout 实施过程将支持多个计时器。 要创建新的计时器，请在 virEventRegisterImpl 或 virEventRegisterDefaultImpl 函数已被调用。 可以使用 virEventRemoveTimeout 移除计时器，或使用 virEventUpdateTimeout 函数更新计时器。","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"8 libvirt错误处理","slug":"libvirt文档/8-libvirt错误处理","date":"2024-03-08T07:50:26.000Z","updated":"2025-03-15T08:00:39.705Z","comments":true,"path":"libvirt文档/8-libvirt错误处理/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/8-libvirt%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/","excerpt":"","text":"Libvirt 错误处理libvirt 错误函数的设计目的是在普通 libvirt 函数或方法返回错误的情况下，提供更详细的失败原因信息。Python libvirt 错误报告中需要注意的一点是，错误是按线程而不是按连接存储的。 libvirtError 类libvirt Python 模块定义了一个标准异常类 libvirtError，可以对其进行子类化，以便在引发 libvirt 异常时添加其他功能。libvirtError 类的部分定义如下： 示例 8.1：Libvirt 模块 libvirtError 类定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class libvirtError(exceptions.Exception): def __init__(self, defmsg, conn=None, dom=None, net=None, pool=None, vol=None): # 切勿调用 virGetLastError()。 # virGetLastError() 现在是线程本地 err = virGetLastError() if err is None: msg = defmsg else: msg = err[2] Exception.__init__(self, msg) self.err = err def get_error_code(self): if self.err is None: return None return self.err[0] def get_error_domain(self): if self.err is None: return None return self.err[1] def get_error_message(self): if self.err is None: return None return self.err[2] def get_error_level(self): if self.err is None: return None return self.err[3] def get_str1(self): if self.err is None: return None return self.err[4] def get_str2(self): if self.err is None: return None return self.err[5] def get_str3(self): if self.err is None: return None return self.err[6] def get_int1(self): if self.err is None: return None return self.err[7] def get_int2(self): if self.err is None: return None return self.err[8] 错误代码方法 get_error_code 返回从错误中返回的错误代码。这是 Python libvirt 模块的数据定义之一。此列表中某些编号较高的条目可能在您的 Python libvirt 模块中不可用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293VIR_ERR_OK = 0VIR_ERR_INTERNAL_ERROR = 1 # 内部错误VIR_ERR_NO_MEMORY = 2 # 内存分配失败VIR_ERR_NO_SUPPORT = 3 # 不支持此函数VIR_ERR_UNKNOWN_HOST = 4 # 无法解析主机名VIR_ERR_NO_CONNECT = 5 # 无法连接到管理程序VIR_ERR_INVALID_CONN = 6 # 连接对象无效VIR_ERR_INVALID_DOMAIN = 7 # 域对象无效VIR_ERR_INVALID_ARG = 8 # 函数参数无效VIR_ERR_OPERATION_FAILED = 9 # 向管理程序发出的命令失败VIR_ERR_GET_FAILED = 10 # HTTP GET 命令失败VIR_ERR_POST_FAILED = 11 # HTTP POST 命令失败VIR_ERR_HTTP_ERROR = 12 # 意外的 HTTP 错误代码VIR_ERR_SEXPR_SERIAL = 13 # 序列化 S-Expr 失败VIR_ERR_NO_XEN = 14 # 无法打开 Xen 虚拟机管理程序控件VIR_ERR_XEN_CALL = 15 # 管理程序调用失败VIR_ERR_OS_TYPE = 16 # 未知操作系统类型VIR_ERR_NO_KERNEL = 17 # 缺少内核信息VIR_ERR_NO_ROOT = 18 # 缺少根设备信息VIR_ERR_NO_SOURCE = 19 # 缺少源设备信息VIR_ERR_NO_TARGET = 20 # 缺少目标设备信息VIR_ERR_NO_NAME = 21 # 缺少域名信息VIR_ERR_NO_OS = 22 # 缺少域操作系统信息VIR_ERR_NO_DEVICE = 23 # 缺少域设备信息VIR_ERR_NO_XENSTORE = 24 # 无法打开 Xen 存储控制VIR_ERR_DRIVER_FULL = 25 # 注册的驱动程序过多VIR_ERR_CALL_FAILED = 26 # 驱动程序不支持（已删除）VIR_ERR_XML_ERROR = 27 # XML 描述不完整或已损坏VIR_ERR_DOM_EXIST = 28 # 域已经存在VIR_ERR_OPERATION_DENIED = 29 # 只读连接上禁止操作VIR_ERR_OPEN_FAILED = 30 # 打开配置文件失败VIR_ERR_READ_FAILED = 31 # 读取配置文件失败VIR_ERR_PARSE_FAILED = 32 # 解析配置文件失败VIR_ERR_CONF_SYNTAX = 33 # 解析配置文件语法失败VIR_ERR_WRITE_FAILED = 34 # 写入配置文件失败VIR_ERR_XML_DETAIL = 35 # XML 错误细节VIR_ERR_INVALID_NETWORK = 36 # 无效的网络对象VIR_ERR_NETWORK_EXIST = 37 # 网络已经存在VIR_ERR_SYSTEM_ERROR = 38 # 一般系统调用故障VIR_ERR_RPC = 39 # 某种 RPC 错误VIR_ERR_GNUTLS_ERROR = 40 # 因调用 GNUTLS 而出错VIR_WAR_NO_NETWORK = 41 # 启动网络失败VIR_ERR_NO_DOMAIN = 42 # 域未找到或意外消失VIR_ERR_NO_NETWORK = 43 # 网络未找到VIR_ERR_INVALID_MAC = 44 # 无效的 MAC 地址VIR_ERR_AUTH_FAILED = 45 # 认证失败VIR_ERR_INVALID_STORAGE_POOL = 46 # 无效的存储池对象VIR_ERR_INVALID_STORAGE_VOL = 47 # 无效的存储卷对象VIR_WAR_NO_STORAGE = 48 # 启动存储失败VIR_ERR_NO_STORAGE_POOL = 49 # 存储池未找到VIR_ERR_NO_STORAGE_VOL = 50 # 未找到存储卷VIR_WAR_NO_NODE = 51 # 启动节点驱动失败VIR_ERR_INVALID_NODE_DEVICE = 52 # 无效的节点设备对象VIR_ERR_NO_NODE_DEVICE = 53 # 未找到节点设备VIR_ERR_NO_SECURITY_MODEL = 54 # 未找到安全模型VIR_ERR_OPERATION_INVALID = 55 # 操作此时不适用VIR_WAR_NO_INTERFACE = 56 # 未能启动接口驱动程序VIR_ERR_NO_INTERFACE = 57 # 接口驱动程序未运行VIR_ERR_INVALID_INTERFACE = 58 # 接口对象无效VIR_ERR_MULTIPLE_INTERFACES = 59 # 发现一个以上匹配的接口VIR_WAR_NO_NWFILTER = 60 # 启动 nwfilter 驱动失败VIR_ERR_INVALID_NWFILTER = 61 # 无效的 nwfilter 对象VIR_ERR_NO_NWFILTER = 62 # 未找到 nw 过滤器池VIR_ERR_BUILD_FIREWALL = 63 # 未找到 nw 过滤器池VIR_WAR_NO_SECRET = 64 # 启动秘密存储失败VIR_ERR_INVALID_SECRET = 65 # 无效密文VIR_ERR_NO_SECRET = 66 # 未找到密文VIR_ERR_CONFIG_UNSUPPORTED = 67 # 不支持配置结构VIR_ERR_OPERATION_TIMEOUT = 68 # 运行期间发生超时VIR_ERR_MIGRATE_PERSIST_FAILED = 69 # 迁移成功，但让虚拟机在目标主机上持续运行失败VIR_ERR_HOOK_SCRIPT_FAILED = 70 # 同步钩子脚本失败VIR_ERR_INVALID_DOMAIN_SNAPSHOT = 71 # 无效的域快照VIR_ERR_NO_DOMAIN_SNAPSHOT = 72 # 未找到域快照VIR_ERR_INVALID_STREAM = 73 # 无效的 i/o 流VIR_ERR_ARGUMENT_UNSUPPORTED = 74 # 参数不支持VIR_ERR_STORAGE_PROBE_FAILED = 75 # 存储探测失败VIR_ERR_STORAGE_POOL_BUILT = 76VIR_ERR_SNAPSHOT_REVERT_RISKY = 77VIR_ERR_OPERATION_ABORTED = 78 # 操作被中止VIR_ERR_AUTH_CANCELLED = 79VIR_ERR_NO_DOMAIN_METADATA = 80 # 没有找到域元数据VIR_ERR_MIGRATE_UNSAFE = 81VIR_ERR_OVERFLOW = 82 # 检测到溢出情况VIR_ERR_BLOCK_COPY_ACTIVE = 83VIR_ERR_OPERATION_UNSUPPORTED = 84 # 该操作不支持VIR_ERR_SSH = 85 # 检测到一个 ssh 错误VIR_ERR_AGENT_UNRESPONSIVE = 86 # 检测到一个代理超时VIR_ERR_RESOURCE_BUSY = 87VIR_ERR_ACCESS_DENIED = 88VIR_ERR_DBUS_SERVICE = 89VIR_ERR_STORAGE_VOL_EXIST = 90VIR_ERR_CPU_INCOMPATIBLE = 91VIR_ERR_XML_INVALID_SCHEMA = 92 错误域get_error_domain 方法之所以命名为 get_error_domain，是出于传统原因，但实际上代表了 libvirt 中产生错误的部分。这是 Python libvirt 模块的数据定义之一。此列表中一些编号较高的条目可能在您的 Python libvirt 模块中不可用。完整列表如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061VIR_FROM_NONE = 0VIR_FROM_XEN = 1 # Xen 虚拟机管理程序层出现错误VIR_FROM_XEND = 2 # 与 xend 守护进程连接时出错VIR_FROM_XENSTORE = 3 # 与 xen 存储器连接时出错VIR_FROM_SEXPR = 4 # S-Expression 代码中出错VIR_FROM_XML = 5 # XML 代码中的错误VIR_FROM_DOM = 6 # 在域上运行时出错VIR_FROM_RPC = 7 # XML-RPC 代码中出错VIR_FROM_PROXY = 8 # 代理代码中的错误VIR_FROM_CONF = 9 # 配置文件处理错误VIR_FROM_QEMU = 10 # QEMU 守护进程出错VIR_FROM_NET = 11 # 在网络上运行时出错VIR_FROM_TEST = 12 # 测试驱动程序出错VIR_FROM_REMOTE = 13 # 远程驱动程序出错VIR_FROM_OPENVZ = 14 # OpenVZ 驱动程序出错VIR_FROM_XENXM = 15 # Xen XM 层出错VIR_FROM_STATS_LINUX = 16 # Linux 统计代码中的错误VIR_FROM_LXC = 17 # Linux 容器驱动程序出错VIR_FROM_STORAGE = 18 # 存储驱动程序出错VIR_FROM_NETWORK = 19 # 网络配置出错VIR_FROM_DOMAIN = 20 # 域配置出错VIR_FROM_UML = 21 # UML 驱动出错VIR_FROM_NODEDEV = 22 # 来自节点设备监控器的错误VIR_FROM_XEN_INOTIFY = 23 # 来自 xen inotify 层的错误VIR_FROM_SECURITY = 24 # 安全框架出错VIR_FROM_VBOX = 25 # VirtualBox 驱动程序出错VIR_FROM_INTERFACE = 26 # 在接口上操作时出错VIR_FROM_ONE = 27 # OpenNebula 驱动程序出错VIR_FROM_ESX = 28 # ESX 驱动程序出错VIR_FROM_PHYP = 29 # 来自 IBM Power 管理程序的错误VIR_FROM_SECRET = 30 # 来自秘密存储的错误VIR_FROM_CPU = 31 # 来自 CPU 驱动程序的错误VIR_FROM_XENAPI = 32 # 来自 XenAPI 的错误VIR_FROM_NWFILTER = 33 # 网络过滤器驱动程序出错VIR_FROM_HOOK = 34 # 同步钩子出错VIR_FROM_DOMAIN_SNAPSHOT = 35 # 域快照中的错误VIR_FROM_AUDIT = 36VIR_FROM_SYSINFO = 37VIR_FROM_STREAMS = 38VIR_FROM_VMWARE = 39VIR_FROM_EVENT = 40VIR_FROM_LIBXL = 41VIR_FROM_LOCKING = 42VIR_FROM_HYPERV = 43VIR_FROM_CAPABILITIES = 44VIR_FROM_URI = 45VIR_FROM_AUTH = 46VIR_FROM_DBUS = 47VIR_FROM_PARALLELS = 48VIR_FROM_DEVICE = 49VIR_FROM_SSH = 50VIR_FROM_LOCKSPACE = 51VIR_FROM_INITCTL = 52VIR_FROM_IDENTITY = 53VIR_FROM_CGROUP = 54VIR_FROM_ACCESS = 55VIR_FROM_SYSTEMD = 56VIR_FROM_BHYVE = 57VIR_FROM_CRYPTO = 58VIR_FROM_FIREWALL = 59VIR_FROM_POLKIT = 60 错误级别get_error_level 方法描述了错误的严重程度。这是 Python libvirt 模块的数据定义之一。错误级别的完整列表如下： 123VIR_ERR_NONE = 0VIR_ERR_WARNING = 1 # 简单警告VIR_ERR_ERROR = 2 # 错误 其他错误信息 get_error_message 方法是一个描述错误的可读字符串。 get_error_str1 方法提供了额外的人类可读信息。 get_error_str2 方法提供额外的可读信息。 get_error_str3 方法提供了额外的可读信息。 get_error_int1 方法提供了额外的数字信息，这些信息可能有助于进一步对错误进行分类。 get_error_int2 方法会提供额外的数字信息，这些信息可能有助于进一步对错误进行分类。 virGetLastErrorvirGetLastError 函数可用于获取一个 Python 列表，其中包含 libvirt 报告的所有错误信息。此信息保存在线程本地存储中，因此不同线程可以安全地同时使用此函数。需要注意的是，该函数不会进行复制，因此如果当前线程随后调用该函数，错误信息可能会丢失。以下代码演示了 virGetLastError 的使用： 示例 8.2：使用 virGetLastError1234567891011121314151617181920212223242526272829303132333435# Example-24.py#!/usr/bin/env python3import sysimport libvirtdef report_libvirt_error(): &quot;&quot;&quot;调用 virGetLastError 函数获取最后一次错误信息&quot;&quot;&quot; err = libvirt.virGetLastError() print(&#x27;Error code: &#x27; + str(err[0]), file=sys.stderr) print(&#x27;Error domain: &#x27; + str(err[1]), file=sys.stderr) print(&#x27;Error message: &#x27; + err[2], file=sys.stderr) print(&#x27;Error level: &#x27; + str(err[3]), file=sys.stderr) if err[4] != None: print(&#x27;Error string1: &#x27; + err[4], file=sys.stderr) else: print(&#x27;Error string1:&#x27;, file=sys.stderr) if err[5] != None: print(&#x27;Error string2: &#x27; + err[5], file=sys.stderr) else: print(&#x27;Error string2:&#x27;, file=sys.stderr) if err[6] != None: print(&#x27;Error string3: &#x27; + err[6], file=sys.stderr) else: print(&#x27;Error string3:&#x27;, file=sys.stderr) print(&#x27;Error int1: &#x27; + str(err[7]), file=sys.stderr) print(&#x27;Error int2: &#x27; + str(err[8]), file=sys.stderr) exit(1)try: conn = libvirt.open(&#x27;qemu:///system&#x27;) # 使参数无效以强制出错except: report_libvirt_error()conn.close()exit(0) 子类化 libvirtError可以对 libvirtError 类进行子类化以添加功能。默认 libvirtError 不提供任何保存错误信息或向用户显示信息的功能。通过子类化 libvirtError，程序员可以灵活地添加任何所需的功能。下面是一个例子： 示例 8.3：子类化 libvirtError12345678910111213141516171819202122232425262728293031323334353637# Example-25.py#!/usr/bin/env python3import sysimport libvirtclass report_libvirt_error(libvirt.libvirtError): &quot;&quot;&quot;子类 virError 可获取最后一条错误信息。&quot;&quot;&quot; def __init__(self, defmsg, conn=None, dom=None, net=None, pool=None, vol=None): libvirt.libvirtError.__init__(self, defmsg, conn=None, dom=None, net=None, pool=None, vol=None) print(&#x27;Default msg: &#x27; + str(defmsg), file=sys.stderr) print(&#x27;Error code: &#x27; + str(self.get_error_code()), file=sys.stderr) print(&#x27;Error domain: &#x27; + str(self.get_error_domain()), file=sys.stderr) print(&#x27;Error message: &#x27; + self.get_error_message(), file=sys.stderr) print(&#x27;Error level: &#x27; + str(self.get_error_level()), file=sys.stderr) if self.err[4] != None: print(&#x27;Error string1: &#x27; + self.get_str1(), file=sys.stderr) else: print(&#x27;Error string1:&#x27;, file=sys.stderr) if self.err[5] != None: print(&#x27;Error string2: &#x27; + self.get_str2(), file=sys.stderr) else: print(&#x27;Error string2:&#x27;, file=sys.stderr) if self.err[6] != None: print(&#x27;Error string3: &#x27; + self.get_str3(), file=sys.stderr) else: print(&#x27;Error string3:&#x27;, file=sys.stderr) print(&#x27;Error int1: &#x27; + str(self.get_int1()), file=sys.stderr) print(&#x27;Error int2: &#x27; + str(self.get_int2()), file=sys.stderr) exit(1)try: conn = libvirt.open(&#x27;qemu:///system&#x27;) # 使参数无效以强制出错except libvirt.libvirtError: raise report_libvirt_error(&#x27;连接错误&#x27;)conn.close()exit(0) 注册错误处理程序函数Libvirt 还支持设置错误处理程序 Python 函数。可以使用 libvirt 函数 registerErrorHandler。成功时返回 1。 已注册的函数会以 f(ctx, error) 的形式被调用，其中 error 是一个有关错误信息的列表。 示例 8.4：子类化 libvirtError123456789101112131415161718192021222324252627282930313233# Example-26.py#!/usr/bin/env python3import sysimport libvirtdef libvirt_error_handler(ctx, err): print(&#x27;Error code: &#x27; + str(err[0]), file=sys.stderr) print(&#x27;Error domain: &#x27; + str(err[1]), file=sys.stderr) print(&#x27;Error message: &#x27; + err[2], file=sys.stderr) print(&#x27;Error level: &#x27; + str(err[3]), file=sys.stderr) if err[4] != None: print(&#x27;Error string1: &#x27; + err[4], file=sys.stderr) else: print(&#x27;Error string1:&#x27;, file=sys.stderr) if err[5] != None: print(&#x27;Error string2: &#x27; + err[5], file=sys.stderr) else: print(&#x27;Error string2:&#x27;, file=sys.stderr) if err[6] != None: print(&#x27;Error string3: &#x27; + err[6], file=sys.stderr) else: print(&#x27;Error string3:&#x27;, file=sys.stderr) print(&#x27;Error int1: &#x27; + str(err[7]), file=sys.stderr) print(&#x27;Error int2: &#x27; + str(err[8]), file=sys.stderr) exit(1)ctx = &#x27;只是一些信息&#x27;libvirt.registerErrorHandler(libvirt_error_handler, ctx)conn = libvirt.open(&#x27;qemu:///system&#x27;)# 使参数无效以强制出错conn.close()exit(0)","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"7 libvirt网络接口","slug":"libvirt文档/7-libvirt网络接口","date":"2024-03-07T07:50:26.000Z","updated":"2025-03-15T08:00:36.794Z","comments":true,"path":"libvirt文档/7-libvirt网络接口/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/7-libvirt%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3/","excerpt":"","text":"网络接口本节介绍使用libvirt virInterface管理物理网络接口类 7.1 概述可以使用 virInterface 类中的方法检查和修改物理主机上的网络接口配置。这对于设置主机以便在多个希望直接连接到网络的客户域之间共享一个物理接口（简而言之——将一个物理接口奴役到网桥，然后为每个希望共享接口的虚拟机创建一个分路设备），以及对于一般的主机网络接口管理都很有用。除物理硬件外，这些方法还可用于配置网桥、绑定接口和 VLAN 接口。 virInterface 类不用于配置虚拟网络（用于将客户域的接口隐藏在 NAT 后面）；而是使用第 6 章（虚拟网络）中描述的 virNetwork 类来配置虚拟网络。 每个主机接口都由 virInterface 类的一个实例表示，每个实例都有一个唯一标识符： name 方法返回主机上所有接口（活动或非活动）中唯一的字符串。该字符串与操作系统用于标识接口的字符串相同（例如：eth0 或 br1）。 MACString 方法返回该接口 MAC 地址的 ASCII 字符串。由于多个接口可以共享相同的 MAC 地址（例如，在 VLAN 的情况下），因此这不是一个唯一的标识符。不过，它仍可用于搜索接口。 使用 libvirt 配置的所有接口都应视为持久接口，因为 libvirt 实际上是在更改主机自身的持久配置数据（通常包含在 /etc 下的某个文件中），而不是接口本身。 定义新接口（使用 interfaceDefineXML 方法）或更改现有接口的配置（同样使用 interfaceDefineXML 方法）时，该配置将存储在主机上。在手动重启接口或重启主机之前，接口本身的实时配置不会改变。 7.2 XML 接口描述格式interfaceDefineXML 和 XMLDesc 生成并接受的 XML 的当前 Relax NG 定义可在 libvirt 存储库的 docs&#x2F;schema&#x2F;interface.rng 文件中找到，该文件可在 https://gitlab.com/libvirt/libvirt/ 上获取。下面是一些常见接口配置的示例。 示例 7.1：使用 DHCP 的以太网接口的 XML 定义1234567&lt;interface type=&#x27;ethernet&#x27; name=&#x27;eth0&#x27;&gt; &lt;start mode=&#x27;onboot&#x27;/&gt; &lt;mac address=&#x27;aa:bb:cc:dd:ee:ff&#x27;/&gt; &lt;protocol family=&#x27;ipv4&#x27;&gt; &lt;dhcp/&gt; &lt;/protocol&gt;&lt;/interface&gt; 示例 7.2：使用静态 IP 的以太网接口的 XML 定义12345678&lt;interface type=&#x27;ethernet&#x27; name=&#x27;eth0&#x27;&gt; &lt;start mode=&#x27;onboot&#x27;/&gt; &lt;mac address=&#x27;aa:bb:cc:dd:ee:ff&#x27;/&gt; &lt;protocol family=&#x27;ipv4&#x27;&gt; &lt;ip address=&quot;192.168.0.5&quot; prefix=&quot;24&quot;/&gt; &lt;route gateway=&quot;192.168.0.1&quot;/&gt; &lt;/protocol&gt;&lt;/interface&gt; 示例 7.3：连接有 eth0 和 eth1 的网桥设备的 XML 定义12345678910111213&lt;interface type=&quot;bridge&quot; name=&quot;br0&quot;&gt; &lt;start mode=&quot;onboot&quot;/&gt; &lt;mtu size=&quot;1500&quot;/&gt; &lt;protocol family=&quot;ipv4&quot;&gt; &lt;dhcp/&gt; &lt;/protocol&gt; &lt;bridge stp=&quot;off&quot; delay=&quot;0.01&quot;&gt; &lt;interface type=&quot;ethernet&quot; name=&quot;eth0&quot;&gt; &lt;mac address=&quot;ab:bb:cc:dd:ee:ff&quot;/&gt; &lt;/interface&gt; &lt;interface type=&quot;ethernet&quot; name=&quot;eth1&quot;/&gt; &lt;/bridge&gt;&lt;/interface&gt; 示例 7.4：与 eth0 相关联的 VLAN 接口的 XML 定义123456789&lt;interface type=&quot;vlan&quot; name=&quot;eth0.42&quot;&gt; &lt;start mode=&quot;onboot&quot;/&gt; &lt;protocol family=&quot;ipv4&quot;&gt; &lt;dhcp peerdns=&quot;no&quot;/&gt; &lt;/protocol&gt; &lt;vlan tag=&quot;42&quot;&gt; &lt;interface name=&quot;eth0&quot;/&gt; &lt;/vlan&gt;&lt;/interface&gt; 7.3 检索接口信息枚举接口与主机建立连接后，可以使用 numOfInterfaces 和 numOfDefinedInterfaces 方法确定主机上的接口数量。使用 listInterfaces 方法和 listDefinedInterfaces 方法（”已定义” 接口是指那些已经定义但当前处于非活动状态的接口）可以获得这些接口名称的列表。list 方法返回一个 Python 列表。如果遇到错误，所有四个函数都返回 None。 示例 7.5：获取主机上活动（”up”）接口列表12345678910111213141516171819# Example-5.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)ifaceNames = conn.listInterfaces()print(&quot;Active host interfaces:&quot;)for ifaceName in ifaceNames: print(&#x27; &#x27; + ifaceName)conn.close()exit(0) 示例 7.6：获取主机上不活动（”停机”）接口的列表12345678910111213141516171819# Example-6.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)ifaceNames = conn.listDefinedInterfaces()print(&quot;Inactive host interfaces:&quot;)for ifaceName in ifaceNames: print(&#x27; &#x27; + ifaceName)conn.close()exit(0) 为接口获取 virInterface 实例许多操作要求您有 virInterface 的实例，但您可能只有接口的名称或 MAC 地址。在这种情况下，您可以使用 interfaceLookupByName 和 interfaceLookupByMACString 获取 virInterface 实例。 示例 7.7：获取给定接口名称的 virInterface 实例12345678910111213141516# Example-7.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.interfaceLookupByName(&#x27;eth0&#x27;)print(&quot;The interface name is: &quot; + iface.name())conn.close()exit(0) 示例 7.8：获取给定接口 MAC 地址的 virInterface 实例12345678910111213141516# Example-8.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.interfaceLookupByMACString(&#x27;00:01:02:03:04:05&#x27;)print(&quot;The interface name is: &quot; + iface.name())conn.close()exit(0) 检索详细的接口信息您也可能会发现自己有一个 virInterface 实例，需要该接口的名称或 MAC 地址，或想检查完整的接口配置。name、MACString 和 XMLDesc 方法提供了这种功能。 示例 7.9：从接口对象获取名称和 MAC 地址1234567891011121314151617# Example-9.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.interfaceLookupByName(&#x27;eth0&#x27;)print(&quot;The interface name is: &quot; + iface.name())print(&quot;The interface mac string is: &quot; + iface.MACString())conn.close()exit(0) 示例 7.10：从接口对象获取 XML 配置字符串12345678910111213141516# Example-10.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.interfaceLookupByName(&#x27;eth0&#x27;)print(&quot;The interface XML description is:\\n &quot; + iface.XMLDesc(0))conn.close()exit(0) 检索接口网络地址您可能会发现自己有一个 virDomain 实例，并需要一个或多个访客域接口的 IP 地址。interfaceAddresses 方法提供了这种功能。 示例 7.11：获取所有客户域网络接口的 IP 地址123456789101112131415161718192021222324252627282930# Example-14.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)domainName = &quot;CentOS7&quot;dom = conn.lookupByName(domainName)if dom == None: print(&#x27;Failed to get the domain object&#x27;, file=sys.stderr) exit(1)ifaces = dom.interfaceAddresses(libvirt.VIR_DOMAIN_INTERFACE_ADDRESSES_SRC_AGENT, 0)print(&quot;The interface IP addresses:&quot;)for (name, val) in ifaces.iteritems(): if val[&#x27;addrs&#x27;]: for ipaddr in val[&#x27;addrs&#x27;]: if ipaddr[&#x27;type&#x27;] == libvirt.VIR_IP_ADDR_TYPE_IPV4: print(ipaddr[&#x27;addr&#x27;] + &quot; VIR_IP_ADDR_TYPE_IPV4&quot;) elif ipaddr[&#x27;type&#x27;] == libvirt.VIR_IP_ADDR_TYPE_IPV6: print(ipaddr[&#x27;addr&#x27;] + &quot; VIR_IP_ADDR_TYPE_IPV6&quot;)conn.close()exit(0) 7.4 管理接口配置文件在 libvirt 中，”定义” 接口意味着创建或更改配置，而 “取消定义” 意味着从系统中删除该配置。新用户有时可能会将这两个操作与创建&#x2F;删除（实际用于激活和停用现有接口，请参见*第 7.5 节 “Interface 生命周期管理”*）混淆。 定义接口配置interfaceDefineXML 方法用于添加新的接口配置和修改现有配置。它可以添加一个新接口（包括接口名称在内的所有信息都在 XML 数据中给出），也可以修改现有接口的配置。新定义的接口将处于非活动状态，直到采取单独的措施使新配置生效（例如，重启主机或调用创建（参见第 7.5 节 “接口生命周期管理”））。 如果在主机配置中成功添加&#x2F;修改了接口，interfaceDefineXML 会返回一个 virInterface 实例。该实例可用作对新界面执行进一步操作的句柄，例如通过创建使其激活。 目前，flags 参数应始终为 0。 示例 7.12：定义新接口1234567891011121314151617181920212223242526272829303132333435363738# Example-11.py#!/usr/bin/env python3import sysimport libvirtxml = &quot;&quot;&quot;&lt;interface type=&#x27;ethernet&#x27; name=&#x27;eth0&#x27;&gt; &lt;start mode=&#x27;onboot&#x27;/&gt; &lt;mac address=&#x27;aa:bb:cc:dd:ee:ff&#x27;/&gt; &lt;protocol family=&#x27;ipv4&#x27;&gt; &lt;ip address=&quot;192.168.0.5&quot; prefix=&quot;24&quot;/&gt; &lt;route gateway=&quot;192.168.0.1&quot;/&gt; &lt;/protocol&gt;&lt;/interface&gt;&quot;&quot;&quot;conn = Nonetry: conn = libvirt.open(&#x27;qemu:///system&#x27;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)# 创建/修改网络接口iface = conn.interfaceDefineXML(xml, 0)# 激活接口try: iface.create(0)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) iface.undefine() conn.close() exit(1)print(&quot;The interface name is: &quot; + iface.name())iface.destroy()iface.undefine()conn.close()exit(0) 取消定义接口配置undefine 方法会从主机配置文件中彻底、永久删除给定接口的配置。如果将来要重新创建该配置，应调用 XMLDesc 方法，并在 undefine 之前保存字符串。 示例 7.13：保存 XML 数据后取消定义 br0 接口1234567891011121314151617181920# Example-12.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.interfaceLookupByName(&#x27;br0&#x27;)# 在取消定义接口之前获取 XMLxml = iface.XMLDesc(0)# 现在取消定义接口iface.undefine()# 接口现在未定义，iface 变量不再可用conn.close()exit(0) 更改回滚该方法回滚自上次调用 changeCommit 方法被调用。 注意：如果没有待定的接口定义，则会出错。 示例 7.14：使用 changeRollback123456789101112131415# Example-30.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.changeRollback()conn.close()exit(0) 更改开始该方法会创建一个还原点，以后可以通过调用 changeRollback 方法返回该点。该函数应在使用接口配置的任何事务之前调用。一旦知道新配置有效，就可以通过 changeCommit 方法提交新配置，从而释放还原点。 如果在事务已打开的情况下调用 changeBegin 方法，则会出现错误。 示例 7.15：使用 changeBegin123456789101112131415# Example-31.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.changeBegin()conn.close()exit(0) 更改提交这将提交对界面所做的更改，并释放由 changeBegin 方法。 示例 7.16：使用 changeCommit123456789101112131415# Example-32.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.changeCommit()conn.close()exit(0) changeRollback 方法可用于回滚未提交的变更。 7.5 接口生命周期管理在 libvirt 术语中，”创建” 接口意味着使其处于活动状态或 “启动”，而 “删除” 接口意味着使其处于非活动状态或 “关闭”。在使用 netcf 后端进行接口配置的主机（如 Fedora 和 Red Hat Enterprise Linux）上，这与为接口调用系统 shell 脚本 ifup 和 ifdown 相同。 激活接口创建方法会将给定的非活动接口激活（”up”）。如果在使接口激活时出现任何问题，则返回 -1。*例 7.12 “定义新接口 “*展示了该方法的典型用法。 停用接口destroy 方法会使给定的接口处于非活动状态（”停机”）。如果在使接口处于激活状态时出现任何问题，则返回 -1。 示例 7.17：暂时关闭 eth2，然后将其恢复正常1234567891011121314151617181920# Example-12.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)iface = conn.interfaceLookupByName(&#x27;br0&#x27;)# 在取消定义接口之前获取 XMLxml = iface.XMLDesc(0)# 现在取消定义接口iface.undefine()# 接口现在未定义，iface 变量不再可用conn.close()exit(0)","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"6 libvirt虚拟网络","slug":"libvirt文档/6-libvirt虚拟网络","date":"2024-03-06T07:50:26.000Z","updated":"2025-03-15T08:00:32.848Z","comments":true,"path":"libvirt文档/6-libvirt虚拟网络/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/6-libvirt%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"libvirt虚拟网络6.1 概述虚拟网络提供了一种连接单个主机内一个或多个访客域的网络设备的方法。虚拟网络可以： 与宿主保持隔离。 允许通过主机操作系统的活动网络接口路由节点外流量。这包括对 IPv4 流量应用 NAT 的选项。 虚拟网络由 virNetwork 对象表示，有两个唯一标识符： name：短字符串，在一台主机上的所有虚拟网络（包括运行中和未激活的）中都是唯一的。为最大限度地提高虚拟机管理程序之间的可移植性，应用程序只能使用字符 a-Z、0-9、-、_。 UUID：16 个无符号字节，保证在任何主机上的所有虚拟网络中都是唯一的。RFC 4122 定义了 UUID 的格式，并提供了生成具有唯一性的 UUID 的推荐算法。 虚拟网络可以是瞬时的，也可以是持久的。瞬态虚拟网络在创建时启动，只能在主机上运行时对其进行管理。脱机后，所有痕迹都会消失。持久虚拟网络的配置以实施定义的格式保存在主机上的数据存储中。因此，当持久网络离线时，仍可管理其非活动配置。瞬态网络可以通过为其定义配置，即时转变为持久网络。 安装 libvirt 后，每台主机都将获得一个名为 “default” 的虚拟网络实例，为访客提供 DHCP 服务，并允许通过 NAT 连接主机接口的 IP。这项服务对网络连接时断时续的主机最有用。例如，使用无线网络的笔记本电脑。该网络被配置为自动启动。 6.2 监听网络使用 networkLookupByName、networkLookupByUUID、networkLookupByUUIDString 和 listNetworks 方法可以发现虚拟网络。下面的示例展示了如何使用这些方法。 示例 6.1：发现和查找虚拟网络1234567891011121314151617181920212223242526272829303132333435363738# Example-1.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)# 发现所有虚拟网络networks = conn.listNetworks()print(&#x27;Virtual networks:&#x27;)for network in networks: print(&#x27; &#x27; + network)print()# 按名称查找默认网络network = conn.networkLookupByName(&#x27;default&#x27;)print(&#x27;Virtual network default:&#x27;)print(&#x27; name: &#x27; + network.name())uuid = network.UUIDString()print(&#x27; UUID: &#x27; + uuid)print(&#x27; bridge: &#x27; + network.bridgeName())print()# 按 UUID 查找默认网络network = conn.networkLookupByUUIDString(uuid)print(&#x27;Virtual network default:&#x27;)print(&#x27; name: &#x27; + network.name())print(&#x27; UUID: &#x27; + network.UUIDString())print(&#x27; bridge: &#x27; + network.bridgeName())conn.close()exit(0) 6.3 生命周期控制下面的示例展示了如何使用 networkDefineXML、networkCreateXML、destroy 和 undefine 方法。 示例 6.2：创建和销毁虚拟网络1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# Example-2.py#!/usr/bin/env python3import sysimport libvirtxml = &quot;&quot;&quot;&lt;network&gt; &lt;name&gt;mynetwork&lt;/name&gt; &lt;bridge name=&quot;virbr1&quot;/&gt; &lt;forward mode=&quot;nat&quot;/&gt; &lt;ip address=&quot;192.168.142.1&quot; netmask=&quot;255.255.255.0&quot;&gt; &lt;dhcp&gt; &lt;range start=&quot;192.168.142.2&quot; end=&quot;192.168.142.254&quot;/&gt; &lt;/dhcp&gt; &lt;/ip&gt;&lt;/network&gt;&quot;&quot;&quot;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)# 创建一个持久的虚拟网络network = conn.networkDefineXML(xml)if network == None: print(&#x27;Failed to create a virtual network&#x27;, file=sys.stderr) exit(1)active = network.isActive()if active == 1: print(&#x27;The new persistent virtual network is active&#x27;)else: print(&#x27;The new persistent virtual network is not active&#x27;)network.create() # 设置网络激活active = network.isActive()if active == 1: print(&#x27;The new transient virtual network is active&#x27;)else: print(&#x27;The new transient virtual network is not active&#x27;)# 现在销毁持久虚拟网络network.destroy()# 删除持久虚拟网络network.undefine()print()# 创建临时虚拟网络network = conn.networkCreateXML(xml)if network == None: print(&#x27;Failed to define a virtual network&#x27;, file=sys.stderr) exit(1)active = network.isActive()if active == 1: print(&#x27;The new transient virtual network is active&#x27;)else: print(&#x27;The new transient virtual network is not active&#x27;)# 现在销毁暂存虚拟网络network.destroy()conn.close()exit(0) 6.4 网络配置下面的示例展示了如何使用 XMLDesc、autostart、isActive、isPersistent 和 setAutostart 方法。 示例 6.3：配置虚拟网络12345678910111213141516171819202122232425262728293031323334353637# Example-3.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)# 按名称查找默认网络network = conn.networkLookupByName(&#x27;default&#x27;)print(&#x27;Virtual network default:&#x27;)print(&#x27; name: &#x27; + network.name())print(&#x27; UUID: &#x27; + network.UUIDString())print(&#x27; bridge: &#x27; + network.bridgeName())print(&#x27; autostart: &#x27; + str(network.autostart()))print(&#x27; is active: &#x27; + str(network.isActive()))print(&#x27; is persistent: &#x27; + str(network.isPersistent()))print()print(&#x27;Unsetting autostart&#x27;)network.setAutostart(0)print(&#x27; autostart: &#x27; + str(network.autostart()))print(&#x27;Setting autostart&#x27;)network.setAutostart(1)print(&#x27; autostart: &#x27; + str(network.autostart()))print()xml = network.XMLDesc(0)print(&#x27;XML description:&#x27;)print(xml)conn.close()exit(0)","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"5 libvirt存储管理","slug":"libvirt文档/5-libvirt存储管理","date":"2024-03-05T07:50:26.000Z","updated":"2025-03-15T08:00:30.417Z","comments":true,"path":"libvirt文档/5-libvirt存储管理/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/5-libvirt%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/","excerpt":"","text":"Libvirt 存储管理Libvirt 通过存储池和存储卷在物理主机上提供存储管理。存储池是由管理员（通常是专门的存储管理员）为虚拟机预留的一定量的存储空间。存储池被存储管理员或系统管理员划分为存储卷，这些存储卷作为块设备分配给虚拟机。 存储池和存储卷的示例NFS 示例负责 NFS 服务器的存储管理员创建了一个共享以存储虚拟机的数据。系统管理员在虚拟化主机上定义一个包含共享详细信息的池（例如，nfs.example.com:/path/to/share 应挂载在 /vm_data）。当池启动时，libvirt 将共享挂载到指定目录，就像系统管理员登录并执行 mount nfs.example.com:/path/to/share /vmdata 一样。如果池配置为自动启动，当 libvirt 启动时，libvirt 会确保 NFS 共享挂载到指定目录。一旦池启动，NFS 共享中的文件将被报告为卷，可以使用 libvirt API 查询存储卷的路径。这些卷的路径可以复制到虚拟机的 XML 定义部分，用于描述虚拟机块设备的源存储。对于 NFS，使用 libvirt 方法的应用程序可以在池中创建和删除卷（NFS 共享中的文件），直到达到池的大小限制（共享的存储容量）。并非所有池类型都支持创建和删除卷。停止池（virsh 和 API 中不幸地称为“池销毁”）会撤销启动操作，在这种情况下会卸载 NFS 共享。尽管名称如此，但销毁操作不会修改共享上的数据。更多详细信息请参阅 man virsh。 iSCSI 示例存储管理员配置一个 iSCSI 目标以向运行虚拟机的主机提供一组 LUN。当 libvirt 配置为管理该 iSCSI 目标作为池时，libvirt 将确保主机登录到 iSCSI 目标，然后 libvirt 可以报告可用的 LUN 作为存储卷。可以查询这些卷的路径并在虚拟机的 XML 定义中使用，如 NFS 示例中那样。在这种情况下，LUN 在 iSCSI 服务器上定义，libvirt 无法创建和删除卷。 存储池和存储卷的必要性存储池和存储卷不是虚拟机正常运行所必需的。池和卷为 libvirt 提供了一种确保特定存储可供虚拟机使用的方法，但一些管理员可能更愿意自己管理存储，虚拟机在没有定义任何池或卷的情况下也能正常运行。在不使用池的系统上，系统管理员必须使用他们喜欢的工具确保虚拟机存储的可用性，例如，将 NFS 共享添加到主机的 fstab 中，以便在启动时挂载共享。 Libvirt 的远程协议功能如果此时池和卷相对于传统系统管理工具的价值尚不清楚，请注意 libvirt 的一个功能是其远程协议，因此可以管理虚拟机生命周期的所有方面以及虚拟机所需资源的配置。这些操作可以在远程主机上完全通过 Python libvirt 模块执行。换句话说，使用 libvirt 的管理应用程序可以让用户执行配置虚拟机主机所需的所有任务：分配资源、运行虚拟机、关闭虚拟机和释放资源，而无需 shell 访问或任何其他控制通道。 Libvirt 支持的存储池类型Libvirt 支持以下存储池类型： 目录后台 本地文件系统后台 网络文件系统后台 逻辑后台 磁盘后台 iSCSI 后端 SCSI 后端 多路径后端 RBD（RADOS 块设备）后台 Sheepdog 后台 Gluster 后端 ZFS 后端 存储池 XML 格式在 https://libvirt.org/formatstorage.html#StoragePool 中指定。 5.1 概述存储池是存储卷的容器。一个系统可以根据需要拥有任意多个存储池，每个存储池可以包含任意多个存储卷。 5.2 列出存储池存储池对象的列表可以使用 virConnect 类。 flags 参数可以是以下一个或多个常量： VIR_CONNECT_LIST_STORAGE_POOLS_INACTIVE VIR_CONNECT_LIST_STORAGE_POOLS_ACTIVE VIR_CONNECT_LIST_STORAGE_POOLS_PERSISTENT VIR_CONNECT_LIST_STORAGE_POOLS_TRANSIENT VIR_CONNECT_LIST_STORAGE_POOLS_AUTOSTART VIR_CONNECT_LIST_STORAGE_POOLS_NO_AUTOSTART VIR_CONNECT_LIST_STORAGE_POOLS_DIR VIR_CONNECT_LIST_STORAGE_POOLS_FS VIR_CONNECT_LIST_STORAGE_POOLS_NETFS VIR_CONNECT_LIST_STORAGE_POOLS_LOGICAL VIR_CONNECT_LIST_STORAGE_POOLS_DISK VIR_CONNECT_LIST_STORAGE_POOLS_ISCSI VIR_CONNECT_LIST_STORAGE_POOLS_SCSI VIR_CONNECT_LIST_STORAGE_POOLS_MPATH VIR_CONNECT_LIST_STORAGE_POOLS_RBD VIR_CONNECT_LIST_STORAGE_POOLS_SHEEPDOG VIR_CONNECT_LIST_STORAGE_POOLS_GLUSTER VIR_CONNECT_LIST_STORAGE_POOLS_ZFS 示例 5.1：获取存储池列表12345678910111213141516171819202122# Example-1.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)pools = conn.listAllStoragePools(0)if pools == None: print(&#x27;Failed to locate any StoragePool objects.&#x27;, file=sys.stderr) exit(1)for pool in pools: print(&#x27;Pool: &#x27; + pool.name())conn.close()exit(0) 5.3 存储池的使用virStoragePool 类中有许多可用的方法。下面的示例程序介绍了其中一些描述池某些属性的方法。 示例 5.2：展示一些存储池方法的用法12345678910111213141516171819202122232425262728293031# Example-2.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)pool = conn.storagePoolLookupByName(&#x27;default&#x27;)if pool == None: print(&#x27;Failed to locate any StoragePool objects.&#x27;, file=sys.stderr) exit(1)info = pool.info()print(&#x27;Pool: &#x27; + pool.name())print(&#x27; UUID: &#x27; + pool.UUIDString())print(&#x27; Autostart: &#x27; + str(pool.autostart()))print(&#x27; Is active: &#x27; + str(pool.isActive()))print(&#x27; Is persistent: &#x27; + str(pool.isPersistent()))print(&#x27; Num volumes: &#x27; + str(pool.numOfVolumes()))print(&#x27; Pool state: &#x27; + str(info[0]))print(&#x27; Capacity: &#x27; + str(info[1]))print(&#x27; Allocation: &#x27; + str(info[2]))print(&#x27; Available: &#x27; + str(info[3]))conn.close()exit(0) 示例 5.3：获取存储池的 XML 描述12345678910111213141516171819202122# Example-3.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)pool = conn.storagePoolLookupByName(&#x27;default&#x27;)if pool == None: print(&#x27;Failed to locate any StoragePool objects.&#x27;, file=sys.stderr) exit(1)xml = pool.XMLDesc(0)print(xml)conn.close()exit(0) 5.4 生命周期控制示例 5.4：创建和销毁存储池123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Example-4.py#!/usr/bin/env python3import sysimport libvirtxmlDesc = &quot;&quot;&quot;&lt;pool type=&#x27;dir&#x27;&gt; &lt;name&gt;my-pool&lt;/name&gt; &lt;uuid&gt;8c79f996-cb2a-d24d-9822-ac7547ab2d01&lt;/uuid&gt; &lt;capacity unit=&#x27;bytes&#x27;&gt;4306780815&lt;/capacity&gt; &lt;allocation unit=&#x27;bytes&#x27;&gt;237457858&lt;/allocation&gt; &lt;available unit=&#x27;bytes&#x27;&gt;4069322956&lt;/available&gt; &lt;source&gt; &lt;/source&gt; &lt;target&gt; &lt;path&gt;/home/dashley/images&lt;/path&gt; &lt;permissions&gt; &lt;mode&gt;0755&lt;/mode&gt; &lt;owner&gt;-1&lt;/owner&gt; &lt;group&gt;-1&lt;/group&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/pool&gt;&quot;&quot;&quot;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)# 创建一个新的持久性存储池pool = conn.storagePoolDefineXML(xmlDesc, 0)if pool == None: print(&#x27;Failed to create StoragePool object.&#x27;, file=sys.stderr) exit(1)# 销毁存储池pool.undefine()# 创建一个新的非持久性存储池pool = conn.storagePoolCreateXML(xmlDesc, 0)if pool == None: print(&#x27;Failed to create StoragePool object.&#x27;, file=sys.stderr) exit(1)# 销毁存储池pool.undefine()conn.close()exit(0) 5.5 发现存储池源示例 5.5：发现存储池的源123456789101112131415161718192021222324252627282930313233343536373839404142434445# Example-5.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidompoolName = &#x27;default&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)sp = conn.storagePoolLookupByName(poolName)if sp == None: print(&#x27;Failed to find storage pool &#x27; + poolName, file=sys.stderr) exit(1)raw_xml = sp.XMLDesc(0)xml = minidom.parseString(raw_xml)name = xml.getElementsByTagName(&#x27;name&#x27;)print(&#x27;pool name: &#x27; + poolName)spType = xml.getElementsByTagName(&#x27;source&#x27;)for spType in spType: attr = spType.getAttribute(&#x27;name&#x27;) if attr != None: print(&#x27; name = &#x27; + attr) attr = spType.getAttribute(&#x27;path&#x27;) if attr != None: print(&#x27; path = &#x27; + attr) attr = spType.getAttribute(&#x27;dir&#x27;) if attr != None: print(&#x27; dir = &#x27; + attr) attr = spType.getAttribute(&#x27;type&#x27;) if attr != None: print(&#x27; type = &#x27; + attr) attr = spType.getAttribute(&#x27;username&#x27;) if attr != None: print(&#x27; username = &#x27; + attr)conn.close()exit(0) 示例 5.6：演示 setAutostart 方法12345678910111213141516171819202122232425262728# Example-6.py#!/usr/bin/env python3import sysimport libvirtpoolName = &#x27;default&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)sp = conn.storagePoolLookupByName(poolName)if sp == None: print(&#x27;Failed to find storage pool &#x27; + poolName, file=sys.stderr) exit(1)print(&#x27;Current autostart setting: &#x27; + str(sp.autostart()))if sp.autostart() == True: sp.setAutostart(0)else: sp.setAutostart(1)print(&#x27;Current autostart setting: &#x27; + str(sp.autostart()))conn.close()exit(0) 5.7 存储卷概述存储卷是满足客户域存储需求的基本存储单元。存储卷封装了用于容纳客户域的所有必要分区。存储卷又包含在存储池中。底层磁盘分区能容纳多少存储池，存储池就能容纳多少存储池。 存储卷 XML 格式在 https://libvirt.org/formatstorage.html#StorageVol 中指定。 5.8 列出存储卷示例 5.7：演示列出存储卷1234567891011121314151617181920212223242526# Example-7.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidompoolName = &quot;default&quot;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)sp = conn.storagePoolLookupByName(poolName)if sp == None: print(&#x27;Failed to find storage pool &#x27; + poolName, file=sys.stderr) exit(1)stgvols = sp.listVolumes()print(&#x27;Storage pool: &#x27; + poolName)for stgvol in stgvols: print(&#x27; Storage volume: &#x27; + stgvol)conn.close()exit(0) 5.9 存储卷信息示例 5.8：列出存储卷信息1234567891011121314151617181920212223242526272829# Example-8.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)pool = conn.storagePoolLookupByName(&#x27;default&#x27;)if pool == None: print(&#x27;Failed to locate any StoragePool objects.&#x27;, file=sys.stderr) exit(1)stgvols = pool.listVolumes()print(&#x27;Pool: &#x27; + pool.name())for stgvolname in stgvols: print(&#x27; volume: &#x27; + stgvolname) stgvol = pool.storageVolLookupByName(stgvolname) info = stgvol.info() print(&#x27; Type: &#x27; + str(info[0])) print(&#x27; Capacity: &#x27; + str(info[1])) print(&#x27; Allocation: &#x27; + str(info[2]))conn.close()exit(0) 5.10 创建和删除存储卷示例 5.9：创建存储卷1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Example-9.py#!/usr/bin/env python3import sysimport libvirtstgvol_xml = &quot;&quot;&quot;&lt;volume&gt; &lt;name&gt;sparse.img&lt;/name&gt; &lt;allocation&gt;0&lt;/allocation&gt; &lt;capacity unit=&quot;G&quot;&gt;2&lt;/capacity&gt; &lt;target&gt; &lt;path&gt;/var/lib/virt/images/sparse.img&lt;/path&gt; &lt;permissions&gt; &lt;owner&gt;107&lt;/owner&gt; &lt;group&gt;107&lt;/group&gt; &lt;mode&gt;0744&lt;/mode&gt; &lt;label&gt;virt_image_t&lt;/label&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/volume&gt;&quot;&quot;&quot;pool = &#x27;default&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)pool = conn.storagePoolLookupByName(pool)if pool == None: print(&#x27;Failed to locate any StoragePool objects.&#x27;, file=sys.stderr) exit(1)stgvol = pool.createXML(stgvol_xml, 0)if stgvol == None: print(&#x27;Failed to create a StorageVol object.&#x27;, file=sys.stderr) exit(1)# 删除存储卷# 从底层磁盘介质中物理移除存储卷stgvol.wipe(0)# 从逻辑上将存储卷从存储池中删除stgvol.delete(0)conn.close()exit(0) 5.11 克隆存储卷示例 5.10：克隆现有存储卷1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# Example-10.py#!/usr/bin/env python3import sysimport libvirtstgvol_xml = &quot;&quot;&quot;&lt;volume&gt; &lt;name&gt;sparse.img&lt;/name&gt; &lt;allocation&gt;0&lt;/allocation&gt; &lt;capacity unit=&quot;G&quot;&gt;2&lt;/capacity&gt; &lt;target&gt; &lt;path&gt;/var/lib/virt/images/sparse.img&lt;/path&gt; &lt;permissions&gt; &lt;owner&gt;107&lt;/owner&gt; &lt;group&gt;107&lt;/group&gt; &lt;mode&gt;0744&lt;/mode&gt; &lt;label&gt;virt_image_t&lt;/label&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/volume&gt;&quot;&quot;&quot;stgvol_xml2 = &quot;&quot;&quot;&lt;volume&gt; &lt;name&gt;sparse2.img&lt;/name&gt; &lt;allocation&gt;0&lt;/allocation&gt; &lt;capacity unit=&quot;G&quot;&gt;2&lt;/capacity&gt; &lt;target&gt; &lt;path&gt;/var/lib/virt/images/sparse.img&lt;/path&gt; &lt;permissions&gt; &lt;owner&gt;107&lt;/owner&gt; &lt;group&gt;107&lt;/group&gt; &lt;mode&gt;0744&lt;/mode&gt; &lt;label&gt;virt_image_t&lt;/label&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/volume&gt;&quot;&quot;&quot;pool = &#x27;default&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)pool = conn.storagePoolLookupByName(pool)if pool == None: print(&#x27;Failed to locate any StoragePool objects.&#x27;, file=sys.stderr) exit(1)# 创建一个新的存储卷stgvol = pool.createXML(stgvol_xml, 0)if stgvol == None: print(&#x27;Failed to create a StorageVol object.&#x27;, file=sys.stderr) exit(1)# 现在克隆现有存储卷print(&#x27;This could take some time...&#x27;)stgvol2 = pool.createXMLFrom(stgvol_xml2, stgvol, 0)if stgvol2 == None: print(&#x27;Failed to clone a StorageVol object.&#x27;, file=sys.stderr) exit(1)# 删除克隆的存储卷# 从底层磁盘介质中物理移除存储卷stgvol2.wipe(0)# 从逻辑上将存储卷从存储池中删除stgvol2.delete(0)# 删除存储卷# 从底层磁盘介质中物理移除存储卷stgvol.wipe(0)# 从逻辑上将存储卷从存储池中删除stgvol.delete(0)conn.close()exit(0)","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"4 libvirt客户域Domain","slug":"libvirt文档/4-libvirt客户域Domain","date":"2024-03-04T07:50:26.000Z","updated":"2025-03-15T08:00:27.746Z","comments":true,"path":"libvirt文档/4-libvirt客户域Domain/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/4-libvirt%E5%AE%A2%E6%88%B7%E5%9F%9FDomain/","excerpt":"","text":"4.1 Domain 概述域是运行在虚拟化机器上的操作系统实例。一个客域可以指正在运行的虚拟机或可用于启动虚拟机的配置。连接对象提供了枚举客域、创建新客域和管理现有域的方法。客域由 virDomainPtr 对象表示，并具有多个唯一标识符： 唯一标识符 ID：正整数，在一台主机上运行的客户域中是唯一的。非活动域没有 ID。如果主机操作系统是虚拟域，默认情况下 ID 为 0。例如，在 Xen 虚拟机管理程序中，Dom0 表示访客域。其他域 ID 将从 1 开始分配，每次启动新域时都会递增。通常情况下，在整个 ID 空间被包围之前，域 ID 不会被重复使用。域 ID 空间的大小至少为 16 位，但通常会扩展到 32 位。 name：短字符串，在一台主机上的所有客户域（包括运行中和非活动域）中都是唯一的。为了在不同管理程序之间实现最大的可移植性，应用程序只能在名称中使用 a-Z、0-9、-、_ 字符。许多管理程序会根据域名将非活动域配置作为文件存储在磁盘上。 UUID：16 个无符号字节，保证在任何主机上的所有客户域中都是唯一的。RFC 4122 定义了 UUID 的格式，并提供了生成具有唯一性的 UUID 的推荐算法。如果主机操作系统本身就是一个虚拟域，那么按照惯例，它将获得一个全为 0 的 UUID。Xen 虚拟机管理程序就是这种情况，其中 Dom0 本身就是一个客户域。 客户域可以是短暂的，也可以是持久的。瞬时访客域只能在主机上运行时进行管理，关闭电源后，所有痕迹都会消失。持久客户域的配置由管理程序以实施定义的格式保存在主机上的数据存储中。因此，当持久客户机关闭电源时，仍可管理其非活动配置。通过为临时客户机定义配置，可以将其快速转变为持久客户机。 当应用程序获得域的唯一标识符后，通常会希望获取相应的 virDomain 对象。有三种方法可用于查找现有域，它们的名字很有想象力，分别是 lookupByID、lookupByName 和 lookupByUUID。每个方法都将域标识符作为参数。如果没有匹配的域存在，它们将返回 None。如果需要，可以查询错误对象，以查找错误的具体细节。 例 4.1. 从 ID 获取域对象12345678910111213141516171819# Example-1.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)domainID = 6dom = conn.lookupByID(domainID)if dom == None: print(&#x27;Failed to get the domain object&#x27;, file=sys.stderr)conn.close()exit(0) 需要注意的是，如果域未激活，lookupByID 方法将不起作用。非活动域的 ID 都是 -1。 例 4.2. 从名称中获取域对象12345678910111213141516171819# Example-2.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)domainName = &#x27;someguest&#x27;dom = conn.lookupByName(domainName)if dom == None: print(&#x27;Failed to get the domain object&#x27;, file=sys.stderr)conn.close()exit(0) 例 4.3. 从 UUID 获取域对象12345678910111213141516171819# Example-3.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)domainUUID = &quot;00311636-7767-71d2-e94a-26e7b8bad250&quot;dom = conn.lookupByUUID(domainUUID)if dom == None: print(&#x27;Failed to get the domain object&#x27;, file=sys.stderr)conn.close()exit(0) 上面的 UUID 示例使用的是 UUID 的可打印格式。Python 不支持使用等价的原始字节。 列举 Domainslibvirt 类公开了两个域列表，第一个列表包含运行中的域，第二个列表包含非活动的持久域。这两个列表是互不重叠的排他性集合，但在每个集合的查询过程中，域停止或启动的可能性很小。本节后面描述的事件类提供了一种跟踪所有生命周期变化的方法，可以避免这种潜在的竞赛条件。 该方法用于列出活动域，返回域 ID 列表。每个运行中的域都有一个正整数 ID，在主机上所有运行中的域中唯一标识。列出活动域的方法 listDomainsID 不需要任何参数。如果出错，返回值将是 None，或者是以 ints 表示的 ID 的 Python 列表。 例 4.4. 列出活动域12345678910111213141516171819202122232425# Example-4.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)domainIDs = conn.listDomainsID()if domainIDs == None: print(&#x27;Failed to get a list of domain IDs&#x27;, file=sys.stderr)print(&quot;Active domain IDs:&quot;)if len(domainIDs) == 0: print(&#x27;None&#x27;)else: for domainID in domainIDs: print(&#x27; &#x27; + str(domainID))conn.close()exit(0) 除了运行中的域外，主机上还可能存储有一些持久的非活动域配置。由于非活动域没有任何 ID 标识符，因此非活动域列表将以名称字符串列表的形式显示。如果出错，返回值将是 None，或者是一个由名称（字符串）填充的 Python 元素列表。 例 4.5. 列出非活动域12345678910111213141516171819202122232425262728293031323334353637383940# Example-5.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)domainNames = conn.listDefinedDomains()if domainNames == None: print(&#x27;Failed to get a list of domain names&#x27;, file=sys.stderr)domainIDs = conn.listDomainsID()if domainIDs == None: print(&#x27;Failed to get a list of domain IDs&#x27;, file=sys.stderr)domainNames = &quot;&quot;try: if len(domainIDs) != 0: for domainID in domainIDs: domain = conn.lookupByID(domainID) domainNames.append(domain.name)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) conn.close() exit(1)print(&quot;All (active and inactive domain names:&quot;)if len(domainNames) == 0: print(&#x27;none&#x27;)else: for domainName in domainNames: print(&#x27; &#x27; + domainName)conn.close()exit(0) 列出域的方法不会直接返回 virDomain 对象，因为这可能会给希望频繁查询域列表的应用程序带来不必要的性能损失。不过，Python libvirt 模块提供了 listAllDomains 方法，该方法可列出所有活动或非活动域。该方法会返回一个 virDomain 实例的 Python 列表，如果出错则返回 None。当没有持久域存在时，列表可能为空。 例 4.6. 获取所有域对象1234567891011121314151617181920212223# Example-6.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)print(&quot;All (active and inactive) domain names:&quot;)domains = conn.listAllDomains(0)if len(domains) != 0: for domain in domains: print(&#x27; &#x27; + domain.name())else: print(&quot;None&quot;)conn.close()exit(0) 4.3 获取域名的状态信息获得域实例后，就可以获取有关域状态的信息。这些信息包括托管的操作系统类型、运行状态、ID、UUID 等。以下方法将演示如何获取这些信息。 获取域的 ID使用 ID 方法可以获取域的 ID。只有运行中的域才有 ID，获取非运行中域的 ID 总是返回 -1。 例 4.7. 获取域的 ID1234567891011121314151617181920212223242526272829# Example-43.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)id = dom.ID()if id == -1: print(&#x27;The domain is not running so has no ID.&#x27;)else: print(&#x27;The ID of the domain is &#x27; + str(id))conn.close()exit(0) 获取域的 UUID域的 UUID 可以通过 UUID 或 UUIDString 方法获得。UUID 方法对 Python 程序并不十分有用，因为它是一个二进制值。UUIDString 方法则有用得多，因为它返回的是一个格式化的字符串值，可以很容易地进行解析。 UUID 与域的运行状态无关，并始终返回一个有效的 UUID。 例 4.8. 获取域的 UUID1234567891011121314151617181920212223242526# Example-44.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)uuid = dom.UUIDString()print(&#x27;The UUID of the domain is &#x27; + uuid)conn.close()exit(0) 获取域的操作系统类型域托管的操作系统类型也可用。只有运行中的域才有 ID，获取非运行中域的 ID 总是返回 -1。同样的信息也可以通过 info 方法获取。 例 4.9. 获取域的 ID1234567891011121314151617181920212223242526# Example-45.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)type = dom.OSType()print(&#x27;The OS type of the domain is &quot;&#x27; + type + &#x27;&quot;&#x27;)conn.close()exit(0) 确定域是否有当前快照hasCurrentSnapshot 方法返回一个布尔值，表示当前快照是否可用。该方法始终返回有效值，且不依赖于域的运行状态。 例 4.10. 确定域是否有当前快照1234567891011121314151617181920212223242526# Example-47.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)flag = dom.hasCurrentSnapshot()print(&#x27;The value of the current snapshot flag is &#x27; + str(flag))conn.close()exit(0) 确定域是否已管理保存镜像hasManagedSaveImages 方法返回一个布尔值，表示域是否有托管的保存镜像。请注意，运行中的域不应该有已保存的镜像，因为该镜像应在重启域时删除。 例 4.11. 确定域是否有托管保存镜像1234567891011121314151617181920212223242526# Example-48.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)flag = dom.hasManagedSaveImage()print(&#x27;The value of the managed save images flag is &#x27; + str(flag))conn.close()exit(0) 获取域名的主机名hostname 方法返回域的主机名。 hostname 方法高度依赖于管理程序和&#x2F;或 qemu-guest-agent。如果该方法无法成功完成，可能会出错。 例 4.12. 获取域的主机名1234567891011121314151617181920212223242526# Example-49.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)name = dom.hostname()print(&#x27;The hostname of the domain is &#x27; + str(name))conn.close()exit(0) 获取网域硬件信息info 方法会返回域硬件的一些常规信息。在 Python 列表中应返回八个条目，包括域的状态、最大内存、内存、cpus 和 cpu 时间。 例 4.13. 获取域名信息123456789101112131415161718192021222324252627282930# Example-50.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)state, maxmem, mem, cpus, cput = dom.info()print(&#x27;The state is &#x27; + str(state))print(&#x27;The max memory is &#x27; + str(maxmem))print(&#x27;The memory is &#x27; + str(mem))print(&#x27;The number of cpus is &#x27; + str(cpus))print(&#x27;The cpu time is &#x27; + str(cput))conn.close()exit(0) 确定网域是否正在运行isActive 方法返回一个布尔标志，表示域是否处于活动（运行）状态。 例 4.14. 确定网域是否正在运行1234567891011121314151617181920212223242526272829# Example-51.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)flag = dom.isActive()if flag == True: print(&#x27;The domain is active.&#x27;)else: print(&#x27;The domain is not active.&#x27;)conn.close()exit(0) 确定网域是否具有持久性isPersistent 方法返回一个布尔标志，表示域是否持久（重启后域将持久）。 例 4.15. 确定域是否持久123456789101112131415161718192021222324252627282930313233343536# Example-52.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)flag = Nonetry: flag = dom.isPersistent()except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) conn.close() exit(1)if flag == 1: print(&#x27;The domain is persistent.&#x27;)elif flag == 0: print(&#x27;The domain is not persistent.&#x27;)conn.close()exit(0) 确定网域是否已更新isUpdated 方法返回一个布尔标志，表示域自创建以来是否更新过。 例 4.16. 确定域是否已更新123456789101112131415161718192021222324252627282930313233343536# Example-58.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)flag = Nonetry: flag = dom.isUpdated()except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) conn.close() exit(1)if flag == 1: print(&#x27;The domain is updated.&#x27;)elif flag == 0: print(&#x27;The domain is not updated.&#x27;)conn.close()exit(0) 确定域的最大内存maxMemory 方法返回分配给域的最大内存。 例 4.17. 确定域的最大内存1234567891011121314151617181920212223242526272829# Example-53.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)mem = dom.maxMemory()if mem 0: print(&#x27;The max memory for domain is &#x27; + str(mem) + &#x27;MB&#x27;)else: print(&#x27;There was an error.&#x27;)conn.close()exit(0) 确定网域的最大 VcpusmaxVcpus 方法返回分配给域的虚拟 CPU 的最大数量。 例 4.18. 确定域的最大 Vcpus1234567891011121314151617181920212223242526272829# Example-54.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)cpus = dom.maxVcpus()if cpus != -1: print(&#x27;The max Vcpus for domain is &#x27; + str(cpus))else: print(&#x27;There was an error.&#x27;)conn.close()exit(0) 获取网域名称name 方法返回域名的名称。 例 4.19. 获取网域名称1234567891011121314151617181920212223242526# Example-55.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)name = dom.name()print(&#x27;The name of the domain is &quot;&#x27; + name + &#x27;&quot;.&#x27;)conn.close()exit(0) 获取域的状态状态 方法返回域的状态。 例 4.20. 获取域的状态123456789101112131415161718192021222324252627282930313233343536373839404142434445# Example-56.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Cent0S7&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = Nonetry: dom = conn.lookupByName(domName)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)state, reason = dom.state()if state == libvirt.VIR_DOMAIN_NOSTATE: print(&#x27;The state is VIR_DOMAIN_NOSTATE&#x27;)elif state == libvirt.VIR_DOMAIN_RUNNING: print(&#x27;The state is VIR_DOMAIN_RUNNING&#x27;)elif state == libvirt.VIR_DOMAIN_BLOCKED: print(&#x27;The state is VIR_DOMAIN_BLOCKED&#x27;)elif state == libvirt.VIR_DOMAIN_PAUSED: print(&#x27;The state is VIR_DOMAIN_PAUSED&#x27;)elif state == libvirt.VIR_DOMAIN_SHUTDOWN: print(&#x27;The state is VIR_DOMAIN_SHUTDOWN&#x27;)elif state == libvirt.VIR_DOMAIN_SHUTOFF: print(&#x27;The state is VIR_DOMAIN_SHUTOFF&#x27;)elif state == libvirt.VIR_DOMAIN_CRASHED: print(&#x27;The state is VIR_DOMAIN_CRASHED&#x27;)elif state == libvirt.VIR_DOMAIN_PMSUSPENDED: print(&#x27;The state is VIR_DOMAIN_PMSUSPENDED&#x27;)else: print(&#x27;The state is unknown&#x27;)print(&#x27;The reason code is &#x27; + str(reason))conn.close()exit(0) 从域中提取时间信息getTime 方法从域中提取当前时间戳。该方法返回与 Python time.struct_time 函数相同的值。 例 4.21. 获取域的状态12345# Example-57.py#!/usr/bin/env python3import sys, timeimport libvirtfrom xml.dom import minidom 抵御主机重启抵御主机重启，而不会认为自己已经重启。libvirt 重启后，访客域将自动恢复。由于 libvirt 必须知道路径而无需用户输入，因此这将由独立于常规保存和还原的 API 来处理。 例 4.34. 为域设置自动启动123456789101112131415161718192021# Example-25.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(6)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)dom.setAutostart(1) # 打开自动启动conn.close()exit(0) 网域配置libvirt 中使用 XML 定义域。与域相关的所有内容（如内存和 CPU）都在域 XML 中定义。域 XML 格式指定在 http://libvirt.org/ formatdomain.html。可在本地访问 &#x2F;usr&#x2F;share&#x2F;doc&#x2F;libvirt-devel-version&#x2F; 如果系统已安装 libvirt-devel 软件包。 访客域的配置信息可通过 XMLDesc 方法获取。该方法以 XML 数据流的形式返回域的当前描述。然后可以对该数据流进行解析，以获取有关域和构成域的所有部分的详细信息。 flags 参数可包含以下任意常数： VIR_DOMAIN_XML_SECURE VIR_DOMAIN_XML_INACTIVE VIR_DOMAIN_XML_UPDATE_CPU VIR_DOMAIN_XML_MIGRATABLE 例 4.35. 从域的 XML 描述中获取基本域信息12345678910111213141516171819202122232425262728# Example-36.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)raw_xml = dom.XMLDesc(0)xml = minidom.parseString(raw_xml)domainTypes = xml.getElementsByTagName(&#x27;type&#x27;)for domainType in domainTypes: print(domainType.getAttribute(&#x27;machine&#x27;)) print(domainType.getAttribute(&#x27;arch&#x27;))conn.close()exit(0) Emulator要发现访客域的模拟器，请查找并显示模拟器 XML 标记的内容。 例 4.36. 获取域的模拟器信息1234567891011121314151617181920212223242526# Example-37.py#!/usr/bin/env python3import sysimport libvirtfrom xml.dom import minidomdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)raw_xml = dom.XMLDesc(0)xml = minidom.parseString(raw_xml)domainEmulator = xml.getElementsByTagName(&#x27;emulator&#x27;)print(&#x27;emulator: &#x27; + domainEmulator[0].firstChild.data)conn.close()exit(0) 仿真器的 XML 配置通常如下： 例 4.37. 域仿真器 XML 信息12345&lt;domain type=&#x27;kvm&#x27;&gt; ... &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt; ...&lt;/domain&gt; 启动模式通过 BIOS 启动适用于支持完全虚拟化的管理程序。在这种情况下，BIOS 有一个启动顺序优先级（软盘、硬盘、cdrom、网络），决定从哪里获取&#x2F;查找启动映像。 例 4.38. 设置启动模式1234567891011121314&lt;domain&gt; ... &lt;os&gt; &lt;type&gt;HVM&lt;/type&gt; &lt;loader readonly=&#x27;yes&#x27; type=&#x27;rom&#x27;&gt;/usr/lib/xen/boot/hvmloader&lt;/loader&gt; &lt;nvram template=&#x27;/usr/share/OVMF/OVMF_VARS.fd&#x27;&gt;/var/lib/libvirt/nvram/guest_VARS.fd&lt;/nvram&gt; &lt;boot dev=&#x27;hd&#x27;/&gt; &lt;boot dev=&#x27;cdrom&#x27;/&gt; &lt;bootmenu enable=&#x27;yes&#x27; timeout=&#x27;3000&#x27;/&gt; &lt;smbios mode=&#x27;sysinfo&#x27;/&gt; &lt;bios useserial=&#x27;yes&#x27; rebootTimeout=&#x27;0&#x27;/&gt; &lt;/os&gt; ...&lt;/domain&gt; 内存和 CPU 资源CPU 和内存资源可以在创建域时设置，也可以在域处于活动或非活动状态时动态设置。 CPU 资源是在创建域时使用 XML 定义中的标记设置的。管理程序定义了虚拟 CPU 的数量限制，无论是在创建域时还是以后都不能超过。这个最大值可能取决于许多资源和管理程序限制。下面是 CPU XML 规范的示例。 12345&lt;domain&gt; ... &lt;vcpu placement=&#x27;static&#x27; cpuset=&quot;1-4,^3,6&quot; current=&quot;1&quot;&gt;2&lt;/vcpu&gt; ...&lt;/domain&gt; 内存资源也可在创建域时使用 XML 定义中的标记进行设置。域的最大内存分配和当前内存分配都应设置。内存 XML 规范示例如下。 1234567&lt;domain&gt; ... &lt;maxMemory slots=&#x27;16&#x27; unit=&#x27;KiB&#x27;&gt;1524288&lt;/maxMemory&gt; &lt;memory unit=&#x27;KiB&#x27;&gt;524288&lt;/memory&gt; &lt;currentMemory unit=&#x27;KiB&#x27;&gt;524288&lt;/currentMemory&gt; ...&lt;/domain&gt; 域创建后，可通过 setVcpus 或 setVcpusFlags 方法增加虚拟 CPU 的数量。CPU 数量不得超过上文讨论的管理程序最大值。 例 4.39. 设置域的最大虚拟 CPU 数量123456789101112131415161718192021# Example-29.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(6)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)dom.setVcpus(2)conn.close()exit(0) 创建域后，还可以通过 setMemory 或 setMemoryFlags 方法更改内存大小。内存大小应以千字节为单位。 例 4.40. 设置域的内存量123456789101112131415161718192021# Example-30.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(6)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)dom.setMemory(4096) # 4 GigaBytesconn.close()exit(0) 除了 setMemory 方法外，还有另一种方法 setMemoryFlags。 监测性能统计指标可用于监控域、vCPU、内存、块设备和网络接口的利用率。 域块设备性能blockStats 方法提供了磁盘使用统计信息： 例 4.41. 获取磁盘块 I&#x2F;O 统计信息123456789101112131415161718192021222324252627# Example-31.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(6)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)rd_req, rd_bytes, wr_req, wr_bytes, err = dom.blockStats(&#x27;/path/to/linux-0.2.img&#x27;)print(&#x27;Read requests issued: &#x27; + str(rd_req))print(&#x27;Bytes read: &#x27; + str(rd_bytes))print(&#x27;Write requests issued: &#x27; + str(wr_req))print(&#x27;写入字节数: &#x27; + str(wr_bytes))print(&#x27;错误次数: &#x27; + str(err))conn.close()exit(0) 返回的元组包含发出的读（写）请求数和实际传输的字节数。块设备由映像文件路径或域 XML 中的 devices&#x2F;disk&#x2F;target[@dev] 元素设置的设备总线名称指定。 除了 blockStats 方法外，还有 blockStatsFlags 备选方法。 vCPU 性能要获取单个 VCPU 统计数据，请使用 getCPUStats 方法。 例 4.42. 获取单个 CPU 统计数据123456789101112131415161718192021222324# Example-33.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)cpu_stats = dom.getCPUStats(False)for (i, cpu) in enumerate(cpu_stats): print(&#x27;CPU &#x27; + str(i) + &#x27; Time: &#x27; + str(cpu[&#x27;cpu_time&#x27;] / 1000000000.))conn.close()exit(0) getCPUStats 使用一个布尔参数。当使用 True 时，报告的统计数据是所有 CPU 的总和。如果使用 False，则每个 CPU 都会报告各自的统计数据。无论采用哪种方式，都会返回一个列表。统计数据以纳秒为单位报告。如果一台主机有四个 CPU，cpu_stats 列表中就会有四个条目。 getCPUStats(True) 汇集主机上所有 CPU 的统计数据： 例 4.43. 获取 CPU 综合统计数据12345678910111213141516171819202122232425# Example-34.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)stats = dom.getCPUStats(True)print(&#x27;cpu_time: &#x27; + str(stats[0][&#x27;cpu_time&#x27;]))print(&#x27;system_time: &#x27; + str(stats[0][&#x27;system_time&#x27;]))print(&#x27;user_time: &#x27; + str(stats[0][&#x27;user_time&#x27;]))conn.close()exit(0) 内存统计要获取域当前使用的内存量，可以使用 memoryStats 方法。 例 4.44. 获取内存统计信息12345678910111213141516171819202122232425# Example-35.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)stats = dom.memoryStats()print(&#x27;memory used:&#x27;)for name in stats: print(&#x27; &#x27; + str(stats[name]) + &#x27; (&#x27; + name + &#x27;)&#x27;)conn.close()exit(0) 请注意，memoryStats 返回的是一个字典对象。根据管理程序和客户域的功能，该对象将包含数量不等的条目。 网络 I&#x2F;O 统计数据要获取网络统计信息，需要域所连接主机接口的名称（通常为 vnetX）。要找到它，请检索域 XML 描述（libvirt 会在运行时修改它）。然后，查找 devices&#x2F;interface&#x2F;target[@dev] 元素： 例 4.45. 获取网络 I&#x2F;O 统计信息12345# Example-32.py#!/usr/bin/env python3import sysimport libvirtfrom xml.etree import ElementTree 监测性能统计指标可用于监控域、vCPU、内存、块设备和网络接口的利用率。 域块设备性能blockStats 方法提供了磁盘使用统计信息： 例 4.41. 获取磁盘块 I&#x2F;O 统计信息123456789101112131415161718192021222324252627# Example-31.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(6)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)rd_req, rd_bytes, wr_req, wr_bytes, err = dom.blockStats(&#x27;/path/to/linux-0.2.img&#x27;)print(&#x27;Read requests issued: &#x27; + str(rd_req))print(&#x27;Bytes read: &#x27; + str(rd_bytes))print(&#x27;Write requests issued: &#x27; + str(wr_req))print(&#x27;写入字节数: &#x27; + str(wr_bytes))print(&#x27;错误次数: &#x27; + str(err))conn.close()exit(0) 返回的元组包含发出的读（写）请求数和实际传输的字节数。块设备由映像文件路径或域 XML 中的 devices/disk/target[@dev] 元素设置的设备总线名称指定。 除了 blockStats 方法外，还有 blockStatsFlags 备选方法。 vCPU 性能要获取单个 VCPU 统计数据，请使用 getCPUStats 方法。 例 4.42. 获取单个 CPU 统计数据123456789101112131415161718192021222324# Example-33.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)cpu_stats = dom.getCPUStats(False)for (i, cpu) in enumerate(cpu_stats): print(&#x27;CPU &#x27; + str(i) + &#x27; Time: &#x27; + str(cpu[&#x27;cpu_time&#x27;] / 1000000000.))conn.close()exit(0) getCPUStats 使用一个布尔参数。当使用 True 时，报告的统计数据是所有 CPU 的总和。如果使用 False，则每个 CPU 都会报告各自的统计数据。无论采用哪种方式，都会返回一个列表。统计数据以纳秒为单位报告。如果一台主机有四个 CPU，cpu_stats 列表中就会有四个条目。 getCPUStats(True) 汇集主机上所有 CPU 的统计数据： 例 4.43. 获取 CPU 综合统计数据12345678910111213141516171819202122232425# Example-34.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)stats = dom.getCPUStats(True)print(&#x27;cpu_time: &#x27; + str(stats[0][&#x27;cpu_time&#x27;]))print(&#x27;system_time: &#x27; + str(stats[0][&#x27;system_time&#x27;]))print(&#x27;user_time: &#x27; + str(stats[0][&#x27;user_time&#x27;]))conn.close()exit(0) 内存统计要获取域当前使用的内存量，可以使用 memoryStats 方法。 例 4.44. 获取内存统计信息12345678910111213141516171819202122232425# Example-35.py#!/usr/bin/env python3import sysimport libvirtdomName = &#x27;Fedora22-x86_64-1&#x27;conn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = conn.lookupByID(5)if dom == None: print(&#x27;Failed to find the domain &#x27; + domName, file=sys.stderr) exit(1)stats = dom.memoryStats()print(&#x27;memory used:&#x27;)for name in stats: print(&#x27; &#x27; + str(stats[name]) + &#x27; (&#x27; + name + &#x27;)&#x27;)conn.close()exit(0) 请注意，memoryStats 返回的是一个字典对象。根据管理程序和客户域的功能，该对象将包含数量不等的条目。 网络 I&#x2F;O 统计数据要获取网络统计信息，需要域所连接主机接口的名称（通常为 vnetX）。要找到它，请检索域 XML 描述（libvirt 会在运行时修改它）。然后，查找 devices/interface/target[@dev] 元素： 例 4.45. 获取网络 I&#x2F;O 统计信息12345# Example-32.py#!/usr/bin/env python3import sysimport libvirtfrom xml.etree import ElementTree PCI 设备直通PCI 设备直通功能可将主机中的物理 PCI 设备直接分配给客户机，客户机操作系统驱动程序可直接使用设备硬件，而无需依赖主机操作系统的任何驱动程序功能。 使用 PCI 设备直通时有一些注意事项。当 PCI 设备直接分配给客户机时，如果不先将设备从客户机热拔插，就无法进行迁移。此外，libvirt 并不保证直接分配设备是安全的，因此安全策略决定与底层虚拟化技术有关。安全 PCI 设备直通通常需要特殊的硬件功能，如英特尔芯片组的 VT-d 功能或 AMD 芯片组的 IOMMU。 连接 PCI 设备有两种模式：”托管” 或 “非托管” 模式，但在撰写本文时，只有 KVM 支持 “托管” 模式连接。在托管模式下，当客户机启动时，配置的设备将自动从主机操作系统驱动程序中分离，然后在客户机关闭时重新连接。在非托管模式下，必须在启动客户机之前明确分离设备。如果设备仍连接到主机操作系统，客户机将拒绝启动。libvirt 的 “节点设备” API 提供了从主机驱动程序分离&#x2F;重新连接 PCI 设备的方法。另外，也可以配置主机操作系统，将用于客户机的 PCI 设备列入黑名单，使其永远不会连接到主机操作系统驱动程序。 在这两种模式下，虚拟化技术始终会在启动客户机之前和客户机关闭之后对设备执行重置。这对于确保主机和客户操作系统之间的隔离至关重要。 复位 PCI 设备有多种方式。一些重置技术的范围仅限于单个设备&#x2F;功能，而另一些则可能同时影响多个设备。在后一种情况下，有必要将所有受影响的设备共同分配给同一客户，否则重置将不可能安全完成。节点设备 API 可用于确定设备是否需要共同分配，方法是手动分离设备，然后尝试执行重置操作。如果成功，就可以将设备单独分配给访客。如果失败，则需要将设备与同一 PCI 总线上的其他设备共同分配。 PCI 设备使用 “hostdevice” 元素连接到客户机。”模式” 属性应始终设置为 “子系统”，”类型” 属性应始终设置为 “pci”。”managed” 属性可根据应用需要设置为 “yes” 或 “no”。在 “hostdevice” 元素中有一个 “source” 元素，其中的 “address” 元素用于指定要连接的 PCI 设备。地址元素需要 “域”、”总线”、”插槽” 和 “功能” 属性。通过一个简短的示例最容易理解： 例 4.52. 获取域的输入设备信息12345&lt;hostdev mode=&#x27;subsystem&#x27; type=&#x27;pci&#x27; managed=&#x27;yes&#x27;&gt; &lt;source&gt; &lt;address domain=&#x27;0x0000&#x27; bus=&#x27;0x06&#x27; slot=&#x27;0x12&#x27; function=&#x27;0x5&#x27;/&gt; &lt;/source&gt;&lt;/hostdev&gt; 实时配置更改区块设备工作Libvirt 提供了通用的块任务方法，可用于启动和管理对属于域的磁盘的操作。通过调用与所需的操作（如 blockPull）。一旦启动，所有区块任务都以相同的方式进行管理。它们可以被中止、节流和查询。完成后，会发出一个异步事件来指示最终状态。 可以启动以下程序块任务： blockPull() 启动指定磁盘的块拉动操作。blockPull 会从磁盘的后备映像中提取数据填充磁盘映像。一旦从其后备映像中提取了所有数据，磁盘就不再依赖于后备映像。 可以使用 blockJobInfo 查询磁盘上的活动分块任务。如果找到，作业信息将以结构形式报告，其中包含：作业类型、带宽节流设置和进度信息。 virDomainBlockJobAbort() 可用于取消指定磁盘上的活动块任务。使用 blockJobSetSpeed() 可以限制块任务可能消耗的带宽。 带宽以 MB&#x2F;秒为单位指定。 例 4.53. 获取域的输入设备信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Example-40.py#!/usr/bin/env python3import sysimport libvirtdomxml = &quot;&quot;&quot;&lt;domain type=&#x27;kvm&#x27;&gt; &lt;name&gt;示例&lt;/name&gt; &lt;memory&gt;131072&lt;/memory&gt; &lt;vcpu&gt;1&lt;/vcpu&gt; &lt;os&gt; &lt;type arch=&#x27;x86_64&#x27; machine=&#x27;pc-0.13&#x27;&gt;hvm&lt;/type&gt; &lt;/os&gt; &lt;devices&gt; &lt;disk type=&#x27;file&#x27; device=&#x27;disk&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;qed&#x27;/&gt; &lt;source file=&#x27;/var/lib/libvirt/images/example.qed&#x27;/&gt; &lt;target dev=&#x27;vda&#x27; bus=&#x27;virtio&#x27;/&gt; &lt;/disk&gt; &lt;/devices&gt;&lt;/domain&gt;&quot;&quot;&quot;def do_cmd(cmdline): status = os.system(cmdline) if status &lt; 0: return -1 return WEXITSTATUS(status)def make_domain(conn): do_cmd(&quot;qemu-img create -f raw /var/lib/libvirt/images/backing.qed 100M&quot;) do_cmd(&quot;qemu-img create -f qed -b /var/lib/libvirt/images/backing.qed &quot; + &quot;/var/lib/libvirt/images/example.qed&quot;) dom = conn.createXML(domxml, 0) return domconn = Nonedom = Nonedisk = &quot;/var/lib/libvirt/images/example.qed&quot;try: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)dom = make_domain(conn)if dom == None: print(&quot;Failed to create domain&quot;, file=sys.stderr) exit(1)if dom.blockPull(disk, 0, 0) &lt; 0: print(&quot;Failed to start block pull&quot;, file=sys.stderr) exit(1)while (1): info = dom.blockJobInfo(disk, 0) if info != None: print(&quot;BlockPull progress: %0.0f %%&quot; % float(100 * info.cur / info.end)) elif info.cur == info.end: print(&quot;BlockPull complete&quot;) break else: print(&quot;Failed to query block jobs&quot;, file=os.stderr) break time.sleep(1)os.unlink(&quot;/var/lib/libvirt/images/backing.qed&quot;)os.unlink(&quot;/var/lib/libvirt/images/example.qed&quot;)if dom != None: conn.destroy(dom)conn.close()exit(0)","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"3 libvirt连接","slug":"libvirt文档/3-libvirt连接","date":"2024-03-03T07:50:26.000Z","updated":"2025-03-15T08:00:25.244Z","comments":true,"path":"libvirt文档/3-libvirt连接/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/3-libvirt%E8%BF%9E%E6%8E%A5/","excerpt":"","text":"3 连接在libvirt中，连接是系统中每个操作和对象的基础。任何希望与libvirt交互的实体，无论是virsh、virt-manager，还是使用libvirt库的程序，都需要首先获得与目标主机上libvirt守护进程的连接。连接不仅描述了代理希望交互的虚拟化技术类型（如qemu、xen、uml等），还描述了连接到该资源所需的任何认证方法。 3.1 概述libvirt 代理必须做的第一件事就是调用 virInitialize 函数或 Python libvirt 连接函数之一来获取 virConnect 类的实例。该实例将在后续操作中使用。Python libvirt 模块为连接到资源提供了 3 个不同的函数： 123conn = libvirt.open(name)conn = libvirt.openAuth(uri, auth, flags)conn = libvirt.openReadOnly(name) 在这三种情况下，都有一个名为 name 的参数，实际上指的是要连接的 hypervisor 的 URI。前面的 第 2.2 节 “驱动程序模型 “ 和 第 3.2.2 节 “远程 URI “ 详细介绍了可接受的各种 URI 格式。如果 URI 为 None，那么 libvirt 将应用一些启发式算法并探测适合的 hypervisor 驱动程序。虽然这对于开发人员进行临时测试可能很方便，但强烈建议应用程序不要依赖探测逻辑，因为它可能随时更改。应用程序应始终通过提供 URI 显式请求所需的 hypervisor 连接。 上述三种方法的区别在于它们进行身份验证的方式及其提供的授权级别。 3.1.1 Open 函数open 函数将尝试为完全读写访问打开一个连接。该函数不允许提供身份验证回调，因此只有在可以根据应用程序的凭证进行身份验证的连接上才会成功。 例 3.1. 使用 open 1234567891011121314# Example-1.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.close()exit(0) 上述示例打开了与系统 qemu 管理程序驱动程序的读写连接，检查是否成功，如果成功则关闭连接。有关 libvirt URI 的更多信息，请参阅 *第 3.2 节 “URl 格式”*。 3.1.2 openReadOnlyopenReadOnly 函数将尝试为只读访问打开一个连接。这种连接只允许调用一组受限制的方法，通常用于监控不允许更改的应用程序。与 open 方法一样，该方法不支持身份验证回调，因此需要依赖凭证。 例 3.2. 使用 openReadOnly 1234567891011121314# Example-2.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.openReadOnly(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.close()exit(0) 上述示例打开了与系统 qemu 管理程序驱动程序的只读连接，检查是否成功，如果成功则关闭连接。有关 libvirt URI 的更多信息，请参阅 *第 3.2 节 “URl 格式”*。 3.1.3 openAuthopenAuth 函数是最灵活的函数，实际上取代了前两个函数。它需要一个额外的参数，提供一个 Python 列表，其中包含客户端应用程序的身份验证凭据。如果需要，flags 参数允许应用程序使用 VIR_cONNEcT_RO 标志请求只读连接。下面是一个使用 openAuth 和用户名与密码凭据的简单 Python 程序示例。与 open 方法一样，该方法没有身份验证回调范围，因此依赖于凭据。 例 3.3. 使用 openAuth 123456789101112131415161718192021222324252627# Example-3.py#!/usr/bin/env python3import sysimport libvirtSASL_USER = &quot;my-super-user&quot;SASL_PASS = &quot;my-super-user&quot;def request_cred(credentials, user_data): for credential in credentials: if credential[0] == libvirt.VIR_CRED_AUTHNAME: credential[4] = SASL_USER elif credential[0] == libvirt.VIR_CRED_PASSPHRASE: credential[4] = SASL_PASS return 0auth = [[libvirt.VIR_CRED_AUTHNAME, libvirt.VIR_CRED_PASSPHRASE], request_cred, None]conn = Nonetry: conn = libvirt.openAuth(&#x27;qemu+tcp://localhost/system&#x27;, auth, 0)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.close()exit(0) 要测试上述程序，必须具备以下配置： &#x2F;etc&#x2F;libvirt&#x2F;libvirtd.conf 123listen_tls = 0listen_tcp = 1auth_tcp = &quot;sasl&quot; &#x2F;etc&#x2F;sasl2&#x2F;libvirt.conf 1mech_list: digest-md5 SASL 数据库中已添加 virt 用户： 123$ saslpasswd2 -a libvirt virtPassword:Again (for verification): libvirtd 已使用 –listen 启动 配置完成后，例 3.3 “使用 openAuth “ 就可以使用配置的用户名和密码，并允许对 libvirtd 进行读写访问。 与 libvirt C 接口不同，Python 不提供用于收集凭证的自定义回调。 3.1.4 Close不再需要连接时，必须调用 virConnection 类的 close 方法来释放连接。连接是有引用计数的对象，因此每次 open 函数调用都应相应地调用 close 方法。 连接是有引用计数的；初始化（open、openAuth 等）会明确增加引用计数；其他依赖于连接存活的方法也会临时增加引用计数。open 函数调用应具有匹配的 close，所有其他引用将在相应操作完成后释放。 在 Python 中，当类实例离开作用域或程序结束时，引用计数会自动减少。 例 3.4. 使用带有附加引用的 close 123456789101112131415161718# Example-5.py#!/usr/bin/env python3import sysimport libvirtconn1 = libvirt.open(&#x27;qemu:///system&#x27;)if conn1 == None: print(&#x27;Failed to open connection to qemu:///system&#x27;, file=sys.stderr) exit(1)conn2 = libvirt.open(&#x27;qemu:///system&#x27;)if conn2 == None: print(&#x27;Failed to open connection to qemu:///system&#x27;, file=sys.stderr) exit(1)conn1.close()conn2.close()exit(0) 还要注意的是，与连接（virDomain、virNetwork 等）相关的每个其他类实例也将持有连接的引用。 3.2 URI 格式Libvirt 使用统一资源标识符 (URI) 来标识管理程序连接。libvirt 使用 URI 对本地和远程管理程序进行寻址。URI 方案和路径定义了要连接的管理程序，而 URI 的主机部分则决定了管理程序的位置。 3.2.1 本地 URILibvirt 本地 URI 有以下几种形式： 1234driver:///systemdriver:///sessiondriver+unix:///systemdriver+unix:///session libvirt URI 的所有其他用途都被视为远程，即使连接到 localhost 也是如此。有关远程 URI 的详细信息，请参见 *第 3.2.2 节 “远程 URI”*。 目前支持以下驱动程序： 驱动 说明 qemu 用于管理 qemu 和 KVM 客户端 xen 用于管理旧式（Xen 3.1 及更旧版本）Xen 客户端 xenapi 用于管理新式 Xen 客户端 uml 用于管理 UML 来宾 lxc 用于管理 Linux 容器 vbox 用于管理 VirtualBox 客户端 openvz 用于管理 OpenVZ 容器 esx 用于管理 VMware ESX 客户端 gsx 用于管理 VMware GSX 客户端 vpx 用于管理 VMware VPX 客户端 hyperv 用于管理 Microsoft Hyper-V 客户端 下面的示例展示了如何使用本地 URI 连接到本地 QEMU 虚拟机管理程序。 例 3.5. 连接本地 QEMU 虚拟机管理程序 1234567891011121314# Example-6.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.close()exit(0) 请注意，并非所有列出的驱动程序都可以这样使用。两个明显的例外是 LXC 驱动程序和仅客户端驱动程序。与 LXC 驱动程序建立连接的方法见下例。 例 3.6. 连接本地 LXC 虚拟机管理程序 1234567891011121314# Example-32.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&#x27;lxc://&#x27;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.close()exit(0) 客户端专用驱动程序包括 ESX、VPX、GPX 和 Hyper-V。与其他驱动程序相比，这些驱动程序不能与远程传输机制一起使用。这意味着以下 URI 无效： 1esx+ssh://example.com 下面举例说明如何正确使用这些驱动程序。 例 3.7. 连接到客户端驱动程序 123456789101112131415# Example-33.py#!/usr/bin/env python3import sysimport libvirt# use VPX over HTTPS, select ESX server &#x27;srv1&#x27; in datacenter &#x27;dc1&#x27;conn = Nonetry: conn = libvirt.open(&#x27;vpx://example-vcenter.com/dc1/srv1&#x27;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.close()exit(0) 3.2.2 远程 URI远程 URI 采用一般形式（”[…]“ 表示可选部分）： 1driver[+transport]://[username@][hostname][:port]/[path][?extraparameters] 下表介绍了 URI 的各个组成部分。 组件 说明 driver 要连接到的 libvirt hypervisor 驱动程序的名称。这与本地 URI 中使用的名称相同。一些示例包括 xen、qemu、lxc、openvz 和 test。作为特殊情况，可以使用伪驱动程序名称 remote，这将导致远程守护进程探测活动的 hypervisor，并选择一个来使用。作为一般规则，如果应用程序知道它需要哪种 hypervisor，应始终明确指定驱动程序名称，而不依赖于自动探测。 transport 数据传输之一的名称。可能的值包括 tls、tcp、unix、ssh 和 ext。如果省略，则在提供主机名的情况下默认为 tls，在未提供主机名的情况下默认为 unix。 username 使用 SSH 数据传输时，可以选择与客户端当前登录名不同的用户名。 hostname 远程计算机的全限定主机名。如果使用带有 x509 证书的 TLS 或带有 GSSAPI&#x2F;Keberos 插件的 SASL，该主机名必须与服务器 x509 证书&#x2F;Kerberos 原理中使用的主机名一致。不匹配的主机名将导致验证失败。 port 很少需要，除非 SSH 或 libvirtd 已配置为在非标准 TCP 端口上运行。SSH 数据传输默认端口为 22，16509 用于 TCP 数据传输，16514 用于 TLS 数据传输。 path 路径应与管理程序驱动程序的本地 URI 所用路径相同。对于 Xen，路径始终是 **&#x2F;**，而对于 QEMU，路径则是 &#x2F;system。 extraparameters URI 查询参数可以对远程连接的某些方面进行微调，下一节将对此进行深入讨论。 根据此处描述的信息，并参考本文件前面提到的管理程序专用 URI，现在可以举例说明一些远程访问 URI。 使用 ssh 隧道数据传输和 ssh 用户名 root 连接到主机 node.example.com 上的远程 Xen 虚拟机管理程序： 1xen+ssh://root@node.example.com/ 使用 TLS 和 x509 证书连接到主机 node.example.com 上的远程 QEMU 虚拟机： 1qemu://node.example.com/system 使用 TLS 连接到主机 node.example.com 上的远程 Xen 虚拟机，跳过对服务器 x509 证书的验证（注意：这会危及你的安全）： 1xen://node.example.com/?no_verify=1 通过非标准 Unix 套接字连接本地 QEMU 实例（在这种情况下要明确提供 Unix 套接字的完整路径）： 1qemu+unix:///system?socket=/opt/libvirt/run/libvirt/libvirt-sock 连接到 libvirtd 守护进程，在另一个 TCP 端口 5000 上提供未加密的 TCP&#x2F;IP 连接，并使用默认配置的测试驱动程序： 1test+tcp://node.example.com:5000/default Extra parameters远程 URI 可以在查询字符串（”?” 后面的部分）中添加额外参数。远程 URI 可以理解以下所示的额外参数。任何其他参数都将不加修改地传递到后端。请注意，参数值必须使用 URI 转义。更多信息请参阅 http://xmlsoft.org/ html&#x2F;libxml-uri.html#xmlURlEscapeStr。 名称 运输 说明 name 任何运输 传递给远程 virConnectOpen 函数的本地管理程序 URI。该 URI 通常是通过删除远程 URI 中的传输、主机名、端口号、用户名和额外参数形成的，但在某些非常复杂的情况下，可能需要明确提供名称。例如：name&#x3D;qemu:&#x2F;&#x2F;&#x2F;system command ssh、ext 外部命令。对于 ext 传输，这是必需的。对于 ssh，默认为 ssh。搜索 PATH 以查找命令。例如：command&#x3D;&#x2F;opt&#x2F;openssh&#x2F;bin&#x2F;ssh socket unix, ssh 外部命令。对于 ext 传输，这是必需的。对于 ssh，默认为 ssh。会在 PATH 中搜索命令。示例：socket&#x3D;&#x2F;opt&#x2F;libvirt&#x2F;run&#x2F;libvirt&#x2F;libvirt-sock netcat ssh 远程计算机上 netcat 命令的名称。默认为 nc。对于 ssh 传输，libvirt 构建的 ssh 命令类似于 command -p port [-l username] hostname netcat -U socket，其中端口、用户名、主机名可作为远程 URI 的一部分指定，而命令、netcat 和 socket 则来自额外参数（或合理的默认值）。例如：netcat&#x3D;&#x2F;opt&#x2F;netcat&#x2F;bin&#x2F;nc no_verify tls 如果设置的值不为零，则会禁用客户端对服务器证书的检查。请注意，要禁用服务器对客户端的检查，您必须更改 libvirtd 配置。示例：no_verify&#x3D;1 no_tty ssh 如果设置为非零，当 ssh 无法自动登录远程机器时（例如，使用 ssh-agent 时），该值将阻止 ssh 询问密码。在无法访问终端时使用此功能，例如在使用 libvirt 的图形程序中。例如 no_tty&#x3D;1 3.3 Capability Information Methods下面的示例展示了如何使用远程 URI 连接 QEMU 虚拟机管理程序。 例 3.8. 连接远程 QEMU 虚拟机管理程序 1234567891011121314# Example-7.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&#x27;qemu+tls://host2/system&#x27;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)conn.close()exit(0) getCapabilities 方法调用可用于获取有关虚拟化主机功能的信息。如果调用成功，它将返回一个包含 capabilities XML 的 Python 字符串（如下所述）。如果发生错误，则返回 None。以下代码演示了 getCapabilities 方法的使用： 例 3.9. 使用 getCapabilities 1234567891011121314151617# Example-8.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)caps = conn.getCapabilities() # caps 将是一个 XML 字符串print(&#x27;Capabilities:\\n&#x27; + caps)conn.close()exit(0) capabilities XML 格式提供了主机虚拟化技术的相关信息。特别是，它描述了虚拟化主机的功能、虚拟化驱动程序以及虚拟化技术可启动的客户类型。请注意，根据使用的 libvirt 驱动程序，能力 XML 可能（也确实）有所不同。能力 XML 示例如下： 例 3.10. QEMU 驱动程序功能示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;capabilities&gt; &lt;host&gt; &lt;cpu&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;/cpu&gt; &lt;migration_features&gt; &lt;live/&gt; &lt;uri_transports&gt; &lt;uri_transport&gt;tcp&lt;/uri_transport&gt; &lt;/uri_transports&gt; &lt;/migration_features&gt; &lt;topology&gt; &lt;cells num=&#x27;1&#x27;&gt; &lt;cell id=&#x27;0&#x27;&gt; &lt;cpus num=&#x27;2&#x27;&gt; &lt;cpu id=&#x27;0&#x27;/&gt; &lt;cpu id=&#x27;1&#x27;/&gt; &lt;/cpus&gt; &lt;/cell&gt; &lt;/cells&gt; &lt;/topology&gt; &lt;/host&gt; &lt;guest&gt; &lt;os_type&gt;hvm&lt;/os_type&gt; &lt;arch name=&#x27;i686&#x27;&gt; &lt;wordsize&gt;32&lt;/wordsize&gt; &lt;emulator&gt;/usr/bin/qemu&lt;/emulator&gt; &lt;machine&gt;pc&lt;/machine&gt; &lt;machine&gt;isapc&lt;/machine&gt; &lt;domain type=&#x27;qemu&#x27;/&gt; &lt;domain type=&#x27;kvm&#x27;&gt; &lt;emulator&gt;/usr/bin/qemu-kvm&lt;/emulator&gt; &lt;/domain&gt; &lt;/arch&gt; &lt;features&gt; &lt;pae/&gt; &lt;nonpae/&gt; &lt;acpi default=&#x27;on&#x27; toggle=&#x27;yes&#x27;/&gt; &lt;apic default=&#x27;on&#x27; toggle=&#x27;no&#x27;/&gt; &lt;/features&gt; &lt;/guest&gt; &lt;guest&gt; &lt;os_type&gt;hvm&lt;/os_type&gt; &lt;arch name=&#x27;x86_64&#x27;&gt; &lt;wordsize&gt;64&lt;/wordsize&gt; &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt; &lt;machine&gt;pc&lt;/machine&gt; &lt;machine&gt;isapc&lt;/machine&gt; &lt;domain type=&#x27;qemu&#x27;/&gt; &lt;domain type=&#x27;kvm&#x27;&gt; &lt;emulator&gt;/usr/bin/qemu-kvm&lt;/emulator&gt; &lt;/domain&gt; &lt;/arch&gt; &lt;features&gt; &lt;acpi default=&#x27;on&#x27; toggle=&#x27;yes&#x27;/&gt; &lt;apic default=&#x27;on&#x27; toggle=&#x27;no&#x27;/&gt; &lt;/features&gt; &lt;/guest&gt;&lt;/capabilities&gt; 在 capabilities XML 中，总有一个 &#x2F;host 子文档，以及零个或多个 &#x2F;guest 子文档（虽然允许零个 guest 子文档，但这意味着该特定驱动程序的任何 guest 都不能在该特定主机上启动）。 &#x2F;host 子文档描述主机的功能。 &#x2F;host&#x2F;uuid 显示主机的 UUID。如果 SMBIOS UUID 可用且有效，则从 SMBIOS UUID 派生，或者在 libvirtd.conf 中用自定义值覆盖。如果以上两项都未正确设置，则每次重启 libvirtd 时都会生成一个临时 UUID。 &#x2F;host&#x2F;cpu 子文档描述了主机 CPU 的能力。libvirt 在决定是否能在这台特定机器上正确启动客户机时会用到它，在实时迁移过程中也会参考它，以确定目标机器是否提供继续运行客户机所需的标志。 &#x2F;host&#x2F;cpu&#x2F;arch 是描述底层主机 CPU 架构的必要 XML 节点。截至本文撰写时，所有 libvirt 驱动程序都会根据 uname(2) 的输出将其初始化。 &#x2F;host&#x2F;cpu&#x2F;features 是一个可选的子文档，用于描述主机上存在的其他 CPU 功能。目前，xen 驱动程序仅使用它来报告是否存在 svm 或 vmx 标志，以及是否存在 pae 标志。 &#x2F;host&#x2F;cpu&#x2F;model 是一个可选元素，用于描述与主机 CPU 最相似的 CPU 型号。libvirt 当前了解的 CPU 型号列表位于 cpu_map.xml 文件中。 &#x2F;host&#x2F;cpu&#x2F;feature 是零个或多个元素，用于描述主机 CPU 所具有的其他 CPU 特性，这些特性不在 &#x2F;host&#x2F;cpu&#x2F;model 中涵盖。 &#x2F;host&#x2F;migration_features 是一个可选的子文档，用于描述该驱动程序在该主机上支持的迁移功能（如果有）。如果该子文档不存在，则不支持迁移。目前，xen、qemu 和 esx 驱动程序支持迁移。 如果驱动程序支持实时迁移，则存在 &#x2F;host&#x2F;migration_features&#x2F;live XML 节点。 &#x2F;host&#x2F;migration_features&#x2F;uri_transports 是一个可选的子文档，描述了备用的迁移连接机制。这些备用连接机制在多主机虚拟化系统中非常有用。例如，virsh migrate 命令可以通过 10.0.0.1 连接到迁移源，通过 10.0.0.2 连接到迁移目标。但是，由于安全策略的原因，迁移源可能只能通过 192.168.0.0&#x2F;24 直接与迁移目标通信。在这种情况下，使用备用迁移连接机制将使迁移成功。截至本文撰写之时，xen 驱动程序支持备用迁移机制 “xenmigr”，而 qemu 驱动程序则支持备用迁移机制 “tcp”。更多信息，请参阅迁移文档。 &#x2F;host&#x2F;topology 子文档描述主机的 NUMA 拓扑；每个 NUMA 节点由一个 &#x2F;host&#x2F;topology&#x2F;cells&#x2F;cell 表示，并描述该 NUMA 节点中的 CPU。如果主机是 UMA（非 NUMA）机器，则只有一个单元，所有 CPU 都在该单元中。这与硬件的具体情况密切相关，因此在不同的机器上必然会有所不同。 &#x2F;host&#x2F;secmodel 是一个可选的子文档，用于描述主机上使用的安全模型。 &#x2F;host&#x2F;secmodel&#x2F;model 显示安全模型的名称，而 &#x2F;host&#x2F;secmodel&#x2F;doi 则显示解释域。有关安全的更多信息，请参阅安全部分。 每个 &#x2F;guest 子文档都描述了该主机驱动程序可以启动的一种客户机。该描述包括客户机的架构（即 i686）以及提供给客户机的 ABI（即 hvm、xen 或 uml）。 &#x2F;guest&#x2F;os_type 是描述来宾类型的必填元素。 驱动 客人类型 qemu 总是 “hvm” xen 准虚拟化客户机可以使用 “xen”，完全虚拟化客户机可以使用 “hvm”。 uml 总是 “uml” lxc 始终为 “exe” vbox 总是 “hvm” openvz 始终为 “exe” one 总是 “hvm” ex 暂不支持 &#x2F;guest&#x2F;arch 是描述该客户机类型各种虚拟硬件的 XML 子文档的根。它有一个名为 “name” 的属性，可用于回溯该子文档。 &#x2F;guest&#x2F;arch&#x2F;wordsize 是一个必填元素，用于描述该客户机类型每个字使用的位数。通常为 32 或 64。 &#x2F;guest&#x2F;arch&#x2F;emulator 是一个可选元素，用于描述该客户机类型模拟器的默认路径。请注意，对于需要备用二进制文件的客户机类型，仿真器可以被 &#x2F;guest&#x2F;arch&#x2F;domain&#x2F;emulator 元素（如下所述）覆盖。 &#x2F;guest&#x2F;arch&#x2F;loader 是一个可选元素，用于描述该客户机类型固件加载器的默认路径。请注意，对于使用备用加载器的客户机类型，默认加载器路径可被 &#x2F;guest&#x2F;arch&#x2F;domain&#x2F;loader 元素（如下所述）覆盖。目前，只有用于 HVM 客户机的 xen 驱动程序使用了这一功能。 可以有零个或多个 &#x2F;guest&#x2F;arch&#x2F;machine 元素来描述该客户机模拟器可以模拟的默认机器类型。这些 “机器” 通常代表 guest 可以启动的 ABI 或硬件接口。请注意，对于提供其他机器类型的虚拟化技术，这些机器类型可以被 &#x2F;guest&#x2F;arch&#x2F;domain&#x2F;machine 元素（如下所述）覆盖。其典型值为 “pc” 和 “isapc”，分别表示基于 PCI 的普通 PC 和基于 ISA 的旧 PC。 可以有零个或多个 &#x2F;guest&#x2F;arch&#x2F;domain XML 子树（但如果 &#x2F;guest&#x2F;arch&#x2F;domain XML 子树为零，则无法启动该驱动程序的任何客户）。每个 &#x2F;guest&#x2F;arch&#x2F;domain XML 子树都有可选的 &lt;emulator&gt;、&lt;loader&gt; 和 &lt;machine&gt; 元素，这些元素会覆盖上文指定的默认值。对于缺失的元素，将使用默认值。 &#x2F;guest&#x2F;features 可选子文档描述了可启用或禁用的各种附加访客功能，以及它们的默认状态和是否可以打开或关闭。 3.4 主机信息有多种 Python virConnection 方法可用于获取虚拟化主机的信息，包括主机名、最大支持客户 CPU 等。 获取主机名getHostname 方法可用于获取 gethostname() 返回的虚拟化主机的主机名。该方法通过 virConnectioninstance 调用，如果成功，将返回一个包含主机名的字符串，该主机名可能扩展为全称域名。如果发生错误，则返回 NULL。以下代码演示了如何使用 getHostname： 例 3.11. 使用 getHostname 1234567891011121314151617# Example-9.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)host = conn.getHostname()print(&#x27;Hostname:&#x27; + host)conn.close()exit(0) getMaxVcpusgetMaxVcpus 方法可用于获取底层虚拟化技术所支持的每个虚拟机的最大虚拟 CPU 数量。该方法将虚拟化 “类型” 作为输入（可以是 None），如果成功，则返回所支持的虚拟 CPU 数量。如果出错，则返回 -1。以下代码演示了 getMaxVcpus 的使用： 例 3.12. 使用 getMaxVcpus 1234567891011121314151617# Example-10.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)vcpus = conn.getMaxVcpus(None)print(&#x27;Maximum support virtual CPUs:&#x27; + str(vcpus))conn.close()exit(0) getInfogetInfo 方法可用于获取虚拟化主机的各种信息。该方法如果成功则返回一个 Python 列表，如果出错则返回 None。Python 列表 包含以下成员： 成员 说明 list[0] 表示 CPU 型号的字符串 list[1] 内存大小（兆字节） list[2] 运行中的 CPU 数量 list[3] 预期 CPU 频率（兆赫） list[4] NUMA 节点的数量，1 表示统一内存访问 list[5] 每个节点的 CPU 插座数量 list[6] 每个插座的内核数 list[7] 每个核心的线程数 以下代码演示了 virNodeGetInfo 的使用： 例 3.13. 使用 getInfo 123456789101112131415161718192021222324# Example-12.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)nodeinfo = conn.getInfo()print(&#x27;Model:&#x27; + str(nodeinfo[0]))print(&#x27;Memory size: &#x27; + str(nodeinfo[1]) + &#x27;MB&#x27;)print(&#x27;Number of CPUs:&#x27; + str(nodeinfo[2]))print(&#x27;CPU 的主频:&#x27; + str(nodeinfo[3]))print(&#x27;NUMA 节点数:&#x27; + str(nodeinfo[4]))print(&#x27;CPU 插座数:&#x27; + str(nodeinfo[5]))print(&#x27;Number of CPU cores per socket:&#x27; + str(nodeinfo[6]))print(&#x27;每个内核的 CPU 线程数:&#x27; + str(nodeinfo[7]))conn.close()exit(0) getCellsFreeMemorygetCellsFreeMemory 方法可用于获取系统中部分或全部 NUMA 节点的可用内存量（以千字节为单位）。该方法需要输入起始单元和要检索数据的最大单元数。如果成功，将返回 Python 列表，其中包含每个节点的可用内存量。如果失败，则返回 “none“。以下代码演示了 getCellsFreeMemory 的使用： 例 3.14. 使用 getCellsFreeMemory 123456789101112131415161718192021222324# Example-13.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)nodeinfo = conn.getInfo()numnodes = nodeinfo[4]memlist = conn.getCellsFreeMemory(0, numnodes)cell = 0for cellfreemem in memlist: print(&#x27;Node &#x27; + str(cell) + &#x27;: &#x27; + str(cellfreemem) + &#x27; bytes free memory&#x27;) cell += 1conn.close()exit(0) getTypegetType 方法可用于获取此连接上使用的虚拟化类型。如果成功，它将返回一个字符串，表示正在使用的虚拟化类型。如果发生错误，则返回 None。以下代码演示了 getType 的使用： 例 3.15. 使用 virConnectGetType 123456789101112131415# Example-14.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)print(&#x27;Virtualization type: &#x27; + conn.getType())conn.close()exit(0) 获取库和驱动程序版本libvirt 库具有软件包和库本身的版本信息。如下所示，可以打印这些信息。 例 3.16. 使用 virConnectGetVersion 123456789# Example-34.py#!/usr/bin/env python3import sysimport libvirtprint(&quot;Package Version: &quot; + libvirt.sys.version)print(&quot;Library Version:&quot; + str(libvirt.sys.version_info))exit(0) 程序库支持的每个驱动程序都有自己的版本。版本号为整数：1000000主版本 + 1000 次版本 + 发行版本。下面的示例演示了如何打印驱动程序的版本号。 例 3.17. 使用 virConnectGetVersion 1234567891011121314151617# Example-15.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)ver = conn.getVersion()print(&#x27;Version: &#x27; + str(ver))conn.close()exit(0) getURIgetURI 方法可用于获取当前连接的 URI。虽然这通常与传入 打开 调用的字符串相同，但底层驱动程序有时会将字符串规范化。此方法将返回规范版本。如果成功，它将返回一个 URI 字符串。如果发生错误，则返回 None。以下代码演示了如何使用 getURI： 例 3.18. 使用 virConnectGetURI 1234567891011121314151617# Example-17.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)uri = conn.getURI()print(&#x27;Canonical URI: &#x27; + uri)conn.close()exit(0) isEncryptedisEncrypted 方法可用于查找给定连接是否已加密。如果成功，加密连接将返回 1，未加密连接将返回 0。如果发生错误，则返回 -1。下面的代码演示了 isEncrypted 的使用： 例 3.19. 使用 virConnectIsEncrypted 123456789101112131415# Example-15.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)print(&#x27;Connection is encrypted:&#x27; + str(conn.isEncrypted()))conn.close()exit(0) isSecureisSecure 方法可用于查找给定连接是否安全。如果一个连接是加密的，或者是在一个不会受到以下攻击的信道上运行，那么它就会被归类为安全连接： 窃听（如 UNIX 域套接字）。如果连接成功，则返回 1 表示连接安全，返回 0 表示连接不安全。如果发生错误，则返回 -1。以下代码演示了 isSecure 的使用： 例 3.20. 使用 virConnectIsSecure 123456789101112131415# Example-19.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)print(&#x27;Connection is secure:&#x27; + str(conn.isSecure()))conn.close()exit(0) isAlive该方法可确定与管理程序的连接是否仍然有效。如果连接是本地连接，或通过未关闭的通道（TCP 或 UNIX 套接字）运行，则该连接将被归类为 “存活” 连接。 例 3.21. 使用 isAlive 12345678910111213141516# Example-21.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)alive = conn.isAlive()print(&quot;Connection is alive = &quot; + str(alive))conn.close()exit(0) compareCPU该方法将给定的 CPU 描述与主机 CPU 进行比较。该 XML 描述参数与用于域描述的 XML 描述参数相同。 例 3.22. 使用 compareCPU 12345678910111213141516171819202122232425262728293031# Example-22.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)xml = &#x27;&lt;cpu mode=&quot;custom&quot; match=&quot;exact&quot;&gt;&#x27; +\\ &#x27;&lt;model fallback=&quot;forbid&quot;&gt;kvm64&lt;/model&gt;&#x27; +\\ &#x27;&lt;/cpu&gt;&#x27;retc = conn.compareCPU(xml)if retc == libvirt.VIR_CPU_COMPARE_ERROR: print(&quot;CPU 不一致或出错&quot;)elif retc == libvirt.VIR_CPU_COMPARE_INCOMPATIBLE: print(&quot;CPU 不兼容&quot;)elif retc == libvirt.VIR_CPU_COMPARE_IDENTICAL: print(&quot;CPU 完全相同&quot;)elif retc == libvirt.VIR_CPU_COMPARE_SUPERSET: print(&quot;The host CPU is better than the one specified.&quot;)else: print(&quot;An Unknown return code was emitted.&quot;)conn.close()exit(0) getFreeMemorygetFreeMemory 方法可用于查找主机节点上的可用内存量。注意：大多数 libvirt API 以千字节为单位提供内存大小，但在此函数中，返回值以字节为单位。必要时除以 1024。 例 3.23. 使用 getFreeMemory 12345678910111213141516# Example-23.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)mem = conn.getFreeMemory()print(&quot;Free memory on the node (host) is &quot; + str(mem) + &quot; bytes.&quot;)conn.close()exit(0) getFreePages该方法用于查询主机系统中指定大小的空闲页面。对于输入，pages 参数是调用者感兴趣的页面大小的 Python 列表（大小单位是千字节，例如传递 2048 表示 2MB），start 参数指的是应该收集信息的第一个 NUMA 节点，cellcount 参数告诉应该查询多少个连续节点。函数会返回一个 Python 列表，其中包含是否有指定输入大小的页面的指示符。 如果主机系统不支持所请求大小的内存页，则会出现错误。 例 3.24. 使用 getFreePages 12345678910111213141516171819202122232425# Example-24.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)pages = [2048]start = 0cellcount = 4buf = conn.getFreePages(pages, start, cellcount)i = 0for page in buf: print(&quot;Page Size: &quot; + str(pages[i]) + &quot; Available pages:&quot; + str(page)) ++iconn.close()exit(0) 获取内存参数该方法以字符串形式返回所有可用的内存参数。 例 3.25. 使用 getFreePages 123456789101112131415161718# Example-25.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)buf = conn.getMemoryParameters()for parm in buf: print(parm)conn.close()exit(0) getMemoryStats此方法返回单个或所有单个节点（主机）的内存统计信息。它返回一个 Python 字符串列表。 例 3.26. 使用 getMemoryStats 123456789101112131415161718# Example-26.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)buf = conn.getMemoryStats(libvirt.VIR_NODE_MEMORY_STATS_ALL_CELLS)for parm in buf: print(parm)conn.close()exit(0) 获取安全模型此方法返回当前使用的安全模型（列表）（如果有）。 例 3.27. 使用 getSecurityModel 12345678910111213141516# Example-27.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)model = conn.getSecurityModel()print(model[0] + &quot; &quot; + model[1])conn.close()exit(0) 获取系统信息该方法以 XML 定义的形式返回系统信息。格式与域 XML 定义相同。 例 3.28. 使用 getSysinfo 1234567891011121314151617# Example-28.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)xmlInfo = conn.getSysinfo()print(xmlInfo)conn.close()exit(0) getCPUMap获取主机节点 CPU 的 CPU 映射。 例 3.29. 使用 getCPUMap 123456789101112131415161718# Example-29.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)map = conn.getCPUMap()print(&quot;CPUs:&quot; + str(map[0]))print(&quot;Available:&quot; + str(map[1]))conn.close()exit(0) getCPUStats获取单个或所有 CPU 的统计数据。该方法需要一个代表 CPU 编号的参数，以获取单个 CPU 的统计数据；或使用 VIR_NODE_CPU_STATS_ALL_CPUS 值，以获取所有 CPU 的 Python 统计数据列表。 例 3.30. 使用 getCPUStats 1234567891011121314151617181920# Example-30.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)stats = conn.getCPUStats(0)print(&quot;kernel:&quot; + str(stats[&#x27;kernel&#x27;]))print(&quot;idle: &quot; + str(stats[&#x27;idle&#x27;]))print(&quot;user: &quot; + str(stats[&#x27;user&#x27;]) + str(stats[&#x27;user&#x27;]))print(&quot;iowait: &quot; + str(stats[&#x27;iowait&#x27;]))conn.close()exit(0) getCPUModelNames获取与架构类型匹配的 CPU 名称列表。 例 3.31. 使用 getCPUModelNames 123456789101112131415161718# Example-31.py#!/usr/bin/env python3import sysimport libvirtconn = Nonetry: conn = libvirt.open(&quot;qemu:///system&quot;)except libvirt.libvirtError as e: print(repr(e), file=sys.stderr) exit(1)models = conn.getCPUModelNames(&#x27;x86_64&#x27;)for model in models: print(model)conn.close()exit(0)","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"2 libvirt对象模型","slug":"libvirt文档/2-libvirt体系结构","date":"2024-03-02T07:50:26.000Z","updated":"2025-03-15T08:00:21.967Z","comments":true,"path":"libvirt文档/2-libvirt体系结构/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/2-libvirt%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/","excerpt":"","text":"Libvirt 对象模型本章介绍了 libvirt API 和 Python libvirt 模块定义背后的主要原理和架构选择。 虚拟机连接连接是 libvirt API 和 Python libvirt 模块中的主要或顶级对象。在尝试使用几乎所有类或方法之前，需要此对象的一个实例。连接与特定的 hypervisor 相关联，该 hypervisor 可以在与 libvirt 客户端应用程序相同的本地计算机上运行，也可以在网络上的远程计算机上运行。在所有情况下，连接由 virConnect 类的一个实例表示，并由一个 URI 标识。URI 的方案和路径定义了要连接的 hypervisor，而 URI 的主机部分则确定了其所在位置。有关有效 URI 的完整说明，请参阅*第 3.2 节 “URI 格式”*。 应用程序允许同时打开多个连接，即使在单台机器上使用多种类型的 hypervisor。例如，主机可以提供 KVM 完整机虚拟化和 LXC 容器虚拟化。连接对象可以跨多个线程同时使用。一旦建立了连接，就可以获取到其他受管理的对象的句柄或创建新的受管理对象，第 2.1.2 节 “客户域 “ 中对此进行了讨论。 访客域名客户域可以指正在运行的虚拟机或可用于启动虚拟机的配置。连接对象提供方法来枚举客户域、创建新的客户域和管理现有的域。客户域由 virDomain 类的一个实例表示，并具有多个唯一标识符。 唯一标识符 ID：正整数，在一台主机上运行的客户域中唯一，非活动域没有 ID。 name：短字符串，在一台主机上的所有客户域（包括运行中和非活动域）中都是唯一的。为确保管理程序之间的最大可移植性，建议名称只包含字母数字（a - Z，0 - 9）、连字符 (-) 和下划线 (_) 字符。 UUID：16 个无符号字节，保证在任何主机上的所有客户域中都是唯一的。RFC 4122 定义了 UUID 的格式，并提供了一个保证唯一性的推荐算法用于生成 UUID。 客户域可以是瞬态的或持久的。瞬态客户域只能在主机上运行时进行管理。一旦它被关闭电源，所有与其相关的痕迹都将消失。持久客户域的配置由 hypervisor 在主机上的数据存储中维护，以实现定义的格式。因此，当持久客户域关闭电源时，仍然可以管理其非活动配置。可以在运行时将瞬态客户域转换为持久客户域，方法是为其定义一个配置。有关使用客户域对象的更多信息，请参阅第 4 章，客户域。 虚拟网络虚拟网络提供了一种连接单个主机内一个或多个客户域的网络设备的方法。虚拟网络可以是： 与宿主保持隔离 允许通过主机操作系统的活动网络接口将流量路由至节点外。这包括对 IPv4 流量应用 NAT 的选项。 虚拟网络由 virNetwork 类的一个实例表示，有两个唯一标识符： 唯一标识符 name：短字符串，在一台主机上的所有虚拟网络（包括运行中和未激活的）中都是唯一的。为了在不同管理程序之间实现最大的可移植性，应用程序只能在名称中使用 a-Z、0-9、-、_ 字符。 UUID：16 个无符号字节，保证在任何主机上的所有虚拟网络中都是唯一的。RFC 4122 定义了 UUID 的格式，并提供了生成具有唯一性的 UUID 的推荐算法。 虚拟网络可以是瞬态的或持久的。瞬态虚拟网络只能在主机上运行时进行管理。一旦将其停止，所有与其相关的痕迹都将消失。持久虚拟网络的配置由 hypervisor 在主机上的数据存储中维护，以实现定义的格式。因此，当持久网络被停止时，仍然可以管理其非活动配置。可以通过为其定义配置来将瞬态网络实时转换为持久网络。 在安装 libvirt 后，每个主机都将获得一个名为 “default” 的虚拟网络实例，该实例为客户提供 DHCP 服务，并允许 NAT 转换的 IP 连接到主机的接口。这项服务对于具有间歇性网络连接的主机特别有用，例如使用无线网络的笔记本电脑。 有关使用虚拟网络对象的详细信息，请参阅第 6 章虚拟网络。 存储池存储池对象提供了一种管理主机上所有类型存储的机制，如本地磁盘、逻辑卷组、iSCSI 目标器、光纤通道 HBA 和本地&#x2F;网络文件系统。存储池指的是可分配形成单个卷的存储量。存储池由 virStoragePool 类的一个实例表示，并有一对唯一标识符。 唯一标识符 name：短字符串，在一台主机上的所有存储池（包括运行中和未激活的）中都是唯一的。为了在不同管理程序之间实现最大的可移植性，应用程序只能在名称中使用 a-Z、0-9、-、_ 字符。 UUID：16 个无符号字节，保证在任何主机上的所有存储池中都是唯一的。RFC 4122 定义了 UUID 的格式，并提供了生成具有唯一性的 UUID 的推荐算法。 存储池可以是瞬时的，也可以是持久的。瞬时存储池只能在主机上运行时进行管理，一旦关闭电源，所有痕迹都将消失（当然，底层物理存储仍然存在！）。持久存储池的配置由管理程序以实施定义的格式保存在主机上的数据存储中。因此，当持久性存储池停用时，仍然可以管理其未激活的配置。通过为临时存储池定义配置，可将其快速转变为持久存储池。 有关使用存储池对象的详细信息，请参阅*第 5 章 “存储池”*。 存储容量存储卷对象提供对池中已分配存储块的管理，无论是磁盘分区、逻辑卷、SCSI&#x2F;iSCSI LUN 还是本地&#x2F;网络文件系统中的文件。 卷分配后，可用于为一个（或多个）虚拟域提供磁盘。卷由 virStorageVol 类的一个实例表示，有三个标识符： 唯一标识符 name：短字符串，在存储池中的所有存储卷中唯一。为了在不同实现之间实现最大的可移植性，应用程序只能在名称中使用 a-Z、0-9、-、_ 字符。即使存储池在主机间共享，也不能保证名称在重启或主机间保持稳定。 密钥：由任意可打印字符组成的不透明字符串，用于唯一标识池中的加密卷。该密钥在重启时和主机间保持稳定。 路径：指向卷的文件系统路径。该路径在一台主机上的所有存储卷中都是唯一的。如果存储池配置了合适的目标路径，卷路径在重启时和主机间都会保持稳定。 有关使用存储卷对象的更多信息，请参阅*第 5.7 节 “存储卷概述”*。 主机设备主机设备提供主机上可用硬件设备的视图。这既包括物理 USB 或 PCI 设备，也包括这些设备提供的逻辑设备，如网卡、磁盘、磁盘控制器、声卡等。设备可以排列成树形结构，以便识别它们之间的关系。主机设备由 virNodeDev 类的一个实例表示，有一个通用标识符，但特定设备类型可能有自己的唯一标识符。 唯一标识符 name：短字符串，在主机上的所有设备中唯一。命名方案由主机操作系统决定。该名称不能保证在重启时保持稳定。 物理设备可以从主机操作系统驱动程序中分离出来，这意味着删除所有相关的逻辑设备，然后分配给访客域。物理设备信息在使用存储和网络 API 确定哪些资源可供配置时也很有用。 驱动模式libvirt 库提供了一个稳定的 API 和 ABI，与任何特定的虚拟化技术解耦。此外，许多 API 都有关联的 XML 模式，被视为稳定 ABI 保证的一部分。在内部，有多个公共 ABI 的实现，每个针对不同的虚拟化技术。每个实现称为一个驱动程序（driver）。当获取 virConnect 类的实例时，应用程序开发者可以提供一个 URI 来确定哪个 hypervisor 驱动程序被激活。 没有两种虚拟化技术具有完全相同的功能。libvirt 的目标不是将应用程序限制在最低公共分母上，因为这将导致 API 功能过于受限。相反，libvirt 试图定义一个与 hypervisor 无关的概念和配置表示，并且可适应未来的扩展。因此，如果两个 hypervisor 实现了类似的功能，libvirt 会为该功能提供统一的控制机制或配置格式。 如果一个 libvirt 驱动程序没有实现特定的 API，那么它将返回一个 VIR_ERR_NO_SUPPORT 错误代码，以便应用程序可以检测到这一情况。还有一个 API 允许应用程序查询 hypervisor 的某些能力，比如支持的客户 ABIs 的类型。 在内部，一个 libvirt 驱动程序会尝试利用针对特定虚拟化技术可用的任何管理通道。对于某些驱动程序，这可能要求 libvirt 直接在被管理的主机上运行，并与本地的 hypervisor 进行通信，而其他驱动程序可能能够通过远程 RPC 服务进行通信。对于没有本地远程通信能力的驱动程序，libvirt 提供了一个通用的安全 RPC 服务。这些内容将在本章后面详细讨论。 Hypervisor 驱动程序 Xen: 开源的 Xen hypervisor，提供半虚拟化和完全虚拟化的机器。一个系统驱动程序运行在 Dom0 主机中，直接与 hypervisor、xenstored 和 xend 通信。示例本地 URI 方案为 xen:///。 QEMU: 基于任何开源 QEMU 的虚拟化技术，包括 KVM。一个特权系统驱动程序运行在主机中管理 QEMU 进程。每个非特权用户帐户也有一个私有的驱动程序实例。示例特权 URI 方案为 qemu:///system，示例非特权 URI 方案为 qemu:///session。 UML (User Mode Linux): 纯粹的半虚拟化技术。一个特权系统驱动程序运行在主机中管理 UML 进程。每个非特权用户帐户也有一个私有的驱动程序实例。示例特权 URI 方案为 uml:///system，示例非特权 URI 方案为 uml:///session。 OpenVZ: 基于容器的虚拟化技术，使用修改后的 Linux 主机内核。一个特权系统驱动程序运行在主机中与 OpenVZ 工具通信。示例特权 URI 方案为 openvz:///system。 LXC: 原生的基于容器的 Linux 虚拟化技术，自 Linux 内核 2.6.25 版本以来可用。一个特权系统驱动程序运行在主机中与内核通信。示例特权 URI 方案为 lxc:///。 Remote: 通用的安全 RPC 服务，与 libvirtd 守护进程通信。使用 TLS、x509 证书、SASL (GSSAPI&#x2F;Kerberos) 和 SSH 隧道进行加密和认证。URI 遵循所需驱动程序的方案，但带有主机名，并在 URI 方案中附加数据传输名称。示例 URI，通过 TLS 通道与 Xen 通信为 xen+tls://somehostname/。通过 SASL 通道与 QEMU 通信为 qemu+tcp:///somehost/system。 Test: 一个模拟驱动程序，提供覆盖所有 libvirt API 的虚拟内存 hypervisor。通过允许运行自动化测试来测试使用 libvirt 的应用程序，而不需要处理实际的 hypervisor。示例默认 URI 方案为 test:///default，示例自定义 URI 方案为 test:///path/to/driver/config.xml。 图 2.1. libvirt 驱动程序架构远程管理虽然许多虚拟化技术提供远程管理能力，但 libvirt 并不假定这一点，并提供了一个专用驱动程序，允许远程管理任何 libvirt hypervisor 驱动程序。该驱动程序具有多种数据传输方式，为数据通信提供了相当的安全性。设计该驱动程序的目的是确保无论是本地与 libvirt 驱动程序通信还是通过 RPC 服务通信，都具有 100% 的功能等效性。 除了 libvirt 包含的本地 RPC 服务外，还有一些用于远程管理的替代方案，这些方案在本文档中不会讨论。libvirt-qpid 项目为 QPid 消息服务提供了一个代理，通过消息总线公开所有 libvirt 管理的对象和操作，这与 libvirt 中的 C API 保持了相当接近的 1 对 1 映射。libvirt-CIM 项目提供了一个 CIM 代理，将 libvirt 对象模型映射到 DMTF 虚拟化架构。 基本用法RPC 服务的服务器端由 libvirtd 守护进程提供，必须在被管理的主机上运行。在默认部署中，该守护进程只会监听本地 UNIX 域套接字上的连接，这仅允许 libvirt 客户端使用 SSH 隧道数据传输。通过配置合适的 x509 证书或 SASL 凭据，可以让 libvirtd 守护进程监听 TCP 套接字，接受直接的、非隧道的客户端连接。 从之前的 libvirt 驱动程序 URI 示例可以看出，对于本地 libvirt 连接，URI 中的主机名字段总是留空。要使用 libvirt RPC 驱动程序，本地 URI 只需进行两处更改。至少需要指定一个主机名，此时 libvirt 将尝试使用直接的 TLS 数据传输。可以通过在 URI 方案中附加数据传输名称来请求替代的数据传输方式。本文档第 3.2.2 节 “远程 URI “ 将详细介绍 URI 格式。 数据传输为了应对各种各样的部署环境，libvirt RPC 服务支持多种数据传输，所有这些传输都可以配置符合行业标准的加密和身份验证功能。 表 2.1. 传输 传输 说明 tls 通过 TLS 协议运行的 TCP 套接字。这是默认的数据传输方式，如果没有明确请求其他方式，将使用端口 16514 上的 TCP 连接。至少需要为服务器配置一个 x509 证书颁发机构并颁发服务器证书。libvirtd 服务器可以选择配置为要求客户端出示 x509 证书作为认证手段。 tcp 通过不使用 TLS 协议的 TCP 套接字进行数据传输。除非启用了 SASL 认证服务并配置了提供加密的插件，否则不应在不受信任的网络上使用此数据传输方式。TCP 连接使用端口 16509。 unix 仅限本地数据传输，允许用户连接到以不同用户账户运行的 libvirtd 守护进程。由于只能在本地计算机上访问，因此未加密。标准套接字名称为 /var/run/libvirt/libvirt-sock，用于完全管理功能；/var/run/libvirt/libvirt-sock-ro，用于仅限于只读操作的套接字。 ssh RPC 数据通过 SSH 连接隧道传输到远程机器。这要求远程机器上安装了 Netcat (nc)，并且 libvirtd 运行时启用了 UNIX 域套接字。建议将 SSH 配置为不需要客户端应用程序输入密码。例如，如果使用 SSH 公钥认证，建议运行 ssh-agent 来缓存密钥凭据。GSSAPI 是 SSH 传输的另一种有用的认证模式，允许使用预初始化的 Kerberos 凭证缓存。 ext 任何能够通过 libvirt 范围之外的方式与远程机器建立连接的外部程序。如果内置的数据传输方式都不令人满意，这允许应用程序提供一个辅助程序，通过自定义通道代理 RPC 数据。 认证方案为了应对各种各样的部署环境，libvirt RPC 服务在其数据传输上支持多种身份验证方案，并具有符合行业标准的加密和身份验证功能。身份验证方案的选择由管理员在 /etc/libvirt/libvirtd.conf 文件中配置。 表 2.2. 计划 计划 说明 sasl SASL 是一个行业标准，用于可插拔的认证机制。每个插件具有多种功能，其优劣讨论超出了本文档的范围。对于 TLS 数据传输，有多种插件可供选择，因为 TLS 为网络通道提供数据加密。对于 TCP 数据传输，libvirt 会拒绝使用任何不支持数据加密的插件，这实际上将选择限制为 GSSAPI&#x2F;Kerberos。如果需要对本地用户进行强认证，也可以选择在 UNIX 域套接字数据传输上启用 SASL。 polkit PolicyKit 是一种适用于本地桌面虚拟化部署的认证方案，仅用于 UNIX 域套接字数据传输。它使 libvirtd 守护进程能够验证客户端应用程序是否在本地 X 桌面会话中运行。可以配置为自动允许已登录用户访问，或提示他们输入自己的密码，或超级用户（root）的密码。 x509 TLS 数据传输虽然不是严格意义上的身份验证方案，但可以配置为强制使用客户端 x509 证书。然后，服务器可将客户端区分名称列入白名单，以控制访问。 生成 TLS 证书Libvirt 支持用于验证服务器和客户端身份的 TLS 证书。其中涉及两种不同的检查： 客户端通过匹配服务器发送的证书和服务器的主机名来检查是否连接到正确的服务器。可以通过添加 ?no_verify=1 来禁用这种检查。详情请参阅*表 3.3 “远程 URI 的附加参数”*。 服务器会进行检查，确保只有允许的客户端才能连接。这可以通过： 客户的 IP 地址。 客户 IP 地址和客户证书。 可使用 libvirtd.conf 文件启用或禁用服务器检查。要进行全面的证书检查，您需要为服务器和所有客户端配备由公认的证书颁发机构（CA）签发的证书。为了避免从商业 CA 获取证书的费用，可以选择建立自己的 CA，并告知服务器和客户端信任由自己的 CA 签发的证书。为此，请按照下一节中的说明进行操作。 请注意，libvirtd.conf 的默认配置允许任何客户端进行连接，只要它们拥有 CA 为其 IP 地址签发的有效证书。根据您的要求，这一设置可能需要更多或更少的许可。 公钥基础设施的设置表 2.3. 公钥设置 地点 机器 说明 必填字段 &#x2F;etc&#x2F;pki&#x2F;CA&#x2F;cacert.pem 已在服务器上安装 服务器私钥 不适用 &#x2F;etc&#x2F;pki&#x2F;libvirt&#x2F;servercert.pem 已在服务器上安装 由 CA 签发的服务器证书 通用名 (CN) 必须是客户端看到的服务器主机名 &#x2F;etc&#x2F;pki&#x2F;libvirt&#x2F;private&#x2F;clientkey.pem 安装在客户端 客户的私人密钥 不适用 &#x2F;etc&#x2F;pki&#x2F;CA&#x2F;cacert.pem 安装在客户端 由 CA 签发的客户证书 可根据访问控制列表检查区分名称 (DN)（tls_allowed_dn_list）","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"1 libvirt概述","slug":"libvirt文档/1-libvirt概述","date":"2024-03-01T07:50:26.000Z","updated":"2025-03-15T08:00:16.606Z","comments":true,"path":"libvirt文档/1-libvirt概述/","permalink":"https://watsonlu6.github.io/libvirt%E6%96%87%E6%A1%A3/1-libvirt%E6%A6%82%E8%BF%B0/","excerpt":"","text":"Libvirt 概述Libvirt 是一个独立于管理程序的虚拟化 API 和工具包，能够与一系列操作系统的虚拟化功能进行交互。它是 GNU 宽通用公共许可证下的自由软件。本章介绍 libvirt 并定义本指南中的常用术语。 概述Libvirt 提供了一个通用、通用和稳定的层，用于安全地管理节点上的域。由于节点可能位于远程位置，libvirt 在管理程序支持这些操作的范围内，提供了配置、创建、修改、监控、控制、迁移和停止域所需的所有方法。虽然 libvirt 可以同时访问多个节点，但其方法仅限于单节点操作。Libvirt 设计用于在多个虚拟化环境中运行，这意味着更多的通用功能将作为方法和函数提供。因此，可能无法提供某些特定功能。例如，它不提供高级虚拟化策略或多节点管理功能，如负载平衡。但是，方法的稳定性确保了这些功能可以在 libvirt 上实现。为了保持这种稳定性，libvirt 试图将应用程序与虚拟化框架底层的频繁变更隔离开来。Libvirt 旨在作为更高级管理工具和应用程序的构建模块，侧重于单个节点的虚拟化，唯一的例外是多节点功能之间的域迁移。它提供了枚举、监控和使用受管节点上可用资源的方法，包括 CPU、内存、存储、网络和非统一内存访问（NUMA）分区。虽然管理节点可以与管理程序位于不同的物理机上，但这只能通过安全协议来实现。本指南无意作为整个 Python libvirt 接口的完整参考。本指南中涉及的函数、类和方法列表只是部分内容，远远不够完整。在安装了 libvirt-python 模块的任何主机（大多数主机）上都可以找到完整的参考资料。在任何命令行终端运行 pydoc libvirt 命令即可获得。 术语表为避免对本指南中使用的术语和概念产生歧义，请参阅下表了解其定义。 表 1.1. 术语 术语 定义 Domain 在管理程序提供的虚拟机上运行的操作系统（或容器虚拟化情况下的子系统）实例。 Hypervisor 允许在一组虚拟机中虚拟化节点的软件层，这些虚拟机的配置可能与节点本身不同。 Node 单个物理服务器。节点可以是多种不同类型中的任何一种，通常按其主要用途来称呼。例如，存储节点、集群节点和数据库节点。 Storage Pool 存储介质（如物理硬盘驱动器）的集合。存储池被细分为更小的容器，称为卷，然后可分配给一个或多个域。 Volume 从存储池中分配的存储空间。卷可分配给一个或多个域使用，通常在域内用作虚拟硬盘驱动器。","categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"}]},{"title":"Cpeh PG常见故障处理","slug":"Storage/Ceph/Ceph-PG常见故障处理","date":"2022-10-08T06:53:31.000Z","updated":"2024-09-08T09:47:49.358Z","comments":true,"path":"Storage/Ceph/Ceph-PG常见故障处理/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-PG%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/","excerpt":"","text":"PG故障排除PG 永远无法变干净如果在创建集群后，任何 PG 保持在 active 状态、active+remapped 状态或 active+degraded 状态，并且永远无法达到 active+clean 状态，那么可能是配置出现了问题。在这种情况下，可能需要查看 Pool、PG 和 CRUSH 配置参考的设置，并进行适当调整。一般来说，运行集群时应使用多个 OSD，并且池的大小应大于两个对象副本。 单节点集群 Ceph 不再提供单节点操作的文档。按定义，分布式计算系统不应在单节点上运行。在包含 Ceph 守护进程的单节点上安装客户端内核模块可能会由于 Linux 内核本身的问题而导致死锁（除非使用虚拟机作为客户端）。尽管有上述限制，您仍可以在单节点配置中实验 Ceph。 要在单节点上创建集群，必须在创建监视器和 OSD 之前，将 Ceph 配置文件中的 osd_crush_chooseleaf_type 设置从默认的 1（表示主机或节点）更改为 0（表示 OSD）。这告诉 Ceph 允许在同一主机上放置另一个 OSD。如果您尝试设置单节点集群，并且 osd_crush_chooseleaf_type 大于 0，Ceph 将尝试将一个 OSD 的 PG 与另一个 OSD 的 PG 放置在另一个节点、机箱、机架、行或数据中心中，具体取决于设置。 提示 不要将内核客户端直接安装在与 Ceph 存储集群相同的节点上。这样可能会产生内核冲突。然而，您可以在单节点上通过虚拟机 (VMs) 安装内核客户端。 如果您使用单个磁盘创建 OSD，则必须先手动创建数据目录。 OSDs 少于副本 如果两个 OSD 处于 up 和 in 状态，但 PG 并未处于 active + clean 状态，可能是 osd_pool_default_size 设置大于 2。 解决这种情况有几种方法。如果您想以 active + degraded 状态运行集群并保持两个副本，您可以将 osd_pool_default_min_size 设置为 2，这样可以在 active + degraded 状态下写入对象。您也可以将 osd_pool_default_size 设置为 2，这样只会有两个存储副本（原始副本和一个副本）。在这种情况下，集群应达到 active + clean 状态。 注意 您可以在集群运行时进行更改。如果您在 Ceph 配置文件中进行更改，可能需要重新启动集群。 POOL SIZE &#x3D; 1 如果将 osd_pool_default_size 设置为 1，则对象只有一个副本。OSDs 依赖其他 OSDs 来告诉它们应该拥有哪些对象。如果一个 OSD 有一个对象副本而没有第二个副本，那么就没有第二个 OSD 来告诉第一个 OSD 它应该拥有这个副本。对于映射到第一个 OSD 的每个 PG（请参见 ceph pg dump），您可以通过运行以下命令来强制第一个 OSD 注意它需要的 PG： 1ceph osd force-create-pg &lt;pgid&gt; CRUSH MAP 错误 如果集群中的任何 PG 不干净，则可能是 CRUSH 映射中存在错误。 PG卡住PG 进入“degraded”或“peering”状态在组件故障后是正常的。这些状态通常反映了故障恢复过程中的预期进展。然而，若一个 PG 长时间停留在这些状态中，可能是更大问题的迹象。因此，Ceph 监视器会在 PG “卡住”在非最佳状态时发出警告。具体检查的状态包括： inactive - PG 长时间未处于活跃状态（即无法处理读写请求）。 unclean - PG 长时间未处于干净状态（即无法完全从先前的故障中恢复）。 stale - PG 状态未被 ceph-osd 更新。这表明存储该 PG 的所有节点可能都已宕机。通过运行以下命令来列出卡住的PG： 123ceph pg dump_stuck staleceph pg dump_stuck inactiveceph pg dump_stuck unclean 卡住的 stale PG 通常表明关键的 ceph-osd 守护进程未运行。 卡住的 inactive PG 通常表明存在 peering 问题（见 PG Down - Peering Failure）。 卡住的 unclean PG 通常表明某些因素阻止了恢复完成，可能是未找到的对象（见 Unfound Objects）。 PG 下线 - 对等连接失败在某些情况下，ceph-osd peering 过程可能会遇到问题，导致 PG 无法变为活跃和可用。在这种情况下，运行 ceph health detail 命令会报告类似以下内容： 1234567ceph health detailHEALTH_ERR 7 pgs degraded; 12 pgs down; 12 pgs peering; 1 pgs recovering; 6 pgs stuck unclean; 114/3300 degraded (3.455%); 1/3 in osds are down...pg 0.5 is down+peeringpg 1.4 is down+peering...osd.1 is down since epoch 69, last address 192.168.106.220:6801/8651 查询集群以确定为什么 PG 被标记为 down，可以运行如下命令： 123456789101112131415161718192021222324ceph pg 0.5 query&#123; &quot;state&quot;: &quot;down+peering&quot;, ... &quot;recovery_state&quot;: [ &#123; &quot;name&quot;: &quot;Started\\/Primary\\/Peering\\/GetInfo&quot;, &quot;enter_time&quot;: &quot;2012-03-06 14:40:16.169679&quot;, &quot;requested_info_from&quot;: []&#125;, &#123; &quot;name&quot;: &quot;Started\\/Primary\\/Peering&quot;, &quot;enter_time&quot;: &quot;2012-03-06 14:40:16.169659&quot;, &quot;probing_osds&quot;: [ 0, 1], &quot;blocked&quot;: &quot;peering is blocked due to down osds&quot;, &quot;down_osds_we_would_probe&quot;: [ 1], &quot;peering_blocked_by&quot;: [ &#123; &quot;osd&quot;: 1, &quot;current_lost_at&quot;: 0, &quot;comment&quot;: &quot;starting or marking this osd lost may let us proceed&quot;&#125;]&#125;, &#123; &quot;name&quot;: &quot;Started&quot;, &quot;enter_time&quot;: &quot;2012-03-06 14:40:16.169513&quot;&#125; ]&#125; recovery_state 部分告诉我们 peering 被阻止是由于 ceph-osd 守护进程宕机，具体是 osd.1。在这种情况下，我们可以启动该 ceph-osd，恢复将继续进行。或者，如果 osd.1 发生灾难性故障（例如磁盘故障），可以告知集群该 OSD 已丢失，并指示集群尽可能应对。 重要告知集群一个 OSD 已丢失是危险的，因为集群不能保证其他副本的数据是一致和最新的。要报告 OSD 丢失并指示 Ceph 继续尝试恢复，可以运行如下命令： 1ceph osd lost 1 恢复将继续进行。 未找到的对象在某些故障组合下，Ceph 可能会报告未找到的对象，例如： 123ceph health detailHEALTH_WARN 1 pgs degraded; 78/3778 unfound (2.065%)pg 2.4 is active+degraded, 78 unfound 这意味着存储集群知道一些对象（或现有对象的新副本）存在，但没有找到它们的副本。以下是这种情况可能发生的示例：一个 PG 的数据在两个 OSD 上，我们称它们为“1”和“2”： OSD 1 发生故障。 OSD 2 单独处理一些写操作。 OSD 1 重新上线。 OSD 1 和 OSD 2 重新进行 peering，OSD 1 上缺失的对象被排队等待恢复。 在新的对象被复制之前，OSD 2 发生故障。此时，OSD 1 知道这些对象存在，但没有活跃的 ceph-osd 拥有这些对象的副本。在这种情况下，对这些对象的 IO 请求将被阻塞，集群希望故障节点尽快恢复。这被认为比直接返回 IO 错误给用户更可取。 注意上述情况是将 size=2 设置在复制池和 m=1 设置在纠删码池时可能导致数据丢失的原因之一。 通过运行以下命令来识别哪些对象未找到： 1234567891011121314151617181920212223242526272829303132333435ceph pg 2.4 list_unfound [starting offset, in json]&#123; &quot;num_missing&quot;: 1, &quot;num_unfound&quot;: 1, &quot;objects&quot;: [ &#123; &quot;oid&quot;: &#123; &quot;oid&quot;: &quot;object&quot;, &quot;key&quot;: &quot;&quot;, &quot;snapid&quot;: -2, &quot;hash&quot;: 2249616407, &quot;max&quot;: 0, &quot;pool&quot;: 2, &quot;namespace&quot;: &quot;&quot; &#125;, &quot;need&quot;: &quot;43&#x27;251&quot;, &quot;have&quot;: &quot;0&#x27;0&quot;, &quot;flags&quot;: &quot;none&quot;, &quot;clean_regions&quot;: &quot;clean_offsets: [], clean_omap: 0, new_object: 1&quot;, &quot;locations&quot;: [ &quot;0(3)&quot;, &quot;4(2)&quot; ] &#125; ], &quot;state&quot;: &quot;NotRecovering&quot;, &quot;available_might_have_unfound&quot;: true, &quot;might_have_unfound&quot;: [ &#123; &quot;osd&quot;: &quot;2(4)&quot;, &quot;status&quot;: &quot;osd is down&quot; &#125; ], &quot;more&quot;: false&#125; 如果单次结果中列出的对象过多，more 字段将为 true，你可以查询更多信息。（最终命令行工具将会隐藏这一点，但现在还没有。）接下来，你可以识别哪些 OSD 已被探测或可能包含数据。在列表的末尾（在 more: false 之前），might_have_unfound 是在 available_might_have_unfound 为 true 时提供的。这相当于 ceph pg #.# query 的输出。它可以避免直接使用 query。提供的 might_have_unfound 信息与 query 的输出方式相同，仅不同的是，状态为“已探测”的 OSD 会被忽略。查询的使用： 1234567ceph pg 2.4 query&quot;recovery_state&quot;: [ &#123; &quot;name&quot;: &quot;Started\\/Primary\\/Active&quot;, &quot;enter_time&quot;: &quot;2012-03-06 15:15:46.713212&quot;, &quot;might_have_unfound&quot;: [ &#123; &quot;osd&quot;: 1, &quot;status&quot;: &quot;osd is down&quot;&#125;]&#125; 在这种情况下，集群知道 osd.1 可能有数据，但它已宕机。以下是可能的状态范围： 已探测 正在查询 OSD 已宕机 尚未查询 有时集群需要一些时间来查询可能的位置。可能还有其他未列出的对象可能存在的位置。例如：如果一个 OSD 停止并从集群中移除，然后集群完全恢复，然后通过随后的故障集群最终出现未找到的对象，集群将忽略已移除的 OSD。（然而，这种情况不太可能发生。） 如果所有可能的位置都已被查询，且对象仍然丢失，你可能需要放弃这些丢失的对象。这仅在发生了不寻常的故障组合，导致集群在写入之前了解了写入操作的情况时才可能发生。要将“未找到”的对象标记为“丢失”，运行以下命令： 1ceph pg 2.5 mark_unfound_lost revert|delete 这里的最后一个参数（revert|delete）指定了集群应如何处理丢失的对象。 delete 选项将使集群完全忘记这些对象。 revert 选项（对于纠删码池不可用）将回滚到对象的先前版本，或（如果它是新对象）完全忘记该对象。使用 revert 时请小心，因为它可能会混淆期望对象存在的应用程序。 无家 PG如果所有具有特定 PG 副本的 OSD 都发生故障，那么包含这些 PG 的对象存储子集将变得不可用，监视器将无法接收到这些 PG 的状态更新。监视器会将主 OSD 已故障的任何 PG 标记为 stale。例如： 12ceph healthHEALTH_WARN 24 pgs stale; 3/300 in osds are down 通过运行以下命令来识别哪些 PG 是 stale 的，以及最后一个存储这些 stale PG 的 OSD 是哪些： 12345678ceph health detailHEALTH_WARN 24 pgs stale; 3/300 in osds are down...pg 2.5 is stuck stale+active+remapped, last acting [2,0]...osd.10 is down since epoch 23, last address 192.168.106.220:6800/11080osd.11 is down since epoch 13, last address 192.168.106.220:6803/11539osd.12 is down since epoch 24, last address 192.168.106.220:6806/11861 此输出表明 PG 2.5（pg 2.5）最后由 osd.0 和 osd.2 管理。重启这些 OSD 以允许集群恢复该 PG。 只有少数 OSD 接收数据如果集群中的只有少数节点在接收数据，请检查池中的 PG 数量，如 PG 文档中所述。由于 PG 会在涉及将集群中的 PG 数量除以集群中 OSD 数量的操作中映射到 OSD，因此在这种操作中，少量的 PG（余数）有时不会在集群中分布。在这种情况下，创建一个 PG 数量是 OSD 数量倍数的池。有关详细信息，请参见 PG。有关如何更改用于确定每个池分配多少 PG 的默认值的说明，请参见 Pool、PG 和 CRUSH 配置参考。 无法写入数据如果集群正常运行，但一些 OSD 已经关闭且无法写入数据，请确保在池中运行了最小数量的 OSD。如果池中没有运行最小数量的 OSD，Ceph 不会允许你向其写入数据，因为无法保证 Ceph 可以复制你的数据。有关详细信息，请参见 Pool、PG 和 CRUSH 配置参考中的 osd_pool_default_min_size。 PG 状态不一致如果命令 ceph health detail 返回 active + clean + inconsistent 状态，这可能表示在清理过程中发生了错误。通过运行以下命令来识别不一致的 PG： 1234$ ceph health detailHEALTH_ERR 1 pgs inconsistent; 2 scrub errorspg 0.6 is active+clean+inconsistent, acting [0,1,2]2 scrub errors 或者，如果你希望以编程方式检查输出，可以运行以下命令： 12$ rados list-inconsistent-pg rbd[&quot;0.6&quot;] 在最坏的情况下，我们可能会在多个对象中发现不同的不一致。如果 PG 0.6 中名为 foo 的对象被截断，rados list-inconsistent-pg rbd 的输出可能类似于： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950rados list-inconsistent-obj 0.6 --format=json-pretty&#123; &quot;epoch&quot;: 14, &quot;inconsistents&quot;: [ &#123; &quot;object&quot;: &#123; &quot;name&quot;: &quot;foo&quot;, &quot;nspace&quot;: &quot;&quot;, &quot;locator&quot;: &quot;&quot;, &quot;snap&quot;: &quot;head&quot;, &quot;version&quot;: 1 &#125;, &quot;errors&quot;: [ &quot;data_digest_mismatch&quot;, &quot;size_mismatch&quot; ], &quot;union_shard_errors&quot;: [ &quot;data_digest_mismatch_info&quot;, &quot;size_mismatch_info&quot; ], &quot;selected_object_info&quot;: &quot;0:602f83fe:::foo:head(16&#x27;1 client.4110.0:1 dirty|data_digest|omap_digest s 968 uv 1 dd e978e67f od ffffffff alloc_hint [0 0 0])&quot;, &quot;shards&quot;: [ &#123; &quot;osd&quot;: 0, &quot;errors&quot;: [], &quot;size&quot;: 968, &quot;omap_digest&quot;: &quot;0xffffffff&quot;, &quot;data_digest&quot;: &quot;0xe978e67f&quot; &#125;, &#123; &quot;osd&quot;: 1, &quot;errors&quot;: [], &quot;size&quot;: 968, &quot;omap_digest&quot;: &quot;0xffffffff&quot;, &quot;data_digest&quot;: &quot;0xe978e67f&quot; &#125;, &#123; &quot;osd&quot;: 2, &quot;errors&quot;: [ &quot;data_digest_mismatch_info&quot;, &quot;size_mismatch_info&quot; ], &quot;size&quot;: 0, &quot;omap_digest&quot;: &quot;0xffffffff&quot;, &quot;data_digest&quot;: &quot;0xffffffff&quot; &#125; ] &#125; ]&#125; 在这种情况下，输出表示以下内容： 唯一的不一致对象名为 foo，其头部存在不一致。 不一致分为两类： errors：这些错误表示分片之间的不一致，没有指示哪些分片有问题。检查 shards 数组中的错误，以确定问题所在。 data_digest_mismatch：从 OSD.2 读取的副本摘要与从 OSD.0 和 OSD.1 读取的副本摘要不同。 size_mismatch：从 OSD.2 读取的副本大小为 0，但 OSD.0 和 OSD.1 报告的大小为 968。 union_shard_errors：分片数组中所有特定分片错误的联合。这些错误包括 read_error 和其他类似错误。以 oi 结尾的错误表示与 selected_object_info 进行比较。检查 shards 数组以确定哪些分片存在哪些错误。 data_digest_mismatch_info：对象信息中存储的摘要不是 0xffffffff，这是从 OSD.2 读取的分片计算出的。 size_mismatch_info：对象信息中存储的大小与从 OSD.2 读取的大小不同，后者为 0。 警告：如果 read_error 列在分片的错误属性中，可能是由于物理存储错误导致的不一致。在这种情况下，请检查该 OSD 使用的存储。 在尝试修复驱动器之前，请检查 dmesg 和 smartctl 的输出。 要修复不一致的 PG，运行如下命令： 1ceph pg repair &#123;placement-group-ID&#125; 例如： 1ceph pg repair 1.4 注意：PG ID 形式为 N.xxxxx，其中 N 是包含 PG 的池的编号。命令 ceph osd listpools 和 ceph osd dump | grep pool 返回池编号的列表。 如果由于时钟偏差而定期收到 active + clean + inconsistent 状态，请考虑配置监视主机上的 NTP 守护进程以充当对等体。有关更多信息，请参见《网络时间协议和 Ceph 时钟设置》。 关于 PG 修复的更多信息Ceph 存储并更新集群中存储对象的校验和。当在 PG 上执行清理时，主 OSD 尝试从其副本中选择一个权威副本。只有一个可能的情况是一致的。执行深度清理后，Ceph 计算从磁盘读取的每个对象的校验和，并将其与先前记录的校验和进行比较。如果当前校验和与先前记录的校验和不匹配，则该不匹配被视为不一致。在复制池的情况下，任何副本的校验和与权威副本的校验和之间的任何不匹配都意味着存在不一致。发现这些不一致会导致 PG 状态被设置为不一致。 pg repair 命令尝试修复各种类型的不一致。当 pg repair 发现不一致的 PG 时，它尝试用权威副本的摘要覆盖不一致副本的摘要。当 pg repair 在复制池中发现不一致副本时，它将不一致副本标记为丢失。在复制池的情况下，恢复超出了 pg repair 的范围。 对于编码和 BlueStore 池，Ceph 会自动执行修复，如果 osd_scrub_auto_repair（默认值为 false）设置为 true 并且发现的错误不超过 osd_scrub_auto_repair_num_errors（默认值为 5）。 pg repair 命令不会解决所有问题。Ceph 不会自动修复发现有不一致的 PG。 RADOS 对象或 omap 的校验和并不总是可用。校验和是逐步计算的。如果一个复制对象非顺序更新，涉及的写入操作会更改对象并使其校验和无效。在重新计算校验和时不会读取整个对象。即使校验和不可用，pg repair 命令也能够进行修复，如 Filestore 中的情况。使用复制 Filestore 池的用户可能会更倾向于手动修复，而不是使用 ceph pg repair。 该材料适用于 Filestore，但不适用于 BlueStore，后者具有自己的内部校验和。匹配记录的校验和和计算的校验和不能证明任何特定副本实际上是权威的。如果没有校验和可用，pg repair 倾向于主数据，但这可能不是未损坏的副本。因此，在发现不一致时需要人工干预。这种干预有时涉及使用 ceph-objectstore-tool。 编码池 PG 不是 active+clean如果 CRUSH 无法找到足够的 OSD 映射到 PG，它将显示为 2147483647，这是 ITEM_NONE 或未找到 OSD。例如： 1[2,1,6,0,5,8,2147483647,7,4] OSD 数量不足 如果 Ceph 集群只有八个 OSD，而一个编码池需要九个 OSD，则集群会显示“OSD 不足”。在这种情况下，你可以创建 另一个需要更少 OSD 的编码池，运行如下命令： 12ceph osd erasure-code-profile set myprofile k=5 m=3ceph osd pool create erasurepool erasure myprofile 或者添加新的 OSD，PG 将自动使用它们。 CRUSH 约束无法满足如果集群中有足够的 OSD，可能是 CRUSH 规则施加了无法满足的约束。如果两个主机上有十个 OSD，而 CRUSH 规则要求同一主机上的两个 OSD 不得在同一 PG 中使用，则映射可能失败，因为只会找到两个 OSD。通过显示（“转储”）规则来检查约束，如下所示： 12345678910111213141516ceph osd crush rule ls[ &quot;replicated_rule&quot;, &quot;erasurepool&quot;]$ ceph osd crush rule dump erasurepool&#123; &quot;rule_id&quot;: 1, &quot;rule_name&quot;: &quot;erasurepool&quot;, &quot;type&quot;: 3, &quot;steps&quot;: [ &#123; &quot;op&quot;: &quot;take&quot;, &quot;item&quot;: -1, &quot;item_name&quot;: &quot;default&quot;&#125;, &#123; &quot;op&quot;: &quot;chooseleaf_indep&quot;, &quot;num&quot;: 0, &quot;type&quot;: &quot;host&quot;&#125;, &#123; &quot;op&quot;: &quot;emit&quot;&#125;]&#125; 通过运行以下命令解决此问题： 12ceph osd erasure-code-profile set myprofile crush-failure-domain=osdceph osd pool create erasurepool erasure myprofile CRUSH 过早放弃如果 Ceph 集群只有足够的 OSD 来映射 PG（例如，总共九个 OSD 的集群和每个 PG 需要九个 OSD 的编码池），则可能 CRUSH 在找到映射之前就放弃了。可以通过以下方式解决此问题： 降低编码池的要求，以使用更少的 OSD（这需要创建另一个池，因为编码配置文件不能动态修改）。 向集群中添加更多 OSD（这不需要修改编码池，因为它会自动变干净）。 使用手动创建的 CRUSH 规则，尝试更多次找到合适的映射。可以通过设置 set_choose_tries 为大于默认值的值来修改现有的 CRUSH 规则。 首先，通过在从集群中提取 crushmap 后使用 crushtool 验证问题。这可以确保你的实验不会修改 Ceph 集群，只在本地文件上操作： 123456789101112131415161718192021ceph osd crush rule dump erasurepool&#123; &quot;rule_id&quot;: 1, &quot;rule_name&quot;: &quot;erasurepool&quot;, &quot;type&quot;: 3, &quot;steps&quot;: [ &#123; &quot;op&quot;: &quot;take&quot;, &quot;item&quot;: -1, &quot;item_name&quot;: &quot;default&quot;&#125;, &#123; &quot;op&quot;: &quot;chooseleaf_indep&quot;, &quot;num&quot;: 0, &quot;type&quot;: &quot;host&quot;&#125;, &#123; &quot;op&quot;: &quot;emit&quot;&#125;]&#125;$ ceph osd getcrushmap &gt; crush.mapgot crush map from osdmap epoch 13$ crushtool -i crush.map --test --show-bad-mappings \\ --rule 1 \\ --num-rep 9 \\ --min-x 1 --max-x $((1024 * 1024))bad mapping rule 8 x 43 num_rep 9 result [3,2,7,1,2147483647,8,5,6,0]bad mapping rule 8 x 79 num_rep 9 result [6,0,2,1,4,7,2147483647,5,8]bad mapping rule 8 x 173 num_rep 9 result [0,4,6,8,2,1,3,7,2147483647] 在这里，--num-rep 是编码规则需要的 OSD 数量，--rule 是 ceph osd crush rule dump 显示的 rule_id 值。此测试将尝试映射一百万个值（在此示例中，范围定义为 [–min-x,–max-x]），并且必须显示至少一个错误映射。如果此测试没有输出任何内容，则表示所有映射都成功，你可以确定集群中的问题不是由于坏映射造成的。 更改 set_choose_tries 的值将 CRUSH 映射解压以编辑 CRUSH 规则，运行以下命令： 1crushtool --decompile crush.map &gt; crush.txt 在规则中添加以下行： 1step set_choose_tries 100 crush.txt 文件的相关部分将类似于： 123456789rule erasurepool &#123; id 1 type erasure step set_chooseleaf_tries 5 step set_choose_tries 100 step take default step chooseleaf indep 0 type host step emit&#125; 重新编译并重新测试 CRUSH 规则： 1crushtool --compile crush.txt -o better-crush.map 当所有映射成功时，通过使用 crushtool 命令的 --show-choose-tries 选项显示找到所有映射所需的尝试次数的直方图，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849crushtool -i better-crush.map --test --show-bad-mappings \\ --show-choose-tries \\ --rule 1 \\ --num-rep 9 \\ --min-x 1 --max-x $((1024 * 1024)) ... 11: 42 12: 44 13: 54 14: 45 15: 35 16: 34 17: 30 18: 25 19: 19 20: 22 21: 20 22: 17 23: 13 24: 16 25: 13 26: 11 27: 11 28: 13 29: 11 30: 10 31: 6 32: 5 33: 10 34: 3 35: 7 36: 5 37: 2 38: 5 39: 5 40: 2 41: 5 42: 4 43: 1 44: 2 45: 2 46: 3 47: 1 48: 0 ... 102: 0 103: 1 104: 0 ... 这个输出表示映射 42 个 PG 需要 11 次尝试，映射 44 个 PG 需要 12 次尝试等。最大尝试次数是防止坏映射的 set_choose_tries 的最小值（例如，上述输出中的 103，因为没有超过 103 次尝试的 PG 被映射）。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Cpeh OSD常见故障处理","slug":"Storage/Ceph/Ceph-OSD常见故障处理","date":"2022-10-07T06:53:31.000Z","updated":"2024-09-08T14:58:28.932Z","comments":true,"path":"Storage/Ceph/Ceph-OSD常见故障处理/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-OSD%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/","excerpt":"","text":"OSD故障排除在故障排除集群的 OSD 之前，请先检查监视器和网络。首先，确定监视器是否有法定人数。运行 ceph health 命令或 ceph -s 命令，如果 Ceph 显示 HEALTH_OK，则表示有监视器法定人数。如果监视器没有法定人数或监视器状态出现错误，请在继续之前处理监视器问题。接下来，检查网络以确保其正常运行。网络对 OSD 的操作和性能有显著影响。检查主机端是否有丢包，并检查交换机端是否有 CRC 错误。 获取 OSD 数据在故障排除 OSD 时，收集关于 OSD 的不同信息非常有用。一些信息来自于对 OSD 的监控（例如，运行 ceph osd tree 命令）。额外的信息涉及到集群的拓扑结构，以下部分将讨论这些内容。 Ceph 日志 Ceph 的日志文件存储在 /var/log/ceph 下。除非路径已被更改（或者您在存储日志位置不同的容器化环境中），否则可以通过以下命令列出日志文件： 1ls /var/log/ceph 如果日志详情不够，请调整日志级别。要确保 Ceph 在高日志量下表现良好，请参阅“日志记录和调试”。 管理员套接字使用管理员套接字工具来检索运行时信息。首先，列出 Ceph 守护进程的套接字，通过以下命令： 1ls /var/run/ceph 接下来，运行如下命令（将 &#123;daemon-name&#125; 替换为特定守护进程的名称，例如 osd.0）： 1ceph daemon &#123;daemon-name&#125; help 或者，指定 &#123;socket-file&#125;（“套接字文件”是 /var/run/ceph 中的特定文件）运行命令： 1ceph daemon &#123;socket-file&#125; help 管理员套接字可以完成许多任务，包括： 列出 Ceph 配置运行时 转储历史操作 转储操作优先级队列状态 转储正在进行的操作 转储性能计数器 显示空闲空间可能会出现文件系统问题。要显示文件系统的空闲空间，请运行以下命令： 1df -h 要查看此命令支持的语法和选项，请运行 df --help。 I&#x2F;O 统计iostat 工具可用于识别 I&#x2F;O 相关的问题。运行以下命令： 1iostat -x 诊断信息要从内核中检索诊断信息，请运行 dmesg 命令，并使用 less、more、grep 或 tail 进行指定输出。例如： 1dmesg | grep scsi 停止而不重新平衡有时可能需要对集群的子集进行维护或解决影响故障域（例如机架）的问题。然而，当停止 OSD 进行维护时，可能希望防止 CRUSH 自动重新平衡集群。为避免这种重新平衡行为，可以通过运行以下命令将集群设置为 noout 状态： 1ceph osd set noout 警告这更多是一个思考练习，用于让读者理解故障域和 CRUSH 行为，而不是建议在 Luminous 版本后的环境中运行 ceph osd set noout。当 OSD 返回到正常状态时，重新平衡将恢复，ceph osd set noout 命令引入的更改将被撤销。 然而，在 Luminous 及后续版本中，更安全的做法是只标记受影响的 OSD。要添加或删除特定 OSD 的 noout 标志，可以运行如下命令： 12ceph osd add-noout osd.0ceph osd rm-noout osd.0 也可以标记整个 CRUSH 组。例如，如果计划停用 prod-ceph-data1701 以增加 RAM，可以运行以下命令： 1ceph osd set-group noout prod-ceph-data1701 设置标志后，停止 OSD 和需要维护的故障域内的其他 Ceph 服务： 1systemctl stop ceph\\*.service ceph\\*.target 注意当 OSD 被停止时，该 OSD 内的任何放置组将被标记为降级。维护完成后，需重新启动 OSD 和其他停止的守护进程。然而，如果主机在维护过程中重启，则不需要重新启动，系统会自动恢复。要重新启动 OSD 或其他守护进程，使用如下命令： 1sudo systemctl start ceph.target 最后，根据需要取消 noout 标志，可以运行如下命令： 12ceph osd unset nooutceph osd unset-group noout prod-ceph-data1701 许多现代 Linux 发行版使用 systemd 进行服务管理。然而，对于某些操作系统（尤其是旧版本），可能需要发出等效的服务或启动&#x2F;停止命令。 OSD 未运行在正常情况下，重新启动 ceph-osd 守护进程将允许它重新加入集群并恢复。 OSD 无法启动如果集群已启动，但某个 OSD 无法启动，请检查以下内容： 配置文件：如果您在新的安装中无法启动 OSD，请检查配置文件以确保其符合标准（例如，确保使用的是 host 而不是 hostname 等）。 检查路径：确保配置中指定的路径与实际存在的数据和元数据路径对应（例如，日志、WAL 和 DB 的路径）。将 OSD 数据与元数据分开，以查看配置文件和实际挂载是否存在错误。如果有，这些错误可能解释了为何 OSD 无法启动。要将元数据存储在单独的块设备上，可以对驱动器进行分区或使用 LVM，并为每个 OSD 分配一个分区。 检查最大线程数：如果集群中有一个节点的 OSD 数量特别高，可能会达到默认的最大线程数（通常是 32,000）。这在恢复过程中尤其可能发生。将最大线程数增加到允许的最大线程数（4194303）可能有助于解决问题。要将线程数增加到最大值，请运行以下命令： 1sysctl -w kernel.pid_max=4194303 如果增加线程数解决了问题，您必须通过在 /etc/sysctl.d 文件夹下的文件或在主 /etc/sysctl.conf 文件中包含 kernel.pid_max 设置来使更改永久生效。例如： 1kernel.pid_max = 4194303 **检查 nf_conntrack**：这个连接跟踪和连接限制系统对许多生产 Ceph 集群造成问题。问题往往缓慢而微妙地出现。随着集群拓扑和客户端工作负载的增长，神秘和间歇性的连接失败和性能问题会越来越多，尤其是在一天中的某些时候。为开始评估问题，请检查 syslog 历史中的 “table full” 事件。解决这种问题的一种方法是：首先，使用 sysctl 实用程序将 nf_conntrack_max 设置为更高的值。接下来，将 nf_conntrack_buckets 的值提高到 nf_conntrack_buckets × 8 = nf_conntrack_max；这可能需要在 sysctl 之外运行命令（例如，echo 131072 &gt; /sys/module/nf_conntrack/parameters/hashsize）。另一种解决方法是将相关内核模块列入黑名单，从而完全禁用处理。这种方法强大但脆弱。模块及其列出顺序可能因内核版本而异。即使被列入黑名单，iptables 和 docker 有时仍会激活连接跟踪，因此我们建议对调优参数采用“设置并忘记”的策略。在现代系统中，这种方法不会消耗显著资源。 内核版本：确定正在使用的内核版本和发行版。默认情况下，Ceph 使用的第三方工具可能存在缺陷或与某些发行版或内核版本冲突（例如，Google 的 gperftools 和 TCMalloc）。检查操作系统建议和每个 Ceph 版本的发行说明，以确保您已解决与内核相关的任何问题。 段错误：如果出现段错误，请提高日志级别并重新启动有问题的守护进程。如果段错误重复发生，请在 Ceph bug 跟踪器（https://tracker.ceph.com/projects/ceph）和 dev 及 ceph-users 邮件列表归档（https://ceph.io/resources）中搜索是否有其他人遇到并报告了这些问题。如果这确实是一个新的和独特的故障，请在 dev 邮件列表中发布，并提供以下信息：正在运行的具体 Ceph 版本、ceph.conf（秘密已用 XXX 替换）、监视器状态输出和日志文件摘录。 OSD 失败当 OSD 失败时，表示 ceph-osd 进程无响应或已死亡，相应的 OSD 已被标记为下线。存活的 ceph-osd 守护进程将向监视器报告该 OSD 似乎已经下线，并且 ceph health 命令的输出中将显示新的状态，如下例所示： 12ceph healthHEALTH_WARN 1/3 in osds are down 当有一个或多个 OSD 被标记为下线时，会引发此健康警报。要查看哪些 OSD 已下线，请为命令添加详细信息，如下所示： 123ceph health detailHEALTH_WARN 1/3 in osds are downosd.0 is down since epoch 23, last address 192.168.106.220:6800/11080 或者，运行以下命令： 1ceph osd tree down 如果由于驱动器故障或其他故障导致某个 ceph-osd 守护进程无法工作或重新启动，则其日志文件（位于 /var/log/ceph 下）中应存在错误消息。如果 ceph-osd 守护进程因心跳故障或自杀超时错误而停止，则底层驱动器或文件系统可能无响应。检查 dmesg 输出和 syslog 输出中的驱动器错误或内核错误。可能需要指定某些标志（例如，dmesg -T 以查看人类可读的时间戳）以避免将旧错误误认为新错误。 如果整个主机的 OSD 都下线，请检查是否存在网络错误或主机的硬件问题。如果 OSD 问题是软件错误（例如，断言失败或其他意外错误）的结果，请在 bug 跟踪器、dev 邮件列表归档和 ceph-users 邮件列表归档中搜索是否有问题报告。如果没有明确的修复或现有 bug，请向 ceph-devel 邮件列表报告问题。 没有可用驱动器空间如果 OSD 已满，Ceph 会通过确保不会将新数据写入 OSD 来防止数据丢失。在正常运行的集群中，当集群的 OSD 和池接近某些“满”比率时，会引发健康检查。mon_osd_full_ratio 阈值默认为 0.95（即 95% 的容量）：这是防止客户端写入数据的点。mon_osd_backfillfull_ratio 阈值默认为 0.90（即 90% 的容量）：这是防止开始回填的点。mon_osd_nearfull_ratio 阈值默认为 0.85（即 85% 的容量）：这是引发 OSD_NEARFULL 健康检查的点。集群中的 OSD 在 Ceph 分配的数据量上会有所不同。要通过显示每个 OSD 的数据使用情况来检查“满”状态，请运行以下命令： 1ceph osd df 要通过显示集群的总体数据使用情况和池之间的数据分布来检查“满”状态，请运行以下命令： 1ceph df 在检查 ceph df 命令的输出时，请特别注意最满的 OSD，而不是原始空间使用的百分比。如果单个 OSD 变满，所有对该 OSD 池的写入可能会失败。当 ceph df 报告池的可用空间时，它会考虑相对于池中最满 OSD 的比率设置。为了平衡数据分布，可以采取两种方法：（1）使用 reweight-by-utilization 命令逐步将数据从过满的 OSD 移动到不足满的 OSD，或者（2）在 Luminous 的后续版本及以后的版本中，利用 ceph-mgr balancer 模块自动执行相同任务。要调整“满”比率，请运行如下命令： 123ceph osd set-nearfull-ratio &lt;float[0.0-1.0]&gt;ceph osd set-full-ratio &lt;float[0.0-1.0]&gt;ceph osd set-backfillfull-ratio &lt;float[0.0-1.0]&gt; 有时，集群问题的原因是 OSD 失败。这可能发生在测试过程中，或者因为集群较小、非常满或不平衡。当 OSD 或节点占据集群数据的过高比例时，组件故障或自然增长可能导致接近满和满的比率被超过。在测试 Ceph 对 OSD 故障的恢复能力时，建议保留足够的空闲磁盘空间，并考虑暂时降低 OSD 的满比率、回填满比率和接近满比率。OSD 的“满”状态在 ceph health 命令的输出中可见，如下例所示： 12ceph healthHEALTH_WARN 1 nearfull osd(s) 有关详细信息，请使用详细命令，如下所示： 1234567ceph health detailHEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)osd.3 is full at 97%osd.4 is backfill full at 91%osd.2 is near full at 87% 为解决满集群问题，建议通过添加 OSD 来增加容量。添加新 OSD 允许集群将数据重新分配到新提供的存储空间。查找浪费空间的 rados bench 孤儿对象。 如果旧版 Filestore OSD 由于已满而无法启动，可以通过删除满 OSD 上的一小部分放置组目录来回收空间。 重要 如果您选择在满 OSD 上删除放置组目录，请勿在其他满 OSD 上删除相同的放置组目录。否则，您将丢失数据。您必须在至少一个 OSD 上保留数据的至少一个副本。删除放置组目录是一种罕见且极端的干预，不应轻易进行。 OSD 运行缓慢&#x2F;无响应OSD 有时会运行缓慢或无响应。在排查这个常见问题时，建议在调查 OSD 性能问题之前排除其他可能性。例如，确保网络正常工作，确认 OSD 正在运行，并检查 OSD 是否限制了恢复流量。提示在 Luminous 版本之前，某些 up 和 in 状态的 OSD 有时不可用或运行缓慢，因为恢复中的 OSD 消耗了系统资源。更新版本通过防止这种现象提供了更好的恢复处理。 网络问题 作为分布式存储系统，Ceph 依赖网络进行 OSD 对等和复制、故障恢复以及周期性心跳。网络问题可能导致 OSD 延迟和抖动的 OSD。更多信息请参见“抖动 OSD”。要确保 Ceph 进程和 Ceph 相关进程连接正常并在监听，请运行以下命令： 123netstat -a | grep cephnetstat -l | grep cephsudo netstat -p | grep ceph 要检查网络统计信息，请运行以下命令： 1netstat -s 驱动器配置 SAS 或 SATA 存储驱动器应仅容纳一个 OSD，但 NVMe 驱动器可以轻松容纳两个或更多。然而，如果其他进程共享驱动器，读写吞吐量可能会受到瓶颈。此类进程包括：日志&#x2F;元数据、操作系统、Ceph 监视器、syslog 日志、其他 OSD 和非 Ceph 进程。 由于 Ceph 在日志记录后确认写入，快速 SSD 是加速响应时间的一个有吸引力的选项——特别是在使用 XFS 或 ext4 文件系统作为传统 FileStore OSD 时。相比之下，Btrfs 文件系统可以同时进行写入和日志记录。（然而，不推荐在生产环境中使用 Btrfs。） 注意 对驱动器进行分区不会改变其总吞吐量或顺序读&#x2F;写限制。通过在单独的分区中运行日志，吞吐量可能会有所改善，但更好的做法是将日志运行在单独的物理驱动器中。 警告 Reef 不支持 FileStore。Reef 之后的版本不再支持 FileStore。提到 FileStore 的信息仅适用于 Quincy 版本及 Quincy 之前的版本。 坏扇区&#x2F;碎片化磁盘 检查驱动器是否存在坏块、碎片化和其他可能导致性能显著下降的错误。检查驱动器错误的有用工具包括 dmesg、syslog 日志和 smartctl（在 smartmontools 包中）。 注意 smartmontools 7.0 及更高版本提供 NVMe 状态直通和 JSON 输出。 共存的监视器&#x2F;OSD 尽管监视器是相对轻量的进程，但当监视器与 OSD 运行在同一主机上时，可能会出现性能问题。监视器发出许多 fsync() 调用，这可能干扰其他工作负载。当监视器与 OSD 共存在同一存储驱动器上时，性能问题尤其严重。此外，如果监视器运行的是较旧的内核（3.0 之前）或没有 syncfs(2) 系统调用的内核，那么同一主机上运行的多个 OSD 可能会进行太多提交，从而影响彼此的性能。这种问题有时会导致所谓的“突发写入”。 共存进程 在与 OSD 运行在同一硬件上时，处理写入数据到 Ceph 的进程（例如基于云的解决方案和虚拟机）可能会导致显著的 OSD 延迟。因此，一般不推荐将这些进程与 OSD 共存在同一硬件上。推荐的做法是优化某些主机用于 Ceph，其他主机用于其他进程。这种将 Ceph 操作与其他应用程序分开的做法可能有助于提高性能，并简化故障排除和维护。在同一硬件上运行共存进程有时被称为“融合”。使用 Ceph 时，仅在具备专业知识和经过考虑后再进行融合。 日志级别 高日志级别可能导致性能问题。操作人员有时会提高日志级别以跟踪问题，然后忘记在之后降低它们。在这种情况下，OSD 可能会消耗宝贵的系统资源，将不必要的详细日志写入磁盘。任何希望使用高日志级别的人都应考虑将驱动器挂载到日志的默认路径（例如，&#x2F;var&#x2F;log&#x2F;ceph&#x2F;$cluster-$name.log）。 恢复限制 根据您的配置，Ceph 可能会减少恢复速率以保持客户端或 OSD 性能，或者可能会增加恢复速率到影响客户端或 OSD 性能的程度。检查客户端或 OSD 是否正在恢复。 内核版本 检查您运行的内核版本。较旧的内核可能缺少改进 Ceph 性能的更新。 内核 SyncFS 问题 如果您遇到 SyncFS 的内核问题，请尝试每个主机运行一个 OSD 以查看性能是否提高。旧的内核可能没有足够新的 glibc 版本来支持 syncfs(2)。 文件系统问题 在 Luminous 版本之后，我们建议使用 BlueStore 后端部署集群。当运行 Luminous 版本之前的版本时，或者如果您有特别的理由使用旧的 Filestore 后端，我们建议使用 XFS。 我们不推荐使用 Btrfs 或 ext4。Btrfs 文件系统有许多吸引人的特性，但可能会导致性能问题和虚假的 ENOSPC 错误。由于 xattr 限制破坏了对长对象名称的支持，我们不推荐在 Filestore OSD 中使用 ext4。 内存不足 我们建议每个 OSD 守护进程至少配备 4GB 内存，并建议将其增加到 6GB 或 8GB。在正常操作期间，您可能会注意到 ceph-osd 进程只使用了其中的一小部分。您可能会被诱使将多余的内存用于共存应用程序，或减少每个节点的内存容量。然而，当 OSD 正在恢复时，其内存使用会急剧增加。如果在恢复过程中没有足够的内存，OSD 性能会显著下降，守护进程可能会崩溃或被 Linux OOM Killer 杀死。 请求阻塞或请求缓慢 当 ceph-osd 守护进程对请求响应缓慢时，集群日志会收到报告操作处理时间过长的消息。警告阈值默认为 30 秒，可以通过 osd_op_complaint_time 设置进行配置。 旧版 Ceph 报告旧请求： 1osd.0 192.168.106.220:6800/18813 312 : [WRN] old request osd_op(client.5099.0:790 fatty_26485_object789 [write 0~4096] 2.5e54f643) v4 received at 2012-03-06 15:42:56.054801 currently waiting for sub ops 新版 Ceph 报告慢请求： 12&#123;date&#125; &#123;osd.num&#125; [WRN] 1 slow requests, 1 included below; oldest blocked for &gt; 30.005692 secs&#123;date&#125; &#123;osd.num&#125; [WRN] slow request 30.005692 seconds old, received at &#123;date-time&#125;: osd_op(client.4240.0:8 benchmark_data_ceph-1_39426_object7 [write 0~4194304] 0.69848840) v4 currently waiting for subops from [610] 可能的原因包括： 驱动器故障（检查 dmesg 输出） 内核文件系统中的错误（检查 dmesg 输出） 集群过载（检查系统负载、iostat 等） ceph-osd 守护进程中的错误 可能的解决方案： 从 Ceph 主机中移除虚拟机 升级内核 升级 Ceph 重启 OSD 更换故障或有问题的组件 调试缓慢请求 如果您运行 ceph daemon osd.&lt;id&gt; dump_historic_ops 或 ceph daemon osd.&lt;id&gt; dump_ops_in_flight，您将看到一组操作和每个操作经历的事件列表。这些事件简要描述如下。 来自 Messenger 层的事件： header_read: Messenger 开始从网络读取消息的时间。 throttled: Messenger 尝试获取内存节流空间以将消息读入内存的时间。 all_read: Messenger 完成从网络读取消息的时间。 dispatched: Messenger 将消息交给 OSD 的时间。 initiated: 这与 header_read 相同。存在这两个事件是历史上的异常。 来自 OSD 处理操作的事件: - queued_for_pg: 操作已被放入队列等待 PG 处理。 - reached_pg: PG 开始执行操作。 - waiting for *: 操作在等待其他工作完成后才能继续（例如，新 OSDMap；对象目标的检查；PG 的对等完成；这些都在消息中指定）。 - started: 操作已被接受为 OSD 应执行的任务，并且正在执行中。 - waiting for subops from: 操作已被发送到副本 OSD。 来自 Filestore 的事件： - commit_queued_for_journal_write: 操作已交给 FileStore。 - write_thread_in_journal_buffer: 操作在日志的缓冲区中，等待持久化（作为下一次磁盘写入）。 - journaled_completion_queued: 操作已被写入日志，回调已排队等待调用。 来自 OSD 在数据已交给底层存储后的事件： - op_commit: 操作已由主 OSD 提交（即，写入日志）。 - op_applied: 操作已写入主 OSD 的后台文件系统（即，在内存中应用但未刷新到磁盘）。 - sub_op_applied: 副本的 op_applied。 - sub_op_committed: 副本的 op_commit（仅对 EC 池）。 - sub_op_commit_rec/sub_op_apply_rec from &lt;X&gt;: 主 OSD 在听到上述消息后进行标记，但针对特定副本（即 ）。 - commit_sent: 我们向客户端（或主 OSD，针对子操作）发送了回复。 虽然一些事件可能看起来冗余，但它们跨越了内部代码中的重要边界（例如，跨锁将数据传递到新线程中）。 OSD抖动“抖动”是指 OSD 被快速重复标记为上线然后下线的现象。本节解释如何识别抖动以及如何减轻它。 当 OSD 进行对等和检查心跳时，它们使用集群（后端）网络。如果您的 OSD 节点有两个网络端口，将一个端口专用于公共网络，另一个端口专用于私有网络，可以避免网络维护和网络故障对集群或客户端造成的重大影响。在这种情况下，可以考虑将两个链接仅用于公共网络：使用绑定（LACP）或等成本路由（例如 FRR），可以获得更高的吞吐量容差、容错能力和减少 OSD 抖动。 当私有网络（甚至单个主机链接）故障或降级时，而公共网络正常运行，OSD 可能无法很好地处理这种情况。在这种情况下，OSD 使用公共网络向监视器报告彼此故障，同时将自己标记为上线。然后，监视器再次通过公共网络发送更新的集群地图，将受影响的 OSD 标记为下线。这些 OSD 向监视器回复“我还没死！”，然后循环重复。我们称这种情况为“抖动”，它可能很难隔离和修复。没有私有网络时，这种恼人的动态被避免了：OSD 通常要么上线要么下线，没有抖动。 如果某些原因导致 OSD “抖动”（被反复标记为下线然后再上线），您可以通过暂时冻结其状态来强制监视器停止抖动： 12ceph osd set noup # 防止 OSD 被标记为上线ceph osd set nodown # 防止 OSD 被标记为下线 这些标志记录在 osdmap 中： 12ceph osd dump | grep flagsflags no-up,no-down 您可以使用以下命令清除这些标志： 12ceph osd unset noupceph osd unset nodown 还有两个其他标志 noin 和 noout，它们分别防止启动的 OSD 被标记为分配数据（in）或保护 OSD 被标记为最终移除（out），无论 mon_osd_down_out_interval 的当前值如何。 注意 noup、noout 和 nodown 是临时性的，因为清除标志后，它们阻止的操作应该很快能够恢复。但是 noin 标志防止 OSD 在启动时被标记为在线，任何在标志设置期间启动的守护进程将保持这种状态。 注意 通过仔细调整 mon_osd_down_out_subtree_limit、mon_osd_reporter_subtree_level 和 mon_osd_min_down_reporters 可以在一定程度上缓解抖动的原因和效果。最佳设置的推导取决于集群大小、拓扑结构和使用的 Ceph 版本。这些因素的交互很微妙，超出了本文档的范围。 由 Ceph 基金会提供 Ceph 文档是由非营利组织 Ceph 基金会资助和托管的社区资源。如果您想支持我们以及其他努力，请考虑立即加入。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Cpeh Mon常见故障处理","slug":"Storage/Ceph/Ceph-Mon常见故障处理","date":"2022-10-06T06:53:31.000Z","updated":"2024-09-08T09:47:41.847Z","comments":true,"path":"Storage/Ceph/Ceph-Mon常见故障处理/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-Mon%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/","excerpt":"","text":"监视器故障排查即使集群出现与监视器相关的问题，集群也不一定会面临宕机的风险。即使集群丢失了多个监视器，只要还有足够的监视器存活并能够形成法定人数（quorum），集群就能继续运行。如果集群遇到了监视器相关的问题，可以参考以下的故障排查信息。 初步故障排查Ceph 监视器故障排查的第一步是确保监视器正在运行，并且它们能够与网络进行通信。按照本节的步骤操作，以排除监视器故障的最简单原因。 确保监视器正在运行确保监视器守护进程（ceph-mon）正在运行。可能是由于升级后监视器没有重新启动。检查这个简单的疏忽可以节省大量的故障排查时间。同时，也要确保管理守护进程（ceph-mgr）正在运行。记住，典型的集群配置中，每个监视器（ceph-mon）都有一个相应的管理器（ceph-mgr）。注意： 在 v1.12.5 之前的版本中，Rook 不会运行超过两个管理器。 确保可以访问监视器节点在某些罕见情况下，iptables 规则可能会阻止访问监视器节点或 TCP 端口。这些规则可能是以前压力测试或规则开发时遗留下来的。要检查是否存在这样的规则，可以通过 SSH 登录每个监视器节点，并使用 telnet、nc 或类似工具尝试连接到其他监视器节点的 tcp&#x2F;3300 和 tcp&#x2F;6789 端口。 确保“ceph status”命令能够运行并从集群接收回复如果 ceph status 命令从集群接收到回复，说明集群正在运行。监视器只有在形成法定人数时才会响应状态请求。确认是否有一个或多个管理器（mgr）守护进程正在运行。在没有任何问题的集群中，ceph status 将报告所有管理器守护进程都在运行。如果 ceph status 命令未能从集群接收到回复，那么很可能是由于没有足够的监视器来形成法定人数。如果运行 ceph -s 命令时未指定进一步的选项，它会连接到任意选择的一个监视器。然而，在某些情况下，添加 -m 标志来连接特定的监视器（或按顺序连接几个特定的监视器）可能更有帮助，例如：ceph status -m mymon1。 如果以上解决方案未能解决问题，可能需要逐一检查每个监视器。即使没有形成法定人数，仍然可以单独联系每个监视器，并使用 ceph tell mon.ID mon_status 命令请求其状态（此处 ID 是监视器的标识符）。对集群中的每个监视器运行 ceph tell mon.ID mon_status 命令。关于此命令的输出。还有另一种联系各个监视器的方法：通过 SSH 登录每个监视器节点并查询守护进程的管理套接字。 使用监视器的管理套接字监视器的管理套接字允许通过 Unix 套接字文件直接与特定的守护进程交互。此套接字文件位于监视器的运行目录中。管理套接字的默认目录是 /var/run/ceph/ceph-mon.ID.asok。可以覆盖管理套接字的默认位置。如果默认位置被覆盖，那么管理套接字会出现在其他位置。这种情况通常发生在集群的守护进程部署在容器中时。要查找管理套接字的目录，请检查ceph.conf 文件以获取备用路径，或运行以下命令： 1ceph-conf --name mon.ID --show-config-value admin_socket 管理套接字只有在监视器守护进程运行时可用。每次监视器正常关闭时，管理套接字都会被移除。如果监视器未运行但管理套接字仍存在，可能是由于监视器未正确关闭。如果监视器未运行，将无法使用管理套接字，并且 ceph 命令很可能返回错误 111：连接被拒绝。 要访问管理套接字，请运行以下形式的 ceph tell 命令： 1ceph tell mon.&lt;id&gt; mon_status 此命令通过管理套接字将帮助命令传递给指定的运行中的监视器守护进程 &lt;id&gt;。如果知道管理套接字文件的完整路径，可以更直接地运行以下命令： 1ceph --admin-daemon &lt;full_path_to_asok_file&gt; &lt;command&gt; 运行 ceph help 显示通过管理套接字可用的所有受支持的命令。特别参考 config get、config show、mon stat 和 quorum_status。 理解 mon_status监视器的状态（由 ceph tell mon.X mon_status 命令报告）可以通过管理套接字获得。ceph tell mon.X mon_status 命令输出关于监视器的大量信息（包括在 quorum_status 命令输出中找到的信息）。 注意命令 ceph tell mon.X mon_status 不应被字面输入。运行命令时，mon.X 的 X 部分应替换为 Ceph 集群中的特定值。 为了理解此命令的输出，考虑以下例子，看到 ceph tell mon.c mon_status 的输出： 123456789101112131415161718192021222324&#123; &quot;name&quot;: &quot;c&quot;, &quot;rank&quot;: 2, &quot;state&quot;: &quot;peon&quot;, &quot;election_epoch&quot;: 38, &quot;quorum&quot;: [ 1, 2], &quot;outside_quorum&quot;: [], &quot;extra_probe_peers&quot;: [], &quot;sync_provider&quot;: [], &quot;monmap&quot;: &#123; &quot;epoch&quot;: 3, &quot;fsid&quot;: &quot;5c4e9d53-e2e1-478a-8061-f543f8be4cf8&quot;, &quot;modified&quot;: &quot;2013-10-30 04:12:01.945629&quot;, &quot;created&quot;: &quot;2013-10-29 14:14:41.914786&quot;, &quot;mons&quot;: [ &#123; &quot;rank&quot;: 0, &quot;name&quot;: &quot;a&quot;, &quot;addr&quot;: &quot;127.0.0.1:6789/0&quot;&#125;, &#123; &quot;rank&quot;: 1, &quot;name&quot;: &quot;b&quot;, &quot;addr&quot;: &quot;127.0.0.1:6790/0&quot;&#125;, &#123; &quot;rank&quot;: 2, &quot;name&quot;: &quot;c&quot;, &quot;addr&quot;: &quot;127.0.0.1:6795/0&quot;&#125;]&#125;&#125; 该输出报告 monmap 中有三个监视器（a, b, 和 c），法定人数由两个监视器组成，而 c 是 peon。 哪个监视器不在法定人数中？答案是 a（即 mon.a）。mon.a 不在法定人数中。 我们如何知道在此示例中 mon.a 不在法定人数中？我们知道 mon.a 不在法定人数中，因为它的 rank 是 0，而 rank 为 0 的监视器根据定义不在法定人数中。如果我们检查法定人数集，可以看到集合中明显有两个监视器：1 和 2。但这些不是监视器名称，而是当前 monmap 中确立的监视器 rank。法定人数集不包括 rank 为 0 的监视器，根据 monmap，该监视器是 mon.a。 监视器的 rank 是如何确定的？每当监视器被添加或从集群中移除时，监视器的 rank 会被计算（或重新计算）。rank 的计算遵循一个简单的规则：IP:PORT 组合越大，rank 越低。在此情况下，因为 127.0.0.1:6789（mon.a）在数值上小于其他两个 IP:PORT 组合（即 “监视器 b”的 127.0.0.1:6790 和 “监视器 c”的 127.0.0.1:6795），mon.a 拥有最高的 rank，即 rank 0。 最常见的监视器问题集群有法定人数但至少有一个监视器关闭当集群有法定人数但至少有一个监视器关闭时，ceph health detail 会返回类似以下的消息： 123$ ceph health detail[snip]mon.a (rank 0) addr 127.0.0.1:6789/0 is down (out of quorum) 如何排查 Ceph 集群有法定人数但至少有一个监视器关闭的问题？确保 mon.a 正在运行。确保可以从其他监视器节点连接到 mon.a 的节点。也检查 TCP 端口。检查所有节点上的 iptables 和 nf_conntrack，并确保未丢弃&#x2F;拒绝连接。 如果这些初步故障排查无法解决问题，那么需要进一步调查。 首先，通过管理套接字检查有问题的监视器的 mon_status，如“使用监视器的管理套接字”和“理解 mon_status”中所述。如果监视器不在法定人数中，则其状态将是以下之一：probing（探测）、electing（选举中）或 synchronizing（同步中）。如果监视器的状态是 leader（领导者）或 peon（跟随者），则监视器认为自己在法定人数中，但集群的其余部分认为它不在法定人数中。排查过程中，可能处于 probing、electing 或 synchronizing 状态的监视器已进入法定人数。再次检查 ceph status 以确定排查过程中监视器是否已进入法定人数。如果监视器仍未进入法定人数，则继续参考本文件中的相关调查。 监视器状态为 probing 是什么意思？如果 ceph health detail 显示监视器的状态为 probing，则该监视器仍在寻找其他监视器。每个监视器启动时都会在这个状态停留一段时间。当监视器连接到 monmap 中指定的其他监视器时，它就不再处于 probing 状态。监视器处于 probing 状态的时间取决于其所在集群的参数。例如，当监视器是单监视器集群的一部分时（在生产环境中绝对不要这样做），监视器几乎是瞬间通过 probing 状态。在多监视器集群中，监视器保持在 probing 状态，直到找到足够的监视器形成法定人数——这意味着如果集群中的三个监视器中有两个关闭，则剩下的一个监视器将无限期地保持在 probing 状态，直到您启动其他监视器之一。 如果已建立法定人数，则只要守护进程能够被访问，监视器守护进程应能够快速找到其他监视器。如果监视器卡在 probing 状态，并且您已经完成了上面描述的监视器之间通信的故障排查，那么可能是有问题的监视器尝试以错误地址连接其他监视器。mon_status 会输出监视器已知的 monmap：确定 monmap 中指定的其他监视器的位置是否与网络中监视器的位置匹配。如果不匹配，请参阅“恢复监视器的损坏 monmap”。如果 monmap 中指定的监视器位置与网络中的监视器位置匹配，则持久的 probing 状态可能与监视器节点之间严重的时钟偏差有关。 监视器的状态为“electing”时意味着什么？如果 ceph health detail 显示某个监视器的状态为“electing”，这表明该监视器正在进行选举。选举通常会很快完成，但有时监视器可能会陷入所谓的选举风暴。如果选举状态持续存在，可以将有问题的监视器置于停机状态以进行调查。这只有在集群中有足够的存活监视器以形成仲裁的情况下才可行。 监视器的状态为“synchronizing”时意味着什么？如果 ceph health detail 显示某个监视器的状态为“synchronizing”，这意味着该监视器正在与集群的其他部分同步，以便加入仲裁。监视器与仲裁的其余部分同步所需的时间取决于集群监视器存储的大小、集群的规模以及集群的状态。通常较大且已降级的集群会使监视器在“synchronizing”状态停留的时间比较小且新建的集群更长。如果监视器的状态在“synchronizing”和“electing”之间来回切换，这表明可能存在问题：集群状态可能正在快速推进（即生成新映射），而同步过程无法跟上新映射的生成速度。这个问题在 Cuttlefish 版本之前更为常见，因为同步过程在较新的版本中已经被重构和增强，以避免这种情况。如果您在较新的版本中遇到此问题，请在 Ceph 错误追踪系统中报告问题。准备并提供日志以支持您报告的任何错误。有关日志准备的信息，请参阅《日志准备》。 监视器的状态为“leader”或“peon”时意味着什么？在 Ceph 正常运行期间，当集群处于 HEALTH_OK 状态时，Ceph 集群中的一个监视器处于“leader”状态，其余的监视器处于“peon”状态。可以通过查看命令 ceph tell &lt;mon_name&gt; mon_status 返回的 state 键的值来确定给定监视器的状态。如果 ceph health detail 显示监视器处于“leader”状态或“peon”状态，很可能存在时钟偏移。请遵循《时钟偏移》中的说明。如果您已经按照这些说明进行操作，但 ceph health detail 仍然显示监视器处于“leader”状态或“peon”状态，请在 Ceph 错误追踪系统中报告问题。如果您提出问题，请提供日志以支持它。有关日志准备的信息，请参阅《日志准备》。 修复监视器的损坏“monmap”可以使用类似 ceph tell mon.c mon_status 的命令来检索 monmap。下面是一个 monmap 的示例： 1234567epoch 3fsid 5c4e9d53-e2e1-478a-8061-f543f8be4cf8last_changed 2013-10-30 04:12:01.945629created 2013-10-29 14:14:41.9147860: 127.0.0.1:6789/0 mon.a1: 127.0.0.1:6790/0 mon.b2: 127.0.0.1:6795/0 mon.c 这个 monmap 是正常的，但您可能的 monmap 可能不正常。在某个节点上的 monmap 可能会因为节点长时间宕机，而期间集群的监视器发生了变化，从而变得过时。 更新监视器过时的 monmap 有两种方法： 报废并重新部署监视器仅在确保不会丢失所报废监视器中保留的信息时使用此方法。确保有其他状态良好的监视器，以便新监视器能够与存活的监视器同步。请记住，如果没有其他监视器内容的副本，销毁监视器可能会导致数据丢失。 将 monmap 注入监视器可以通过从集群中存活的监视器中检索最新的 monmap 并将其注入到损坏或缺失 monmap 的监视器中来修复它。实施此解决方案请执行以下步骤： 按以下方式之一检索 monmap： 如果存在监视器仲裁：从仲裁中检索 monmap： 1ceph mon getmap -o /tmp/monmap 如果没有监视器仲裁： 直接从已停止的监视器中检索 monmap： 1ceph-mon -i ID-FOO --extract-monmap /tmp/monmap 在此示例中，已停止监视器的 ID 是 ID-FOO。 停止将注入 monmap 的监视器： 1service ceph -a stop mon.&#123;mon-id&#125; 将 monmap 注入已停止的监视器： 1ceph-mon -i ID --inject-monmap /tmp/monmap 启动监视器。 警告将 monmap 注入监视器可能会引起严重问题。注入 monmap 会覆盖监视器上存储的最新 monmap。请小心操作！ 时钟偏移Paxos 共识算法需要紧密的时间同步，这意味着仲裁中的监视器之间的时钟偏移会对监视器的操作产生严重影响，导致一些令人困惑的行为。为避免这种问题，应在监视器节点上运行时钟同步工具，例如 Chrony 或传统的 ntpd 工具。配置每个监视器节点时确保启用了 iburst 选项，并确保每个监视器有多个对等节点，包括以下内容： 其他监视器 内部 NTP 服务器 多个外部公共池服务器 注意iburst 选项在初始同步时会发送八个数据包，而不是通常的一个。此外，建议将集群中的所有节点与内部和外部服务器同步，甚至与监视器同步。请在物理机上运行 NTP 服务器，因为虚拟机的虚拟化时钟不适合稳定的时间保持。 时钟偏移问题及解答容忍的最大时钟偏移是多少？默认情况下，监视器允许时钟最大漂移 0.05 秒（50 毫秒）。 我可以增加最大容忍的时钟偏移吗？可以，但我们强烈建议不要这样做。最大容忍的时钟偏移可通过 mon-clock-drift-allowed 选项进行配置，但更改此选项几乎肯定是一个糟糕的决定。设定的时钟偏移上限是因为时钟不同步的监视器不可靠。当前的默认值已证明其在监视器遇到严重问题之前提醒用户方面的有效性。更改此值可能会对监视器的稳定性和整体集群健康状况造成不可预见的影响。 我如何知道是否存在时钟偏移？当存在时钟偏移时，监视器会通过集群状态 HEALTH_WARN 发出警告。执行 ceph health detail 和 ceph status 命令时，输出类似如下内容： 1mon.c addr 10.10.0.1:6789/0 clock skew 0.08235s &gt; max 0.05s (latency 0.0045s) 在此示例中，监视器 mon.c 被标记为存在时钟偏移。在 Luminous 及更高版本中，可以通过运行 ceph time-sync-status 命令来检查时钟偏移。注意，主监视器通常具有数值最低的 IP 地址。它始终显示 0：其他监视器报告的偏移是相对于主监视器的，而不是任何外部参考源。 如果存在时钟偏移，我该怎么办？同步时钟。使用 NTP 客户端可能会有所帮助。但是，如果已经在使用 NTP 客户端并且仍然遇到时钟偏移问题，请确定使用的 NTP 服务器是否位于网络之外，还是托管在网络中。托管自己的 NTP 服务器往往可以缓解时钟偏移问题。 客户端无法连接或挂载如果客户端无法连接到集群或挂载，请检查您的 iptables。一些操作系统安装程序会向 iptables 添加一个 REJECT 规则。iptables 规则会拒绝所有尝试连接到主机的客户端（除了 ssh）。如果您的监视器主机的 iptables 具有 REJECT 规则，则从单独节点连接的客户端会失败，并引发超时错误。检查 iptables 规则，看看是否有任何拒绝尝试连接到 Ceph 守护进程的客户端。例如： 1REJECT all -- anywhere anywhere reject-with icmp-host-prohibited 可能还需要在 Ceph 主机上的 iptables 添加规则，以确保客户端能够访问与 Ceph 监视器（默认：端口 6789）和 Ceph OSD（默认：6800 到 7568）关联的 TCP 端口。例如： 1iptables -A INPUT -m multiport -p tcp -s &#123;ip-address&#125;/&#123;netmask&#125; --dports 6789,6800:7568 -j ACCEPT 监视器存储故障存储损坏的症状Ceph 监视器在键值存储中维护集群地图。如果键值存储损坏导致监视器失败，则监视器日志可能包含以下错误消息之一： 1Corruption: error in middle of record 或者： 1Corruption: 1 missing files; e.g.: /var/lib/ceph/mon/mon.foo/store.db/1234567.ldb 使用健康的监视器恢复如果集群中有幸存的监视器，损坏的监视器可以用新的监视器替换。新监视器启动后，它会与健康的对等体同步。新监视器完全同步后，将能够为客户端提供服务。 使用 OSDs 进行恢复即使所有监视器同时失效，也可以使用存储在 OSDs 中的信息恢复监视器存储。建议在 Ceph 集群中部署至少三个（最好是五个）监视器。在这种部署中，完全的监视器故障是不太可能的。然而，如果数据中心在磁盘设置或文件系统设置配置不当的情况下发生意外断电，可能会导致底层文件系统故障，这可能会导致所有监视器故障。在这种情况下，OSDs 中的数据可用于恢复监视器。以下是可以在这种情况下使用的脚本来恢复监视器： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546ms=/root/mon-storemkdir $ms# collect the cluster map from stopped OSDsfor host in $hosts; do rsync -avz $ms/. user@$host:$ms.remote rm -rf $ms ssh user@$host &lt;&lt;EOF for osd in /var/lib/ceph/osd/ceph-*; do ceph-objectstore-tool --data-path \\$osd --no-mon-config --op update-mon-db --mon-store-path $ms.remote doneEOF rsync -avz user@$host:$ms.remote/. $msdone# rebuild the monitor store from the collected map, if the cluster does not# use cephx authentication, we can skip the following steps to update the# keyring with the caps, and there is no need to pass the &quot;--keyring&quot; option.# i.e. just use &quot;ceph-monstore-tool $ms rebuild&quot; insteadceph-authtool /path/to/admin.keyring -n mon. \\ --cap mon &#x27;allow *&#x27;ceph-authtool /path/to/admin.keyring -n client.admin \\ --cap mon &#x27;allow *&#x27; --cap osd &#x27;allow *&#x27; --cap mds &#x27;allow *&#x27;# add one or more ceph-mgr&#x27;s key to the keyring. in this case, an encoded key# for mgr.x is added, you can find the encoded key in# /etc/ceph/$&#123;cluster&#125;.$&#123;mgr_name&#125;.keyring on the machine where ceph-mgr is# deployedceph-authtool /path/to/admin.keyring --add-key &#x27;AQDN8kBe9PLWARAAZwxXMr+n85SBYbSlLcZnMA==&#x27; -n mgr.x \\ --cap mon &#x27;allow profile mgr&#x27; --cap osd &#x27;allow *&#x27; --cap mds &#x27;allow *&#x27;# If your monitors&#x27; ids are not sorted by ip address, please specify them in order.# For example. if mon &#x27;a&#x27; is 10.0.0.3, mon &#x27;b&#x27; is 10.0.0.2, and mon &#x27;c&#x27; is 10.0.0.4,# please passing &quot;--mon-ids b a c&quot;.# In addition, if your monitors&#x27; ids are not single characters like &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, please# specify them in the command line by passing them as arguments of the &quot;--mon-ids&quot;# option. if you are not sure, please check your ceph.conf to see if there is any# sections named like &#x27;[mon.foo]&#x27;. don&#x27;t pass the &quot;--mon-ids&quot; option, if you are# using DNS SRV for looking up monitors.ceph-monstore-tool $ms rebuild -- --keyring /path/to/admin.keyring --mon-ids alpha beta gamma# make a backup of the corrupted store.db just in case! repeat for# all monitors.mv /var/lib/ceph/mon/mon.foo/store.db /var/lib/ceph/mon/mon.foo/store.db.corrupted# move rebuild store.db into place. repeat for all monitors.mv $ms/store.db /var/lib/ceph/mon/mon.foo/store.dbchown -R ceph:ceph /var/lib/ceph/mon/mon.foo/store.db 该脚本执行以下步骤： 从每个 OSD 主机收集地图。 重建存储。 用适当的权限填充 keyring 文件中的实体。 用恢复的副本替换 mon.foo 上的损坏存储。 已知限制上述恢复工具无法恢复以下信息： 某些添加的 keyring：所有使用 ceph auth add 命令添加的 OSD keyring 都会从 OSD 的副本中恢复，并且使用 ceph-monstore-tool 导入 client.admin keyring。但是，MDS keyring 和其他所有 keyring 都会在恢复的监视器存储中缺失，可能需要手动重新添加。 创建池：如果任何 RADOS 池正在创建过程中，则该状态会丢失。恢复工具假设所有池都已创建。如果恢复后部分创建的池中有 PG 卡在未知状态，可以运行 ceph osd force-create-pg 命令强制创建空 PG。只有当你确定池是空的时才采取此操作。 MDS 映射：MDS 映射会丢失。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph Cache Tier源码实现","slug":"Storage/Ceph/Ceph-Cache-Tier源码实现","date":"2022-09-28T12:45:54.000Z","updated":"2024-09-01T13:34:00.917Z","comments":true,"path":"Storage/Ceph/Ceph-Cache-Tier源码实现/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-Cache-Tier%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Cache Tier架构Ceph存储集群如果采用廉价的PC和传统的机械硬盘进行搭建，磁盘的访问速度受到了一定的限制，无法达到理想的IOPS性能水平。为了优化系统的IO性能，可以考虑添加快速的存储设备作为缓存，以减少数据的访问延时。其中，Cache Tier分层存储机制是一种常见的解决方案，在Ceph服务端缓存中被广泛使用，可以有效提升后端存储层的I&#x2F;O性能。Cache Tier需要创建一个由高速且昂贵的存储设备（如SSD）组成的存储池作为缓存层，以及一个相对廉价的设备组成的后端存储池作为经济存储层。缓存层使用多副本模式，存储层可以使用多副本或纠删码模式。Ceph的缓存分层理论基础是数据存在热点，数据访问不均匀。通常，80%的应用只访问20%的数据，这20%的数据被称为热点数据。为了减少响应时间，可以将热点数据保存到性能较高的存储设备（如固态硬盘）中。在Cache Tiering中，有一个分层代理，当保存在缓存层的数据变冷或不再活跃时，该代理会将这些数据刷到存储层，并将其从缓存层中移除。这种操作称为刷新或逐出。在客户端读写数据时，Ceph的对象处理器负责决定对象存储的位置，而Cache Tier则决定何时将缓存层中的对象刷回后端存储层。对于写操作，请求到达缓存层后，完成写操作后直接应答客户端，之后由缓存层的代理线程负责将数据写入存储层。对于读操作，如果命中缓存层，直接在缓存层读取，否则可以重定向到存储层访问。如果数据近期有访问过，说明比较热，可以提升到缓存层中。对于Ceph客户端来说，缓存层和后端存储层是完全透明的。所有Ceph客户端都可以使用缓存层，因此Cache Tier具有提升块设备、Ceph对象存储、Ceph文件系统和原生绑定的I&#x2F;O性能的潜力。 Ceph Cache tier处理流程使用命令add-cache 可以将一个cachepool作为base pool的tier。这时会设置pool的信息，在pool里面记录了cache pool和base pool的关系。客户端在获取pool信息的时候可知，目标base pool存在一个tier，叫做cache pool，那么操作base pool的请求都会发送给cache pool。请求达到cache pool中时，作为tier的pool会有一些特别的处理maybe_cache_handle，具体的流程如下图： 判断操作的object是否在cache pool中命中，如果命中，则直接在cache pool中处理，和在普通pool的请求一样处理。后续会有agent线程将缓存脏数据刷写到base pool中。 没有命中缓存的情况下，才会去判断缓存模式。如果命中缓存，不管是什么模式都会在cache pool中处理。下面的处理都是未命中缓存的情况。 判断是否是writeback模式，读操作，如果可以proxy_read，那就直接do_proxy_read读取数据即可，不可以proxy_read 就使用do_cache_redirect，告诉客户端去base pool中读取。写操作，如果当前是evict_full模式，说明现在缓存中已经达到了阈值，需要等待缓存淘汰一些object，在完成写操作，目前放在等待队列中等待，如果不是evict_full模式，则需要从base pool中promote对应的object到cache pool中，promote结束后继续处理本次的写操作。 判断是否是forward模式。在forward模式下，不再在cachepool中处理请求，会告诉客户端将请求全部发送到base pool中。 判断是不是readonly模式。写操作会告诉客户端直接想base pool写即可，如果是读操作，则会从base pool中promote该object。 判断是不是readforward模式。该模式读操作全部都告诉客户端直接去base pool中读取即可，写操作按着writeback模式处理。 判断是不是readproxy模式。该模式读操作都采用cachepool的proxy read方法，写操作按着writeback模式处理。 针对其中涉及到的几个封装好的方法的操作： do_cache_redirect， do_proxy_read， do_proxy_write，promote_object do_cache_redirect ：客户端请求cache pool，cache pool告诉客户端你应该去base pool中请求，客户端收到应答后，再次发送请求到base pool中请求数据，由base pool告诉客户端请求完成。 do_proxy_read：客户端发送读请求到cache pool，但是未命中，则cache pool自己会发送请求到base pool中，获取数据后，由cache pool将数据发送给客户端，完成读请求。但是值得注意的是，虽然cache pool读取到了该object，但不会保存在cache pool中，下次请求仍然需要调用函数promote_objectbasePool读取该对象请求，然后写入cachePool中。 do_proxy_write：直接写数据到basePool中，同样，cachePool中并没有该数据对象，还需要后续调用promote_object函数把数据对象从basePool中读到cachePool中。 promote_object：当客户端发送请求到cache pool中，但是cache pool未命中，cache pool会选择将该object从base pool中提升到cache pool中，然后在cache pool进行读写操作，操作完成后告知客户端请求完成，在cache pool会缓存该object，下次直接在cache中处理，和proxy_read存在的区别。构造PromoteCallback回调函数，然后调用函数start_copyk拷贝函数。 无论是 Proxy Read 还是 Promote Object 操作最终都是调用了 objecter 的 read 方法来从base storage层读取对象数据 Cache Tier数据结构由于 Tier cache 在 Ceph 中的存在形式是存储池，pg_pool_t保存了存储池的相关属性。(src&#x2F;osd&#x2F;osd_type.h&#x2F;struct pg_pool_t) 123456789101112131415161718set&lt;uint64_t&gt; tiers; //如果当前pool是一个basePool，tiers就记录改basepool的cachePool层，一个base pool可以设置多个cachePoolint64_t tier_of; //如果当前pool是一个cachePool，那么tier_of记录了该cachePool的basePoolint64_t read_tier; //设置basePool的读缓存层，根据Ceph不同的Cache Tier模式来设置int64_t write_tier; //设置basePool的写缓存层，根据Ceph不同的Cache Tier模式来设置cache_mode_t cache_mode; //设置Cache Tier模式uint64_t target_max_bytes; //设置了cachePool的最大字节数uint64_t target_max_objects; //设置了cachePool的最大对象数量uint32_t cache_target_dirty_ratio_micro; // 目标脏数据率：当脏数据比例达到这个值，后台 agent 开始 flush 数据uint32_t cache_target_dirty_high_ratio_micro; // 高目标脏数据率：当脏数据比例达到这个值，后台 agent 开始高速 flush 数据uint32_t cache_target_full_ratio_micro; // 数据满的比率：当数据达到这个比例时，认为数据已满，需要进行缓存淘汰uint32_t cache_min_flush_age; // 对象在 cache 中被刷入到 storage 层的最小时间uint32_t cache_min_evict_age; // 对象在 cache 中被淘汰的最小时间HitSet::Params hit_set_params; // HitSet 相关参数uint32_t hit_set_period; // 每间隔 hit_set_period 一段时间，系统重新产生一个新的 hit_set 对象来记录对象的缓存统计信息uint32_t hit_set_count; // 记录系统保存最近的多少个 hit_set 记录bool use_gmt_hitset; // hitset archive 对象的命名规则 uint32_t hit_set_grade_decay_rate; //当前hit_set在对象温度计数上具有最高优先级，后续hit_set的优先级比预hit_set衰减此参数uint32_t hit_set_search_last_n; //为温度累积，最多N次hit_sets 读写IOAdd Cache在 ceph&#x2F;src&#x2F;mon&#x2F;OSDMonitor.cc 中实现了 add-cache 命令，从命令行中获取对应的参数并绑定 Tier 关系 选择 Cache PoolCache Tier的应用主要体现在计算OSD的过程中，通过判断basepool的参数，来决定是否要更新targetpool：读操作时，如果有read_tier，则更新为read_tier pool；写操作时，如果有write_tier，则更新为write_tier pool。read_tier和write_tier与pool是否开启Cache Tier有关。 在 ceph&#x2F;src&#x2F;osdc&#x2F;Objecter.cc&#x2F;Objecter::_calc_target中指定目标存储池为 Cache Pool，设置之后由后续的代码在该 Pool 中执行 Crush 算法。 123456789101112131415161718//首先根据base_oloc.pool获取pool信息，获取pg_pool_t对象 const pg_pool_t *pi = osdmap-&gt;get_pg_pool(t-&gt;base_oloc.pool);// apply tiering 根据读写操作，分别设置需要操作的 tiert-&gt;target_oid = t-&gt;base_oid; #base_oid //读取的对象 #target_oid; //最终读取的目标对象t-&gt;target_oloc = t-&gt;base_oloc; #base_oloc //对象的pool信息 #//target_oloc //最终目标对象的pool信息if ((t-&gt;flags &amp; CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) &#123;//检查cache tier，如果是读操作，并且有读缓存，就设置t-&gt;target_oloc.pool为该pool的read_tier值。if (is_read &amp;&amp; pi-&gt;has_read_tier()) t-&gt;target_oloc.pool = pi-&gt;read_tier; //如果是写操作，并且有写缓存，就设置t-&gt;target_oloc.pool为该pool的write_tier值。if (is_write &amp;&amp; pi-&gt;has_write_tier()) t-&gt;target_oloc.pool = pi-&gt;write_tier;pi = osdmap-&gt;get_pg_pool(t-&gt;target_oloc.pool);if (!pi) &#123; t-&gt;osd = -1; return RECALC_OP_TARGET_POOL_DNE;&#125;&#125; osd 先接收到客户端发送来的请求，然后OSD::dequeue_op()调用 PrimaryLogPG:: do_request()——&gt;PrimaryLogPG::do_op()中处理，这都是正常的一个 pool 处理请求的流程，在 do_op 中来看看不同于其他普通 pool 的处理。如果开启了Cache Tier，将会在do_op中执行以下操作： 首先判断hit_set中是否包含待操作的对象（hit_set-&gt;contains(obc-&gt;obs.oi.soid)），如果不包含，则把对象添加到hit_set中。添加对象后，如果hit_set满了，或者hit_set超时，则调用hit_set_persist()。 执行agent_choose_mode()，设置agent相关参数，如flush_mode、num_objects、num_bytes等。 执行maybe_handle_cache()。这里处理cache执行逻辑。 如果maybe_handle_cache()中调用maybe_handle_cache_detail()，如果成功处理了op请求，则直接return，否则会继续执行后续操作（说明不需要从datapool读取数据或者转发请求到datapool，可以直接在此osd命中查询的对象），由本OSD执行读取操作。 HitSet在 write back&#x2F;read forward&#x2F;read proxy 模式下需要 HitSet 来记录缓存命中。 HitSet 用于跟踪和统计对象的访问行为，记录对象是否存在缓存中。定义了一个缓存查找到抽象接口，目前提供了三种实现方式：ExplicitHashHitSet，ExplicitObjectHitSet，BloomHitSet ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h 定义了抽象接口，同时该头文件中包含了具体的 HitSet 实现 ExplicitHashHitSet ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h&#x2F;class ExplicitHashHitSet 基于对象的 32 位 HASH 值的 set 来记录对象的命中，每个对象占用 4 bytes 内存空间 优点：空间占用相对较少，但需要根据 HASH 进行全局的扫描遍历比较 ExplicitObjectHitSet ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h&#x2F;class ExplicitObjectHitSet 使用一个基于 ceph&#x2F;src&#x2F;common&#x2F;hobject 的 set 来记录对象的命中，占用的内存取决于对象的关键信息的大小 使用内存中缓存数据结构来进行判断带来的优点就是实现相对简单直观，但占用的内存空间相对较大 BloomHitSet ceph&#x2F;src&#x2F;osd&#x2F;HitSet.h&#x2F;class BloomHitSet 采用了压缩的 Bloom Filter 的方式来记录对象是否在缓存中，进一步减少了内存占用空间 Cache Tier的初始化 src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::hit_set_setup()用来创建并初始化HisSet对象 src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_setup()完成agent相关的初始化工作 Cache Pool 请求处理Cache 的相关请求处理可以通过do_op()进行梳理，主要包含了 agent_choose_mode()和 maybe_handle_cache() 两个主要方法。(src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;do_op(OpRequestRef &amp;)) agent_choose_mode(bool restart, OpRequestRef op) src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;agent_choose_mode 该函数主要计算一个 PG 的 flush_mode 和 evic_mode 的参数值。 返回值如果为 True，表明该请求 Op 被重新加入请求队列（由于 EvictMode 为 Full），其他情况返回 false。 maybe_handle_cache(…) src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;maybe_handle_cache()调用maybe_handle_cache_detail(） 处理有关cache的读写请求 Cache flush &amp; evictcachePool空间不够时，需要选择一些脏数据对象会刷到数据层，即flush操作；将一些clean对象从缓存层剔除，以释放更多的缓存空间，即evict操作。这两种操作都是在后台线程完成的。flush操作和evict操作算法的好坏决定了Cache Tier的缓存命中率。evict是针对cachepool中已经过期或过冷的数据，只需要把它从cachepool中删除即可，evict操作通常会影响缓存命中率。flush是把脏数据刷新到storagePool，flush操作通常不会直接影响缓存命中率。flush操作是将缓存中的数据写回到持久存储介质中，从而保证数据的一致性，但并不会直接影响缓存的访问，脏数据是只保存在cachePool中，经过修改后，还未写入storagePool的数据。 数据结构src&#x2F;osd&#x2F;osd.h&#x2F;OSDServices ：定义了 AgentThread 后台线程，用于完成 flush 和 evict 操作：一是把脏对象从cachePool层适时地会刷到basePool层；二是从cachePool层剔除掉一些不经常访问的clean对象。 1234567891011121314Mutex agent_lock; // agent 线程锁，保护下面所有数据结构Cond agent_cond; // 线程相应的条件变量map&lt;uint64_t, set&lt;PGRef&gt; &gt; agent_queue; // agent线程的工作队列，保存了OSD中所有归属于cachePool的淘汰或者回刷所需的 PG 集合，根据PG集合的优先级，保存在不同的map中set&lt;PGRef&gt;::iterator agent_queue_pos; //当前在扫描的PG集合的一个位置bool agent_valid_iterator; //只有agent_valid_iterator为true时，agent_queue_pos指针才有效，否则从集合的起始处开始扫描int agent_ops; // 所有正在进行的回刷和淘汰操作int flush_mode_high_count; //一旦FLUSH_MODE_HIGH有了一个pg，就可以高速刷新对象set&lt;hobject_t&gt; agent_oids; // 所有正在进行的 agent 操作（回刷或者淘汰）的对象bool agent_active; // agent 是否有效struct AgentThread : public Thread&#123;&#125; agent_thread; // agent 线程，专门用来处理cache tier数据迁移的线程，线程名叫：osd_srv_agent。其作用就是循环遍历agent_queue中的所有pg，并对他们执行agent_work()操作。osd_srv_agent线程是一个OSD上所有PG公用的，为了保证效率，设置了严格的限流参数：osd_pool_default_cache_max_evict_check_size限制依次遍历对象的总数，达到后立刻切换退出循环在osd_srv_agent中切换PG；osd_agent_max_ops设置了一个循环中最多能够处理几次flush或者evict操作。bool agent_stop_flag; // agent 停止的标志 SafeTimer agent_timer; //agent相关定时器：当扫描一个 PG 对象时，该对象既没有剔除操作，也没有回刷操作，就停止 PG 的扫描，把该 PG 加入到定时器中，5S 后继续 src&#x2F;osd&#x2F;TierAgentState.h：TierAgentState用来保存PG相关的agent信息。 123456789hobject_t position; //PG内扫描的对象位置int started; //PG里所有对象扫描完成后，所发起的所有的agent操作数目。如果没有agent操作，就需要延迟一段时间hobject_t start; //本次扫描起始位置bool delaying; //是否延迟pow2_hist_t temp_hist; //历史统计信息int hist_age;map&lt;time_t,HitSetRef&gt; hit_set_map; //Hitset的历史记录list&lt;hobject_t&gt; recent_clean; //最近处于clean的对象unsigned evict_effort; //应该驱逐的对象的大致比例（假设它们均匀分布） flush&#x2F;evict 执行入口src&#x2F;osd&#x2F;osd.cc&#x2F;OSDService::agent_entry：agent_entry 是 agent_thread 的入口函数，它在后台调用pg-&gt;agent_work()，agent_queue的改变是在PrimaryLogPG::agent_choose_mode函数中改变的 src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_work：遍历PG中所有对象，去寻找已经过期的、失效的需要flush或者evict的对象并对它们执行相应操作。 扫描本PG的对象，从 agent_state-&gt;position 开始扫描，结果保存在 ls 中 12vector&lt;hobject_t&gt; ls;int r = pgbackend-&gt;objects_list_partial(agent_state-&gt;position, ls_min, ls_max, &amp;ls, &amp;next); 对扫描的 ls 对象做相应的检查，执行 evict 操作和 flush 操作 1234567for (vector&lt;hobject_t&gt;::iterator p = ls.begin();p != ls.end(); ++p) if (agent_state-&gt;evict_mode != TierAgentState::EVICT_MODE_IDLE &amp;&amp; agent_maybe_evict(obc, false)) ++started;else if (agent_state-&gt;flush_mode!=TierAgentState::FLUSH_MODE_IDLE&amp;&amp;agent_flush_quota&gt;0&amp;&amp;agent_maybe_flush(obc)) &#123; ++started; --agent_flush_quota; &#125; 真正执行操作的方法 evict：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_maybe_evict flush：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::agent_maybe_flush start_flush：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::start_flush 该函数完成实际的 flush 操作 start_manifest_flush：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::start_manifest_flush 真正刷回数据之前的数据准备 do_manifest_flush：src&#x2F;osd&#x2F;PrimaryLogPG.cc&#x2F;PrimaryLogPG::do_manifest_flush 真正刷回数据的过程 flush 操作最终是以 Op 请求的方式传递到底层存储层的，也就意味着需要再执行一次 Ceph 存储池写数据的相关逻辑。Ceph的Cache Tier功能目前在对象访问频率和热点统计上的实现都比较简单，可以通过基于自学习的Cache算法提升缓存命中率。 agent_state在每个函数中都起到决定性地位：在agent_work中，agent_state-&gt;evict_mode和agent_state-&gt;flush_mode的值决定要不要进行evict和flush判断。在agent_maybe_evict和agent_maybe_flush中agent_state-&gt;evict_mode的值决定要不要直接执行evict或者flush。而agent_state值的计算过程是在agent_choose_mode函数中。agent_choose_mode函数计算一个PG的flush和evict行为的相关参数。该函数主要完成以下任务： 统计当前PG中dirty object数量和当前PG中所有的object数量；（dirty object指的是脏数据对象) 统计当前PG中dirty object占用的字节数和当前PG中所有object占用的总的字节数； 分别从object数量角度和object占用的字节数角度计算dirty占比和full占比； 计算当前flush mode和evict mode； 更新agent_state-&gt;flush_mode和agent_state-&gt;evict_mode； 根据当前flush mode和evict mode决定是要将当前PG加入到待处理的PG队列中； 从agent_choose_mode最后可以看到，如果缓存池需要flush或者evict，需要将待处理的PG加入到agent_queue队列中，这一动作是最终通过调用_enqueue函数实现，该函数主要完成以下任务： src&#x2F;osd&#x2F;OSD.h&#x2F;OSDService::_enqueue 判断是否需要调整agent线程要处理哪个pg set； 将待处理的pg加入到pg set中； 唤醒agent线程，执行flush或者evict任务； 从agent_choose_mode最后可以看到，如果缓存池需不需要flush或者evict，但是如果之前agent线程有处理过该PG，需要将待处理的PG从agent_queue队列中移除掉，这一动作最终通过调用_dequeue函数实现，该函数主要完成以下任务： src&#x2F;osd&#x2F;OSD.h&#x2F;OSDService::_dequeue 根据old_priority从agent_queue队列中获取到相应的pg set； 在pg set中查找要移除的PG；如果找到了，从pg set中删除，并调整下一个要处理的PG； 如果删除之后的pg set没有任何一个PG，需要从agent_queue队列中移除，并调整下一个要处理的pg set； agent_choose_mode流程图agent_entry流程图","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph Cache Tier使用介绍","slug":"Storage/Ceph/Ceph-Cache-Tier使用介绍","date":"2022-08-25T08:45:47.000Z","updated":"2024-09-01T13:33:52.799Z","comments":true,"path":"Storage/Ceph/Ceph-Cache-Tier使用介绍/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-Cache-Tier%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"1、Ceph Cache Tier介绍缓存层(Ceph Cache Tier)为 Ceph 客户端提供了存储在后备存储层中的一部分数据的更好 I&#x2F;O 性能。缓存分层涉及创建一个相对较快&#x2F;昂贵的存储设备池（例如固态硬盘），配置为缓存层，以及一个后备池，该池由纠删码或相对较慢&#x2F;更便宜的设备组成，配置为经济的存储层。Ceph 对象处理器决定对象的存放位置，而缓存代理负责决定何时将对象从缓存中刷新到后备存储层。因此，缓存层和后备存储层对 Ceph 客户端完全透明。缓存分层代理会自动处理缓存层和后备存储层之间的数据迁移。不过，管理员可以通过设置缓存模式来配置这种迁移方式。 写回模式 (writeback mode)： 如果基础层和缓存层被配置为写回模式，Ceph 客户端每次将数据写入时会从基础层接收到 ACK 确认。然后，缓存分层代理会判断是否设置了 osd_tier_default_cache_min_write_recency_for_promote。如果已经设置，并且在指定时间间隔内数据被写入的次数超过设定值，那么数据将被提升到缓存层。当 Ceph 客户端需要访问存储在基础层的数据时，缓存分层代理会从基础层读取数据并返回给客户端。当数据从基础层读取时，缓存分层代理会查阅 osd_tier_default_cache_min_read_recency_for_promote 的值，并决定是否将数据从基础层提升到缓存层。当数据从基础层提升到缓存层后，Ceph 客户端可以通过缓存层对其进行 I&#x2F;O 操作。这种模式非常适合处理可变数据（例如，照片&#x2F;视频编辑，事务性数据）。 读代理模式 (readproxy mode)： 该模式将使用缓存层中已存在的对象，但如果缓存中不存在该对象，请求将被代理到基础层。这在从写回模式过渡到禁用缓存的过程中非常有用，因为它允许在缓存被清空的同时，工作负载能够正常运行，而不向缓存添加任何新对象。 只读模式 (readonly)： 该模式在读操作时将对象提升到缓存层；写操作则会被直接转发到基础层。该模式适用于无需存储系统强制保持一致性的只读工作负载。（警告：当对象在基础层中被更新时，Ceph 不会尝试将这些更新同步到缓存中的相应对象。由于该模式被认为是实验性的，因此启用时必须传递 --yes-i-really-mean-it 选项。） 无缓存模式 (none)： 该模式用于完全禁用缓存。 2、使用Ceph Cache Tier要设置缓存分层，您必须拥有两个池。一个将作为后备存储，另一个将作为缓存。在后续示例中，我们将缓存池称为 hot-storage，后备池称为 cold-storage。 2.1、设置后备存储池设置后备存储池通常涉及以下两种场景之一： 标准存储： 在这种情况下，池在 Ceph 存储集群中存储对象的多个副本。 纠删码： 在这种情况下，池使用纠删码来更高效地存储数据，代价是会有一些性能上的折扣。 在标准存储场景中，可以设置一个 CRUSH 规则来确定故障域（例如，osd、主机、机箱、机架、行等）。当规则中的所有存储驱动器尺寸、速度（包括 RPM 和吞吐量）和类型一致时，Ceph OSD 守护进程的性能最佳。关于创建规则的详细信息，请参阅 CRUSH Maps。一旦创建了规则，就可以创建后备存储池。 2.2、设置缓存池设置缓存池的过程与标准存储场景相同，但有以下不同：缓存层的驱动器通常是高性能驱动器，位于各自的服务器中，并拥有自己的 CRUSH 规则。设置此类规则时，应考虑拥有高性能驱动器的主机，同时排除没有这些驱动器的主机。 2.3、创建缓存层设置缓存层需要将一个后备存储池与一个缓存池关联起来： 1ceph osd tier add &#123;storagepool&#125; &#123;cachepool&#125; 要设置缓存模式，请执行以下命令： 1ceph osd tier cache-mode &#123;cachepool&#125; &#123;cache-mode&#125; 缓存层会覆盖后备存储层，因此还需要执行一个额外的步骤：必须将所有客户端流量从存储池直接引导到缓存池。为此，请执行以下命令： 1ceph osd tier set-overlay &#123;storagepool&#125; &#123;cachepool&#125; 2.4、配置缓存层缓存层有多种配置选项。您可以使用以下方式设置缓存层配置选项： 1ceph osd pool set &#123;cachepool&#125; &#123;key&#125; &#123;value&#125; 目标大小和类型Ceph 的生产缓存层使用布隆过滤器作为 hit_set_type： 1ceph osd pool set &#123;cachepool&#125; hit_set_type bloom hit_set_count 和 hit_set_period 定义了要存储的 HitSets 数量以及每个 HitSet 应覆盖的时间长度： 123ceph osd pool set &#123;cachepool&#125; hit_set_count 12ceph osd pool set &#123;cachepool&#125; hit_set_period 14400ceph osd pool set &#123;cachepool&#125; target_max_bytes 1000000000000 注意 较大的 hit_set_count 会消耗更多的 RAM，影响 ceph-osd 进程的内存使用。 通过对访问时间的分类，Ceph 可以判断 Ceph 客户端是否在一定时间内访问了一个对象至少一次或多次（即“年龄” vs “热度”）。 读写缓存设置min_read_recency_for_promote 定义了在处理读操作时检查对象存在性的 HitSets 数量。检查结果用于决定是否异步提升对象。其值应在 0 到 hit_set_count 之间。如果设置为 0，对象将始终被提升；如果设置为 1，则仅检查当前 HitSet，如果对象在当前 HitSet 中，将被提升，否则不提升。对于其他值，将检查对应数量的历史 HitSets，如果对象出现在最近的min_read_recency_for_promote 个 HitSets 中，将会被提升。min_write_recency_for_promote定义了写操作可以设置类似的参数。 12ceph osd pool set &#123;cachepool&#125; min_read_recency_for_promote 2ceph osd pool set &#123;cachepool&#125; min_write_recency_for_promote 2 注意 周期越长、min_read_recency_for_promote 和 min_write_recency_for_promote 值越高，ceph-osd 守护进程消耗的 RAM 就越多。特别是在代理活动冲刷或驱逐缓存对象时，所有 hit_set_count 个 HitSets 都会加载到 RAM 中。 缓存大小设置缓存分层代理执行两个主要功能： 冲刷： 代理识别已修改（或脏）对象并将其转发到存储池以供长期存储。 驱逐： 代理识别未修改（或干净）的对象，并从中驱逐最少最近使用的对象。 绝对大小设置缓存分层代理可以基于字节总数或对象总数来冲刷或驱逐对象。要指定最大字节数，请执行以下命令： 1ceph osd pool set &#123;cachepool&#125; target_max_bytes &#123;bytes&#125; 例如，要在 1 TB 时进行冲刷或驱逐，请执行： 1ceph osd pool set hot-storage target_max_bytes 1099511627776 要指定最大对象数量，请执行以下命令： 1ceph osd pool set &#123;cachepool&#125; target_max_objects &#123;objects&#125; 例如，要在 100 万个对象时进行冲刷或驱逐，请执行： 1ceph osd pool set hot-storage target_max_objects 1000000 注意 Ceph 无法自动确定缓存池的大小，因此这里需要对绝对大小进行配置，否则冲刷&#x2F;驱逐将无法正常工作。如果同时指定了两个限制，当任一阈值触发时，缓存分层代理将开始冲刷或驱逐。 仅当达到 target_max_bytes 或 target_max_objects 时，所有客户端请求才会被阻塞。 相对大小设置缓存分层代理可以相对于缓存池的大小（由绝对大小设置中的 target_max_bytes &#x2F; target_max_objects 指定）来冲刷或驱逐对象。当缓存池包含一定比例的已修改（或脏）对象时，缓存分层代理将冲刷它们到存储池。要设置 cache_target_dirty_ratio，执行以下命令： 1ceph osd pool set &#123;cachepool&#125; cache_target_dirty_ratio &#123;0.0~1.0&#125; 例如，将值设置为 0.4 当已修改（脏）对象达到缓存池容量的 40% 时开始冲刷： 1ceph osd pool set hot-storage cache_target_dirty_ratio 0.4 当已修改（脏）对象达到一定比例时，以更高速度冲刷这些对象。设置 cache_target_dirty_high_ratio： 1ceph osd pool set &#123;cachepool&#125; cache_target_dirty_high_ratio &#123;0.0~1.0&#125; 例如，将值设置为 0.6 当已修改（脏）对象达到缓存池容量的 60% 时开始积极地冲刷脏对象。显然，最好将值设置在 dirty_ratio 和 full_ratio 之间： 1ceph osd pool set hot-storage cache_target_dirty_high_ratio 0.6 当缓存池达到一定容量比例时，缓存分层代理将驱逐对象以保持空闲容量。设置 cache_target_full_ratio，执行以下命令： 1ceph osd pool set &#123;cachepool&#125; cache_target_full_ratio &#123;0.0~1.0&#125; 例如，将值设置为 0.8，当未修改（干净）对象达到缓存池容量的 80% 时开始冲刷： 1ceph osd pool set hot-storage cache_target_full_ratio 0.8 缓存年龄可以指定对象在缓存分层代理将最近修改（或脏）对象冲刷到后备存储池之前的最小年龄： 1ceph osd pool set &#123;cachepool&#125; cache_min_flush_age &#123;seconds&#125; 例如，在 10 分钟后冲刷修改（或脏）对象，请执行： 1ceph osd pool set hot-storage cache_min_flush_age 600 可以指定对象在从缓存层驱逐之前的最小年龄： 1ceph osd pool &#123;cache-tier&#125; cache_min_evict_age &#123;seconds&#125; 例如，在 30 分钟后驱逐对象，请执行： 1ceph osd pool set hot-storage cache_min_evict_age 1800 移除缓存层移除缓存层的步骤取决于缓存是回写类型还是只读类型。移除只读缓存由于只读缓存没有已修改的数据，您可以在不丢失缓存中对象的任何最新更改的情况下禁用并移除它。将缓存模式更改为 none 以禁用它： 1ceph osd tier cache-mode &#123;cachepool&#125; none 从后备池中移除缓存池： 1ceph osd tier remove &#123;storagepool&#125; &#123;cachepool&#125; 移除回写缓存由于回写缓存可能包含已修改的数据，因此在禁用并移除它之前，必须采取措施确保不会丢失缓存中对象的任何最新更改。将缓存模式更改为 proxy，以便新对象和已修改对象将被冲刷到后备存储池： 1ceph osd tier cache-mode &#123;cachepool&#125; proxy 确保缓存池已经被冲刷。此过程可能需要几分钟： 1rados -p &#123;cachepool&#125; ls 如果缓存池中仍有对象，可以手动冲刷它们。例如： 1rados -p &#123;cachepool&#125; cache-flush-evict-all 移除覆盖，以便客户端不会将流量引导到缓存中： 1ceph osd tier remove-overlay &#123;storagetier&#125; 最后，从后备存储池中移除缓存层池： 1ceph osd tier remove &#123;storagepool&#125; &#123;cachepool&#125; 3、注意事项缓存分层会降低大多数工作负载的性能。在使用此功能之前，应格外谨慎。 依赖于工作负载： 缓存是否能够提升性能高度依赖于工作负载。由于将对象移入或移出缓存会产生一定的成本，缓存分层只有在数据集的访问模式存在较大偏斜时才有效，例如大多数请求集中访问少量对象。缓存池的大小应足够大，以捕获工作负载的工作集，从而避免缓存抖动。 难以基准测试： 大多数用户用来衡量性能的基准测试在启用缓存分层时会显示出较差的性能，部分原因是这些测试很少将请求集中在少量对象上，缓存“预热”需要较长时间，而且预热的成本可能很高。 通常更慢： 对于不适合缓存分层的工作负载，其性能通常会比没有启用缓存分层的普通 RADOS 池更慢。 librados 对象枚举： librados 级别的对象枚举 API 在存在缓存时并不保证一致性。如果你的应用程序直接使用 librados 并依赖对象枚举，那么缓存分层可能不会如预期工作。（对于 RGW、RBD 或 CephFS，这不是问题。） 复杂性： 启用缓存分层意味着 RADOS 集群内将使用大量额外的机制和增加的复杂性。这增加了你可能会遇到其他用户尚未遇到的系统错误的概率，并使你的部署面临更高的风险。 3.1、已知表现良好的工作负载 RGW 时间偏斜： 如果 RGW 工作负载的情况是几乎所有读操作都针对最近写入的对象，那么一个简单的缓存分层配置可以很好地工作，该配置在可配置的时间后将最近写入的对象从缓存层降级到基础层。 3.2、已知表现不佳的工作负载以下配置已知与缓存分层配合表现不佳。 RBD 与复制缓存和纠删码基础： 这是一个常见的请求，但通常表现不好。即使是相对倾斜的工作负载，仍然会将一些小的写入发送到冷对象上，而由于纠删码池尚不支持小的写操作，因此必须将整个对象（通常为 4 MB）迁移到缓存中，以满足一个小的（通常为 4 KB）写入请求。只有少数用户成功部署了这种配置，而且对他们而言，这仅在数据极为冷（备份）并且完全不敏感于性能的情况下有效。 RBD 与复制缓存和基础层： 使用复制基础层的 RBD 比使用纠删码基础层的表现稍好，但仍然高度依赖于工作负载中的偏斜程度，并且非常难以验证。用户需要对其工作负载有很好的理解，并需要仔细调整缓存分层参数。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"缓存基础与Ceph分层存储","slug":"Storage/缓存基础与Ceph分层存储","date":"2022-08-10T11:59:00.000Z","updated":"2024-09-01T13:34:14.905Z","comments":true,"path":"Storage/缓存基础与Ceph分层存储/","permalink":"https://watsonlu6.github.io/Storage/%E7%BC%93%E5%AD%98%E5%9F%BA%E7%A1%80%E4%B8%8ECeph%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8/","excerpt":"","text":"缓存基础 缓存命中率：表示从缓存中获取数据的成功率，即缓存命中的次数与总访问次数的比值。缓存命中率越高，表示缓存系统的效率越高，能够更快地响应用户的请求。 缓存失效率：表示从缓存中获取数据失败的次数与总访问次数的比值。缓存失效率越高，表示缓存系统的效率越低，需要从持久存储介质中读取数据的次数也越多，可能会导致系统的响应速度变慢。 缓存容量：指缓存系统能够存储数据的最大容量。缓存容量的大小会影响缓存系统的性能和可靠性，如果缓存容量不足，可能会导致缓存系统频繁地进行evict操作，从而影响系统的响应速度和可用性。 缓存算法：指缓存系统用于决定哪些数据被缓存，哪些数据被删除的算法。常见的缓存算法包括LRU（最近最少使用）、LFU（最不经常使用）、FIFO（先进先出）等。 脏数据（Dirty Data）：指缓存中已经被修改但尚未被写回到持久存储介质（如磁盘）中的数据。这些数据需要及时写回到持久存储介质中以保证数据的一致性。常见的处理策略包括写回（write-back）和写直达（write-through）策略。 干净数据（Clean Data）：指缓存中未被修改或已经被写回到持久存储介质中的数据。干净数据在缓存系统中可以快速读取，减少写入操作，优先选择删除干净数据可以避免写回操作带来的额外开销。 evict操作：从缓存中移除某些数据，以释放缓存空间供其他数据使用。常用的策略包括LRU（Least Recently Used）等，根据最近最少使用的数据进行移除。 flush操作：将缓存中的数据立即写回到持久性存储介质（例如硬盘），以确保缓存中的数据与存储介质中的数据保持一致。 数据一致性和性能考虑 脏数据的处理：存在脏数据可能导致数据一致性问题和性能问题，因此需要及时处理脏数据。选择适当的写回策略可以平衡数据一致性和系统性能。 干净数据的优先删除：在缓存系统中，干净数据的存在可以提高系统性能，因为它们可以快速读取而不需要进行额外的写入操作。当需要从缓存中删除对象时，通常优先选择删除干净数据。 flush操作的选择：通常在以下情况下使用flush操作： 数据一致性要求高的场景，如数据库应用 性能要求不高或系统关闭时需要保证数据的持久性 evict操作的选择：通常在以下情况下使用evict操作： 缓存空间不足，需要释放空间 数据访问模式固定或数据访问频率低 基于缓存替换算法（如LRU、LFU、FIFO等） 缓存替换算法 LRU（Least Recently Used）：根据最近的访问时间来决定删除哪些数据。 LFU（Least Frequently Used）：基于数据的访问频率选择删除数据。 FIFO（First In First Out）：按照数据进入缓存的时间顺序移除数据。 Random（随机）：随机选择数据进行删除，简单但效果不如其他算法。 性能优化 优化evict操作：通过使用动态策略（基于数据使用情况）和静态策略（基于数据属性），可以提升缓存性能。了解系统的负载和压力情况也有助于优化evict操作。 缓存命中率影响：evict操作可能导致缓存命中率下降，因为被删除的数据可能被访问到。flush操作通常不会直接影响缓存命中率，但需要高效的实现以避免影响系统性能。 这些基本要素和策略可以帮助优化缓存系统的性能和可靠性，根据具体的应用需求进行适当的调整和选择。 Ceph分层存储Cache Tier分层存储是存储领域中的一个重要分支，其思想基石是存储的金字塔模型——描述了快速设备通常容量小而性能高，慢速设备通常容量大而性能低。对于数据访问而言，通常在一段时间内，真实数据的访问是具有时间局部性和空间局部性的。时间局部性是指被访问的数据在短时间内可能再次被访问，空间局部性是指与被访问数据临近的数据有更大的概率被访问。故基于时间局部性理论产生了通常所说的缓存，如：cpu缓存、内存等；而基于空间局部性原理，产生了数据预取，如：指令预取（prefetch）、数据预读（read ahead）等。 目前Ceph的OSD主要可以基于SSD或者HDD的裸盘进行构建，机械盘通常比固态盘容量大、价格比固态盘低、但读写比固态盘慢，如何用机械盘和固态盘来提供一个高可靠、高性能、高性价比的分布式存储是需要解决的重要问题。如果全部基于SSD进行构建，其性能一定会最优，但是SSD价格昂贵，出于成本考虑，不可能全部采用SSD进行构建，那么SSD与HDD混合硬件架构就显得很有必要。 Ceph的缓存分层理论基础是数据存在热点，数据访问不均匀。通常，80%的应用只访问20%的数据，这20%的数据被称为热点数据。为了减少响应时间，可以将热点数据保存到性能较高的存储设备（如固态硬盘）中。在Cache Tiering中，有一个分层代理，当保存在缓存层的数据变冷或不再活跃时，该代理会将这些数据刷到存储层，并将其从缓存层中移除。这种操作称为刷新或逐出。在客户端读写数据时，Ceph的对象处理器负责决定对象存储的位置，而Cache Tier则决定何时将缓存层中的对象刷回后端存储层。对于写操作，请求到达缓存层后，完成写操作后直接应答客户端，之后由缓存层的代理线程负责将数据写入存储层。对于读操作，如果命中缓存层，直接在缓存层读取，否则可以重定向到存储层访问。如果数据近期有访问过，说明比较热，可以提升到缓存层中。对于Ceph客户端来说，缓存层和后端存储层是完全透明的。所有Ceph客户端都可以使用缓存层，因此Cache Tier具有提升块设备、Ceph对象存储、Ceph文件系统和原生绑定的I&#x2F;O性能的潜力。 Ceph Cache Tier提供了快速存储池与慢速存储池间的分层缓存特性。通常来说，对于块存储用户而言，数据访问会有明显的时间局部性与空间局部性，故可以通过分层存储思想，改善资源配置及效率。Ceph提供了Cache Tier的解决方案，能够融合两种存储，通过合理配比提供容量与性能介于SSD与HDD之间的虚拟存储资源池。对于对象存储而言，目前主要对外提供基于S3与Swift restful api的访问接口。RGW对象存储可以通过对数据池进行Cache Tier，从而提高其访问效率。 在Ceph中，分层存储系统通过缓存和存储池的方式实现，热资源池可以将数据存储至那些管理SSD磁盘的OSD上，而冷资源池可以将数据存储至那些管理HDD磁盘的OSD上。若客户命中被访问的数据落在热资源池中，可以直接被访问，此时IO速度最快，接近SSD磁盘的性能。 若客户被访问的数据不落在热资源池中，出现缓存丢失的情况，需要转向去HDD盘上读取数据，而HDD盘处理请求访问速度为毫秒级别，故网络延时与请求处理延时可以近似忽略，认为其访问速度接近HDD磁盘的性能。这时候的处理分为两种：代理读写和数据拉取。当读写请求出现缓存丢失时，代理读写向后端请求冷数据，但缓存池不对数据进行缓存，直接将请求内容返回给客户端。 读写请求出现缓存丢失时，缓存池向后端请求冷数据，在向后端请求冷数据后，会将数据读入缓存池，继续处理客户端请求并返回请求内容。此外，短时间内被多次访问的数据会被认为是热数据而拉取到热池中，这将消耗HDD磁盘的读带宽与SSD磁盘的写入带宽。 另一方面，在热池中的数据，需要定期回写入冷池，此时，回写数据将暂用SSD与HDD磁盘的部分带宽，这个过程叫数据回写。 还未回写入冷资源池的数据，在热资源池中再次被修改，这种情况越多，缓存效率越高，即相当于热资源池带宽充分利用，帮助冷资源池挡掉了大量的写入带宽。可以简单的认为，有1%的数据是需要脏回刷的（即回刷后的1%数据为clean状态，所以后续的命中会是非脏命中），如果所有数据都不脏回刷，且都访问命中的话，那么脏命中率为100%。 根据上述原理，不难发现，Ceph Cache Tier的性能取决于访问命中率。访问命中率越高时，存储系统越接近SSD磁盘的性能；反之，访问命中率越低时，越接近HDD磁盘的性能。另一方面，在Ceph中，缓存粒度以对象方式进行拉取与回写，故在实际情况下，如果缓存丢失过多，将会有大量的数据会被拉取，从而占用SSD磁盘的带宽，使得其访问带宽比SATA磁盘更差。然而，在实际生产使用过程中，数据总使用量总是逐步增加的，与此同时，热数据的量也将逐步的增加。那么，在整个使用周期中，随着数据量的增加，就必然会经历以下过程：首先刚刚开始使用时，数据量还很少。此时，所有数据全部能够被缓存，数据命中率为100%，效果很好。随着总数据量与热数据量不断的增加，缓存池已经无法容纳所有数据，只能容纳较多的热数据，此时缓存命中率会随之逐步的下降。随着数据的进一步增加，缓存命中率低于某个临界值了，此时保持同样大小的缓存池已经无法给使用带来足够好的收益。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"存储系统缓存/分层相关论文","slug":"Storage/存储系统缓存相关论文","date":"2022-02-10T07:31:33.000Z","updated":"2024-08-04T07:59:18.239Z","comments":true,"path":"Storage/存储系统缓存相关论文/","permalink":"https://watsonlu6.github.io/Storage/%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E7%BC%93%E5%AD%98%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/","excerpt":"","text":"TDC: Pool-level object cache replacement algorithm based on temperature density 在原生Ceph系统的基础上，提出缓存池的基于热度密度缓存替换算法，计算每个对象消耗空间的热度密度，并以最低的热度密度驱逐对象。通过驱逐对命中率贡献不大的对象，提高缓存池的命中率以及分层存储性能。 在 Ceph 中，Cache Tier通过缓存机制和存储池方式的实现，其将热数据存储SSD池，冷数据存储HDD池。若客户访问的数据直接命中落在SSD池中，可以直接被访问，此时IO速度最快 ，接近SSD磁盘的性能。但如果被访问的数据不在SSD池中，需要转向去HDD池上读取数据，而HDD盘处理请求访问速度为毫秒级别，则认为其访问速度接近HDD磁盘的性能。 目前 Cache Tier 采用的是基于频率估计的类 LRU缓存替换算法，并未充分利用数据对象所携带的元数据信息，这种频率估计概率存在误差，缓存命中率性能有限，且很难达到理论极限。因此实际使用时缓存命中率较低，导致较长的 IO 路径，使得Cache Tier性能表现较差。为进一步提升缓存命中率，考虑 Cache Tier 数据对象可以携带更多信息的特点，提出基于热度密度的缓存替换算法(TDC)。对象的热度计算基于访问频率，而热度密度计算基于热度和缓存对象占用时空的比例。通过引入热度密度计算，可以更准确地评估对象对命中率贡献，从而更有效地驱逐对命中率贡献不大的对象。 在用户文件上传到Ceph集群时，Ceph通过调用file_to_extents()函数将文件分割成若干个对象。面对数目庞大的对象，为了计算每个对象的访问热度值，采用Multiple Bloomfilter记录存储池每个对象访问的频率，通过这种方式有效地捕获更细粒度的最近性和频率。 将数据对象在缓存池中花费的时间和占用缓存空间视为缓存成本，这样，将缓存池中的对象对缓存命中率的贡献转化成一种成本效益分析。热度密度就是一种综合考虑对象大小、访问频率和访问时间等因素的指标，可以用来评估每个对象对缓存命中率的贡献，并根据热度密度来进行缓存替换。整个图形的热度密度可以表示为所有命中对象的贡献之和除以所有对象的占用缓存资源之和。通过比较已缓存对象的热度密度，并按一定比例删除热度密度较低的对象。 A Survey on Tiering and Caching in High-Performance Storage Systems 论文讨论了针对高性能存储系统的缓存和分层解决方案研究。在第2节中，简要介绍了存储设备及其技术。在第3节中，讨论关于缓存解决方案及缓存算法的研究。在第4节中，讨论关于存储分层解决方案的研究。缓存和分层已被长期用于隐藏存储层次结构中慢速设备的长延迟。 基于响应时间，计算机存储系统被设计为有组织的多级层次结构，旨在提高整体性能和存储管理。不同类型的存储介质根据其性能、容量和控制技术指定为级别。通常，层次结构中级别越低，其带宽越小，存储容量越大。层次结构中有四个主要级别：处理器寄存器和缓存（SRAM、触发器或锁存缓冲区）、主存储器（DRAM）、辅助存储（SSD、HDD）、第三级存储（可移动存储设备）。图1概述了当前可用的和新兴的存储技术。表1比较了不同的计算机存储技术。 硬盘驱动器由固定在主轴周围的刚性快速旋转磁盘和使用致动器臂重新定位的磁头组成。数字数据以磁材料薄膜磁化转变的形式存储在每个磁盘上。HDD的机电方面和存储数据的串行化使得HDD比所提到的非易失性存储器技术慢几个数量级。然而，它的低价格和极高的密度使它成为二级和三级存储级别的理想选择。根据表1，HDD的容量可以比DRAM大1000倍，而操作延迟大约慢106倍。 IDC报告[58]预测到2020年，云将只触及数字世界的24%，13%将存储在云中，63%可能根本无法触及[58]。需要保护的数据的速度超过40%，甚至比数字世界本身还要快。因此，大部分数据通常存储在更便宜、更可靠和更大的设备中，而未经处理的部分数据则保存在快速存储介质中。因此，无疑需要具有缓存&#x2F;分层机制的混合存储系统 为了减轻慢设备的长延迟，可以在混合存储系统中使用缓存机制。缓存子系统有两个主要原则：1）在将原始数据保持在层次结构的中等级别时，缓存中存在一个处理不足的数据副本；以及2）缓存层中数据的生命周期短，并且它是临时的。 分层ARC（H-ARC）[12]缓存是一种基于NVM的缓存，它优化了ARC算法，以考虑最近、频率、脏和干净四种状态，并首先将缓存拆分为脏&#x2F;干净页缓存，然后将每个部分拆分为最近&#x2F;频率页缓存。基于与ARC类似的机制，它在每个级别上按层次调整每个部分的大小。因此，H-ARC在缓存中保持较高频率的脏页的时间更长。 如今，多层存储系统中使用了许多具有不同特性和容量的存储介质。缓存和分层之间的主要区别在于，在缓存系统中，数据的副本保存在缓存中，而在分层系统中，原始数据通过升级和降级两种操作在多个层之间迁移。数据根据应用程序需求和可用层的特性进行分类，通常分为热层和冷层。热数据驻留在性能层，冷数据留在容量层。考虑到随机性、传输速度等多种因素，可能有两个以上的层。 eMRC: Efficient Miss Ratio Approximation for Multi-Tier Caching 研究高效的多层缓冲命中率分析技术 未命中率曲线（MRC）是捕获工作负载特性和调整系统行为的有用工具。MRC表示缓存大小和相应的缓存未命中率之间的关系。假设随着时间的推移，工作负载相对稳定，从观察到的IO跟踪得出的MRC对于单层缓存有效工作[13]。 许多存储缓存分配方法使用未命中率曲线（MRC）来提高缓存效率。然而，他们只关注单层缓存架构，并需要整个MRC作为缓存管理的输入，而现代数据中心采用分层缓存架构以最大限度地提高资源利用率。由于每个缓存层的逐出策略和容量不同，为多层缓存生成MRC（我们称之为未命中率函数）要困难得多。我们引入eMRC，一种多维未命中率近似技术，以实现多层缓存的高效MRC生成。论文使用了一种新颖的多维性能悬崖去除方法和凸包近似技术，以使用少量采样点有效地生成没有悬崖的多维MRC。 多层缓存需要一种有效的、低开销的缓存管理方案，因为多层的任意缓存配置可能会因其副作用而对租户不利[27]。为具有不同服务级别目标（SLO）的租户配置缓存需要对缓存的每一层进行高效、准确的缓存性能分析。 扩展：已经有很多关于MRC(miss ratio curves)的理论，来对上层缓存进行建模，得出程序分配缓存大小和性能的关系模型：MissRatio&#x3D;F(capacity)。基本分为两种方法：1、通过数据重新访问距离(reuse distance)，来建立MRC模型。2、通过数据重新访问时间(reuse time)，来建立MRC模型。 PHOEBE: Reuse-Aware Online Caching with Reinforcement Learning for Emerging Storage Models NVMe和SSD是新兴存储技术的公认代表，具有数据持久性、高访问速度、低功耗和字节寻址能力。高性能采用这些技术的一个关键问题是如何正确定义智能缓存层，以便能够很好地弥补新兴技术和主存储器之间的性能差距。快速的主机端内存和慢速的后端存储驱动器之间的延迟差异很大，存储I&#x2F;O仍然是性能瓶颈，这导致了难以置信的长I&#x2F;O等待时间和大量CPU闲置浪费。为了缓解这种延迟差异，缓存层被广泛用于驻留在主存储器和后端存储驱动器之间。缓存系统的性能通常受到三个因素的影响：数据分配策略、数据热识别的准确性和数据驱逐策略。数据分配策略基本上控制数据流，并确定各种数据的接纳，例如只读、只读或两者兼而有之；数据热识别的高精度可以防止不必要数据对缓存的污染，通过局部保护提高缓存性能；数据驱逐策略决定在缓存已满时驱逐哪个数据块，从而间接增加了缓存的有效容量。这三个因素关注三个不同的方面，具有高度相关性。值得一提的是，传统的缓存策略，如LRU和LFU，并不是一个符合所有这些因素的通用解决方案（Li等人2019；Li和Gu 2020；Liu等人2020）。 PHOEBE是一种基于强化学习的在线缓存的可重用优化方案，适用于广泛的新兴存储模型。通过与缓存环境和数据流的持续交互，PHOEBE能够从单个跟踪中提取关键的时间数据依赖性和相对位置信息，随着时间的推移变得越来越智能。实验结果表明，PHOEBE能够将LRU和最先进的基于在线学习的缓存策略Belady最优策略之间的缓存未命中率差距分别缩小70.3%和52.6%。 PHOEBE预测了一个新定义的指标，即停留优先级，以表示每个数据块的相对重要性，而不是显式预测重用距离，重用距离可能是无界值，从而增加了高精度预测的难度和复杂性。当驱逐事件发生时，缓存根据其停留优先级值替换数据块（驱逐具有最低停留优先级的数据块），旨在最大化缓存命中率。停留优先级值具有时间滞后特性，在过时时间戳处的高优先级值不能在最新时间戳处保持同样高，因此它们通常与相应的时间戳组合以得出最终驱逐决定。PHOEBE关注9个特征来提取同时考虑全局和局部模式的重用信息。前六个特征和最后一个是全局特征；第7和第8特征是从当前数据块之前的滑动窗口收集的局部特征。数据块地址、数据块地址增量、频率、重复使用距离、最终重复使用距离、平均重复使用距离、滑动窗口中的频率、滑动窗口中的缓存未命中数、优先级值 将在线缓存问题建模为马尔可夫决策过程 LeCaR（Vietri等人，2018）：一种基于在线学习的缓存策略，根据学习到的概率在LRU和LFU缓存算法之间切换。 相关工作：将机器学习应用于缓存优化有两个主要途径：设计智能预取策略或改进缓存替换策略。 Sprout: A functional caching approach to minimize service latency in erasure-coded storage 利用缓存优化纠删码存储的性能，没有优化缓存策略。 论文提出一种新的具有纠删码存储的缓存框架，称为功能缓存。功能缓存涉及在缓存中使用纠删码块，使得存储节点中的块和缓存组合形成的代码是最大距离可分离（MDS）纠删码。 SSD-HDD混合存储中基于顺序封装的缓存取出 论文提出了一种基于顺序打包的缓存逐出技术，该技术将垃圾收集（GC）块中的相邻冷数据页与位于其他SSD块中的相邻冷数据页数据分组。然后，打包这些数据页一起flush到较低级别的HDD存储中，以充分利用HDD的高顺序带宽。该方法可以减少写放大对SSD缓存的负面影响，并有助于提高SSD-HDD混合存储的I&#x2F;O性能。 Shi等人[13]提出了SSDUP+，该SSDUP＋采用SSD设备来缓冲随机访问的数据，并且顺序访问的数据被刷新到磁盘的低级别存储。在SSDUP+中，需要SSD缓存来缓存热读数据，尽管它们揭示了序列特征并直接刷新到HDD上。也就是说，如果SSD缓存已满，SSDUP+必须使用最近最少使用（LRU）策略来处理缓存逐出。 将热数据放在快速存储中，将冷数据放在慢速存储中并不是一个新想法，分层存储管理通常采用快速存储作为慢速存储的缓冲[5]。已经推出了许多结合HDD和SSD的混合存储解决方案，以提高读写吞吐量[11、12、14、15]。Chen等人[11]提出Hystor将低成本HDD和高速SSD相结合，以识别关键数据（例如元数据），从而将其保存在SSD中以快速响应。此外，它利用SSD作为回写缓冲区来吸收写请求，从而产生更好的写性能。HotDataTrap[20]建议仅缓冲SSD缓存中的热数据，并将冷数据直接刷新到HDD，这为热数据存储在缓存空间中提供了更多机会。类似地，Zhang等人[21]提出了一种基于机器学习的混合存储系统写策略。具体来说，它使用机器学习来识别只写数据，并将其直接刷新到HDD中，以最大限度地减少SSD的写入流量。混合存储系统的性能受到数据刷新例程或缓存逐出方案的严重影响[2，8]。 SeqPack的Hot Read&#x2F;Write Separate模块维护两个固定长度的LRU链接列表，以分别记录最近读取和写入的数据页，用于筛选热读取数据页而不是热写入。具体而言，在发生读或写访问之后，可以将数据页插入或移动到相应链接列表的开头。然后，两个LRU链接列表都维护最近访问的读写页，这样我们可以筛选热读数据页，但不筛选热写数据页，它们可以始终缓存在SSD缓存中，同时其他数据页被视为顺序打包的候选页。sequential packer模块依赖于所提出的顺序打包模型，通过将GC块中的弹出页面与其他SSD块中的冷数据页面打包，将许多随机写入分组为大型顺序写入。GC Selector模块以较小的成本释放SSD空间，引入了一种基于成本的选择方法来定位GC目标块， 性能指标：I&#x2F;O响应时间、缓存命中率、长尾延迟。 Improving in-memory file system reading performance by fine-grained user-space cache mechanisms（大数据场景的缓存优化） 随着服务器的内存容量越来越大，分布式内存文件系统已被广泛使用，该系统使应用程序能够快速与数据交互。然而，现有的分布式内存文件系统在小数据读取中仍然面临数据访问性能低的问题（非混合存储），这严重降低了它们在许多重要的大数据场景中的可用性。论文分析了影响内存文件读取性能的因素，并提出了一种两层用户空间缓存管理机制：在第一层，我们缓存数据包引用以减少频繁的页面故障中断（包级缓存）；在第二层，我们缓存和管理小文件数据单元，以避免冗余的进程间通信（对象级缓存）。设计了一个基于子模块函数优化理论的细粒度缓存模型，以有效地管理客户端具有部分重叠片段的可变长度缓存单元，更准确地识别热碎片，避免不必要的RPC通信。。重点是设计可变长度缓存块的管理机制和替换策略。 [15]提出了一种缓存模型，该模型使用不同的LRU队列来管理不同大小的文件。因此，它可以减少小文件被部分锁定的频率。为了使应用程序能够从客户端节点读取和写入数据，而不会失去全局文件系统命名空间的优势。传统的缓存替换策略包括FIFO、LRU[17]、LFU[18]、ARC[19]、FBR[20]和2Q[21]。根据[19]的结果，ARC通过复杂的自调整机制改进了基本的LRU策略，在各种工作负载上优于上述算法。然而，对大数据工作负载的实验表明，即使使用非常大的缓存空间，传统的缓存方法仍然遭受相对较低的命中率[22]。为了进一步提高I&#x2F;O性能，研究人员提出了大量方法，这些方法通过专门的指标来替代缓存候选[23–26]。 许多智能技术应用于广泛领域[33-35]。受此启发，一些研究人员采用了先进的数学方法或机器学习模型来描述和分析缓存问题。例如，[24，36]考虑马尔可夫决策过程背景下的缓存替换问题，[37，38]提出了基于强化学习和长短期记忆神经网络的更复杂场景下的智能缓存替换框架。特别是，在这些算法中，基于概率模型的EVA[24]通过充分利用所有缓存单元的命中、逐出和年龄分布，实现了最佳性能。 数据压缩是另一种常用于优化缓存空间利用率的策略。通常，压缩用于优化存储级别的访问性能。事实上，压缩也可以用于优化缓存性能[45–48]。压缩可以扩大有效的缓存容量，因此，通过保存更多对象可以减少缓存未命中。然而，由于压缩&#x2F;解压缩过程，压缩方案引入了额外的访问延迟。更糟糕的是，由于压缩比不可预测，这可能会导致性能下降。 数据包级缓存机制：在分布式文件系统中，要读取的数据被拆分为数据包，以便在多次小规模读取时容错和提高性能。当读取文件时，客户端调用mmap()将数据包映射到进程的虚拟地址空间，并调用munmap()读取后立即释放映射区域。在某种情况下，每个计算任务都将调用mmap()和munmap()，导致多页错误中断。为了提高读取性能，论文设计了一种包级缓存机制：在读取后不会立即释放它们，而是设法存储读取数据包的引用以供后续使用。数据包级缓存机制维护缓存队列。如果数据包的引用被减少到0，则该数据包的参考被放入队列。当队列大小超过阈值, 包将由munmap()根据缓存迁移策略调用。（带有优先级计数和缓存队列的数据包级缓存机制） 对象级缓存机制：哈希表：在第一级，整个文件被分成几个桶；每个文件片段根据其起始地址和结束地址被放入特定的存储桶中。红黑树：当单个存储桶中的单元数量超过限制时，缓存模型会根据当前存储桶中所有元素的地址将其放入红黑树中。当存储桶中的单元数小于限制时，原始红黑树将被删除。双重链接列表：文件片段自然按其起始地址和结束地址排序。每个缓存单元包含指向最近单元的前指针和后指针。 为了获得对象级缓存模型中缓存管理问题的近似解，论文实现了[53]提出的两种Greedy和ISK算法 Improving NAND Flash Based Disk Caches 论文介绍了Flash在当今的服务器平台中用作磁盘缓存的研究。提出了两项改进。第一种方法通过将基于Flash的磁盘缓存拆分为单独的读写区域来提高性能和可靠性。第二种通过采用可编程闪存控制器来提高可靠性。它可以根据应用的需求改变错误码强度（可纠正位的数量）和存储单元可以存储的位的数量（单元密度）。Flash的可管理性和可靠性是一个具有挑战性的问题，需要解决这些问题才能将Flash完全集成到数据中心。提出了一种用于NAND闪存的硬件辅助软件管理磁盘缓存。 为了减轻磨损，对基于闪存的磁盘缓存的闪存擦除执行磨损级别管理。对于读缓存和写缓存，首先使用LRU策略选择要逐出的块，该策略针对磁盘缓存容量未命中（对于读缓存）或容量写入（对于写缓存，需要首先擦除块的异地写入）。然而，如果该块的磨损超过最新块的磨损预定阈值，则驱逐与最小磨损相对应的块（最新块）以平衡磨损水平。从整个闪存块集合中选择最新的块。在驱逐最新的块之前，它的内容被迁移到旧块。 Optimizing the SSD Burst Buffer by Traffic Detection（细粒度缓存替换策略） HPC存储系统仍然使用硬盘驱动器（HDD）作为其主要存储设备。固态驱动器（SSD）被广泛部署为HDD的缓冲区。还提出了突发缓冲器来管理突发写入请求的SSD缓冲。虽然突发缓冲区在许多情况下可以提高I&#x2F;O性能，但它具有一些限制，例如需要大的SSD容量以及计算阶段和数据flush阶段之间的和谐重叠。提出了一种称为SSDUP+的方案。SSDUP+旨在通过解决上述限制来改善突发缓冲区。首先，为了减少对SSD容量的需求，只选择一部分数据写入SSD，而其余数据则直接写入HDD，而不牺牲I&#x2F;O性能。开发了一种新的方法来检测和量化写入流量中的数据随机性。此外，提出了一种自适应算法来动态地对随机写入进行分类。通过这样做，需要更少的SSD容量来实现与其他突发缓冲方案类似的性能。然后，为了克服计算阶段和flush阶段完美重叠的困难，提出了SSD缓冲区的流水线机制，在流水线机制中，SSD缓冲区被分成两半。当一半接收写入数据时，另一半完全占用将数据从SSD刷新到HDD。其中数据缓冲和刷新在流水线中执行。为了提高I&#x2F;O吞吐量，采用了流量感知刷新策略来减少HDD中的I&#x2F;O干扰。最后，为了进一步提高SSD中缓冲随机写入的性能，SSDUP+通过使用日志结构存储数据，将SSD中的随机写入转换为顺序写入。此外，SSDUP+使用AVL树结构来存储数据的序列信息。SSDUP+以减少满足突发性大规模I&#x2F;O访问性能所需的SSD容量，从另一个角度来看，在相同的SSD容量下提高I&#x2F;O性能。 硬盘驱动器（HDD）仍然被用作HPC存储系统中的主要永久存储设备，部分原因是其成本低，可以在访问大型连续数据块时提供高带宽。然而，HDD有一个主要缺点：当随机访问数据时，由于磁盘头的缓慢机械移动，它们的性能很差。固态驱动器（SSD）等新的存储设备由于其接近零的寻道延迟和优异的性能（特别是对于随机访问）而被广泛部署在HPC环境中。然而，SSD比HDD昂贵得多。因此，在大规模生产HPC系统中使用SSD作为唯一的存储设备并不是一个经济高效的解决方案，更不用说SSD的技术限制，例如磨损和寿命有限的问题。解决HDD随机数据访问问题的一个流行解决方案是使用SSD缓冲HDD和计算节点之间的数据流。另一方面，对HDD的突发随机写入可能会显著降低HPC存储系统上运行的数据密集型应用程序的性能。为了解决上述问题，引入了突发缓冲器，它使用SSD缓冲器作为计算节点和基于HDD的存储服务器之间的中间层，以吸收突发写入请求。 Exploration and Exploitation for Buffer-Controlled HDD-Writes for SSD-HDD Hybrid Storage Server（细粒度缓存替换策略） 结合固态驱动器（SSD）和硬盘驱动器（HDD）的混合存储服务器为应用程序提供了成本效益和μ级响应能力。会导致HDD通常利用不足，而SSD使用过度，特别是在密集写入下。这会导致SSD的快速磨损和高尾部延迟。HDD的一系列顺序和连续写入呈现出周期性、阶梯状的写入延迟模式，即低（35μs）、中（55μs）和高延迟（12毫秒），这是由HDD控制器内的缓冲写入导致的。可以利用HDD的潜在μs级IO延迟，以吸收过多的SSD写入，而不会降低性能。论文建立了一个描述阶梯行为的HDD写入模型，并设计了一个配置过程来初始化和动态重新校准模型参数。然后，提出了一种缓冲区控制写入方法（BCW），以主动控制缓冲区写入，从而用应用程序数据调度低延迟和中延迟时段，并用填充数据填充高延迟时段。利用BCW，设计了一个混合IO调度器（MIOS），以自适应地将传入数据引导到SSD和HDD。进一步设计了多HDD调度以最小化HDD写入延迟 Cache Replacement Policy Based on Expected Hit Count 现有处理器采用最近最少使用（LRU）策略的变体来确定替换的缓存块。不幸的是，LRU提供的服务与Belady的MIN之间存在很大差距，这是最佳的更换策略。Belady的MIN要求选择具有最长重用距离的缓存块，因此，由于需要了解未来，这是不可行的。在论文研究中，发现缓存块的预期命中数与其重用距离的倒数之间存在很强的相关性。论文提出了用于替换缓存中的缓存块的预期命中计数（EHC）策略，在现有低成本替换策略的基础上，采用基于命中计数的缓存块选择程序，以显著提高最后一级缓存中缓存块选择的质量，而无需相应的区域开销。 现代处理器经常需要从最后一级缓存中移出一段数据，以便为新数据留出空间。替换策略决定了在所有可能的候选项中，在新数据到达时应该从缓存中删除哪个候选项。 使用第二届缓存替换锦标赛（CRC2）发布的模拟框架评估预期命中计数（EHC）策略。 Hystor: Making the best use of solid state drives in high performance storage systems（分层存储） 由于SSD相对较高的价格和较低的容量，需要解决的一个主要系统研究问题是如何以成本和性能有效的方式使SSD在高性能存储系统中发挥最有效的作用。论文设计和实现Hystor高性能混合存储系统，Hystor将SSD和HDD作为一个单块设备进行管理，Hystor可以有效地识别（1）可能导致长延迟或（2）语义关键的块（例如文件系统元数据），并将其存储在SSD中以供将来访问，从而实现显著的性能改进。为了进一步利用最先进SSD中极高的写入性能，Hystor还充当回写缓冲区，以加快写入请求。 将高容量SSD视为存储的一部分，而不是缓存位置。相应地，与基于缓存的传统策略不同，基于缓存的策略在每次数据访问时频繁更新缓存内容，论文只定期和异步地重新组织设备之间的块布局，以实现长期优化。Hystor通过三个主要组件实现其数据管理的优化目标。首先，通过实时监控I&#x2F;O流量，Hystor自动学习工作负载访问模式并识别性能关键块。只有能够带来最大性能优势的块才能从HDD重新映射到高速SSD。第二，通过有效利用现有接口中可用的高级信息，Hystor识别语义关键块（例如文件系统元数据），并及时为它们提供高优先级，使其留在SSD中，这进一步提高了系统性能。第三，传入的写入被缓冲到低延迟SSD中，以提高写入密集型工作负载的性能。 Back to the Future: Leveraging Belady’s Algorithm for Improved Cache Replacement 缓存是减少数据访问的长延迟的重要机制，其有效性受到其替换策略的显著影响。论文解释了缓存替换算法如何通过将其应用于过去的缓存访问来学习Belady的算法，以告知未来的缓存替换决策。并提出了基于Belady的缓存替换算法，将Belady方法的变体应用于过去的内存访问历史。如果过去的行为是未来行为的良好预测，论文提出的策略将接近Belady算法的行为。新缓存替换策略由两部分组成。第一个使用OPTgen算法重构了Belady对过去缓存访问的最佳解决方案。第二个是一个预测器，它可以学习OPT对过去PC的行为，以告知同一PC对未来负载的驱逐决定。 在缺乏明确反馈的情况下，现有的替换策略基于启发式方法，如最近最少使用（LRU）和最近最多使用（MRU），这两种方法都适用于不同的工作负载。然而，即使使用越来越聪明的技术来优化和组合这些策略，这些基于启发式的解决方案也仅限于特定类别的访问模式，无法在更复杂的场景中表现良好。 将缓存替换视为一个二进制分类问题，其目标是确定传入的行是缓存友好的还是缓存厌恶的：缓存友好的行以高优先级插入，而缓存厌恶的行被标记为未来冲突的驱逐候选行。为了确定传入线路应如何分类，Hawkeye重构了Belady对过去访问的最优解决方案，以了解单个加载指令的行为。 Performance Evaluation of Traditional Caching Policies on A Large System with Petabytes of Data 大多数现有的缓存性能研究都评估填充相对较小缓存的、相当小的文件。很少有报告讨论了传统缓存替换策略在超大系统上的性能。论文在PB级存储系统中，全面评估了几种缓存策略的性能，包括先进先出（FIFO）、最近最少使用（LRU）和最不频繁使用（LFU）。研究表明当应用于大型数据集和小型数据集时，传统缓存策略能够提高性能。 在整个评估过程中，FIFO缓存替换策略经常导致比LRU或LFU显著更低的命中率，尽管有一小部分数据点的FIFO命中率较高。LRU缓存替换策略在所有测试的替换策略中获得了最高的比率，但与LFU策略相比，LRU导致平均命中率的标准偏差更高。当排除攻击性用户时，LFU缓存替换策略的平均命中率最高，而当LRU包含攻击性用户后，该策略的命中率仅超过0.29%。 将可用缓存的大小增加一倍，最多可以提高12%的命中率。论文建议额外的需求可以通过简单地扩展缓存大小来降低性能增益的价值。论文认为，对专用缓存策略进行更彻底的检查能够专注于大规模缓存大小的优化。通过将所使用的缓存大小增加一倍，命中率发生了相对较小的变化，这表明，在使用更有效的缓存策略的同时，缩小总体缓存大小将节省大量空间，并减少用作缓存所需的活动磁盘数。 预取是另一种有可能显著提高缓存性能的技术[24]，[25]，[26]，[27]，[28]。事实上，预取比简单地用流行文档加载缓存更有效[33]。有效的预取策略可以帮助缓存将命中率提高50%[32]。智能地预加载数据可以在不增加成本的情况下实现性能提高，因为使用预取的缓存可以与不使用预取缓存的两倍缓存一样有效[30]。已经证明，使用有效的预取方案可以显著减少不同缓存替换策略的命中率之间的差异，增强了格式良好的预取算法的重要性[33]。 Improving Cache Management Policies Using Dynamic Reuse Distances 论文提出了一种新的PDP缓存管理策略，一种使用动态重用距离来进一步改进缓存替换策略，该策略防止替换缓存线，直到对其缓存集进行一定数量的访问，称为保护距离（PD）。该策略保护缓存线足够长，可以重复使用，但不能超过该长度，以避免缓存污染。这可以与旁路机制相结合，该机制也依赖于动态重用分析，以绕过预期重用较少的管线。如果没有未保护的行，则忽略未命中提取。提出了一种基于动态重用历史的命中率模型，并动态计算了使命中率最大的PD。PD会定期重新计算，以跟踪程序的内存访问行为和阶段。 Optimum Caching versus LRU and LFU: Comparison and Combined Limited Look-Ahead Strategies 将基于最近最少使用（LRU）和最不频繁使用（LFU）替换原则的web缓存策略与根据Belady算法的最佳缓存进行比较。研究了一种结合LRU、LFU或其他非预测方法的有限前瞻最优策略的组合方法。、通过模拟，根据请求跟踪和独立参考模型（IRM）的前瞻性程度来评估命中率增益，并对观察到的行为进行分析确认。 将常用缓存策略的命中率和更新工作量与最佳缓存作为性能上限进行比较。缓存策略性能评估的三种基本方法是通过跟踪模拟、根据综合模型模拟运行生成的请求模式和分析。 对一种组合缓存方法的评估表明，优化缓存不仅可以提供缓存命中率上限，而且可以部分用于视频流的缓存和服务于巨大请求工作负载的缓存。对缓存和请求特定参数对有限前瞻方案适用性的影响进行更详细的分析，以供将来研究。 A Distributed Block Storage Optimization Mechanism Based on Ceph 为了应对企业在提高块存储服务的资源利用率和读&#x2F;写速率方面面临的挑战，Ceph提供了缓存分层，以提高异构存储环境中的群集性能。然而，由于缓存污染，缓存分层中最近最少使用的（LRU）算法会驱逐更多有价值的数据，这会导致某些请求的延迟更高；同时，当在存储节点上分配数据时，可扩展哈希下的受控复制（CRUSH）算法只考虑存储节点容量，这使得Ceph无法动态平衡节点的I&#x2F;O负载。为了解决这些问题，提出了一种基于预测模型的存储选择策略，以提高缓存池中对象访问的命中率，提高集群的整体I&#x2F;O性能；此外，还提出了缓存池I&#x2F;O负载平衡策略。与原生机制相比，所提出的块存储优化机制可以实现更高的I&#x2F;O吞吐量和更均衡的I&#x2F;O负载。 当数据在缓存层被逐出时，缓存分层中的LRU算法仅基于最近的访问记录逐出数据，这可能会由于偶尔的冷数据访问而导致逐出更有价值的热数据[3]。基于Ceph的缓存分层机制，论文提出了一种基于预测模型的存储选择策略，该策略根据对象访问频率确定对象请求是访问SSD OSD池还是访问后端HDD OSD池。该策略可以减少冷数据处理所造成的不必要开销，从而提高集群的总体I&#x2F;O性能。 为了有效利用缓存分层中有限的缓存池资源，冷数据应该存储在后端存储池中，而热数据应该存储到缓存池中。因此，在海量数据存储的背景下，区分数据的热量（即访问频率）并采用不同的处理策略可以充分利用Cache Tiering中的存储资源，并减少冷数据处理（例如冷数据从后端存储池进入缓存池，LRU驱逐冷数据）所造成的不必要开销。提出了一种基于预测模型的存储选择策略。该策略适应海量数据存储的特点，分析存储对象的长期访问记录。同时，根据某一时间段内的对象热度，判断是选择访问SSD缓存池还是后端HDD存储池，以减少冷数据处理带来的不必要开销，最终提高集群性能。 Maximizing Cache Performance Under Uncertainty（提出EVA）2017 HPCA 指出Belady理论假设了对未来的完全了解，但这在实践中是不可用的，其明显在信息不完善的情况下是次优的。并建议：对于实际的缓存替换，应该根据其经济增加值（即其预期命中率与平均值的差异）来替代。缓存替换中的两个主要权衡：命中概率和缓存空间，并描述了EVA如何在一个直观的度量中协调它们。通过借鉴马尔可夫决策过程（MDP）理论，证明了EVA最大化了缓存命中率。 最常见的缓存替换策略是使用最近性和频率启发式。大多数缓存替换策略采用某种形式的最近性，有利于最近被引用的候选人：例如，LRU仅使用最近性，而RRIP[17，39]预测较老的候选人需要更长的时间才能被引用。类似地，一些不假设最近的政策仍然基于候选人最后被引用的时间：PDP[14]保护候选人直到某个年龄；IRGD[35]使用年龄的启发式函数。另一种常见的缓存替换策略解释动态行为的方式是通过频率，倾向于先前重用的候选：例如，LFU单独使用频率，ARC[26]和SRRIP[17]等“抗扫描”策略倾向于至少重用一次的候选。 EVA缓存替换策略：本质上是一种成本效益分析，即候选数据的命中概率是否值得其所消耗的缓存空间。EVA将每个候选数据选视为一项投资，试图留住利润最高的候选候选（以命中率衡量）。首先，EVA奖励每个候选数据预期的未来命中率。然后，由于缓存空间是一种稀缺资源，EVA需要考虑每个候选将消耗多少空间。EVA通过对每个候选数据在缓存中花费的时间“收费”来实现这一点。具体而言，EVA以单行的平均命中率（即缓存的命中率除以其大小）对候选项收费，因为这是消耗缓存空间的长期机会成本。EVA &#x3D; Expected hits - (Cache hit rate&#x2F; Cache size) * Expected time EVA策略的实现主要包括以下几个步骤： 计算每个缓存行的经济增值（EVA）：EVA是一个衡量缓存行价值的指标，它考虑了缓存行的命中概率和占用缓存空间的时间成本。具体地，EVA等于缓存行的期望命中次数减去缓存的命中率乘以缓存行在缓存中的时间。 选择EVA最小的缓存行进行替换：当需要替换缓存行时，EVA策略会选择EVA最小的缓存行进行替换。这是因为EVA最小的缓存行对缓存的贡献最小，替换它可以最大化缓存的命中率。 更新缓存行的EVA值：当缓存行被访问时，EVA策略会更新它的EVA值。具体地，EVA策略会根据缓存行的命中情况和占用缓存空间的时间，重新计算缓存行的EVA值。 调整缓存大小：EVA策略还可以根据缓存的命中率和缓存行的EVA值，动态调整缓存的大小。具体地，当缓存的命中率较低时，EVA策略会增加缓存的大小；当缓存的命中率较高时，EVA策略会减小缓存的大小。 EVA策略的实现比较简单，只需要对每个缓存行维护一个EVA值，并选择EVA最小的缓存行进行替换即可。 LHD: Improving Cache Hit Rate by Maximizing Hit Density 2018 NSDI 云应用程序的性能严重依赖于数据中心键值缓存的命中率。键值缓存通常使用最近最少使用（LRU）作为其逐出策略，但在实际工作负载下，LRU的命中率远不是最佳的。论文提出最小命中密度（LHD）缓存替换算法，这是一种针对键值缓存的新驱逐策略。LHD预测每个对象每消耗空间的预期命中率（命中密度），过滤对缓存命中率贡献不大的对象。与先前的驱逐策略不同，LHD不依赖启发式，而是使用条件概率严格地模拟对象的行为，以实时调整其行为。 缓存命中率的小幅增加会对应用程序性能产生巨大影响。例如，将命中率从98%提高到99%，只需1%，就可以将对数据库的请求数量减半。使用上面使用的延迟数，这将平均服务时间从210µs减少到110µs（接近2倍），并且对于云应用程序来说，重要的是，将长延迟请求的尾部减半[21]。为了提高缓存命中率，云提供商通常会扩展服务器数量，从而增加缓存总容量[37]。从长远来看，添加缓存容量是不可行的，因为命中率随着缓存容量的增加呈对数增长[3，13，20]。需要大量内存才能显著影响命中率。在一定的缓存空间条件下，可以采用高效的缓存替换策略来提高缓存命中率。 流行的内存缓存使用最近最少使用（LRU）或LRU的变体作为其逐出策略。然而，LRU远不是缓存工作负载的最佳选择，因为：当工作负载具有可变的对象大小时，LRU的性能会受到影响，以及常见的访问模式暴露了LRU中的病态，导致命中率低。LRU的这些缺点已经得到了充分的记录，先前的工作已经提出了许多针对LRU的驱逐政策[4，14，16，25，35，38，40]。然而，这些策略并没有被广泛采用，因为它们通常需要大量的参数调整，这使得它们的性能不可靠，并且全局同步状态会影响它们的请求吞吐量。 论文提出命中密度的概念，用它衡量对象对缓存命中率的贡献程度。根据每个对象的信息（其年龄或大小）推断出每个对象的命中密度，然后以最小的命中密度（LHD）驱逐该对象。最小命中密度（LHD）是一种基于命中密度的缓存替换策略。LHD在线监控对象，并使用条件概率预测其可能的行为。LHD利用了许多不同的对象特性（例如，年龄、频率、应用程序id和大小），并且很容易支持其他对象。动态排名使LHD能够随时间调整其替换策略，以适应不同的应用程序工作负载，而无需任何手动调整。例如，在某个工作负载上，LHD可能最初接近LRU，然后切换到最近使用的（MRU）、最不频繁使用的（LFU）或其组合。LHD动态预测每个对象每消耗空间的预期命中率或命中密度，并以最低的命中率驱逐对象。通过过滤掉对缓存命中率贡献不大的对象，LHD逐渐提高了平均命中率。 根据Memcachier[36]提供的为期一周的商业memcached跟踪和Microsoft Research提供的存储跟踪对LHD进行了评估[48]。LHD显著提高了先前策略的命中率，例如，与LRU相比，将未命中率减少了一半，与最近的策略相比，减少了四分之一，并且还避免了诸如影响先前策略的性能悬崖等问题。图1显示了实现与LHD相同命中率所需的缓存大小，Memcachier上为256 MB，Microsoft跟踪上为64 GB。LHD需要的空间比以前的驱逐策略少得多，从而节省了现代数据中心数千台服务器的成本。 先前的缓存替换策略以许多不同的方式改进了LRU。几乎所有的政策都通过额外的机制来改善其最坏的病理状况。例如，ARC[35]使用两个LRU列表来区分新进入的对象，并限制来自不常访问对象的污染。类似地，AdaptSize[9]在LRU列表前面添加了一个概率过滤器，以限制大型物体的污染。最近的一些策略将访问划分为多个LRU列表，以消除性能悬崖[6，18，51]或在不同大小的对象之间分配空间[10，17，18，37，41，43，49]。所有这些策略都使用LRU列表作为核心机制，因此保留了最近性作为内置假设。此外，他们增加的机制可以引入新的假设和病理。例如，ARC通过将频繁访问的对象与新允许的对象放在一个单独的LRU列表中，并倾向于驱逐新允许的物体，从而假设频繁访问的物体更有价值。这通常是LRU的改进，但可能表现为病态。 EVA，一种最近针对处理器缓存的驱逐策略[7，8]，引入了使用条件概率来平衡命中与消耗的资源的想法。LHD和EVA之间有几个显著的差异，使LHD能够在关键价值工作负载上表现出色。首先，LHD和EVA使用不同的排名功能。EVA根据对象的命中率（而不是命中密度）对其进行排名。 LHD算法的实现过程如下： 首先，需要为每个对象计算其期望的命中率。这可以通过以下公式计算： Hit density &#x3D; Hit probability * Object size &#x2F; Expected time in cache 其中，Hit probability是对象在其生命周期内被访问的概率，Object size是对象的大小，Expected time in cache是对象在缓存中的期望时间。在LHD算法中，Expected time in cache是通过对象的访问模式和缓存的大小等因素来计算的。具体地，可以使用以下公式计算对象的Expected time in cache： Expected time in cache &#x3D; (Cache size &#x2F; Object size) * (1 &#x2F; Hit probability) 其中，Cache size是缓存的大小，Object size是对象的大小，Hit probability是对象在其生命周期内被访问的概率。这个公式的意思是，如果缓存中有足够的空间来存储对象，那么对象在缓存中的期望时间就是对象被访问的平均间隔时间的倒数。这个期望时间可以用来计算对象的期望命中率，从而帮助LHD算法更好地预测对象的命中率。 然后，需要为每个对象维护一个命中率分布。这可以通过记录对象的命中和驱逐时间来实现。当对象被命中时，将其命中时间添加到命中率分布中。当对象被驱逐时，将其驱逐时间添加到驱逐率分布中。 当需要驱逐一个对象时，LHD算法会选择命中率分布最小的对象进行驱逐。这可以通过计算每个对象的命中率分布的加权平均值来实现。具体地，对于每个对象，将其命中率分布的每个时间点乘以其命中率，然后将所有时间点的乘积相加，得到该对象的加权平均命中率。然后，选择加权平均命中率最小的对象进行驱逐。 在实现过程中，还可以使用其他技术来优化LHD算法的性能。例如，可以使用分类来改进预测，以便更好地考虑对象的特征。还可以使用并发技术来提高算法的吞吐量。 总之，LHD算法的实现过程包括计算对象的期望命中率，维护命中率分布，选择命中率分布最小的对象进行驱逐等步骤。通过这些步骤，LHD算法可以更好地预测对象的命中率，从而提高缓存","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"Ceph PG介绍","slug":"Storage/Ceph/ceph-PG介绍","date":"2021-11-10T10:22:25.000Z","updated":"2024-09-01T13:29:00.495Z","comments":true,"path":"Storage/Ceph/ceph-PG介绍/","permalink":"https://watsonlu6.github.io/Storage/Ceph/ceph-PG%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"Ceph并不是直接通过CRUSH算法将数据对象一一映射到OSD中的，这样做将非常复杂与低效。而且，Ceph用户并不需要了解实际的CRUSH算法是怎么运行的，只需要关心数据保存在哪里即可。Ceph通过存储池Pool和放置组Placement Groups (PGs)来实现CRUSH算法进行数据寻址。PG聚合了池中的对象。按对象跟踪 RADOS 对象的位置和元数据在计算上是昂贵的。对于具有数百万个 RADOS 对象的系统，按对象跟踪位置是不切实际的。Ceph 客户端计算一个 RADOS 对象应位于哪个 PG。作为计算的一部分，客户端会对对象 ID 进行哈希处理，并执行涉及池中 PG 数量和池 ID 的操作。 属于 PG 的 RADOS 对象的内容存储在一组 OSD 中。例如，在一个大小为 2 的复制池中，每个 PG 将在两个 OSD 上存储对象，如下所示：如果 OSD #2 发生故障，另一个 OSD 将被分配到 Placement Group #1，然后用 OSD #1 中所有对象的副本填充。如果池大小从 2 更改为 3，将分配一个额外的 OSD 到 PG，并接收 PG 中所有对象的副本。分配给 PG 的 OSD 并不是专门属于该 PG 的；相反，OSD 也与来自同一池或其他池的其他 PG 共享。在我们的例子中，OSD #2 由 Placement Group #1 和 Placement Group #2 共享。如果 OSD #2 发生故障，那么 Placement Group #2 必须恢复对象副本（利用 OSD #3）。 PG是 Ceph 中的逻辑池的子集。PG 执行将对象（作为一组）放置到 OSDs 中的功能。Ceph 以 PG 为单位管理数据, 这比管理单个 RADOS 对象更具扩展性。在集群中拥有较多的 PG的集群，相比具有较少 PG 的相同集群，数据分布更均衡。每个 Ceph 的内部 RADOS 对象都会映射到特定的 PG，而每个 PG 又只属于一个 Ceph 池。 当 PG 数量增加时，会发生几个后果。新的 PG 被分配到 OSD。CRUSH 函数的结果发生变化，这意味着一些来自已经存在的 PG 的对象被复制到新的 PG 中，并从旧的 PG 中删除。 在理想条件下，对象会在PG之间均匀分布。由于CRUSH计算每个对象的PG，但不知道每个与PG关联的OSD中存储了多少数据，因此PG和OSD的数量比例可能对数据分布产生显著影响。 例如，假设在一个有三个副本的池中，只有一个PG分配给十个OSD。在这种情况下，由于CRUSH没有其他选择，只会使用三个OSD。然而，如果有更多PG可用，RADOS对象更有可能在OSD之间均匀分布。CRUSH会尽力使OSD在所有现有PG之间均匀分布。 只要PG的数量比OSD多一个或两个数量级，分布通常会比较均匀。例如：3个OSD对应256个PG，10个OSD对应512个PG，或者10个OSD对应1024个PG。 但是，不均匀的数据分布可能由于PG与OSD比例之外的因素而出现。例如，由于CRUSH不考虑RADOS对象的大小，一些非常大的RADOS对象的存在可能会造成不平衡。例如，假设有一百万个4 KB的RADOS对象，总计4 GB，均匀分布在10个OSD的1024个PG中。这些RADOS对象将会在每个OSD上消耗4 GB &#x2F; 10 &#x3D; 400 MB。如果再往池中添加一个400 MB的RADOS对象，则在该RADOS对象所在的PG的三个OSD上，每个OSD将被填充到400 MB + 400 MB &#x3D; 800 MB，而其他七个OSD仍然只有400 MB。 每个PG对OSD和MON增加了内存、网络和CPU的需求。这些需求必须始终得到满足，并在恢复期间增加。实际上，PG的主要作用之一是通过将对象聚集在一起分担这些开销。因此，减少PG的数量可以节省大量资源。 自动缩放PGPG是 Ceph 用于分配数据的内部实现细节。自动缩放提供了一种管理 PG，特别是管理不同池中 PG 数量的方法。当启用 pg-autoscaling 时，集群可以根据预期的集群和池使用情况，对每个池的 PG 数量（pgp_num）进行建议或自动调整。每个池都有一个 pg_autoscale_mode 属性，可以设置为以下值： off：禁用该池的自动缩放。管理员需要为每个池选择合适的 pgp_num。有关更多信息，请参见选择 PG 数量。 on：启用对给定池的 PG 数量的自动调整。 warn：当 PG 数量需要调整时发出健康检查警告。 要设置现有池的自动缩放模式，请运行以下命令： 1ceph osd pool set &lt;pool-name&gt; pg_autoscale_mode &lt;mode&gt; 对于在集群初始设置后创建的池，也有一个 pg_autoscale_mode 设置。要更改此设置，请运行以下命令： 1ceph config set global osd_pool_default_pg_autoscale_mode &lt;mode&gt; 可以使用 noautoscale 标志来禁用或启用所有池的自动缩放。默认情况下，该标志设置为 off，但可以通过以下命令将其设置为 on： 1ceph osd pool set noautoscale 要将 noautoscale 标志设置为 off，请运行： 1ceph osd pool unset noautoscale 要获取标志的值，请运行： 1ceph osd pool get noautoscale 查看 PG 缩放状态要查看每个池、其相对利用率以及对 PG 数量的任何推荐更改，请运行以下命令： 1ceph osd pool autoscale-status 输出将类似于： 1234POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE BULKa 12900M 3.0 82431M 0.4695 8 128 warn Truec 0 3.0 82431M 0.0000 0.2000 0.9884 1.0 1 64 warn Trueb 0 953.6M 3.0 82431M 0.0347 8 warn False POOL：池的名称。 SIZE：池中存储的数据量。 TARGET SIZE（如果存在）：预计池中存储的数据量，由管理员指定。系统使用两个值中的较大者进行计算。 RATE：池的倍数，决定了消耗多少原始存储容量。例如，一个三副本池的比例为 3.0，一个 k&#x3D;4 m&#x3D;2 的纠删编码池的比例为 1.5。 RAW CAPACITY：负责存储池数据（以及可能的其他池数据）的特定 OSD 上的总原始存储容量。 RATIO：池使用的存储量与总原始存储容量的比率。换句话说，RATIO 定义为 (SIZE * RATE) &#x2F; RAW CAPACITY。 TARGET RATIO（如果存在）：池的预期存储（即管理员指定的池预计消耗的存储量）与设置了目标比例的所有其他池的预期存储之比。如果同时指定了 target_size_bytes 和 target_size_ratio，则 target_size_ratio 优先。 EFFECTIVE RATIO：对目标比例进行两次调整后的结果： 减去预期被使用的目标容量。 在设置了目标比例的池之间对目标比例进行归一化，以便它们集体瞄准集群容量。例如，四个目标比例为 1.0 的池将具有 0.25 的有效比例。 BIAS：用作倍数，根据之前有关特定池应有多少 PG 的信息手动调整池的 PG 数量。 PG_NUM：池当前的 PG 数量，或如果正在进行 pg_num 更改，则为池正在努力达到的 PG 数量。 NEW PG_NUM（如果存在）：系统建议的池的 pg_num 值。它始终是 2 的幂，并且仅在推荐值与当前值的差异大于默认的 3 倍时出现。 AUTOSCALE：池的 pg_autoscale_mode，设置为 on、off 或 warn。 BULK：确定池是否是批量池。它的值为 True 或 False。批量池预计会很大，应该最初具有大量 PG，以避免性能问题。另一方面，非批量池预计会很小（例如，一个 .mgr 池或元数据池）。 注意如果 ceph osd pool autoscale-status 命令没有返回任何输出，可能至少有一个池跨越了多个 CRUSH 根。这种“跨越池”问题可能发生在以下场景中：当新部署自动在默认 CRUSH 根上创建 .mgr 池时，后续池的创建规则将它们约束到特定的阴影 CRUSH 树。例如，如果您创建一个限制为 deviceclass = ssd 的 RBD 元数据池和一个限制为 deviceclass = hdd 的 RBD 数据池，您将遇到此问题。要解决此问题，将跨越池约束到仅一个设备类。在上述场景中，可能会有一个 replicated-ssd CRUSH 规则生效，可以通过运行以下命令将 .mgr 池约束到 ssd 设备： 1ceph osd pool set .mgr crush_rule replicated-ssd 此干预将导致少量的回填，但通常这些流量很快就会完成。 自动扩缩容在最简单的自动扩缩容方法中，集群允许根据使用情况自动调整 pgp_num。Ceph 会考虑整个系统的总可用存储和目标 PG 数量，考虑每个池中存储的数据量，并相应地分配 PG。系统采取保守的方法，仅在当前 PG 数量（pg_num）与推荐数量的差异超过 3 倍时，才对池进行更改。每个 OSD 的目标 PG 数量由 mon_target_pg_per_osd 参数确定（默认值：100），可以通过以下命令调整： 1ceph config set global mon_target_pg_per_osd 100 自动缩放器会分析池并在每个子树基础上进行调整。由于每个池可能映射到不同的 CRUSH 规则，每条规则可能将数据分配到不同的设备，Ceph 将独立考虑层次结构中每个子树的利用率。例如，映射到 ssd OSD 的池和映射到 hdd OSD 的池将根据这两种不同设备类型的数量来确定各自的最佳 PG 数量。如果一个池使用了两个或更多 CRUSH 根（例如，同时包含 ssd 和 hdd 设备的阴影树），自动缩放器会在管理器日志中发出警告。警告中会列出池的名称和重叠的根集。自动缩放器不会对具有重叠根的池进行扩缩容，因为这种情况可能会导致扩缩容过程出现问题。我们建议将每个池约束到仅一个根（即一个 OSD 类别），以消除警告并确保扩缩容过程成功。 管理标记为批量的池如果一个池被标记为批量池，则自动缩放器会为池分配完整的 PG 数量，然后仅在池的使用比例不均时才缩减 PG 数量。然而，如果一个池未被标记为批量池，则自动缩放器会以最小 PG 数量启动池，仅在池中使用量增加时才创建额外的 PG。要创建一个标记为批量池的池，请运行以下命令： 1ceph osd pool create &lt;pool-name&gt; --bulk 要设置或取消设置现有池的批量标志，请运行以下命令： 1ceph osd pool set &lt;pool-name&gt; bulk &lt;true/false/1/0&gt; 要获取现有池的批量标志，请运行以下命令： 1ceph osd pool get &lt;pool-name&gt; bulk 指定预期池大小当集群或池首次创建时，它只消耗了集群总容量的一小部分，并且系统认为它只需要少量 PG。然而，在某些情况下，集群管理员知道哪些池在长期内可能会消耗大部分系统容量。当 Ceph 提供了这些信息时，可以从一开始就使用更合适的 PG 数量，从而避免后续更改 pg_num 和相关的数据迁移开销。 池的目标大小可以通过两种方式指定：一种是与池的绝对大小（以字节为单位）相关，另一种是作为与所有其他设置了 target_size_ratio 的池的权重关系。 例如，要告诉系统 mypool 预计将消耗 100 TB 的容量，请运行以下命令： 1ceph osd pool set mypool target_size_bytes 100T 或者，要告诉系统 mypool 预计将消耗相对于设置了 target_size_ratio 的其他池的 1.0 比例，请运行以下命令： 1ceph osd pool set mypool target_size_ratio 1.0 如果 mypool 是集群中唯一的池，则预计它将使用集群总容量的 100%。然而，如果集群中包含一个目标比例设置为 1.0 的第二个池，则两个池都预计将使用集群总容量的 50%。 ceph osd pool create 命令具有两个命令行选项，可以在创建池时设置目标大小：--target-size-bytes &lt;bytes&gt; 和 --target-size-ratio &lt;ratio&gt;。 注意，如果指定的目标大小值不合理（例如，大于集群总容量），则会引发健康检查（POOL_TARGET_SIZE_BYTES_OVERCOMMITTED）。 如果为池同时指定了 target_size_ratio 和 target_size_bytes，则会忽略后者，前者将用于系统计算，并会引发健康检查（POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO）。 指定池的 PG 范围 可以指定池的 PG 最小值和最大值。 设置 PG 的最小值和最大值 如果设置了最小值，则 Ceph 不会自行将 PG 数量减少（也不会建议减少）到低于配置值的水平。设置最小值是为了在 I&#x2F;O 期间即使池大部分为空时，也能为客户端提供一定程度的并行性。 如果设置了最大值，则 Ceph 不会自行将 PG 数量增加（也不会建议增加）到高于配置值的水平。 要设置池的 PG 最小值，请运行以下命令： 1ceph osd pool set &lt;pool-name&gt; pg_num_min &lt;num&gt; 要设置池的 PG 最大值，请运行以下命令： 1ceph osd pool set &lt;pool-name&gt; pg_num_max &lt;num&gt; 此外，ceph osd pool create 命令具有两个命令行选项，可用于在创建池时指定池的最小或最大 PG 数量：--pg-num-min &lt;num&gt; 和 --pg-num-max &lt;num&gt;。 预选择PG_NUM在创建池时，可以通过以下命令预选择 pg_num 参数的值： 1ceph osd pool create &#123;pool-name&#125; [pg_num] 如果在命令中选择不指定 pg_num，集群会使用 PG 自动缩放器根据池中存储的数据量自动配置该参数（见上文的自动扩缩容部分）。无论是否在创建时指定 pg_num，都不会影响集群之后是否会自动调整该参数。PG 自动缩放的启用或禁用可以通过以下命令设置： 1ceph osd pool set &#123;pool-name&#125; pg_autoscale_mode (on|off|warn) 没有平衡器时，建议的目标是在每个 OSD 上大约 100 个 PG 副本。使用平衡器时，初始目标是每个 OSD 上大约 50 个 PG 副本。自动缩放器尝试满足以下条件： 每个 OSD 上的 PG 数量应与池中的数据量成正比。 每个池应有 50-100 个 PG，考虑到每个 PG 副本在 OSD 之间的复制开销或纠删编码扩展。 指定 PG_NUM 的相关因素在某种程度上，数据高可用和在 OSD 之间均匀分配的标准倾向于更高的 PG 数量。另一方面，节省 CPU 资源和最小化内存使用的标准则倾向于较低的 PG 数量。数据高可用当一个 OSD 发生故障时，数据丢失的风险增加，直到恢复到配置的复制水平。为了说明这一点，假设以下场景导致一个 PG 的永久数据丢失： OSD 发生故障，所有其包含的对象副本丢失。对于 PG 中的每个对象，其副本数量从三降到两。 Ceph 开始为这个 PG 选择一个新的 OSD，以重新创建每个对象的第三个副本。 在新的 OSD 完全填充第三个副本之前，另一个 OSD 在同一 PG 内发生故障。某些对象将只有一个剩余副本。 Ceph 选择另一个 OSD 并继续复制对象，以恢复所需的副本数量。 在恢复完成之前，PG 内的第三个 OSD 发生故障。如果这个 OSD 恰好包含对象的唯一剩余副本，则对象将永久丢失。 在一个包含 10 个 OSD 的集群中，具有 512 个 PG 和三副本池的情况下，CRUSH 会将每个 PG 分配给三个 OSD。因此，当第一个 OSD 发生故障时，将同时为所有 150 个 PG 开始恢复。被恢复的 150 个 PG 可能会均匀分布在 9 个剩余的 OSD 上。每个剩余的 OSD 因此可能会将对象副本发送到所有其他 OSD，并且还可能接收一些新对象，因为它已成为新的 PG 的一部分。 恢复完成所需的时间取决于 Ceph 集群的架构。比较两种设置：（1）每个 OSD 由一台机器上的 1 TB SSD 托管，所有 OSD 连接到 10 Gb&#x2F;s 交换机，单个 OSD 的恢复在一定时间内完成。（2）每台机器上有两个 OSD，使用 HDD，没有 SSD WAL+DB 和 1 Gb&#x2F;s 交换机。在第二种设置中，恢复将慢至少一个数量级。 在这样的集群中，PG 数量对数据高可用几乎没有影响。无论每个 OSD 有 128 个 PG 还是 8192 个 PG，恢复不会更快或更慢。然而，增加 OSD 的数量可以提高恢复速度。假设我们的 Ceph 集群从 10 个 OSD 扩展到 20 个 OSD。每个 OSD 现在只参与大约 75 个 PG，而不是大约 150 个 PG。所有 19 个剩余的 OSD 仍然需要复制相同数量的对象以进行恢复。但现在有 20 个 OSD 必须复制仅 50 GB 的每个对象，而不是 10 个 OSD 复制 100 GB 的每个对象。如果网络以前是瓶颈，则恢复现在的速度加倍。 类似地，假设我们的集群增长到 40 个 OSD。每个 OSD 仅托管大约 38 个 PG。如果 OSD 发生故障，恢复将比以前更快，除非被其他瓶颈阻碍。然而，假设我们的集群增长到 200 个 OSD。每个 OSD 将只托管大约 7 个 PG。如果 OSD 发生故障，恢复将在最多 7 个 OSD 上进行，这意味着恢复时间将比 40 个 OSD 时长。因此，应该增加 PG 数量。 无论恢复时间多么短，在恢复过程中始终有额外 OSD 失败的可能性。考虑上面的 10 个 OSD 的集群：如果任何 OSD 失败，则大约 150 除以 9 的 PG 将只有一个剩余副本。如果剩余的 8 个 OSD 中的任何一个失败，则大约 17 除以 8 的 PG 可能会丢失其剩余的对象。这是为什么设置 size=2 是有风险的一个原因。 当集群中的 OSD 数量增加到 20 时，损失3个OSD 会显著减少损坏的 PG 数量。第二个 OSD 的损失仅导致大约 17 除以 8 个 PG 损坏，而第三个 OSD 的损失仅导致如果它是包含剩余副本的 4 个 OSD 之一，则才会丢失数据。这意味着——假设恢复期间失去一个 OSD 的概率是 0.0001%——在 10 个 OSD 的集群中丢失三个 OSD 的概率为 X，而在 20 个 OSD 的集群中为 Y。总之，OSD 数量越多，恢复速度越快，因级联故障而永久丢失 PG 的风险越低。就数据高可用而言，在少于 50 个 OSD 的集群中，512 或 4096 个 PG 的数量差异几乎没有影响。 注意最近添加到集群中的 OSD 可能需要较长时间来填充分配给它的 PG。但是，这一过程的缓慢不会导致对象退化或对数据耐用性产生影响，因为 Ceph 会在从旧 PG 中移除数据之前，将数据填充到新的 PG 中。 选择PG的数量如果OSD数量超过50个，我们建议每个OSD大约设置50-100个PG，以平衡资源使用、数据耐久性和数据分布。如果OSD数量少于50个，请参阅预选择部分的指导。对于单个池，使用以下公式获取基线值：[\\text{Total PGs} &#x3D; \\text{OSD数量} \\times \\text{池大小}] 其中池大小是复制池的副本数量或编码池的K+M总和。要检索这个总和，请运行命令 ceph osd erasure-code-profile get。 接下来，检查结果的基线值是否与您设计Ceph集群的方式一致，以最大化数据耐久性和对象分布，并最小化资源使用。 这个值应四舍五入到最接近的2的幂。 每个池的 pg_num 应该是2的幂。其他值可能会导致数据在OSD之间分布不均。最好只在可行且期望的情况下增加 pg_num 到下一个最高的2的幂。注意，这个2的幂规则是针对每个池的；对所有池的 pg_num 的总和对齐到2的幂既不是必要的，也不容易。 例如，如果您有一个200个OSD的集群和一个副本大小为3的单个池，估算PG的数量如下：[\\text{Total PGs} &#x3D; 200 \\times 3 &#x3D; 600] 四舍五入到最接近的2的幂：8192。 当使用多个数据池存储对象时，确保平衡每个池的PG数量和每个OSD的PG数量，以便得到一个合理的PG总数。找到一个为每个OSD提供合理低方差的数字，而不会对系统资源造成过大压力或使配对过程过慢是很重要的。 例如，假设您有一个包含10个池的集群，每个池有512个PG，分布在10个OSD上。这意味着总共有5120个PG分布在10个OSD上，每个OSD上有512个PG。这种集群不会使用过多的资源。然而，在一个包含1000个池的集群中，每个池有512个PG，OSD将处理大约50000个PG。这种集群将需要显著更多的资源和更多的配对时间。 设置PG的数量设置池中初始PG数量必须在创建池时进行。然而，即使在池创建之后，如果未使用 pg_autoscaler 管理 pg_num 值，您仍然可以通过运行以下命令更改PG数量： 1ceph osd pool set &#123;pool-name&#125; pg_num &#123;pg_num&#125; 如果您增加PG数量，集群将不会重新平衡，直到您增加用于放置的PG数量（pgp_num）。pgp_num 参数指定了CRUSH算法在放置时要考虑的PG数量。增加 pg_num 会拆分集群中的PG，但数据不会迁移到新的PG，直到 pgp_num 被增加。pgp_num 参数应与 pg_num 参数相等。要增加用于放置的PG数量，请运行以下命令：shell 1ceph osd pool set &#123;pool-name&#125; pgp_num &#123;pgp_num&#125; 如果您减少PG数量，则 pgp_num 会自动调整。在Nautilus及以后的版本中，当未使用 pg_autoscaler 时，pgp_num 会自动调整以匹配 pg_num。这个过程表现为PG的重新映射和回填，这是正常的预期行为。 获取PG数量要获取池中的PG数量，请运行以下命令： 1ceph osd pool get &#123;pool-name&#125; pg_num 获取集群的PG统计信息要查看集群中PG的详细信息，请运行以下命令： 1ceph pg dump [--format &#123;format&#125;] 有效的格式有plain（默认）和json。 获取卡住PG的统计信息要查看所有处于指定状态的卡住PG的统计信息，请运行以下命令： 1ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format &lt;format&gt;] [-t|--threshold &lt;seconds&gt;] Inactive PGs 不能处理读写操作，因为它们在等待足够的OSD来获得最新的数据。 Undersized PGs 包含未被复制到所需次数的对象。在正常情况下，可以假设这些PG正在恢复。 Stale PGs 处于未知状态 — 托管它们的OSD在一定时间内（由 mon_osd_report_timeout 决定）没有向监视集群报告。 有效的格式有plain（默认）和json。阈值定义PG卡住的最少秒数，超过该时间PG会被包括在返回的统计信息中（默认：300秒）。 获取PG映射要获取特定PG的映射，请运行以下命令： 1ceph pg map &#123;pg-id&#125; Ceph会返回PG映射、PG和OSD状态。输出类似于以下内容： 1osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0] 获取PG的统计信息要查看特定PG的统计信息，请运行以下命令： 1ceph pg &#123;pg-id&#125; query 对PG进行清理要对PG进行清理，请运行以下命令： 1ceph pg scrub &#123;pg-id&#125; Ceph检查主OSD和副本OSD，生成PG中所有对象的目录，并将对象进行对比，以确保没有对象丢失或不匹配，并且其内容是一致的。如果副本全部匹配，则进行最终的语义扫描，以确保所有与快照相关的对象元数据一致。错误会记录在日志中。 要对特定池中的所有PG进行清理，请运行以下命令： 1ceph osd pool scrub &#123;pool-name&#125; 优先恢复&#x2F;回填PG如果遇到多个PG需要恢复或回填的情况，但某些PG中的数据比其他PG中的数据更重要（例如，有些PG包含正在运行的机器使用的镜像数据，而其他PG用于非活动机器，包含的数据不太相关），您可能希望优先恢复或回填特别重要的数据所在的PG，以便尽快恢复集群的性能和数据的可用性。要将特定PG标记为恢复优先，请运行以下命令： 1ceph pg force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...] 要将特定PG标记为回填优先，请运行以下命令： 1ceph pg force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...] 这些命令指示Ceph在处理 其他PG之前，优先对指定的PG进行恢复或回填。优先级不会中断当前的回填或恢复，但会将指定的PG置于队列顶部，以便下一个被处理。如果您改变主意或意识到优先级设置错误，请运行以下命令之一： 12ceph pg cancel-force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]ceph pg cancel-force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...] 这些命令会从指定PG中移除强制标记，使PG按常规顺序处理。与添加强制标记的情况一样，这只会影响那些仍在排队的PG，而不会影响当前正在恢复的PG。 强制标记会在PG的恢复或回填完成后自动清除。 同样，要指示Ceph优先处理指定池中的所有PG（即，首先对这些PG进行恢复或回填），请运行以下命令之一： 12ceph osd pool force-recovery &#123;pool-name&#125;ceph osd pool force-backfill &#123;pool-name&#125; 这些命令也可以被取消。要恢复到默认顺序，请运行以下命令之一： 12ceph osd pool cancel-force-recovery &#123;pool-name&#125;ceph osd pool cancel-force-backfill &#123;pool-name&#125; 警告这些命令可能会打破Ceph内部优先级计算的顺序，因此使用时请小心！如果您有多个池当前共享相同的底层OSD，并且某些池中的数据比其他池中的数据更重要，则建议运行以下命令来为所有池安排自定义恢复&#x2F;回填优先级： 1ceph osd pool set &#123;pool-name&#125; recovery_priority &#123;value&#125; 例如，如果您有二十个池，您可以将最重要的池的优先级设为20，次重要的池设为19，以此类推。另一种选择是仅为适当的池子子集设置恢复&#x2F;回填优先级。在这种情况下，可能会将三个重要的池都分配为优先级1，而所有其他池将没有分配恢复&#x2F;回填优先级。另一种可能性是选择三个重要的池，并将它们的恢复&#x2F;回填优先级分别设置为3、2和1。 重要使用 ceph osd pool set &#123;pool-name&#125; recovery_priority &#123;value&#125; 设置恢复&#x2F;回填优先级时，数值越大优先级越高。例如，优先级为30的池比优先级为15的池具有更高的优先级。 恢复丢失的RADOS对象如果集群丢失了一个或多个RADOS对象，并且您决定放弃对丢失数据的搜索，您必须将未找到的对象标记为丢失。 如果已经查询了所有可能的位置，并且所有OSD都正常，但某些RADOS对象仍然丢失，您可能不得不放弃这些对象。当罕见和异常的故障组合允许集群了解到在写入操作恢复之前执行的写入时，可能会出现这种情况。 标记RADOS对象丢失的命令只有一个支持的选项：revert。revert选项将回滚到RADOS对象的先前版本（如果它足够旧以拥有先前版本），或者完全忽略它（如果它太新而没有先前版本）。要标记“未找到”的对象为丢失，请运行以下命令： 1ceph pg &#123;pg-id&#125; mark_unfound_lost revert|delete 平衡模块平衡模块可以优化放置组（PG）在OSD之间的分配，以实现平衡分布。平衡器可以自动操作，也可以在监督下操作。 状态要检查平衡器的当前状态，请运行以下命令： 1ceph balancer status 自动平衡当平衡器处于 upmap 模式时，自动平衡功能默认启用。要禁用平衡器，请运行以下命令： 1ceph balancer off 平衡器模式可以从 upmap 模式更改为 crush-compat 模式。crush-compat 模式与较旧的客户端向后兼容。在 crush-compat 模式下，平衡器会自动对数据分布进行小的调整，以确保 OSD 被均等利用。 限制如果集群处于降级状态（即，OSD 已失败且系统尚未自愈），则平衡器不会对 PG 分布进行任何调整。 当集群处于健康状态时，平衡器将逐步移动一小部分不平衡的 PG，以改善分布。这个比例不会超过默认的 5% 阈值。要调整这个 target_max_misplaced_ratio 阈值设置，请运行以下命令： 1ceph config set mgr target_max_misplaced_ratio .07 # 7% 平衡器在运行之间会休眠。要设置此休眠间隔的秒数，请运行以下命令： 1ceph config set mgr mgr/balancer/sleep_interval 60 要设置自动平衡开始的时间（HHMM 格式），请运行以下命令： 1ceph config set mgr mgr/balancer/begin_time 0000 要设置自动平衡结束的时间（HHMM 格式），请运行以下命令： 1ceph config set mgr mgr/balancer/end_time 2359 自动平衡可以限制在特定的星期几。要将其限制为特定的星期几或之后的时间（如 crontab 中，0 是星期天，1 是星期一，以此类推），请运行以下命令： 1ceph config set mgr mgr/balancer/begin_weekday 0 要限制自动平衡为特定的星期几或之前的时间（同样，0 是星期天，1 是星期一，以此类推），请运行以下命令： 1ceph config set mgr mgr/balancer/end_weekday 6 自动平衡可以限制在特定的池。默认情况下，这个设置的值是空字符串，这样所有池都会自动平衡。要将自动平衡限制到特定的池，请获取它们的数字池 ID（通过运行 ceph osd pool ls detail 命令），然后运行以下命令： 1ceph config set mgr mgr/balancer/pool_ids 1,2,3 模式支持两种平衡器模式： crush-compat：此模式使用兼容权重集功能（在 Luminous 中引入）来管理 CRUSH 层次结构中设备的备用权重集。当平衡器在此模式下运行时，正常权重应保持为设备的大小，以反映计划存储在设备上的数据量。然后，平衡器将优化权重集值，通过小的增量进行调整，以实现尽可能接近目标分布的分布。（由于 PG 放置是伪随机过程，因此会受到自然变异的影响；优化权重有助于对抗这种自然变异。） 请注意，此模式与较旧的客户端完全向后兼容：当 OSD Map 和 CRUSH 图与较旧的客户端共享时，Ceph 将优化后的权重呈现为“真实”权重。 此模式的主要限制是，平衡器无法处理具有不同放置规则的多个 CRUSH 层次结构，如果层次结构的子树共享任何 OSD。（这种 OSD 共享是不典型的，并且由于管理共享 OSD 上的空间利用的困难，通常不推荐。） upmap：在 Luminous 及更高版本中，OSDMap 可以存储对单个 OSD 的显式映射，作为正常 CRUSH 放置计算的例外。这些 upmap 条目提供了对 PG 映射的细粒度控制。此平衡器模式优化单个 PG 的放置，以实现平衡分布。在大多数情况下，结果分布几乎是完美的：即，每个 OSD 上的 PG 数量相等（±1 PG，因为总数可能无法均匀分割）。 要使用 upmap，所有客户端必须是 Luminous 或更高版本。 默认模式是 upmap。可以通过运行以下命令将模式更改为 crush-compat： 1ceph balancer mode crush-compat 监控优化监督使用平衡器可以分为三个不同的阶段： 制定计划 评估数据分布的质量，无论是当前的 PG 分布还是执行计划后将产生的 PG 分布 执行计划 要评估当前分布，请运行以下命令： 1ceph balancer eval 要评估单个池的分布，请运行以下命令： 1ceph balancer eval &lt;pool-name&gt; 要详细查看评估，请运行以下命令： 1ceph balancer eval-verbose ... 要指示平衡器生成一个计划（使用当前配置的模式），为计划起个名字（任何有用的标识字符串），并运行以下命令： 1ceph balancer optimize &lt;plan-name&gt; 要查看计划的内容，请运行以下命令： 1ceph balancer show &lt;plan-name&gt; 要显示所有计划，请运行以下命令： 1ceph balancer ls 要丢弃旧的计划，请运行以下命令： 1ceph balancer rm &lt;plan-name&gt; 要查看当前记录的计划，请检查以下状态命令的输出： 1ceph balancer status 要评估执行特定计划后将产生的分布，请运行以下命令： 1ceph balancer eval &lt;plan-name&gt; 如果计划预计会改善分布（即计划的评分低于当前集群状态的评分），可以通过运行以下命令执行该计划： 1ceph balancer execute &lt;plan-name&gt;","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"存储性能测试工具","slug":"Storage/存储性能测试","date":"2021-10-11T05:26:35.000Z","updated":"2024-08-03T06:56:02.287Z","comments":true,"path":"Storage/存储性能测试/","permalink":"https://watsonlu6.github.io/Storage/%E5%AD%98%E5%82%A8%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/","excerpt":"","text":"FIO简介FIO是Linux下开源的一款IOPS测试工具，主要用来对磁盘进行压力测试和性能验证。它可以产生许多线程或进程来执行用户特定类型的I&#x2F;O操作，通过编写作业文件或者直接命令去执行测试动作，相当于是一个 多线程的io生成工具，用于生成多种IO模式来测试硬盘设备的性能（大多情况用于测试裸盘性能）。硬盘I&#x2F;O测试主要有以下类型： 随机读 随机写 顺序读 顺序写 混合读写 （可根据需求设置70%读，30%写或100%读等等） FIO的安装与使用github地址：github.com&#x2F;axboe&#x2F;fio下载安装方式： 123456789# 方式1yum -i install fio# 方式2yum -y install libaio-devel #安装libaio引擎，不然执行fio会报“fio: engine libaio not loadable”，必须要在fio安装前安装，不然还要重新编译安装一遍fiowget https://github.com/axboe/fio/archive/refs/tags/fio-3.10.zipcd /root/fio-fio-3.10./configuremke &amp;&amp; make install 常用参数介绍 filename&#x3D;&#x2F;dev&#x2F;sdb 要测试盘的名称，支持文件系统或者裸设备，&#x2F;dev&#x2F;sda2或&#x2F;dev&#x2F;sdb direct&#x3D;1 测试过程绕过机器自带的buffer，使测试结果更真实（Linux在读写时，数据会先写到缓存，再在后台写到硬盘，读的时候也是优先从缓存中读，这样访问速度会加快，但是一旦掉电，缓存中数据就会清空，所有一种模式为DirectIO，可以跳过缓存，直接读写硬盘） ioengine&#x3D;libaio 定义使用什么io引擎去下发io请求 sync：同步IO引擎，使用Linux系统调用实现IO操作，可以测试磁盘性能的上限。 mmap：使用内存映射技术实现IO操作，可以测试文件系统的缓存和文件的预读能力。 libaio：异步IO引擎，使用Linux系统调用libaio实现IO操作，可以测试磁盘的随机读写性能。 posixaio：类似于libaio的异步IO引擎，但使用POSIX AIO接口实现IO操作。 pvsync：使用Linux系统调用实现IO操作，但对写操作进行缓存，并且只在需要时进行刷新，可以提高IO性能。 rbd：用于测试Ceph集群中rados block device (RBD)的性能，支持异步IO和同步IO操作。 iodepth&#x3D;16 队列的深度为16，在异步模式下，CPU不能一直无限的发命令到硬盘设备。比如SSD执行读写如果发生了卡顿，那有可能系统会一直不停的发命令，几千个，甚至几万个，这样一方面SSD扛不住，另一方面这么多命令会很占内存，系统也要挂掉了。这样，就带来一个参数叫做队列深度。 bs&#x3D;4k 单次io的块文件大小为4k numjobs&#x3D;10 并发工作线程数 size&#x3D;5G 每个线程读写的数据量是5GB runtime&#x3D;60 测试时间为60秒，可以设置2m为两分钟。如果不配置此项，会将设置的size大小全部写入或者读取完为止 rw&#x3D;randread 测试随机读的I&#x2F;O rw&#x3D;randwrite 测试随机写的I&#x2F;O rw&#x3D;randrw 测试随机混合写和读的I&#x2F;O rw&#x3D;read 测试顺序读的I&#x2F;O rw&#x3D;write 测试顺序写的I&#x2F;O rw&#x3D;rw 测试顺序混合写和读的I&#x2F;O thread 使用pthread_create创建线程，另一种是fork创建进程。进程的开销比线程要大，一般都采用thread测试 rwmixwrite&#x3D;30 在混合读写的模式下，写占30%（即rwmixread读为70%，单独配置这样的一个参数即可） group_reporting 关于显示结果的，汇总每个进程的信息 name&#x3D;”TDSQL_4KB_read_test” 定义测试任务名称扩展 lockmem&#x3D;1g 只使用1g内存进行测试 zero_buffers 用全0初始化缓冲区，默认是用随机数据填充缓冲区 random_distribution&#x3D;random #默认情况下，fio 会在询问时使用完全均匀的随机分布，有需要的话可以自定义访问区域，zipf、pareto、normal、zoned nrfiles&#x3D;8 每个进程生成文件的数量 测试场景示例100%随机读，5G大小，4k块文件： 1fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=randread -group_reporting -name=&quot;TDSQL_4KB_randread_test&quot; 100%顺序读，5G大小，4k块文件： 1fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=read -group_reporting -name=&quot;TDSQL_4KB_write_test&quot; 70%随机读，30%随机写，5G大小，4k块文件： 1fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=randrw -rwmixread=70 -group_reporting -name=&quot;TDSQL_4KB_randread70-write_test&quot; 70%顺序读，30%随机写，5G大小，4k块文件： 1fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=rw -rwmixread=70 -group_reporting -name=&quot;TDSQL_4KB_read70-write_test&quot; 输出报告1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@localhost test]# fio -filename=/dev/sdb -direct=1 -ioengine=libaio -bs=4k -size=5G -numjobs=10 -iodepth=16 -runtime=60 -thread -rw=randrw -rwmixread=70 -group_reporting -name=&quot;local_randrw_test&quot;# 结果local_randrw_test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=16...fio-3.10Starting 10 threadsJobs: 10 (f=10): [m(10)][100.0%][r=19.4MiB/s,w=8456KiB/s][r=4969,w=2114 IOPS][eta 00m:00s]local_randrw_test: (groupid=0, jobs=10): err= 0: pid=11189: Mon Oct 25 11:01:46 2021 read: IOPS=5230, BW=20.4MiB/s (21.4MB/s)(1226MiB/60031msec) slat (usec): min=2, max=342637, avg=1266.82, stdev=7241.29 clat (usec): min=4, max=459544, avg=20056.81, stdev=24888.90 lat (usec): min=134, max=459586, avg=21329.16, stdev=25378.16 clat percentiles (usec): | 1.00th=[ 1467], 5.00th=[ 1844], 10.00th=[ 2147], 20.00th=[ 2606], | 30.00th=[ 3032], 40.00th=[ 3556], 50.00th=[ 4359], 60.00th=[ 6063], | 70.00th=[ 36439], 80.00th=[ 46924], 90.00th=[ 51643], 95.00th=[ 59507], | 99.00th=[105382], 99.50th=[117965], 99.90th=[137364], 99.95th=[152044], | 99.99th=[219153] bw ( KiB/s): min= 795, max= 4494, per=9.91%, avg=2072.23, stdev=744.04, samples=1195 iops : min= 198, max= 1123, avg=517.74, stdev=186.00, samples=1195 write: IOPS=2243, BW=8972KiB/s (9188kB/s)(526MiB/60031msec) slat (usec): min=2, max=311932, avg=1272.76, stdev=7272.09 clat (usec): min=6, max=458031, avg=20206.30, stdev=24897.71 lat (usec): min=974, max=459755, avg=21484.12, stdev=25400.41 clat percentiles (usec): | 1.00th=[ 1500], 5.00th=[ 1860], 10.00th=[ 2147], 20.00th=[ 2606], | 30.00th=[ 3064], 40.00th=[ 3621], 50.00th=[ 4424], 60.00th=[ 6194], | 70.00th=[ 36439], 80.00th=[ 46924], 90.00th=[ 51643], 95.00th=[ 59507], | 99.00th=[105382], 99.50th=[117965], 99.90th=[137364], 99.95th=[149947], | 99.99th=[200279] bw ( KiB/s): min= 357, max= 1944, per=9.90%, avg=888.57, stdev=325.49, samples=1195 iops : min= 89, max= 486, avg=221.80, stdev=81.37, samples=1195 lat (usec) : 10=0.01%, 50=0.01%, 100=0.01%, 250=0.02%, 500=0.01% lat (usec) : 750=0.01%, 1000=0.01% lat (msec) : 2=7.45%, 4=38.36%, 10=18.10%, 20=1.09%, 50=22.31% lat (msec) : 100=11.42%, 250=1.24%, 500=0.01% cpu : usr=0.26%, sys=19.41%, ctx=12026, majf=0, minf=18 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, &gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, &gt;=64=0.0% issued rwts: total=313975,134655,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=16Run status group 0 (all jobs): READ: bw=20.4MiB/s (21.4MB/s), 20.4MiB/s-20.4MiB/s (21.4MB/s-21.4MB/s), io=1226MiB (1286MB), run=60031-60031msec WRITE: bw=8972KiB/s (9188kB/s), 8972KiB/s-8972KiB/s (9188kB/s-9188kB/s), io=526MiB (552MB), run=60031-60031msecDisk stats (read/write): sdb: ios=314008/134653, merge=0/0, ticks=189470/89778, in_queue=279286, util=99.75% 输出报告分析下面是每个执行的数据方向的I&#x2F;O统计数据信息的代表值含义 read&#x2F;write： 读&#x2F;写的IO操作（还有一个trim没用过） salt： 提交延迟，这是提交I&#x2F;O所花费的时间（min:最小值，max:最大值，avg:平均值，stdev:标准偏差） chat： 完成延迟，表示从提交到完成I&#x2F;O部分的时间 lat： 相应时间，表示从fio创建I&#x2F;O单元到完成I&#x2F;O操作的时间 bw： 带宽统计 iops： IOPS统计 lat(nsec&#x2F;usec&#x2F;msec)： I&#x2F;O完成延迟的分布。这是从I&#x2F;O离开fio到它完成的时间。与上面单独的读&#x2F;写&#x2F;修剪部分不同，这里和其余部分的数据适用于报告组的所有I&#x2F; o。10&#x3D;0.01%意味着0.01%的I&#x2F;O在250us以下完成。250&#x3D;0.02%意味着0.02%的I&#x2F;O需要10到250us才能完成。 cpu： cpu使用率 IO depths： I&#x2F;O深度在作业生命周期中的分布 IO submit： 在一个提交调用中提交了多少个I&#x2F;O。每一个分录表示该数额及其以下，直到上一分录为止——例如，4&#x3D;100%意味着我们每次提交0到4个I&#x2F;O调用 IO complete： 和上边的submit一样，不过这个是完成了多少个 IO issued rwt： 发出的read&#x2F;write&#x2F;trim请求的数量，以及其中有多少请求被缩短或删除 IO latency： 满足指定延迟目标所需的I&#x2F;O深度 bw： 总带宽以及最小和最大带宽 io： 该组中所有线程执行的累计I&#x2F;O run： 这组线程中最小和最长的运行时。 ios： 所有组的I&#x2F; o个数 merge： I&#x2F;O调度器执行的总合并数 ticks： 使磁盘繁忙的滴答数（仅供参考，原文是Number of ticks we kept the disk busy） in_queue： 在磁盘队列中花费的总时间 util： 磁盘利用率。值为100%意味着我们保留了磁盘，如果一直很忙，那么50%的时间磁盘就会闲置一半的时间 FIO通过配置文件运行除了命令行直接执行命令外，也可以通过写配置到xxx.fio文件中，每次只用修改配置即可，使用更方便些，执行方式为fio xxx.fio 12345678910111213141516171819202122232425262728293031323334353637[root@localhost jobs]# cat test.fio[global]filename=/dev/sdbioengine=libaiodirect=1threadgroup_reporting[randread-4k-128M]rw=randreadbs=4ksize=128Mnumjobs=5[randwrite-4k-128M]rw=randwritebs=4ksize=128Mnumjobs=5[write-4k-128M]rw=writebs=4ksize=128Mnumjobs=5#执行fio命令测试[root@localhost jobs]# fio test.fiorandread-4k-128M: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1...randwrite-4k-128M: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1...write-4k-128M: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1...fio-3.10Starting 15 threadsJobs: 6 (f=6): [_(3),r(1),_(1),w(5),E(1),_(4)][92.1%][r=10.8MiB/s,w=29.2MiB/s][r=2777,w=7483 IOPS][eta 00m:05s] 性能关注的重点 顺序读写和随机读写：顺序读写指对大块数据进行读写操作，随机读写则是对随机位置的小块数据进行读写操作。顺序读写通常比随机读写更快，因为它可以利用SSD的顺序读写优势。然而，随机读写需要处理更多的数据以找到所需数据，因此其IOPS和带宽通常低于顺序读写。 SSD与HDD性能比较： HDD：顺硬盘驱动器 (HDD) 的顺序读写和随机读写性能是不同的。顺序读写操作是指读写大块连续的数据，这是硬盘驱动器的强项。因为磁头可以直接读写连续的数据块，因此顺序读写速度快。随机读写操作是指读写随机位置的小块数据，这是硬盘驱动器的弱点。因为磁头需要频繁移动以读写不同的数据块，因此随机读写速度较慢。因此，硬盘驱动器的顺序读写速度通常比随机读写速度快。在评估硬盘驱动器性能时，需要考虑两种读写方式的结果。 SSD：SSD 具有很高的随机读写性能，但顺序读写性能仍然不如随机读写性能。因为 SSD 需要管理大量的块，因此对大块连续的数据的读写可能不如对随机位置的小块数据的读写快。但需要注意的是，SSD 的顺序读写性能仍然高于硬盘驱动器 (HDD)。因此，即使是 SSD 的顺序读写性能不如随机读写性能，它仍然具有很高的性能。 测试配置建议 numjobs和iodepth设置： numjobs过大可能导致任务等待时间过长，建议设置为1、2、4、8。 iodepth可以设置大一些，建议为64、128，以增加IO任务队列。 利用率观察：通过iostat -x -m 5 /dev/sdb查看磁盘的utilize是否达到百分百。如果未达到，可继续增加numjobs直到utilize达到百分百。 关闭写缓存： 关闭写缓存可以提高数据持久性测试的准确性。 使用hdparm -W /dev/device查看当前状态，hdparm -W 0 /dev/device关闭写缓存。 其他建议 获取设备信息：使用smartctl -a /dev/device获取设备型号、连接版本和速度。 numjobs大小:FIO的多线程调度其实还是一个进程，numjobs过大的话会导致任务等待时间过长，任务一直在排队。numjobs不能开太大，建议是1、2、4、8；iodepth任务可以开大点，建议是64、128，因为队列任务相当于IO任务，进行压测时，numjobs从1、2、4、8往上调，同时使用iostat -x -m 5 /dev/sdb查看磁盘的utilize是否达到百分百；如果当前numjobs的utilize还不是百分百，表示不是压测，numjobs再往上加，直到utilize达到百分百为止。 多个fio进程测一个SSD和一个fio进程多线程测一个SSD的区别 多个fio进程测一个SSD和一个fio进程多线程测一个SSD的主要区别在于并发性和资源利用率。多个fio进程可以并发地访问SSD并生成更多的负载，这可以更好地测试SSD的并行读写能力和响应时间，但同时也会占用更多的CPU和内存资源。此外，由于多个fio进程之间的I&#x2F;O请求存在竞争关系，可能会影响测试结果的准确性和一致性。 相比之下，一个fio进程多线程测一个SSD可以更好地利用系统资源并模拟真实的应用程序I&#x2F;O模式。在这种情况下，每个线程可以并发地访问SSD并生成负载，同时避免了多个fio进程之间的竞争关系。这可以更好地测试SSD的性能和稳定性，并提供更准确的测试结果。 总的来说，两种方法都有其优缺点和适用场景。需要根据实际情况选择适当的测试方法来评估SSD的性能和可靠性。 磁盘性能摸底时需要关闭写缓存？ 在使用fio进行性能测试时，是否需要关闭写缓存取决于具体的测试需求和测试方案。如果测试场景需要模拟真实应用程序中的写操作，并希望测试结果反映出SSD的真实性能水平，那么建议关闭写缓存，以便更准确地衡量SSD的写性能和数据持久性。然而，在某些情况下，为了测试SSD的I&#x2F;O性能而不是数据持久性，或者为了测试SSD的读性能，可能需要保持写缓存打开。因此，是否需要关闭写缓存取决于具体的测试需求和测试方案，需要根据实际情况进行决定。 关闭写缓存会使得磁盘的性能更具可预测性，因为每次写入都会立即被持久化到磁盘上，可以更准确地测试磁盘的写入性能和数据持久性。然而，关闭写缓存会使得写入操作变慢，因为每个写入操作都必须等待磁盘确认数据已经被永久写入。 不关闭写缓存会使得磁盘的写入性能更高，因为数据可以先被缓存起来，减少了写入操作对磁盘的访问次数，从而提高了写入性能。然而，数据可能会在缓存中存储一段时间，而不是立即写入磁盘，这可能会导致数据丢失或不一致。 通过合理配置fio参数，可以有效测试存储系统的性能。需要根据实际需求选择适当的测试方法和参数设置，确保测试结果的准确性和代表性。在测试过程中，需特别注意顺序读写和随机读写性能的差异，以及SSD和HDD在不同读写模式下的表现。 Ceph Rados性能测试工具Ceph 提供了 rados bench 和 rados load-gen 两个命令，用于测试和评估集群的性能。以下是这两个命令的用法和选项。 rados bench 命令rados bench 命令用于对 Ceph 集群进行基准测试，以评估集群的读写性能。 语法1rados bench &#123;seconds&#125; &#123;operation&#125; [options] 参数说明 &#123;seconds&#125;：测试运行的持续时间，单位为秒。 &#123;operation&#125;：指定测试的操作类型，包括 write、seq（顺序读）、rand（随机读）。 常用选项 -p &#123;pool&#125; 或 --pool &#123;pool&#125;：指定使用的存储池名称。 -b &#123;block_size&#125; 或 --block-size &#123;block_size&#125;：指定块大小，默认值为 4MB。 -t &#123;threads&#125; 或 --threads &#123;threads&#125;：指定使用的线程数，默认值为 16。 -n &#123;num_objects&#125; 或 --num-objects &#123;num_objects&#125;：指定创建的对象数量。 -c 或 --no-cleanup：在测试结束后保留测试数据。 -D 或 --verify：启用数据验证。 示例写入测试1rados bench 60 write --pool testpool 顺序读测试1rados bench 60 seq --pool testpool 随机读测试1rados bench 60 rand --pool testpool 使用自定义块大小和线程数1rados bench 60 write --pool testpool --block-size 8192 --threads 32 rados load-gen 命令rados load-gen 命令用于生成负载，以测试和评估 Ceph 集群在不同负载下的性能。 语法1rados load-gen [options] 常用选项 -b &#123;bytes&#125; 或 --block-size &#123;bytes&#125;：指定块大小，默认值为 4096 字节。 -t &#123;threads&#125; 或 --threads &#123;threads&#125;：指定使用的线程数，默认值为 16。 -o &#123;objects&#125; 或 --objects &#123;objects&#125;：指定创建的对象数量，默认值为 100。 -p &#123;pool&#125; 或 --pool &#123;pool&#125;：指定使用的存储池名称，默认值为 rbd。 -c &#123;clients&#125; 或 --clients &#123;clients&#125;：指定客户端数量，默认值为 1。 -d &#123;seconds&#125; 或 --duration &#123;seconds&#125;：指定测试运行的持续时间，单位为秒。 --num-objects：指定对象的总数 --min-object-size：指定最小object尺寸 --max-object-size：指定最大object尺寸 --min-op-len：指定操作的最小 io 长度 --max-op-len：指定操作的最大 io 长度 --max-ops：指定最大操作数 --max-backlog：指定最大压测规模 --read-percent：指定读取操作的百分比 --target-throughput：指定目标吞吐量（以字节为单位） --run-length：指定总时间（以秒为单位） 示例使用默认参数测试1rados load-gen 指定块大小和对象数量测试1rados load-gen --block-size 8192 --objects 500 在指定存储池中测试1rados load-gen --pool mypool 使用多个线程和客户端测试1rados load-gen --threads 32 --clients 4 指定时间的测试1rados load-gen --duration 60 指定object大小范围的测试1rados -p hdd_pool load-gen --num-objects 200 --min-object-size 4K --max-object-size 4M --max-ops 20 --read-percent 0 --min-op-len 4K --max-op-len 1M --target-throughput 20G --run-length 20 --num-threads 64","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"rbd相关运维命令","slug":"Storage/Ceph/rbd相关运维命令","date":"2021-09-22T16:31:00.000Z","updated":"2024-08-04T08:03:25.937Z","comments":true,"path":"Storage/Ceph/rbd相关运维命令/","permalink":"https://watsonlu6.github.io/Storage/Ceph/rbd%E7%9B%B8%E5%85%B3%E8%BF%90%E7%BB%B4%E5%91%BD%E4%BB%A4/","excerpt":"","text":"创建 rbd 镜像12rbd create &#123;pool-name&#125;/&#123;image-name&#125; [--size &#123;size&#125;] [--image-format &#123;format&#125;] [--features &#123;feature-list&#125;] 参数说明 {pool-name}&#x2F;{image-name}：指定存储池名称和镜像名称。 –size {size}：设置镜像的大小。 –image-format {format}：指定镜像的格式。有效的格式包括 1（兼容旧版格式）和 2（支持更多特性）。 –features {feature-list}：启用镜像特性，特性名称之间用逗号分隔。例如，layering,exclusive-lock。 查看 rbd 镜像列表1rbd ls &#123;pool-name&#125; 获取 rbd 镜像信息1rbd info &#123;pool-name&#125;/&#123;image-name&#125; 删除 rbd 镜像1rbd rm &#123;pool-name&#125;/&#123;image-name&#125; 映射 rbd 镜像到本地设备1rbd map &#123;pool-name&#125;/&#123;image-name&#125; 取消映射 rbd 镜像1rbd unmap /dev/rbd/&#123;pool-name&#125;/&#123;image-name&#125; 扩展 rbd 镜像大小1rbd resize &#123;pool-name&#125;/&#123;image-name&#125; --size &#123;new-size-in-MB&#125; 从ceph导出 RBD 镜像用于将镜像的数据导出到一个本地文件中。 1rbd export &#123;pool-name&#125;/&#123;image-name&#125; &#123;output-file&#125; 向ceph导入 RBD 镜像用于将本地文件导入到ceph rbd中。 1rbd import &#123;input-file&#125; &#123;pool-name&#125;/&#123;image-name&#125; 启用rbd特性1rbd feature enable &#123;pool-name&#125;/&#123;image-name&#125; &#123;feature-name&#125; rbd镜像特性 layering：支持图层（Layering） exclusive-lock：独占锁（Exclusive Locking） object-map：对象映射（Object Map） 禁用rbd特性1rbd feature disable &#123;pool-name&#125;/&#123;image-name&#125; &#123;feature-name&#125; 启用和禁用所有特性12rbd feature enable &#123;pool-name&#125;/&#123;image-name&#125; --allrbd feature disable &#123;pool-name&#125;/&#123;image-name&#125; --all RBD 复制命令用于复制 RBD 镜像到同一存储池中的新镜像，或复制到不同存储池中。 1rbd cp &#123;source-pool-name&#125;/&#123;source-image-name&#125; &#123;destination-pool-name&#125;/&#123;destination-image-name&#125; 启RBD 深度复制命令用于执行深度复制，包括镜像的所有快照和元数据。这个命令是 RADOS 镜像的完整复制工具。 1rbd deep-copy &#123;source-pool-name&#125;/&#123;source-image-name&#125; &#123;destination-pool-name&#125;/&#123;destination-image-name&#125; RBD 差异命令用于查看两个 RBD 镜像之间的差异，包括哪些块已更改、已删除或已新增。它可以帮助识别镜像之间的更改。 1rbd diff &#123;pool-name&#125;/&#123;image-name&#125; [--snap &#123;snapshot-name&#125;] [--diff &#123;other-image&#125;] RBD 磁盘使用命令用于显示 RBD 镜像的磁盘使用情况，包括镜像占用的总磁盘空间和其他相关信息。 1rbd du &#123;pool-name&#125;/&#123;image-name&#125; RBD 状态命令用于显示 RBD 镜像的状态信息，包括镜像的健康状态和其他相关信息。 1rbd status &#123;pool-name&#125;/&#123;image-name&#125; RBD 观察命令用于观察镜像的实时更改，这个命令允许用户跟踪镜像的变化，包括数据的写入、删除和其他操作。 1rbd watch &#123;pool-name&#125;/&#123;image-name&#125; RBD 稀疏化命令用于将 RBD 镜像的已分配但未使用的空间标记为稀疏。通过稀疏化，可以释放磁盘上未使用的空间，从而优化存储资源。注意事项 影响：稀疏化操作会扫描镜像并更新其内部元数据，可能会占用一定的 I&#x2F;O 带宽和计算资源。 稀疏化条件：只有在镜像的写入操作完成后，才建议执行稀疏化，以避免在镜像空间仍在使用时进行操作。1rbd sparsify &#123;pool-name&#125;/&#123;image-name&#125; 创建 rbd 快照1rbd snap create &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; 保护 rbd 快照1rbd snap protect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; 取消保护 rbd 快照1rbd snap unprotect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; 列出 rbd 快照1rbd snap ls &#123;pool-name&#125;/&#123;image-name&#125; 回滚到 rbd 快照1rbd snap rollback &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; 删除 rbd 快照1rbd snap rm &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; rbd trashrbd trash 功能允许管理员在删除 rbd 镜像时先将其移动到 trash，而不是立即永久删除。这为误删除的镜像提供了恢复的机会。以下是 rbd trash 的详细说明和常用操作指南。rbd trash 功能的主要特点包括： 安全性：防止意外删除镜像。 可恢复性：在一定时间内可以恢复已删除的镜像。 定期清理：可以设置镜像在 trash 中的过期时间，自动清理过期的镜像。 查看 rbd trash 中的镜像1rbd trash ls &#123;pool-name&#125; 将 rbd 镜像移动到 trash1rbd trash mv &#123;pool-name&#125;/&#123;image-name&#125; 恢复 trash 中的 rbd 镜像1rbd trash restore &#123;pool-name&#125;/&#123;image-id or image-name&#125; 永久删除 trash 中的 rbd 镜像1rbd trash rm &#123;pool-name&#125;/&#123;image-id or image-name&#125; 查看 trash 中镜像的详细信息1rbd trash info &#123;pool-name&#125;/&#123;image-id or image-name&#125; 设置 trash 镜像的过期时间1rbd trash mv &#123;pool-name&#125;/&#123;image-name&#125; --expire &#123;time-spec&#125; 其中，&#123;time-spec&#125; 可以是以下格式之一： 1d：1天 1h：1小时 1m：1分钟 查看 trash 镜像的过期时间1rbd trash list &#123;pool-name&#125; --long 清空 trash1rbd trash purge &#123;pool-name&#125; 克隆 rbd 镜像1rbd clone &#123;pool-name&#125;/&#123;parent-image&#125;@&#123;snap-name&#125; &#123;pool-name&#125;/&#123;clone-name&#125; 合并克隆的 rbd 镜像1rbd flatten &#123;pool-name&#125;/&#123;clone-name&#125; 启用镜像同步1rbd mirror image enable &#123;pool-name&#125;/&#123;image-name&#125; &#123;mode&#125; 禁用镜像同步1rbd mirror image disable &#123;pool-name&#125;/&#123;image-name&#125; 查看镜像同步状态1rbd mirror image status &#123;pool-name&#125;/&#123;image-name&#125; 查看镜像配置1rbd config image list &#123;pool-name&#125;/&#123;image-name&#125; 修改镜像配置1rbd config image set &#123;pool-name&#125;/&#123;image-name&#125; &#123;config-key&#125; &#123;value&#125; 删除镜像配置1rbd config image rm &#123;pool-name&#125;/&#123;image-name&#125; &#123;config-key&#125;","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph相关运维命令","slug":"Storage/Ceph/Ceph常见运维命令","date":"2021-09-07T12:01:00.000Z","updated":"2024-08-02T17:38:18.260Z","comments":true,"path":"Storage/Ceph/Ceph常见运维命令/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E5%B8%B8%E8%A7%81%E8%BF%90%E7%BB%B4%E5%91%BD%E4%BB%A4/","excerpt":"","text":"查看 Ceph 的守护进程使用以下命令查看所有 Ceph 守护进程： 1systemctl list-unit-files | grep ceph 按类型在 Ceph 节点上启动特定类型的所有守护进程1234systemctl start/restart/stop ceph-osd.targetsystemctl start/restart/stop ceph-mon.targetsystemctl start/restart/stop ceph-mds.targetsystemctl start/restart/stop ceph-radosgw.target 启动特定守护进程实例1234systemctl start/status/restart/stop ceph-osd@&#123;id&#125;systemctl start/status/restart/stop ceph-mon@&#123;hostname&#125;systemctl start/status/restart/stop ceph-mds@&#123;hostname&#125;systemctl start/restart/stop ceph-radosgw@&#123;hostname&#125; 查看 Ceph 集群状态123ceph -s# 或者ceph health detail 输出信息包括： 集群的 ID 集群健康状况 monitor map 版本和 mon 法定人数状态 OSD map 版本和 OSD 状态摘要 PG map 版本 PG 和 Pool 的数量 集群存储的数据量，对象的总量，以及集群的已用容量&#x2F;总容量&#x2F;可用容量 客户端的 IOPS 信息 观察集群中的状态1ceph -w 输出信息包含： 集群的 ID 集群健康状况 monitor map 版本和 mon 法定人数状态 OSD map 版本和 OSD 状态摘要 PG map 版本 PG 和 Pool 的数量 集群存储的数据量，对象的总量，以及集群的已用容量&#x2F;总容量&#x2F;可用容量 客户端的 IOPS 信息 检查集群的容量情况1ceph df 修改集群配置查看默认配置1ceph --show-config 修改配置Ceph 支持在运行时更改 ceph-osd、ceph-mon、ceph-mds 守护进程的配置。 使用 tell 的方式1234ceph tell &#123;daemon-type&#125;.&#123;id or *&#125; injectargs --&#123;name&#125; &#123;value&#125; [--&#123;name&#125; &#123;value&#125;]# 示例：ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1 使用 daemon 的方式设置在设置的角色所在主机上进行设置。 1234# 查看配置ceph daemon osd.1 config get mon_osd_full_ratio# 修改配置ceph daemon osd.1 config set mon_osd_full_ratio 0.97 在线调整日志级别1ceph tell osd.0 injectargs --debug-osd 0/5 修改配置文件进行调整编辑 /etc/ceph/ceph.conf 中的 [global] 字段添加配置，重启相应服务生效。 查看 mon 状态1ceph mon stat 查看 mon 的详细状态1ceph daemon mon.ceph-xx-mon00 mon_status mon 法定人数状态1ceph quorum_status -f json-pretty 查看 mon 选举状态1ceph quorum_status 查看 mon 的映射信息1ceph mon dump 查看 mon 的 admin socket12ceph-conf --name mon.ceph-xx-mon00 --show-config-value admin_socket/var/run/ceph/ceph-mon.ceph-xx-mon00.asok CRUSH Map创建 bucket1ceph osd crush add-bucket host-xx host 移动 bucket1ceph osd crush move host-xx room=default 提取 CRUSH Map1ceph osd getcrushmap -o crush 反编译 crush map1crushtool -d crush -o de_crush 编译 crush map1crushtool -c de_crush -o new_crush 测试新的 CRUSH Map1crushtool --test -i new_crush --num-rep 3 --rule 1 --show-mappings 注入 CRUSH Map1ceph osd setcrushmap -i new_crush 列出 crush_rule1ceph osd crush rule ls 查看 crush_rule1ceph osd crush rule dump &#123;rule&#125; PG 和 PGP查看 PG 状态1ceph pg stat 查看 PG 组映射信息1ceph pg dump 查看一个 PG 的 map1ceph pg map 1.2f6 查看 PG 详细信息1ceph pg 1.2f6 query 显示集群所有 PG 统计1ceph pg dump --format plain 显示非正常状态的 PG1ceph pg dump_stuck inactive|unclean|stale OSD查看 OSD 状态1ceph osd stat 检查 OSD 容量是否均衡1ceph osd df tree 查看 OSD 映射信息1ceph osd dump 查看 OSD 目录树1ceph osd tree 定位 OSD 在集群中的节点位置1ceph osd find [osd] 查看对象在哪些 OSD 上1ceph osd map test-pool object1 下线某个 OSD1ceph osd down 0 拉起某个 OSD1ceph osd up 0 将某个 OSD 逐出集群1ceph osd out 0 将某个 OSD 加入集群1ceph osd in 0 删除某个 OSD1ceph osd rm 0 查看 OSD 延迟1ceph osd perf 查看当前 OSD 的状态1ceph daemon osd.14 perf dump Pool查看 pool 信息1ceph osd lspools 查看 pool 详细信息1ceph osd pool ls detail 查看 pool 状态1ceph osd pool stats 创建 pool创建副本 pool1ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; &#123;pgp-num&#125; replicated &#123;crush-ruleset-name&#125; 创建 EC pool1ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; &#123;pgp-num&#125; erasure &#123;erasure-code-profile&#125; 创建 erasure-code-profile1ceph osd erasure-code-profile set ec-4-2 k=4 m=2 ruleset-failure-domain=host ruleset-root=hddRoom 列出 erasure-code-profile1ceph osd erasure-code-profile ls 查看 erasure-code-profile1ceph osd erasure-code-profile get [name] 删除 pool1234# 删除 pool 前需要执行ceph tell mon.* injectargs --mon-allow-pool-delete=true# 删除poolceph osd pool delete test_pool test_pool --yes-i-really-really-mean-it #pool的名字需要重复两次 设置 pool 的 PG 数量1ceph osd pool set test_pool pg_num 100 查看 pool 的 PG 数量1ceph osd pool get test_pool pg_num 设置 pool 的 PGP 数量1ceph osd pool set test_pool pgp_num 100 查看 pool 的 PGP 数量1ceph osd pool get test_pool pgp_num 设置 pool 池副本数1ceph osd pool set test_pool size 3 查看 pool 池副本数1ceph osd pool get test_pool size 设置存储池 crush rule1ceph osd pool set &lt;poolname&gt; crush_ruleset &lt;ruleset&gt; 查看存储池 crush rule1ceph osd pool get &lt;poolname&gt; crush_rule RADOS查看对象信息1rados -p test_pool stat test-object-1 获取对象内容1rados -p test_pool get test-object-1 test.txt 将指定文件作为对象写入到资源池1rados -p test_pool put test-object-2 test.txt 删除对象1rados -p test_pool rm test-object-1","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph线程池实现","slug":"Storage/Ceph/Ceph线程池实现","date":"2021-08-27T14:26:42.000Z","updated":"2024-07-28T10:16:50.698Z","comments":true,"path":"Storage/Ceph/Ceph线程池实现/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"线程池和工作队列是紧密相连的，基本流程就是将任务送入到对应的工作队列中，线程池中的线程从工作队列中取出任务并进行处理。Ceph 为了支持高并发读写，源码设计中大量采用线程池来进行io的推进。Ceph的线程池实现了多种不同的工作队列。一般情况下，一个线程池对应一个类型的工作队列。在要求不高的情况下，也可以一个线程池对应多种类型的工作队列，让线程池处理不同类型的任务。 mutex的实现src&#x2F;common&#x2F;mutex.h condition variable的实现src&#x2F;common&#x2F;cond.h 线程的实现Ceph中线程的在src&#x2F;common&#x2F;Thread.h中定义线程编程接口中，一个线程在创建时调用pthread_create函数来传入entry函数，杀死线程调用pthread_kill函数，当线程被杀死之后，必须调用pthread_join函数来进行线程资源的回收，如果不调用此函数，就会出现类似zombie process。如果要想让系统自己回收线程资源，就要将线程与父线程分离即调用pthread_detach。通过接口对比，src&#x2F;common&#x2F;Thread.h中定义的class thread，实际上是Ceph自己封装了一个线程类，这个线程类其实就是对Linux线程接口的一层封装。 Ceph中所有要用的线程必须继承Thread类，通过查找发现如下一些线程： Accepter.h (src\\msg)：class Accepter : public Thread &#x2F;&#x2F;用来socket bind的线程, accepter线程入口函数里定义了poll的网络通讯结构，用来放入管道 Admin_socket.h (src\\common)：class AdminSocket : public Thread Ceph_context.cc (src\\common)：class CephContextServiceThread : public Thread DispatchQueue.h (src\\msg): class DispatchThread : public Thread &#x2F;&#x2F;用来进行消息分发的线程， 在simpleMessenger中有dispatch_queue成员变量, FileJournal.h (src\\os): class Writer : public Thread &#x2F;&#x2F;用来进行写数据到journal中的线程 FileJournal.h (src\\os): class WriteFinisher : public Thread &#x2F;&#x2F;当用aio异步模式写数据到journal完成后，此线程用来接管其他剩余操作 FileStore.h (src\\os): struct SyncThread : public Thread &#x2F;&#x2F;用来同步数据执行同步的线程，主要是将已经完成的journal的序列号写入到文件中 Finisher.h (src\\common): struct FinisherThread : public Thread &#x2F;&#x2F;公用的finisher线程，用来查看某些特定的操作是否结束，结束后进行后续处理工作 MDLog.h (src\\mds): class ReplayThread : public Thread OSD.h (src\\osd): struct T_Heartbeat : public Thread &#x2F;&#x2F;维系osd进程之间互相心跳连接的线程 OutputDataSocket.h (src\\common):class OutputDataSocket : public Thread Pipe.h (src\\msg): class Reader : public Thread &#x2F;&#x2F;用来处理所有对socket的读操作，由acepter线程将socket accept以后打入到SimpleMessenger::dispatch_queue中交由此线程处理 Pipe.h (src\\msg): class Writer : public Thread &#x2F;&#x2F;用来处理所有对socket的写操作，由acepter线程将socket accept以后打入到SimpleMessenger::dispatch_queue中交由此线程处理 Pipe.h (src\\msg): class DelayedDelivery: public Thread &#x2F;&#x2F;用来处理所有对socket的延时操作 Signal_handler.cc (src\\global)：struct SignalHandler : public Thread SimpleMessenger.h (src\\msg): class ReaperThread : public Thread &#x2F;&#x2F;用来进行消息通信的主要线程 reaper是用来在通讯完成时拆除管道，其中成员有accepter线程（用来bind，accept socket文件放入管道），还有dispatch_queue线程 Throttle.cc (src\\test\\common): class Thread_get : public Thread Timer.cc (src\\common)：class SafeTimerThread : public Thread WorkQueue.h (src\\common): struct WorkThread : public Thread 可以将这些线程分为四类线程 普通类线程： 使用此类线程类直接申明继承自Thread，重写一个entry函数，在进程启动最初时，调用了create函数创建了线程，同时使用它的人必须自己定义消息队列。上面大部分线程都是此类，比如FileJournal::write_thread就是一个FileJournal::Writer类对象，它自己定义了消息队列FileJournal::writeq SafeTimerThread类线程: 此类线程使用者可以直接申明一个SafeTimer成员变量，因为SafeTimer中已经封装了SafeTimerThread类和一个消息队列（成员是Context回调类），并完成了entry函数的逻辑流程。使用者使用方法，就是设置回调函数，通过SafeTimer::add_event_after函数将钩子埋入，等待规定时间到达后执行。 FinisherThread类线程: 此类线程使用者可以直接申明一个Finisher成员变量，因为Finsher中已经封装了FinisherThread类和一个消息队列（成员是Context回调类），并完成entry函数的逻辑流程。使用者使用方法，就是设置回调函数，通过Finisher::queue函数将钩子埋入，等待某类操作完成后执行。 ThreadPool内部线程： 这类线程由于是具体工作类线程，所以他们一般都是以线程池形式一下创建多个。ThreadPool类内部有多个线程set&lt;WorkThread*&gt;和多个消息队列vector&lt;WorkQueue_*&gt;组成。工作流程就是线程不断的轮询从队列中拿去数据进行操作。 可以看到Ceph线程的所有接口都只是对相应的Linux接口的封装。继承其的子类主要在于实现entry()函数： 线程池的实现Ceph中线程池的在src&#x2F;common&#x2F;WorkQueue.h中定义线程池和工作队列其实是密不可分的，从Ceph的代码中也可以看出来。让任务推入工作队列，而线程池中的线程负责从工作队列中取出任务进行处理。工作队列和线程池的关系，类似于狡兔和走狗的关系，正是因为有任务，所以才需要雇佣线程来完成任务，没有了狡兔，走狗也就失去了存在的意义。而线程必须要可以从工作队列中认领任务并完成，这就类似于猎狗要有追捕狡兔的功能。正因为两个数据结构拥有如此紧密的关系，因此，Ceph中他们的相关函数都位于WorkQueue.cc和WorkQueue.h中。 void ThreadPool::start()函数ThreadPool::start()用来启动线程池，其在加锁的情况下，调用函数start_threads()，start_threads()检查当前的线程数，如果小于配置的线程池线程数，就创建新的工作线程。 struct WorkThread : public Thread ThreadPool::worker()线程池的关键在于线程的主函数做的事情。首先是工作线程。线程池中会有很多的WorkThread，它的基类就是Thread。线程的主函数为pool-&gt;worker，即ThreadPool::worker函数。其entry函数其实就是调用线程池的worker函数进行具体的工作。 ThreadPool::worker函数内定义了WorkThread类线程的操作逻辑。基本流程就是轮询所有WorkQueue_，当发现某种类型WorkQueue_中有数据时拿出，然后依次调用该WorkQueue_自己定义的函数_void_process和_void_process_finish等函数来顺序执行操作。（worker函数的主要实现其实很常规，就是遍历work_queues，从其中找出每一个消息队列实例，并调用WorkQueue_自己定义的函数_void_process和_void_process_finish等函数来顺序执行操作。） 线程池是支持动态调整线程个数的。所谓调整，有两种可能性，一种是线程个数增加，一种线程个数减少。当添加OSD的时候，数据会重分布，恢复的速度可以调节，其中一个重要的参数为osd-max-recovery-threads，该值修改可以实时生效。 ThreadPool::join_old_threads()线程本身是一个loop，不停地处理WorkQueue中的任务，在一个loop的开头，线程个数是否超出了配置的个数，如果超出了，就需要自杀，所谓自杀即将自身推送到_old_threads中，然后跳出loop，直接返回了。线程池中的其他兄弟在busy-loop开头的join_old_threads函数会判断是否存在自杀的兄弟，如果存在的话，执行join，为兄弟收尸。 ThreadPool::start_threads()start_threads函数不仅仅可以用在初始化时启动所有工作线程，而且可以用于动态增加，它会根据配置要求的线程数_num_threads和当前线程池中线程的个数，来创建WorkThread，当然了，他会调整线程的io优先级。 ThreadPool::handle_conf_change()线程池的线程个数如果不够用，也可以动态的增加，通过配置的变化来做到： ThreadPool::pause()线程池的工作线程，绝大部分时间内，自然是busy－loop中处理工作队列上的任务，但是有一种场景是，需要让工作暂时停下来，停止工作，不要处理WorkQueue中的任务。线程池提供了一个标志为_pause,只要_pause不等于0，那么线程池中线程就在loop中就不会处理工作队列中的任务，而是空转。为了能够及时的醒来，也不是sleep，而是通过条件等待，等待执行的时间。 当下达pause指令的时候，很可能线程池中的某几个线程正在处理工作队列中的任务，这种情况下并不是立刻就能停下的，只有处理完手头的任务，在下一轮loop中检查_pause标志位才能真正地停下。那么pause指令就面临选择，要不要等工作线程WorkThread处理完手头的任务。pause函数是等，pauser_new函数并不等，pause_new函数只负责设置标志位，当其返回的时候，某几个线程可能仍然在处理工作队列中的任务。 struct WorkQueue_在ThreadPool这个类中，set&lt;WorkThread*&gt; _threads保存着线程池中的多个线程，vector&lt;WorkQueue_*&gt; work_queues保存着线程池中的待线程处理的消息队列。整个线程池的原理思想比较简单就是生成一定数目的线程，然后线程从队列中遍历获取队列实例，调用实例自带的处理函数_void_process和_void_process_finish处理。 ThreadPool中的WorkQueue_，这是一种抽象的类，只定义了一个队列应该有的一些特定的函数，这些函数几乎都是虚函数，目的是为了调用到自己三个子类BatchWorkQueue、WorkQueueVal、WorkQueue自己定义的函数。而在三个子类中对应函数_void_process、_void_process_finish中又分别调用了使用者自己继承它们而自己实现的具体操作函数如_process,_process_finish。存放在work_queues里面的WorkQueue_类： 这是一个纯虚基类，也就是说不同的线程池要实现自己的队列，继承WorkQueues_并且实现其接口。线程池已经有4个纯虚基类继承这个类： BatchWorkQueue 批量处理队列 WorkQueueVal 存值队列 WorkQueue 存指针队列 PointerWQ 存指针队列 add_work_queue()&#x2F;remove_work_queue()ThreadPool中的add_work_queue和remove_work_queue就是用来建立和移除与WorkQueue关联的函数 TPHandle超时检查，每次线程函数执行时，都会设置一个grace超时时间，当线程执行超过该时间，就认为是unhealthy的状态。当执行时间超过suicide_grace时，OSD就会产生断言而导致自杀。heartbeat_handle_d记录了相关信息，并把该结构添加到HeartbeatMap的系统链表中保存。OSD会有一个定时器，定时检查是否超时。 线程池使用步骤先创建线程池，然后创建WorkQueue的时候，将线程池作为参数传递给WorkQueue，就能建立关系。 声明线程池成员ThreadPool *_tp 声明队列类型ThreadPool::WorkQueue_*_wq 重写WorkQueue中对应函数_void_process,_void_process_finish 调用*_tp.add_work_queue(*_wq)将队列传入 基本线程池扩展在Ceph中有不少线程池会实现继承以上基类：ThreadPool op_tp: 处理client请求 struct recovery_tp: 处理recovery_tp操作 struct command_tp: 处理命令行来的操作 ShardedThreadPool: Ceph还实现了另外一种线程池ShardedThreadPool，这种线程池与上面的线程池不同之处在于这种线程池是多线程共享队列的方式。只有一个队列，多个线程同时对这个队列进行处理。 SharededWQ: shardedThreadPool类型线程池内部有个比较重要的消息队列SharededWQ，该队列将多种OP放入其中 Ceph 在实际使用中，会用到这种线程池","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_rbd客户端实现","slug":"Storage/Ceph/Ceph-rbd源码实现","date":"2021-08-15T14:26:55.000Z","updated":"2024-09-08T10:08:22.331Z","comments":true,"path":"Storage/Ceph/Ceph-rbd源码实现/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-rbd%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Ceph RBD介绍随着云计算的发展，Ceph已经成为目前最为流行的分布式存储系统，俨然存储界的Linux操作系统。Ceph集块存储、文件存储和对象存储于一身，适用场景广泛，用户众多。RBD是 Ceph 分布式存储系统中提供的块存储服务，Ceph的块存储通过一个客户端模块实现，这个客户端可以直接从数据守护进程读写数据（不需要经过一个网关）。根据客户端整合生态系统的差异，使用Ceph的块设备有两种实现方式：librbd (用户态)和krbd (内核态)。RBD：RADOS Block Devices. Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. 使用Ceph的块设备有两种路径（内核态与用户态）：(rbd map就是内核使用ceph块设备，调用librbd&#x2F;librados API访问ceph块设备是用户态) 通过Kernel Module(内核态RBD)：即创建了RBD设备后，把它映射到内核中（使用rbd map命令映射到操作系统上），成为一个虚拟的块设备，这时这个块设备同其他通用块设备一样，设备文件一般为&#x2F;dev&#x2F;rbd0，后续直接使用这个块设备文件就可以了，可以把&#x2F;dev&#x2F;rbd0格式化后挂载到某目录，也可以直接作为裸设备进行使用。krbd是一个内核模块。其在内核中以一个块设备的方式加以实现。这整个Ceph客户端都是以内核模块的方式实现（没有与之相关的用户态进程或者守护进程）。krbd在内核的源码目录源文件:drivers&#x2F;block&#x2F;rbd.c、drivers&#x2F;block&#x2F;rbd_types.h、net&#x2F;ceph&#x2F;、include&#x2F;linux&#x2F;ceph https://www.likecs.com/show-203739919.html https://github.com/torvalds/linux/blob/cfb92440ee71adcc2105b0890bb01ac3cddb8507/drivers/block/rbd.c https://github.com/torvalds/linux/tree/85c7000fda0029ec16569b1eec8fd3a8d026be73/include/linux/ceph 通过librbd(用户态RBD)：即创建了RBD设备后，使用librbd&#x2F;librados库访问和管理块设备。这种方式直接调用librbd提供的接口，实现对RBD设备的访问和管理，不会在客户端产生设备文件，这种方式主要是为虚拟机提供块存储设备，在虚拟机场景中，一般会用QEMU&#x2F;KVM中的RBD驱动部署Ceph块设备，宿主机通过librbd向客户端提供块存储服务。应用方案有：SPDK+librbd&#x2F;librados https://github.com/ceph/ceph/tree/acf835db0376b1b71152949fdfec36e68f4a8474/src/librbd https://github.com/spdk/spdk/tree/cff525d336fb2c4c087413d4c53474b9e61cbdbe/module/bdev/rbd RBD 的块设备由于元数据信息少而且访问不频繁，故 RBD 在 Ceph 集群中不需要单独的守护进程将元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的 Monitor 服务和 OSD 服务进行交互。 RBD 模块相关IO流图 客户端写数据osd过程： 采用的是 librbd 的形式，使用 librbd 创建一个块设备，向这个块设备中写入数据 在客户端本地同过调用 librados 接口，然后经过 pool，rbd，object，pg 进行层层映射（CRUSH 算法）,在 PG 这一层中，可以知道数据保存在哪几个 OSD 上，这几个 OSD 分为主从的关系 客户端与 primary OSD 建立 SOCKET 通信，将要写入的数据传给 primary OSD，由 primary OSD 再将数据发送给其他 replica OSD 数据节点。 IO 时序图librbd提供了针对image的数据读写和管理操作两种访问接口，其中数据读写请求入io_work_queue，然后由线程池中的线程将io请求以object粒度切分并分别调用rados层的aio接口（IoCtxImpl）下发，当所有的object请求完成时，调用librbd io回调（librbd::io::AioCompletion）完成用户层的数据io。而对image的管理操作通常需要涉及单个或多个对象的多次访问以及对内部状态的多次更新，其第一次访问将从用户线程调用至rados层 aio 接口或更新状态后入 op_work_queue 队列进行异步调用，当 rados aio 层回调或 Context 完成时再根据实现逻辑调用新的 rados aio 或构造 Context 回调，如此反复，最后调用应用层的回调完成管理操作请求。 此外为了支持多客户端共享访问 image，librbd 提供了构建于 rados watch&#x2F;notify 之上的通知、远程执行以及 exclusive lock 分布式锁机制。每个 librbd 客户端在打开 image 时（以非只读方式打开）都会 watch image 的 header 对象，从远程发往本地客户端的通知消息或者内部的 watch 错误消息会通过 RadosClient 的 Finisher 线程入 op_work_queue 队列进行异步处理。 RBD读写流程对于任何RBD客户端的读写都要经过以下步骤： 集群句柄创建、读取配置 集群句柄的创建即是librados:Rados的创建，初始化，读取配置 创建：librados::Rados rados; 初始化：librados::Rados::init(const char * const id) 主要是初始化librados::RadosClient 读取配置： librados::Rados::conf_read_file(const char * const path) const librados::Rados::conf_parse_argv(int argc, const char ** argv) const 集群连接 librados::Rados::connect() IO上下文环境初始化（pool创建读写等） librados::Rados::ioctx_create(const char *name, IoCtx &amp;io) 主要是IoCtxImpl即librados::IoCtx rbd创建 librbd::RBD rbd; RBD::create2(IoCtx&amp; io_ctx, const char *name, uint64_t size,uint64_t features, int *order) rbd的读写 librbd::Image image; RBD::open(IoCtx&amp; io_ctx, Image&amp; image, const char *name) Image::write(uint64_t ofs, size_t len, bufferlist&amp; bl) Image::read(uint64_t ofs, size_t len, bufferlist&amp; bl) IO上下文环境关闭 librbd::Image::close() librados::IoCtx::close() 集群句柄关闭 librados::Rados::shutdown() RBD源码介绍librbd以及librados都是属于ceph 的客户端，其提供ceph的接口向上提供块存储服务。librados提供客户端访问Ceph集群的原生态统一接口。其它接口或者命令行工具都基于该动态库实现。在librados中实现了Crush算法和网络通信等公共功能，数据请求操作在librados计算完成后可以直接与对应的OSD交互进行数据传输。librbd 是Ceph提供的在librados上封装的块存储接口的抽象。 librados主要的类是Rados和IoCtxlibrados::Rados负责初始化集群、读取配置、连接集群 librados::IoCtx负责创建IO上下文环境 librados::bufferlist负责读写缓存 librbd最主要的两个类是：RBD和Imagelibrbd::rbd主要负责 Image 的创建、删除、重命名、克隆映像等操作，包括对存储池的元数据的管理，针对部分操作提供异步接口 librbd::image负责image的读写(read&#x2F;write)，以及快照相关的操作等等。同时提供了相关异步操作的接口。 rbd Image的创建rbd卷的创建接口： 函数输入参数： io_ctx: 针对pool的上下文环境，对pool的操作都要首先建立一个相应的上下文环境 *name：rbd卷名字 size：rbd卷大小 features: rbd卷的特性 order: rbd卷的分块大小其具体实现在internal.cc中： 继续往下调用： 根据format格式调用不同的创建接口，现在主流采用新的format2，所用调用新的接口： 12int create_v2(IoCtx&amp; io_ctx, const char *imgname, uint64_t bid, uint64_t size,int order, uint64_t features, uint64_t stripe_unit,uint64_t stripe_count, uint8_t journal_order,uint8_t journal_splay_width, const std::string &amp;journal_pool,const std::string &amp;non_primary_global_image_id,const std::string &amp;primary_mirror_uuid,bool negotiate_features) 这个接口会做如下工作：创建rbd_id.{volume_name}的object： 然后想这个object写入block_name_prefix中的id号： 然后向rbd_directory写入卷名和id的一一映射。 创建名为rbd_header.id的object，并向这个object写入size,order,features,RBD_DATA_PREFIX等信息。 如果有条带化，则会设置条带化信息： 创建名为rbd_object_map.{id}的对象： rbd Image的打开 其实就是生成一个ImageCtx实例，调用其open接口。 rbd Image的写 rbd Image的读 rbd Image的快照 rbd Image的克隆 rbd Image的删除 rbd的读写 要使用librbd, 需要先安装下面两个包。可以通过yum安装, 也可以通过下载ceph源码编译后, 通过make install进行安装。 123$ yum list | grep librbdlibrbd1.x86_64 1:0.80.7-3.el7 baselibrbd1-devel.x86_64 1:0.80.7-3.el7 base 至于如何使用librbd来编程, 请参考下面的代码, 这是使用librbd的一般流程。编译时记得加上链接参数: g++ librbdtest.cpp -lrados -lrbd。更多函数的使用请参考 librbd.hpp。 另外 这里 有一些不错的示例。 12#include &lt;rbd/librbd.hpp&gt;#include &lt;rados/librados.hpp&gt;","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_rbd介绍与使用","slug":"Storage/Ceph/Ceph-rbd介绍与使用","date":"2021-08-10T14:26:55.000Z","updated":"2024-09-08T11:25:39.559Z","comments":true,"path":"Storage/Ceph/Ceph-rbd介绍与使用/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-rbd%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Ceph RBD介绍随着云计算的发展，Ceph已经成为目前最为流行的分布式存储系统，俨然存储界的Linux操作系统。Ceph集块存储、文件存储和对象存储于一身，适用场景广泛，用户众多。RBD是 Ceph 分布式存储系统中提供的块存储服务，Ceph的块存储通过一个客户端模块实现，这个客户端可以直接从数据守护进程读写数据（不需要经过一个网关）。根据客户端整合生态系统的差异，使用Ceph的块设备有两种实现方式：librbd (用户态)和krbd (内核态)。RBD：RADOS Block Devices. Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. 使用Ceph的块设备有两种路径（内核态与用户态）：(rbd map就是内核使用ceph块设备，调用librbd&#x2F;librados API访问ceph块设备是用户态) 通过Kernel Module(内核态RBD)：即创建了RBD设备后，把它映射到内核中（使用rbd map命令映射到操作系统上），成为一个虚拟的块设备，这时这个块设备同其他通用块设备一样，设备文件一般为&#x2F;dev&#x2F;rbd0，后续直接使用这个块设备文件就可以了，可以把&#x2F;dev&#x2F;rbd0格式化后挂载到某目录，也可以直接作为裸设备进行使用。krbd是一个内核模块。其在内核中以一个块设备的方式加以实现。这整个Ceph客户端都是以内核模块的方式实现（没有与之相关的用户态进程或者守护进程）。krbd在内核的源码目录源文件:drivers&#x2F;block&#x2F;rbd.c、drivers&#x2F;block&#x2F;rbd_types.h、net&#x2F;ceph&#x2F;、include&#x2F;linux&#x2F;ceph https://www.likecs.com/show-203739919.html https://github.com/torvalds/linux/blob/cfb92440ee71adcc2105b0890bb01ac3cddb8507/drivers/block/rbd.c https://github.com/torvalds/linux/tree/85c7000fda0029ec16569b1eec8fd3a8d026be73/include/linux/ceph 通过librbd(用户态RBD)：即创建了RBD设备后，使用librbd&#x2F;librados库访问和管理块设备。这种方式直接调用librbd提供的接口，实现对RBD设备的访问和管理，不会在客户端产生设备文件，这种方式主要是为虚拟机提供块存储设备，在虚拟机场景中，一般会用QEMU&#x2F;KVM中的RBD驱动部署Ceph块设备，宿主机通过librbd向客户端提供块存储服务。应用方案有：SPDK+librbd&#x2F;librados https://github.com/ceph/ceph/tree/acf835db0376b1b71152949fdfec36e68f4a8474/src/librbd https://github.com/spdk/spdk/tree/cff525d336fb2c4c087413d4c53474b9e61cbdbe/module/bdev/rbd RBD 的块设备由于元数据信息少而且访问不频繁，故 RBD 在 Ceph 集群中不需要单独的守护进程将元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的 Monitor 服务和 OSD 服务进行交互。 rbd基本块设备命令rbd命令使你能够创建、列出、检查和删除块设备镜像。还可以使用它来克隆镜像、创建快照、将镜像回滚到快照、查看快照等。 1. 创建块设备池使用 ceph 工具创建一个池，使用 rbd 工具初始化池以供 RBD 使用： 1rbd pool init &lt;pool-name&gt; 2. 创建块设备用户除非另有说明，rbd 命令使用 Ceph 用户 ID admin 来访问 Ceph 集群。admin Ceph 用户 ID 允许对集群进行完全的管理访问。建议使用权限比 admin Ceph 用户 ID 少的 Ceph 用户 ID 访问 Ceph 集群。要创建 Ceph 用户，请使用 ceph auth get-or-create 命令指定 Ceph 用户 ID 名称、监视器权限（capabilities）和 OSD 权限（capabilities）： 1ceph auth get-or-create client.&#123;ID&#125; mon &#x27;profile rbd&#x27; osd &#x27;profile &#123;profile name&#125; [pool=&#123;pool-name&#125;][, profile ...]&#x27; mgr &#x27;profile rbd [pool=&#123;pool-name&#125;]&#x27; 例如，要创建一个名为 qemu 的 Ceph 用户 ID，该用户对池 vms 具有读写权限，对池 images 具有只读权限，运行以下命令： 1ceph auth get-or-create client.qemu mon &#x27;profile rbd&#x27; osd &#x27;profile rbd pool=vms, profile rbd-read-only pool=images&#x27; mgr &#x27;profile rbd pool=images&#x27; ceph auth get-or-create 命令的输出是指定 Ceph 用户 ID 的密钥环，可以写入 /etc/ceph/ceph.client.&#123;ID&#125;.keyring。 3. 创建块设备镜像在将块设备添加到节点之前，你必须在 Ceph 存储集群中创建一个镜像。要创建块设备镜像，请运行如下命令： 1rbd create --size &#123;megabytes&#125; &#123;pool-name&#125;/&#123;image-name&#125; 如果在创建镜像时未指定池，则镜像将存储在默认池 rbd 中。例如，如果运行以下命令，将创建一个大小为 1GB、名称为 foo 的镜像，并将其存储在默认池 rbd 中： 1rbd create --size 1024 foo 4. 列出块设备镜像要列出 rbd 池中的块设备，请运行以下命令： 1rbd ls rbd 是默认的池名称，rbd ls 列出默认池中的命令。要列出特定池中的块设备，请运行以下命令，但将 &#123;poolname&#125; 替换为池的名称： 1rbd ls &#123;poolname&#125; 要列出 rbd 池中的“推迟删除”块设备，请运行以下命令： 1rbd trash ls &#123;poolname&#125; 5.检索镜像信息要从特定镜像中检索信息，请运行以下命令，但将 &#123;image-name&#125; 替换为镜像的名称： 1rbd info &#123;image-name&#125; 要从池中的镜像中检索信息，请运行以下命令，但将 &#123;image-name&#125; 替换为镜像的名称，将 &#123;pool-name&#125; 替换为池的名称： 1rbd info &#123;pool-name&#125;/&#123;image-name&#125; 6.调整块设备镜像大小Ceph 块设备镜像是按需配置的。在开始保存数据之前，它们不会实际使用任何物理存储。但是，它们确实有一个最大容量，你可以使用 --size 选项来设置。如果你想增加（或减少）Ceph 块设备镜像的最大大小，请运行以下命令之一： 增加块设备镜像的大小 1rbd resize --size 2048 foo 减少块设备镜像的大小 1rbd resize --size 2048 foo --allow-shrink 7. 删除块设备镜像要删除一个块设备，请运行以下命令，但将 &#123;image-name&#125; 替换为你想删除的镜像的名称： 1rbd rm &#123;image-name&#125; 8. 从池中删除块设备要从池中删除块设备，请运行以下命令，但将 &#123;image-name&#125; 替换为要删除的镜像的名称，将 &#123;pool-name&#125; 替换为要从中删除镜像的池的名称： 1rbd rm &#123;pool-name&#125;/&#123;image-name&#125; 9. “推迟删除”块设备要推迟删除块设备（即将其移动到“垃圾桶”中并稍后删除），请运行以下命令，但将 &#123;image-name&#125; 替换为要移动到垃圾桶中的镜像的名称，将 &#123;pool-name&#125; 替换为池的名称： 1rbd trash mv &#123;pool-name&#125;/&#123;image-name&#125; 10. 从池中删除推迟的块设备要从池中删除推迟的块设备，请运行以下命令，但将 &#123;image-id&#125; 替换为要删除的镜像的 ID，将 &#123;pool-name&#125; 替换为要从中删除镜像的池的名称： 1rbd trash rm &#123;pool-name&#125;/&#123;image-id&#125; 即使镜像有快照或被克隆使用，你也可以将其移动到垃圾桶中。但是，在这些条件下，你不能从垃圾桶中删除它。你可以使用 --expires-at 设置推迟时间（默认为现在）。如果推迟时间尚未到达，除非使用 --force，否则你不能删除镜像。 11.恢复块设备镜像要恢复 rbd 池中的推迟删除块设备，请运行以下命令，但将 &#123;image-id&#125; 替换为镜像的 ID： 1rbd trash restore &#123;image-id&#125; 12.恢复特定池中的块设备镜像要恢复特定池中的推迟删除块设备，请运行以下命令，但将 &#123;image-id&#125; 替换为镜像的 ID，将 &#123;pool-name&#125; 替换为池的名称： 1rbd trash restore &#123;pool-name&#125;/&#123;image-id&#125; 13.恢复镜像时重命名你还可以在恢复镜像时使用 --image 选项进行重命名。例如： 1rbd trash restore swimmingpool/2bf4474b0dc51 --image new-name rbd快照快照是某个时间点上镜像的只读逻辑副本：一个检查点。Ceph 块设备的一个高级功能是你可以创建镜像的快照，以保留时间点状态历史。Ceph 还支持快照分层，这允许你快速而轻松地克隆镜像（例如，虚拟机镜像）。Ceph 块设备快照是通过 rbd 命令和多个高级接口（包括 QEMU、libvirt、OpenStack、OpenNebula 和 CloudStack）进行管理的。 由于 RBD 对镜像（卷）内的文件系统没有感知，快照仅在发生崩溃时保持一致，除非它们在挂载（附加）操作系统内进行协调。因此，我们建议你在创建快照之前暂停或停止 I&#x2F;O 操作。 如果卷包含文件系统，则在创建快照之前，文件系统应该处于内部一致的状态。没有进行写操作冻结的快照在重新挂载之前可能需要进行 fsck 检查。要冻结 I&#x2F;O 操作，可以使用 fsfreeze 命令。有关更多详细信息，请参见 fsfreeze(8) 手册页。 对于虚拟机，可以使用 qemu-guest-agent 自动冻结文件系统以创建快照。 1.创建快照要创建快照，请使用 rbd snap create 命令，并指定池名称、镜像名称和快照名称： 1rbd snap create &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; 2.列出快照要列出镜像的快照，请使用 rbd snap ls 命令，并指定池名称和镜像名称： 1rbd snap ls &#123;pool-name&#125;/&#123;image-name&#125; 3.回滚快照要回滚到快照，请使用 rbd snap rollback 命令，并指定池名称、镜像名称和快照名称： 1rbd snap rollback &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; 将镜像回滚到快照意味着用快照中的数据覆盖镜像的当前版本。执行回滚所需的时间随着镜像大小的增加而增加。从快照克隆比回滚镜像到快照要快。克隆快照是恢复到先前状态的首选方法。 4.删除快照要删除快照，请使用 rbd snap rm 命令，并指定池名称、镜像名称和快照名称： 1rbd snap rm &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snap-name&#125; Ceph OSD 异步删除数据，因此删除快照不会立即释放底层 OSD 的容量。这个过程被称为“snaptrim”，在 ceph status 输出中也称为此。 5.清除快照要删除所有快照，请使用 rbd snap purge 命令，并指定池名称和镜像名称： 1rbd snap purge &#123;pool-name&#125;/&#123;image-name&#125; rbd分层Ceph 支持创建多个基于写时复制（COW）的块设备快照克隆。快照分层使 Ceph 块设备客户端能够非常快速地创建镜像。例如，你可以创建一个写入 Linux 虚拟机的块设备镜像，快照该镜像，保护快照，然后创建任意数量的基于写时复制的克隆。快照是只读的，因此克隆快照简化了语义，使得创建克隆变得迅速。 术语“父” 和 “子” 指的是 Ceph 块设备快照（父）及从快照克隆的相应镜像（子）。这些术语在下面的命令行用法中很重要。 每个克隆的镜像（子）存储对其父镜像的引用，这使得克隆镜像能够打开父快照并读取它。 基于写时复制的快照克隆表现得完全像任何其他 Ceph 块设备镜像。你可以读取、写入、克隆和调整克隆镜像的大小。克隆镜像没有特殊限制。然而，基于写时复制的快照克隆依赖于快照，因此你必须在克隆之前保护快照。下图描述了这个过程。 Ceph 仅支持克隆 “RBD 格式 2” 镜像（即，没有指定 --image-format 1 创建的镜像）。Linux 内核客户端从 3.10 版本开始支持克隆镜像。 Ceph 块设备分层是一个简单的过程。必须有一个镜像。必须创建镜像的快照。必须保护快照。在执行这些步骤后，你可以开始克隆快照。 克隆的镜像有一个对父快照的引用，并包括池 ID、镜像 ID 和快照 ID。池 ID 的包含意味着你可以将快照从一个池克隆到另一个池中的镜像。 镜像模板：块设备分层的常见用例是创建一个基础镜像和一个作为克隆模板的快照。例如：用户可以创建一个 Linux 发行版的镜像（例如，Ubuntu 22.04）并创建其快照。用户可以定期更新镜像并创建新的快照（通过运行诸如 sudo apt-get update、sudo apt-get upgrade 或 sudo apt-get dist-upgrade 以及 rbd snap create 的命令）。随着镜像的成熟，用户可以克隆任何一个快照。 扩展模板：更高级的用例包括扩展模板镜像以提供比基础镜像更多的信息。例如，用户可以克隆一个镜像（例如，一个虚拟机模板），安装其他软件（例如，数据库、内容管理系统、分析系统），然后对扩展的镜像进行快照，该镜像本身可以像基础镜像一样进行更新。 模板池：使用块设备分层的一种方式是创建一个池，其中包含（1）作为模板的基础镜像和（2）这些模板的快照。然后你可以扩展只读权限给用户，以便他们可以克隆快照，即使他们没有允许在池中写入或执行的权限。 镜像迁移&#x2F;恢复：使用块设备分层的一种方式是将数据从一个池迁移或恢复到另一个池中。 保护快照克隆访问父快照。如果用户不小心删除了父快照，所有克隆都会中断。为了防止数据丢失，你必须在克隆之前保护快照： 1rbd snap protect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; 克隆快照要克隆快照，请指定父池、父镜像和父快照，以及子池和镜像名称。你必须在克隆之前保护快照： 1rbd clone &#123;pool-name&#125;/&#123;parent-image-name&#125;@&#123;snap-name&#125; &#123;pool-name&#125;/&#123;child-image-name&#125; 可以将快照从一个池克隆到另一个池中的镜像。例如，你可以在一个池中维护只读镜像和快照作为模板，而在另一个池中维护可写的克隆。 取消保护快照在删除快照之前，你必须先取消保护它。此外，你不能删除有克隆引用的快照。在取消保护快照之前，你必须先扁平化或删除每个克隆： 1rbd snap unprotect &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; 列出快照的子镜像要列出快照的子镜像，请使用 rbd children 命令，并指定池名称、镜像名称和快照名称： 1rbd children &#123;pool-name&#125;/&#123;image-name&#125;@&#123;snapshot-name&#125; 扁平化克隆镜像克隆镜像保留对父快照的引用。当你从克隆中移除对父快照的引用时，你实际上是通过将快照中存储的数据复制到克隆中来“扁平化”克隆。扁平化克隆所需的时间随着快照的大小增加。要删除快照，你必须首先扁平化子镜像（或删除它们）： 1rbd flatten &#123;pool-name&#125;/&#123;image-name&#125; 由于扁平化镜像包含快照中存储的所有数据，因此扁平化镜像比分层克隆占用更多的存储空间。 RBD独占锁独占锁是一种机制，用于防止多个进程以未协调的方式访问同一个 Rados 块设备（RBD）。独占锁在虚拟化中使用广泛（它们防止虚拟机互相覆盖写操作），以及在 RBD 镜像中（它们是基于日志的镜像中的日志记录和基于快照的镜像中增量差异快速生成的先决条件）。 独占锁功能在新创建的镜像上默认启用。这个默认设置可以通过 rbd_default_features 配置选项或 rbd create 命令的 --image-feature 和 --image-shared 选项进行覆盖。 许多镜像功能，包括对象映射和快速差异，依赖于独占锁定。禁用独占锁功能将会对某些操作的性能产生负面影响。 为了维护多客户端访问，独占锁功能实现了客户端之间的自动协作锁过渡。它确保在任何给定时间只有一个客户端可以写入 RBD 镜像，从而保护像对象映射、日志或 PWL 缓存这样的内部镜像结构免受并发修改。 独占锁对用户基本上是透明的： 每当一个客户端（一个 librbd 进程或在 krbd 客户端的情况下，一个客户端节点的内核）需要处理对启用了独占锁的 RBD 镜像的写操作时，它首先会对该镜像获取一个独占锁。如果锁已被其他客户端持有，则会请求该客户端释放锁。 每当持有 RBD 镜像独占锁的客户端收到释放锁的请求时，它会停止处理写操作，刷新其缓存，并释放锁。 每当持有 RBD 镜像独占锁的客户端正常终止时，锁也会被正常释放。 独占锁的正常释放（无论是由于请求还是客户端终止）使得另一个后续客户端能够获取锁并开始处理写操作。 默认情况下，独占锁功能并不防止两个或更多并发运行的客户端轮流打开同一个 RBD 镜像并写入（无论是在同一节点上还是在不同节点上）。实际上，它们的写入只是被线性化，因为锁会以协作方式自动过渡来回。 要禁用客户端之间的自动锁过渡，可以在获取独占锁时指定 RBD_LOCK_MODE_EXCLUSIVE 标志。这通过 rbd device map 命令的 --exclusive 选项暴露。 独占锁功能与 RBD 顾问锁（rbd lock add 和 rbd lock rm 命令）不兼容。 阻止列表 有时，之前持有 RBD 镜像独占锁的客户端不会正常终止，而是突然终止。这可能是因为客户端进程收到了 KILL 或 ABRT 信号，或者客户端节点经历了硬重启或遭遇了电力故障。在这种情况下，锁从未正常释放。这意味着任何新客户端在启动并尝试写入镜像时，必须打破先前持有的独占锁。 然而，进程（或内核线程）可能会挂起或仅仅丧失与 Ceph 集群的网络连接。在这种情况下，打破锁可能是灾难性的：挂起的进程或连接问题可能会自行解决，原始进程可能会与中间启动的新进程竞争，从而以未协调和破坏性的方式访问 RBD 数据。 如果锁无法以标准的正常方式获取，超越的进程不仅打破锁，还会将之前的锁持有者列入阻止列表。这是新客户端进程与 Ceph 监视器之间的协商结果。 在接收到阻止列表请求后，监视器会指示相关的 OSD 不再服务于旧客户端进程的请求； 在完成相关的 OSD 映射更新后，新客户端可以打破先前持有的锁； 在新客户端获取到锁后，它可以开始向镜像写入数据。 阻止列表因此是一种存储级别的资源隔离形式。 为了使阻止列表功能正常工作，客户端必须具备 osd blocklist 能力。这个能力包含在 rbd 能力配置文件中，通常应在所有使用 RBD 的 Ceph 客户端身份上设置。 RBD 镜像同步RBD 镜像可以在两个 Ceph 集群之间异步镜像。这一功能有两种模式： 基于日志的同步：该模式使用 RBD 日志镜像功能来确保集群之间的时间点、崩溃一致性复制。每次写入 RBD 镜像时，首先将写入记录到相关日志中，然后再修改实际的镜像。远程集群将从这个相关日志中读取并重放更新到其本地镜像副本。由于每次写入 RBD 镜像会导致 Ceph 集群中发生两次写入，因此在使用 RBD 日志镜像功能时，写入延迟几乎会增加一倍。 基于快照的同步：该模式使用定期计划或手动创建的 RBD 镜像镜像快照，在集群之间复制崩溃一致的 RBD 镜像。远程集群将确定两个镜像快照之间的任何数据或元数据更新，并将差异复制到其本地镜像副本。借助 RBD 快速差异图像功能，可以快速确定更新的数据块，而无需扫描整个 RBD 镜像。由于此模式不如日志模式那样精细，因此在故障转移场景中，必须在使用之前同步两个快照之间的完整差异。任何部分应用的差异集将在故障转移时被回滚。 注意： 基于日志的同步需要 Ceph Jewel 版本或更高版本；基于快照的同步需要 Ceph Octopus 版本或更高版本。 镜像同步在对等集群内按池进行配置，并且可以在池内的特定图像子集上进行配置。使用基于日志的同步时，您还可以镜像池内的所有图像。镜像同步使用 rbd 命令配置。rbd-mirror 守护进程负责从远程对等集群提取镜像更新，并将其应用于本地集群中的镜像。 根据复制的需求，RBD 镜像同步可以配置为单向或双向复制： 单向复制：当数据仅从主集群镜像到辅助集群时，rbd-mirror 守护进程仅在辅助集群上运行。 双向复制：当数据从一个集群的主镜像镜像到另一个集群的非主镜像（反之亦然）时，rbd-mirror 守护进程会在两个集群上运行。 每个 rbd-mirror 守护进程必须能够同时连接到本地和远程 Ceph 集群（即所有监视器和 OSD 主机）。此外，网络必须在两个数据中心之间具有足够的带宽，以处理镜像工作负载。 1.池配置以下演示了如何执行基本的管理任务，以使用 rbd 命令配置镜像同步。镜像同步是在池级别配置的。这些池配置步骤应在两个对等集群上执行。这些过程假设名为“site-a”和“site-b”的两个集群从单个主机访问，以便清晰。 以下示例中的集群名称对应于具有相同名称的 Ceph 配置文件（例如 /etc/ceph/site-b.conf）。有关如何配置多个集群的信息，请参见 ceph-conf 文档。请注意，rbd-mirror 不要求源集群和目标集群具有唯一的内部名称；两者都可以并且应称为 ceph。rbd-mirror 需要的本地和远程集群的配置文件可以任意命名，容器化守护进程是一种将它们保持在 /etc/ceph 之外以避免混淆的策略。 2.启用镜像同步要启用池的镜像同步，请使用 rbd 发出 mirror pool enable 子命令，指定池名称、镜像模式和可选的友好站点名称来描述本地集群： 1rbd mirror pool enable [--site-name &#123;local-site-name&#125;] &#123;pool-name&#125; &#123;mode&#125; 镜像模式可以是 image 或 pool： image：在镜像模式下配置时，必须显式启用每个镜像的同步。 pool（默认）：在池模式下配置时，池中所有启用了日志功能的镜像都会被镜像。 创建或导入新的引导令牌时也可以指定站点名称。站点名称可以使用相同的 mirror pool enable 子命令进行更改，但请注意，本地站点名称和远程集群使用的相应站点名称通常必须匹配。 3.禁用镜像同步要禁用池上的镜像同步，请指定 mirror pool disable 命令和池名称： 1rbd mirror pool disable &#123;pool-name&#125; 当以这种方式禁用池上的镜像同步时，池内所有显式启用镜像同步的图像也将禁用镜像同步。例如： 12$ rbd --cluster site-a mirror pool disable image-pool$ rbd --cluster site-b mirror pool disable image-pool 4.引导对等集群为了使 rbd-mirror 守护进程发现其对等集群，必须注册对等集群并创建一个用户帐户。可以使用 rbd 和 mirror pool peer bootstrap create 及 mirror pool peer bootstrap import 命令自动完成此过程。 要使用 rbd 手动创建新的引导令牌，请发出 mirror pool peer bootstrap create 子命令，指定池名称和可选的友好站点名称来描述本地集群： 1rbd mirror pool peer bootstrap create [--site-name &#123;local-site-name&#125;] &#123;pool-name&#125; mirror pool peer bootstrap create 的输出将是一个令牌，应该提供给 mirror pool peer bootstrap import 命令。例如，在 site-a 上： 12$ rbd --cluster site-a mirror pool peer bootstrap create --site-name site-a image-pooleyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ== 要使用 rbd 手动导入由另一个集群创建的引导令牌，请指定 mirror pool peer bootstrap import 命令，池名称、创建的令牌的文件路径（或使用 - 从标准输入读取），以及可选的友好站点名称来描述本地集群和镜像方向（默认为双向镜像，但也可以设置为单向镜像）： 1rbd mirror pool peer bootstrap import [--site-name &#123;local-site-name&#125;] [--direction &#123;rx-only or rx-tx&#125;] &#123;pool-name&#125; &#123;token-path&#125; 例如，在 site-b 上： 1234$ cat &lt;&lt;EOF &gt; tokeneyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==EOF$ rbd --cluster site-b mirror pool peer bootstrap import --site-name site-b image-pool token 5.手动添加集群对等体如果需要或当前安装的 Ceph 版本不支持上述引导命令，可以手动指定集群对等体。远程 rbd-mirror 守护进程需要访问本地集群以执行镜像同步。应为远程守护进程创建一个新的本地 Ceph 用户。 用户可以是具有最低权限的 client.radosgw，例如，但最好为 rbd-mirror 创建一个新用户。要创建该用户，请使用以下 ceph auth 命令： 1ceph auth add client.rbd-mirror mon &#x27;allow r&#x27; osd &#x27;allow rwx&#x27; mgr &#x27;allow rw&#x27; mds &#x27;allow r&#x27; 远程集群应有一个对等集群条目以允许从远程 Ceph 集群接收镜像更新。要创建本地 Ceph 集群的对等体条目，请使用 ceph 命令添加对等集群的配置信息： 1ceph config-key put client.radosgw.rgw_mirror.pool.&#123;pool-name&#125;.peers.&#123;remote-site-name&#125; &#123;remote-hostname&#125;:&#123;remote-port&#125; &#123;remote-client-key&#125; 例如： 1ceph config-key put client.radosgw.rgw_mirror.pool.image-pool.peers.site-b 192.168.1.1:6800 AQBzYvU5y5JYkFABx8T/8E6bErfHk8m1IoV9EA== remote-site-name 是要向其发送镜像更新的远程集群的站点名称（相当于远程集群上的站点名称）。 remote-hostname 和 remote-port 是远程 Ceph 集群的主机名和端口。 remote-client-key 是用来连接远程 Ceph 集群的密钥。 一旦完成，rbd-mirror 守护进程将开始从对等集群同步镜像。 6.启动镜像守护进程要启动 rbd-mirror 守护进程，请在每个 Ceph 集群的所有监视器节点上运行 radosgw 守护进程。通过发出 systemctl 命令来完成这一操作： 1systemctl start ceph-radosgw 要检查 radosgw 守护进程是否正在运行，请使用以下命令： 1systemctl status ceph-radosgw 完成这些步骤后，rbd-mirror 守护进程将在 Ceph 集群之间镜像 RBD 镜像。 镜像实时迁移RBD 镜像可以在同一个 Ceph 集群内的不同池、镜像格式和&#x2F;或布局之间进行实时迁移；也可以从另一个 Ceph 集群的镜像进行迁移；或从外部数据源进行迁移。当迁移开始时，源镜像将被深度复制到目标镜像，同时保留所有快照历史，并尽可能保持数据的稀疏分配。 默认情况下，当在同一个 Ceph 集群内进行 RBD 镜像的实时迁移时，源镜像将被标记为只读，所有客户端将重定向 I&#x2F;O 到新的目标镜像。此外，这种模式还可以选择保留源镜像的父级链接以保留稀疏性，或者在迁移过程中将镜像扁平化，以消除对源镜像父级的依赖。 实时迁移过程还可以在仅导入模式下使用，其中源镜像保持不变，目标镜像可以链接到另一个 Ceph 集群中的镜像或外部数据源，如备份文件、HTTP(s) 文件、S3 对象或 NBD 导出。 实时迁移复制过程可以在新的目标镜像被使用时安全地在后台运行。在不使用仅导入模式的情况下，目前要求在准备迁移之前暂时停止使用源镜像。这有助于确保使用镜像的客户端被更新为指向新的目标镜像。 实时迁移需要 Ceph Nautilus 版本或更高版本。对外部数据源的支持需要 Ceph Pacific 版本或更高版本。krbd 内核模块目前不支持实时迁移。 实时迁移过程包含三个步骤： 准备迁移：初始步骤创建新的目标镜像并将其链接到源镜像。如果没有配置为仅导入模式，源镜像也会被链接到目标镜像，并标记为只读。类似于分层镜像，尝试读取目标镜像内的未初始化数据区段将内部重定向读取到源镜像，写入未初始化区段将内部深度复制重叠的源镜像块到目标镜像。 执行迁移：这是一个后台操作，将源镜像中的所有初始化块深度复制到目标镜像。这一步可以在客户端正在积极使用新目标镜像时运行。 完成迁移：一旦后台迁移过程完成，可以提交或中止迁移。提交迁移将移除源镜像和目标镜像之间的交叉链接，并在未配置为仅导入模式时移除源镜像。中止迁移将移除交叉链接，并移除目标镜像。 1.准备迁移在同一个 Ceph 集群内的镜像进行实时迁移的默认过程是通过运行 rbd migration prepare 命令来启动，提供源镜像和目标镜像： 1$ rbd migration prepare migration_source [migration_target] rbd migration prepare 命令接受与 rbd create 命令相同的布局选项，这允许更改不可变的镜像磁盘布局。如果目标镜像是仅用于更改磁盘布局而保持原始镜像名称，则可以跳过 migration_target。 在准备实时迁移之前，必须停止所有使用源镜像的客户端。如果在准备步骤中发现任何客户端以读写模式打开镜像，则准备步骤将失败。一旦准备步骤完成，客户端可以使用新的目标镜像名称重新启动。尝试使用源镜像名称重新启动客户端将导致失败。 rbd status 命令将显示实时迁移的当前状态： 123456$ rbd status migration_targetWatchers: noneMigration: source: rbd/migration_source (5e2cba2f62e) destination: rbd/migration_target (5e2ed95ed806) state: prepared 请注意，为避免在迁移过程中误用，源镜像将被移动到 RBD 垃圾箱中： 1234$ rbd info migration_sourcerbd: error opening image migration_source: (2) No such file or directory$ rbd trash ls --all5e2cba2f62e migration_source 2.准备仅导入迁移仅导入的实时迁移过程是通过运行相同的 rbd migration prepare 命令来启动，但添加 --import-only 可选项，并提供 JSON 编码的源规格来描述如何访问源镜像数据。这个源规格可以通过 --source-spec 可选项直接传递，或通过 --source-spec-path 可选项通过文件或标准输入传递： 1$ rbd migration prepare --import-only --source-spec &quot;&lt;JSON&gt;&quot; migration_target rbd migration prepare 命令接受与 rbd create 命令相同的布局选项。 rbd status 命令将显示实时迁移的当前状态： 123456$ rbd status migration_targetWatchers: noneMigration: source: &#123;&quot;stream&quot;:&#123;&quot;file_path&quot;:&quot;/mnt/image.raw&quot;,&quot;type&quot;:&quot;file&quot;&#125;,&quot;type&quot;:&quot;raw&quot;&#125; destination: rbd/migration_target (ac69113dc1d7) state: prepared 源规格 JSON 的一般格式如下： 12345678&#123; &quot;type&quot;: &quot;&lt;format-type&gt;&quot;, &lt;format unique parameters&gt; &quot;stream&quot;: &#123; &quot;type&quot;: &quot;&lt;stream-type&gt;&quot;, &lt;stream unique parameters&gt; &#125;&#125; 当前支持的格式包括：native、qcow 和 raw。当前支持的流类型包括：file、http、s3 和 nbd。 格式 native 格式 可以用来描述 Ceph 集群内的本地 RBD 镜像作为源镜像。其源规格 JSON 编码如下： 123456789101112&#123; &quot;type&quot;: &quot;native&quot;, [&quot;cluster_name&quot;: &quot;&lt;cluster-name&gt;&quot;,] (如果镜像在其他集群中，指定，要求 `&lt;cluster-name&gt;.conf` 文件) [&quot;client_name&quot;: &quot;&lt;client-name&gt;&quot;,] (用于连接到其他集群，默认为 `client.admin`) &quot;pool_name&quot;: &quot;&lt;pool-name&gt;&quot;, [&quot;pool_id&quot;: &lt;pool-id&gt;,] (可选，作为 &quot;pool_name&quot; 的替代) [&quot;pool_namespace&quot;: &quot;&lt;pool-namespace&quot;,] (可选) &quot;image_name&quot;: &quot;&lt;image-name&gt;&quot;, [&quot;image_id&quot;: &quot;&lt;image-id&gt;&quot;,] (如果镜像在垃圾箱中，指定) &quot;snap_name&quot;: &quot;&lt;snap-name&gt;&quot;, [&quot;snap_id&quot;: &quot;&lt;snap-id&gt;&quot;,] (可选，作为 &quot;snap_name&quot; 的替代)&#125; 注意，native 格式不包括 stream 对象，因为它利用了 Ceph 的本地操作。例如，要从镜像 rbd/ns1/image1@snap1 导入，源规格可以编码为： 1234567&#123; &quot;type&quot;: &quot;native&quot;, &quot;pool_name&quot;: &quot;rbd&quot;, &quot;pool_namespace&quot;: &quot;ns1&quot;, &quot;image_name&quot;: &quot;image1&quot;, &quot;snap_name&quot;: &quot;snap1&quot;&#125; qcow 格式 可用于描述 QCOW（QEMU copy-on-write）块设备。目前支持 QCOW（v1）和 QCOW2 格式，但不支持高级功能，如压缩、加密、备份文件和外部数据文件。未来版本可能会添加对这些缺失功能的支持。qcow 格式数据可以链接到任何支持的流源。例如，其基本源规格 JSON 编码如下： 123456&#123; &quot;type&quot;: &quot;qcow&quot;, &quot;stream&quot;: &#123; &lt;stream unique parameters&gt; &#125;&#125; raw 格式 可用于描述厚配置的原始块设备导出（即 rbd export --export-format 1 &lt;snap-spec&gt;）。raw 格式数据可以链接到任何支持的流源。例如，其基本源规格 JSON 编码如下： 123456789101112131415&#123; &quot;type&quot;: &quot;raw&quot;, &quot;stream&quot;: &#123; &lt;stream unique parameters for HEAD, non-snapshot revision&gt; &#125;, &quot;snapshots&quot;: [ &#123; &quot;type&quot;: &quot;raw&quot;, &quot;name&quot;: &quot;&lt;snapshot-name&gt;&quot;, &quot;stream&quot;: &#123; &lt;stream unique parameters for snapshot&gt; &#125; &#125;, ] (可选，按从旧到新的顺序排列的快照)&#125; 快照数组的包含是可选的，目前仅支持厚配置的原始快照导出。未来版本将添加更多格式，如 RBD export-format v2 和 RBD export-diff 快照。 file 流 可用于从本地可访问的 POSIX 文件源导入。其源规格 JSON 编码如下： 1234567&#123; &lt;format unique parameters&gt; &quot;stream&quot;: &#123; &quot;type&quot;: &quot;file&quot;, &quot;file_path&quot;: &quot;&lt;file-path&gt;&quot; &#125;&#125; 例如，要从位于 /mnt/image.raw 的文件导入 raw 格式镜像，其源规格 JSON 编码如下： 1234567&#123; &quot;type&quot;: &quot;raw&quot;, &quot;stream&quot;: &#123; &quot;type&quot;: &quot;file&quot;, &quot;file_path&quot;: &quot;/mnt/image.raw&quot; &#125;&#125; http 流 可用于从远程 HTTP 或 HTTPS 网络服务器导入。其源规格 JSON 编码如下： 123456789&#123; &lt;format unique parameters&gt; &quot;stream&quot;: &#123; &quot;type&quot;: &quot;http&quot;, &quot;url&quot;: &quot;&lt;url-path&gt;&quot; &#125;&#125; 例如，要从位于 http://download.ceph.com/image.raw 的文件导入 raw 格式镜像，其源规格 JSON 编码如下： 1234567&#123; &quot;type&quot;: &quot;raw&quot;, &quot;stream&quot;: &#123; &quot;type&quot;: &quot;http&quot;, &quot;url&quot;: &quot;http://download.ceph.com/image.raw&quot; &#125;&#125; s3 流 可用于从远程 S3 存储桶导入。其源规格 JSON 编码如下： 123456789&#123; &lt;format unique parameters&gt; &quot;stream&quot;: &#123; &quot;type&quot;: &quot;s3&quot;, &quot;url&quot;: &quot;&lt;url-path&gt;&quot;, &quot;access_key&quot;: &quot;&lt;access-key&gt;&quot;, &quot;secret_key&quot;: &quot;&lt;secret-key&gt;&quot; &#125;&#125; 例如，要从位于 http://s3.ceph.com/bucket/image.raw 的文件导入 raw 格式镜像，其源规格 JSON 编码如下： 123456789&#123; &quot;type&quot;: &quot;raw&quot;, &quot;stream&quot;: &#123; &quot;type&quot;: &quot;s3&quot;, &quot;url&quot;: &quot;http://s3.ceph.com/bucket/image.raw&quot;, &quot;access_key&quot;: &quot;NX5QOQKC6BH2IDN8HC7A&quot;, &quot;secret_key&quot;: &quot;LnEsqNNqZIpkzauboDcLXLcYaWwLQ3Kop0zAnKIn&quot; &#125;&#125; access_key 和 secret_key 参数支持将密钥存储在 MON 配置密钥存储中，可以通过在密钥值前加上 config:// 前缀和路径来实现。可以通过 ceph config-key set &lt;key-path&gt; &lt;value&gt; 来存储值（例如 ceph config-key set rbd/s3/access_key NX5QOQKC6BH2IDN8HC7A）。 nbd 流 可用于从远程 NBD 导出导入。其源规格 JSON 编码如下： 1234567&#123; &lt;format unique parameters&gt; &quot;stream&quot;: &#123; &quot;type&quot;: &quot;nbd&quot;, &quot;uri&quot;: &quot;&lt;nbd-uri&gt;&quot; &#125;&#125; 例如，要从位于 nbd://nbd.ceph.com 的 NBD 导出中导入 raw 格式镜像，导出名称为 image.raw，其源规格 JSON 编码如下： 1234567&#123; &quot;type&quot;: &quot;raw&quot;, &quot;stream&quot;: &#123; &quot;type&quot;: &quot;nbd&quot;, &quot;uri&quot;: &quot;nbd://nbd.ceph.com/image.raw&quot; &#125;&#125; nbd-uri 参数应遵循 NBD URI 规范。默认 NBD 端口是 10809。 3.执行迁移准备好实时迁移后，必须将源镜像的块复制到目标镜像。这是通过运行 rbd migration execute 命令来完成的： 12$ rbd migration execute migration_targetImage migration: 100% complete...done. rbd status 命令还会提供迁移块深度复制过程的进度反馈： 1234567$ rbd status migration_targetWatchers: watcher=1.2.3.4:0/3695551461 client.123 cookie=123Migration: source: rbd/migration_source (5e2cba2f62e) destination: rbd/migration_target (5e2ed95ed806) state: executing (32% complete) 4.提交迁移一旦实时迁移完成了从源镜像到目标镜像的所有数据块的深度复制，可以提交迁移： 12345678$ rbd status migration_targetWatchers: noneMigration: source: rbd/migration_source (5e2cba2f62e) destination: rbd/migration_target (5e2ed95ed806) state: executed$ rbd migration commit migration_targetCommit image migration: 100% complete...done. 如果 migration_source 镜像是一个或多个克隆的父镜像，则需要在确保所有后代克隆镜像未使用的情况下指定 --force 选项。 提交实时迁移将移除源镜像和目标镜像之间的交叉链接，并移除源镜像： 1$ rbd trash list --all 5.中止迁移如果希望撤销准备或执行步骤，可以运行 rbd migration abort 命令以撤销迁移过程： 12$ rbd migration abort migration_targetAbort image migration: 100% complete...done. 中止迁移将导致目标镜像被删除，并恢复对原始源镜像的访问： 12$ rbd lsmigration_source RBD 持久只读缓存共享只读父镜像缓存克隆的 RBD 镜像通常只修改父镜像的一小部分。例如，在 VDI 使用场景中，虚拟机是从相同的基础镜像克隆的，最初的区别仅在于主机名和 IP 地址。在启动过程中，所有这些虚拟机读取的是相同父镜像数据的不同部分。如果我们有一个本地的父镜像缓存，这将加速缓存主机上的读取操作，同时减少客户端到集群的网络流量。RBD 缓存必须在 ceph.conf 中显式启用。ceph-immutable-object-cache 守护进程负责在本地磁盘上缓存父内容，未来对该数据的读取将从本地缓存中提供。RBD 共享只读父镜像缓存需要 Ceph Nautilus 版本或更高版本。 启用 RBD 共享只读父镜像缓存要启用 RBD 共享只读父镜像缓存，需要在 ceph.conf 文件的 [client] 部分中添加以下 Ceph 设置： 12rbd parent cache enabled = truerbd plugins = parent_cache 不可变对象缓存守护进程ceph-immutable-object-cache 守护进程负责在其本地缓存目录中缓存父镜像内容。建议使用 SSD 作为底层存储，因为这样可以提供更好的性能。守护进程的主要组件包括： 基于域套接字的进程间通信（IPC）：守护进程在启动时监听本地域套接字，并等待来自 librbd 客户端的连接。 基于 LRU 的提升&#x2F;降级策略：守护进程维护每个缓存文件的内存中缓存命中统计。如果容量达到配置的阈值，则降级冷缓存。 基于文件的缓存存储：守护进程维护一个简单的文件缓存存储。在提升时，RADOS 对象从 RADOS 集群中获取并存储在本地缓存目录中。 当每个克隆的 RBD 镜像被打开时，librbd 尝试通过其 Unix 域套接字连接到缓存守护进程。在 librbd 成功连接后，它在每次后续读取时与守护进程协调。在未缓存读取的情况下，守护进程将 RADOS 对象提升到本地缓存目录中，下一个读取操作将从缓存中服务。守护进程维护简单的 LRU 统计信息，用于在需要时逐出冷缓存文件（例如，当缓存达到容量并且受到压力时）。 以下是一些重要的缓存配置设置： immutable_object_cache_sock描述：用于 librbd 客户端和 ceph-immutable-object-cache 守护进程之间通信的域套接字路径。类型：字符串必需：否默认值：/var/run/ceph/immutable_object_cache_sock immutable_object_cache_path描述：不可变对象缓存数据目录。类型：字符串必需：否默认值：/tmp/ceph_immutable_object_cache immutable_object_cache_max_size描述：不可变缓存的最大大小。类型：大小必需：否默认值：1G immutable_object_cache_watermark描述：缓存的高水位线。值在 (0, 1) 之间。如果缓存大小达到此阈值，守护进程将开始根据 LRU 统计信息删除冷缓存。类型：浮点数必需：否默认值：0.9 ceph-immutable-object-cache 守护进程可以在可选的 ceph-immutable-object-cache 发行包中找到。ceph-immutable-object-cache 守护进程需要能够连接到 RADOS 集群。 运行不可变对象缓存守护进程ceph-immutable-object-cache 守护进程应使用唯一的 Ceph 用户 ID。要创建 Ceph 用户，可以使用 ceph 指定 auth get-or-create 命令、用户名、监视器权限和 OSD 权限： 1ceph auth get-or-create client.ceph-immutable-object-cache.&#123;unique id&#125; mon &#x27;allow r&#x27; osd &#x27;profile rbd-read-only&#x27; ceph-immutable-object-cache 守护进程可以通过 systemd 管理，将用户 ID 作为守护进程实例指定： 1systemctl enable ceph-immutable-object-cache@ceph-immutable-object-cache.&#123;unique id&#125; ceph-immutable-object-cache 也可以通过 ceph-immutable-object-cache 命令在前台运行： 1ceph-immutable-object-cache -f --log-file=&#123;log_path&#125; QOS 设置不可变对象缓存支持限流，由以下设置控制： immutable_object_cache_qos_schedule_tick_min描述：不可变对象缓存的最小调度间隔。类型：毫秒必需：否默认值：50 immutable_object_cache_qos_iops_limit描述：每秒所需的不可变对象缓存 IO 操作限制。类型：无符号整数必需：否默认值：0 immutable_object_cache_qos_iops_burst描述：不可变对象缓存 IO 操作的突发限制。类型：无符号整数必需：否默认值：0 immutable_object_cache_qos_iops_burst_seconds描述：不可变对象缓存 IO 操作的突发持续时间（秒）。类型：秒必需：否默认值：1 immutable_object_cache_qos_bps_limit描述：每秒所需的不可变对象缓存 IO 字节限制。类型：无符号整数必需：否默认值：0 immutable_object_cache_qos_bps_burst描述：不可变对象缓存 IO 字节的突发限制。类型：无符号整数必需：否默认值：0 immutable_object_cache_qos_bps_burst_seconds描述：不可变对象缓存 IO 字节的突发持续时间（秒）。类型：秒必需：否默认值：1 RBD 持久写日志缓存持久写日志缓存（PWL）为基于 librbd 的 RBD 客户端提供了一个持久且容错的写回缓存。 该缓存采用日志顺序写回设计，内部维护检查点，以确保写入被刷新回集群时始终保持崩溃一致性。即使客户端缓存完全丢失，磁盘镜像仍然是一致的，但数据可能会显得过时。 此缓存可以使用 PMEM 或 SSD 作为缓存设备。对于 PMEM，缓存模式称为副本写日志（rwl）。目前仅支持本地缓存，副本功能尚在开发中。对于 SSD，缓存模式称为 ssd。 使用PWL 缓存在持久设备中管理缓存数据。它会在配置的目录中查找并创建缓存文件，然后将数据缓存到文件中。PWL 缓存依赖于独占锁功能。只有在获取了独占锁后，缓存才能加载。 缓存提供两种不同的持久性模式。在持久写模式下，写入操作仅在写入被持久化到缓存设备后才会完成，并且在崩溃后仍然可读。在持久刷新模式下，写入操作一旦不再需要调用者的数据缓冲区即可完成，但不保证在崩溃后写入数据可读。数据在收到刷新请求时持久化到缓存设备。默认情况下，缓存模式为持久写模式，在第一次刷新请求后切换到持久刷新模式。 1.启用缓存要启用 PWL 缓存，请设置以下配置项： 12rbd_persistent_cache_mode = &#123;cache-mode&#125;rbd_plugins = pwl_cache &#123;cache-mode&#125; 的值可以是 rwl、ssd 或 disabled。默认情况下，缓存是禁用的。 rwl 缓存模式依赖于 libpmem 库（PMDK 的一部分）。它在 x86_64 架构上应普遍可用，并且在某些发行版的 ppc64le 和 aarch64 架构上也可能可用。在 s390x 架构上不可用。 以下是一些缓存配置项： rbd_persistent_cache_path描述：缓存数据的文件夹。在使用 rwl 模式时，这个文件夹必须启用 DAX（参见 DAX），以避免性能下降。类型：字符串必需：否默认值：无 rbd_persistent_cache_size描述：每个镜像的缓存大小。最小缓存大小为 1 GB。类型：大小必需：否默认值：无 上述配置可以按主机、池、镜像等进行设置。例如，要按主机设置，请将覆盖项添加到主机的 ceph.conf 文件的相应部分。要按池、镜像等设置，请参阅 rbd 配置命令。 2.缓存状态当获取到独占锁时，PWL 缓存被启用；当释放独占锁时，缓存被关闭。要检查缓存状态，可以使用命令 rbd status。 1rbd status &#123;pool-name&#125;/&#123;image-name&#125; 状态信息将显示，包括当前状态、清理状态、缓存大小、位置以及一些基本指标。例如： 12345678910111213141516171819$ rbd status rbd/fooWatchers: watcher=10.10.0.102:0/1061883624 client.25496 cookie=140338056493088Persistent cache state: host: sceph9 path: /mnt/nvme0/rbd-pwl.rbd.101e5824ad9a.pool size: 1 GiB mode: ssd stats_timestamp: Sun Apr 10 13:26:32 2022 present: true empty: false clean: false allocated: 509 MiB cached: 501 MiB dirty: 338 MiB free: 515 MiB hits_full: 1450 / 61% hits_partial: 0 / 0% misses: 924 hit_bytes: 192 MiB / 66% miss_bytes: 97 MiB 3.刷新缓存要刷新 RBD 缓存文件，请指定持久缓存刷新命令、池名称和镜像名称： 1rbd persistent-cache flush &#123;pool-name&#125;/&#123;image-name&#125; 如果应用程序意外崩溃，此命令也可以用于将缓存刷新回 OSDs。例如： 1$ rbd persistent-cache flush rbd/foo 4.使缓存失效要使 RBD 缓存文件失效（丢弃），请指定持久缓存失效命令、池名称和镜像名称： 1rbd persistent-cache invalidate &#123;pool-name&#125;/&#123;image-name&#125; 该命令会移除对应镜像的缓存元数据，禁用缓存功能，并删除本地缓存文件（如果存在）。例如： 1$ rbd persistent-cache invalidate rbd/foo 镜像加密从 Pacific 版本开始，镜像级别的加密可以由 RBD 客户端内部处理。这意味着你可以设置一个秘密密钥，用于加密特定的 RBD 镜像。此页面描述了 RBD 加密功能的范围。 当前 krbd 内核模块不支持加密。也可以使用外部工具（如 dm-crypt、QEMU）来加密 RBD 镜像，这些工具的功能集和限制可能与这里描述的有所不同。 加密格式默认情况下，RBD 镜像是未加密的。要加密 RBD 镜像，必须将其格式化为支持的加密格式之一。格式化操作会将加密元数据持久化到镜像中。加密元数据通常包括加密格式和版本、加密算法和模式规范，以及用于保护加密密钥的信息。加密密钥本身由用户保管的秘密（通常是密码短语）保护，该秘密不会持久化。基本的加密格式操作需要指定加密格式和一个秘密。 一些加密元数据可能作为镜像数据的一部分存储，通常一个加密头会写入原始镜像数据的开头。这意味着加密镜像的有效镜像大小可能低于原始镜像大小。有关更多详细信息，请参见支持的格式部分。 除非明确（重新）格式化，否则加密镜像的克隆会使用相同的格式和秘密进行加密。 加密镜像的克隆始终会被加密。重新格式化为明文是不支持的。 格式化前写入镜像的数据可能变得不可读，尽管它仍然可能占用存储资源。 启用了日志功能的镜像不能由 RBD 客户端格式化和加密。 加密加载格式化镜像是启用加密的必要前提。然而，格式化的镜像仍将被所有 RBD API 视为原始未加密镜像。特别是，加密的 RBD 镜像可以通过与其他镜像相同的 API 打开，并可以读取&#x2F;写入原始未加密数据。这种原始 IO 可能会影响加密格式的完整性，例如覆盖位于镜像开头的加密元数据。 为了安全地对格式化的镜像执行加密 IO，应在打开镜像后应用额外的加密加载操作。加密加载操作需要提供加密格式和一个秘密，以解锁镜像及其所有显式格式化祖先镜像的加密密钥。成功的加密加载操作后，所有对已打开镜像的 IO 将被加密&#x2F;解密。对于克隆镜像，这也包括祖先镜像的 IO。加密密钥将由 RBD 客户端保存在内存中，直到镜像被关闭。 一旦加载了加密，不能对已打开镜像的上下文应用其他加密加载&#x2F;格式化操作。一旦加载了加密，使用打开的镜像上下文的 API 调用将返回有效镜像大小和有效父重叠。一旦加载了加密，调整镜像大小的 API 调用将把指定的目标大小解释为有效镜像大小。 如果加密镜像的克隆被显式格式化，则克隆镜像的扁平化操作不再透明，因为父数据必须根据克隆镜像的格式重新加密，因为它是从父快照中复制的。如果在执行扁平化操作之前没有加载加密，则克隆镜像中之前可访问的任何父数据可能变得不可读。 如果加密镜像的克隆被显式格式化，则克隆镜像的缩小操作不再透明，因为在某些情况下（例如，如果克隆镜像有快照或克隆镜像被缩小到不与对象大小对齐的大小），它涉及从父快照复制一些数据，类似于扁平化。如果在执行缩小操作之前没有加载加密，则克隆镜像中之前可访问的任何父数据可能变得不可读。 当通过 rbd-nbd 挂载 RBD 镜像作为块设备时，可以自动应用加密加载。 支持的格式LUKS支持 LUKS1 和 LUKS2。数据布局完全符合 LUKS 规范。因此，RBD 格式化的镜像可以使用外部 LUKS 支持工具（如 dm-crypt 或 QEMU）加载。此外，现有的 LUKS 数据（在 RBD 之外创建的）可以导入（通过将原始 LUKS 数据复制到镜像中）并由 RBD 加密加载。 LUKS 格式仅在基于 Linux 的系统上受支持。 当前，仅支持 AES-128 和 AES-256 加密算法。此外，xts-plain64 目前是唯一支持的加密模式。 要使用 LUKS 格式，请首先格式化镜像： 1rbd encryption format [--cipher-alg &#123;aes-128|aes-256&#125;] &#123;image-spec&#125; &#123;luks1|luks2&#125; &#123;passphrase-file&#125; 加密格式操作会生成一个 LUKS 头并将其写入镜像的开头。头部附加了一个密钥槽，其中包含一个随机生成的加密密钥，并由从 passphrase-file 读取的密码短语保护。 在较早版本中，如果 passphrase-file 的内容以换行符结束，则会被去除。 默认情况下，将使用 AES-256 的 xts-plain64 模式（这是当前推荐的模式，也是其他工具的通常默认模式）。格式化操作也允许选择 AES-128。目前，RBD 不支持添加&#x2F;移除密码短语，但可以使用兼容工具（如 cryptsetup）应用于原始 RBD 数据。 LUKS 头的大小可能会有所不同（在 LUKS2 中最大为 136MiB），但通常为 16MiB，具体取决于安装的 libcryptsetup 版本。为了优化性能，加密格式会将数据偏移量设置为与镜像条带周期大小对齐。例如，如果使用配置为 8MiB 对象大小的镜像，则期望最小开销为 8MiB；如果使用配置为 4MiB 对象大小且条带计数为 3 的镜像，则期望最小开销为 12MiB。 在 LUKS1 中，扇区是最小的加密单位，固定为 512 字节。LUKS2 支持更大的扇区，为了获得更好的性能，我们将默认扇区大小设置为最大 4KiB。写入小于扇区的内容或不对齐的写入将触发客户端的受保护读写链，并带来相当大的延迟。这样的未对齐写入的批次可能导致 IO 竞争，进一步恶化性能。因此，建议在无法保证传入写入与扇区对齐的情况下，避免使用 RBD 加密。 要映射一个 LUKS 格式化的镜像，请运行： 1rbd device map -t nbd -o encryption-passphrase-file=&#123;passphrase-file&#125; &#123;image-spec&#125; 请注意，出于安全原因，加密格式和加密加载操作都是 CPU 密集型的，可能需要几秒钟才能完成。对于实际镜像 IO 的加密操作，假设启用了 AES-NI，相对较小的微秒延迟应被添加，以及少量的 CPU 利用率增加。 示例创建一个 LUKS2 格式化的镜像，实际大小为 50GiB： 123rbd create --size 50G mypool/myimagerbd encryption format mypool/myimage luks2 passphrase.binrbd resize --size 50G --encryption-passphrase-file passphrase.bin mypool/myimage rbd resize 命令的最后一步将镜像扩展以补偿与 LUKS2 头相关的开销。 给定一个 LUKS2 格式化的镜像，创建一个具有相同有效大小的 LUKS2 格式化克隆： 1234rbd snap create mypool/myimage@snaprbd snap protect mypool/myimage@snaprbd clone mypool/myimage@snap mypool/myclonerbd encryption format mypool/myclone luks2 clone-passphrase.bin 给定一个 LUKS2 格式化的镜像，实际大小为 50GiB，创建一个具有相同有效大小的 LUKS1 格式化克隆： 1234567rbd snap create mypool/myimage@snaprbd snap protect mypool/myimage@snaprbd clone mypool/myimage@snap mypool/myclonerbd encryption format mypool/myclone luks1 clone-passphrase.binrbd resize --size 50G --allow-shrink --encryption-passphrase-file clone-passphrase.bin --encryption-passphrase-file passphrase.bin mypool/myclone 由于 LUKS1 头通常小于 LUKS2 头，rbd resize 命令的最后一步将克隆镜像缩小，以去除不需要的空间分配。 给定一个 LUKS1 格式化的镜像，实际大小为 50GiB，创建一个具有相同有效大小的 LUKS2 格式化克隆： 1234567rbd resize --size 51G mypool/myimagerbd snap create mypool/myimage@snaprbd snap protect mypool/myimage@snaprbd clone mypool/myimage@snap mypool/myclonerbd encryption format mypool/myclone luks2 clone-passphrase.binrbd resize --size 50G --allow-shrink --encryption-passphrase-file passphrase.bin mypool/myimagerbd resize --size 50G --allow-shrink --encryption-passphrase-file clone-passphrase.bin --encryption-passphrase-file passphrase.bin mypool/myclone 由于 LUKS2 头通常大于 LUKS1 头，rbd resize 命令的开始步骤会暂时扩展父镜像，以保留一些额外的空间在父快照中，从而使克隆镜像中的所有父数据可访问。rbd resize 命令的最后一步将父镜像缩小回其原始大小（这不会影响父快照），并将克隆镜像缩小，以去除未使用的保留空间。 这同样适用于创建格式化克隆未格式化（明文）镜像，因为未格式化镜像完全没有头。 要映射一个格式化的克隆，请为克隆本身和所有显式格式化的父镜像提供加密格式和密码短语。加密格式和密码短语文件选项的顺序应基于镜像层次结构：从克隆镜像开始，然后是其父镜像，依此类推。 以下是映射格式化克隆的命令示例： 1rbd device map -t nbd -o encryption-passphrase-file=clone-passphrase.bin,encryption-passphrase-file=passphrase.bin mypool/myclone RBD 配置设置通用设置1.rbd_compression_hint写操作时发送给 OSD 的提示。如果设置为 compressible 并且 OSD 的 bluestore_compression_mode 设置为 passive，则 OSD 会尝试压缩数据。如果设置为 incompressible 并且 OSD 的压缩设置为 aggressive，则 OSD 不会尝试压缩数据。 类型：字符串 (str) 默认值：none 可选值： none compressible incompressible 2.rbd_read_from_replica_policy确定哪个 OSD 接收读取操作的策略。如果设置为 default，每个 PG 的主 OSD 将始终用于读取操作。如果设置为 balance，读取操作将发送到副本集中的随机选择的 OSD。如果设置为 localize，读取操作将发送到由 CRUSH 图确定的最接近的 OSD。与 rbd_balance_snap_reads 和 rbd_localize_snap_reads 或 rbd_balance_parent_reads 和 rbd_localize_parent_reads 不同，它影响所有读取操作，而不仅仅是快照或父镜像。注意：此功能需要集群配置为最低兼容 OSD 版本 Octopus。 类型：字符串 (str) 默认值：default 可选值： default balance localize 3.rbd_default_order配置新镜像的默认对象大小。值作为二的幂使用，意味着 default_object_size = 2 ^ rbd_default_order。配置一个介于 12 和 25 之间的值（包含 12 和 25），对应于 4KiB 下限和 32MiB 上限。 类型：无符号整数 (uint) 默认值：22 缓存设置1.内核缓存Ceph 块设备的内核驱动程序可以使用 Linux 页面缓存来提高性能。 Ceph 块设备的用户空间实现（即 librbd）无法利用 Linux 页面缓存，因此它包含自己的内存缓存，称为“RBD 缓存”。RBD 缓存的行为类似于良好工作的硬盘缓存。当操作系统发送障碍或刷新请求时，所有脏数据会被写入 OSD。这意味着使用回写缓存的安全性与使用良好工作的物理硬盘一样（即 Linux 内核 &gt;&#x3D; 2.6.32）。缓存使用最少最近使用（LRU）算法，在回写模式下，它可以合并连续请求以提高吞吐量。 librbd 缓存默认启用，支持三种不同的缓存策略：写绕过、回写和写直达。在写绕过和回写策略下，写入会立即返回，除非有超过 rbd_cache_max_dirty 的未写入字节到存储集群。写绕过策略与回写策略的不同之处在于它不会尝试从缓存中服务读取请求，因此对于高性能写工作负载更快。在写直达策略下，写入仅在数据在所有副本上都存在时返回，但读取可能来自缓存。 在收到刷新请求之前，缓存表现为写直达缓存，以确保旧操作系统（不会发送刷新以确保崩溃一致性行为）安全操作。 如果禁用 librbd 缓存，写入和读取会直接到达存储集群，写入仅在数据在所有副本上都存在时返回。 缓存位于客户端的内存中，每个 RBD 镜像都有自己的缓存。由于缓存是客户端本地的，如果有其他人访问镜像，缓存之间不会保持一致。运行 GFS 或 OCFS 在 RBD 上时，启用缓存将无法正常工作。 RBD 的选项设置应在配置文件或中央配置存储的 [client] 部分中设置。这些设置包括： 2.rbd_cache启用 RADOS 块设备（RBD）的缓存。 类型：布尔值 (bool) 默认值：true 3.rbd_cache_policy选择 librbd 的缓存策略。 类型：字符串 (str) 默认值：writearound 可选值： writethrough writeback writearound 4.rbd_cache_writethrough_until_flush开始时使用写直达模式，并在收到第一个刷新请求后切换到回写模式。启用此选项是一种保守但安全的策略，适用于运行在 RBD 卷上的虚拟机（如 Linux 内核版本低于 2.6.32 的 virtio 驱动程序）不能发送刷新请求的情况。 类型：布尔值 (bool) 默认值：true 5.rbd_cache_size每卷 RBD 客户端缓存的大小（以字节为单位）。 类型：大小 (size) 默认值：32Mi 策略：回写和写直达 6.rbd_cache_max_dirty缓存触发回写的脏数据限制（以字节为单位）。如果为 0，则使用写直达缓存。 类型：大小 (size) 默认值：24Mi 约束：必须小于 rbd_cache_size 策略：写绕过和回写 7.rbd_cache_target_dirty缓存开始将数据写入数据存储之前的脏数据目标值。不会阻止写入缓存。 类型：大小 (size) 默认值：16Mi 约束：必须小于 rbd_cache_max_dirty 策略：回写 8.rbd_cache_max_dirty_age脏数据在缓存中存在的秒数，然后开始回写。 类型：浮点数 (float) 默认值：1.0 策略：回写 预读设置librbd 支持预读&#x2F;预取，以优化小的、顺序的读取。这通常应该由虚拟机的操作系统处理，但引导加载程序可能不会发出有效的读取。如果缓存被禁用或策略为写绕过，则预读会自动禁用。1.rbd_readahead_trigger_requests触发预读所需的顺序请求数量。 类型：无符号整数 (uint) 默认值：10 2.rbd_readahead_max_bytes预读请求的最大大小。如果为零，则禁用预读。 类型：大小 (size) 默认值：512Ki 3.rbd_readahead_disable_after_bytes从 RBD 镜像中读取此字节数后，预读对该镜像将被禁用，直到它关闭。这允许来宾操作系统在启动后接管预读。如果为零，则预读保持启用。 类型：大小 (size) 默认值：50Mi 镜像特性RBD 支持通过命令行创建镜像时指定高级功能，或通过 rbd_default_features = &lt;特性数值总和&gt; 或 rbd_default_features = &lt;逗号分隔的 CLI 值列表&gt; 配置默认特性。 layering描述层叠启用克隆。 内部值：1 CLI 值：layering 添加于：v0.52 (Bobtail) KRBD 支持：从 v3.10 起 默认值：是 striping v2描述条带将数据分散到多个对象上。条带有助于顺序读&#x2F;写工作负载的并行性。 内部值：2 CLI 值：striping 添加于：v0.55 (Bobtail) KRBD 支持：从 v3.10 起（仅默认条带，v4.17 添加了“Fancy”条带） 默认值：是 exclusive-lock描述启用时，要求客户端在进行写入之前对对象进行锁定。排他锁应仅在任何时候只有一个客户端访问镜像时启用。 内部值：4 CLI 值：exclusive-lock 添加于：v0.92 (Hammer) KRBD 支持：从 v4.9 起 默认值：是 object-map描述对象映射支持依赖于排他锁支持。块设备是瘦配置的，这意味着它们仅存储实际写入的数据，即它们是稀疏的。对象映射支持有助于跟踪哪些对象实际存在（在设备上存储有数据）。启用对象映射支持可以加快克隆、导入和导出稀疏填充镜像以及删除的 I&#x2F;O 操作。 内部值：8 CLI 值：object-map 添加于：v0.93 (Hammer) KRBD 支持：从 v5.3 起 默认值：是 fast-diff描述快速差异支持依赖于对象映射支持和排他锁支持。它为对象映射添加了另一个属性，使生成镜像快照之间的差异变得更快。它还更快地计算快照或卷的实际数据使用量（rbd du）。 内部值：16 CLI 值：fast-diff 添加于：v9.0.1 (Infernalis) KRBD 支持：从 v5.3 起 默认值：是 deep-flatten描述深度扁平化使 rbd flatten 能在镜像的所有快照上工作，除了镜像本身。没有它，镜像的快照仍将依赖于父镜像，因此在删除父镜像之前必须先删除快照。深度扁平化使父镜像独立于其克隆，即使它们有快照，也不会受到影响，但会使用额外的 OSD 设备空间。 内部值：32 CLI 值：deep-flatten 添加于：v9.0.2 (Infernalis) KRBD 支持：从 v5.1 起 默认值：是 journaling描述日志记录支持依赖于排他锁支持。日志记录按发生顺序记录对镜像的所有修改。RBD 镜像可以利用日志记录将崩溃一致的镜像复制到远程集群。最好让 rbd-mirror 仅在需要时管理此功能，因为长期启用可能导致大量额外的 OSD 空间消耗。 内部值：64 CLI 值：journaling 添加于：v10.0.1 (Jewel) KRBD 支持：无 默认值：否 Data pool描述在纠删码池上，镜像数据块对象需要存储在与镜像元数据不同的池中。 内部值：128 添加于：v11.1.0 (Kraken) KRBD 支持：从 v4.11 起 默认值：否 Operations描述用于限制旧客户端对镜像执行某些维护操作（例如克隆、创建快照）。 内部值：256 添加于：v13.0.2 (Mimic) KRBD 支持：从 v4.16 起 Migrating描述用于限制旧客户端在镜像处于迁移状态时打开镜像。 内部值：512 添加于：v14.0.1 (Nautilus) KRBD 支持：无 Non-primary描述用于限制使用基于快照的镜像镜像的非主副本进行更改。 内部值：1024 添加于：v15.2.0 (Octopus) KRBD 支持：无 QOS 设置librbd 支持以几种方式限制每个镜像的 I&#x2F;O。这些限制适用于给定进程中的给定镜像——例如，在多个地方使用的同一镜像（例如两个独立的虚拟机）会有独立的限制。 IOPS：每秒 I&#x2F;O 操作数（任何类型的 I&#x2F;O） 读取 IOPS：每秒读取 I&#x2F;O 操作数 写入 IOPS：每秒写入 I&#x2F;O 操作数 bps：每秒字节数（任何类型的 I&#x2F;O） 读取 bps：每秒读取字节数 写入 bps：每秒写入字节数 这些限制彼此独立操作。默认情况下，它们都处于关闭状态。每种限制类型使用令牌桶算法来限制 I&#x2F;O，能够配置限制（时间上的平均速度）和较高的速率（突发）在短时间内（burst_seconds）。当达到这些限制并且没有剩余的突发容量时，librbd 会将该类型 I&#x2F;O 的速率降低到限制值。 例如，如果配置了 100MB 的读取 bps 限制，但写入没有限制，则写入可以尽可能快地进行，而读取将被限制为 100MB&#x2F;s。如果设置了 150MB 的读取突发和 5 秒的读取突发时间，则读取可以以 150MB&#x2F;s 的速度进行长达 5 秒，然后恢复到 100MB&#x2F;s 的限制。 以下选项配置这些限制： rbd_qos_iops_limit每秒 I&#x2F;O 操作数的期望限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_iops_burstI&#x2F;O 操作的期望突发限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_iops_burst_secondsI&#x2F;O 操作的期望突发持续时间（以秒为单位）。 类型：无符号整数 (uint) 默认值：1 最小值：1 rbd_qos_read_iops_limit每秒读取操作数的期望限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_read_iops_burst读取操作的期望突发限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_read_iops_burst_seconds读取操作的期望突发持续时间（以秒为单位）。 类型：无符号整数 (uint) 默认值：1 最小值：1 rbd_qos_write_iops_limit每秒写入操作数的期望限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_write_iops_burst写入操作的期望突发限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_write_iops_burst_seconds写入操作的期望突发持续时间（以秒为单位）。 类型：无符号整数 (uint) 默认值：1 最小值：1 rbd_qos_bps_limit每秒 I&#x2F;O 字节数的期望限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_bps_burstI&#x2F;O 字节的期望突发限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_bps_burst_secondsI&#x2F;O 字节的期望突发持续时间（以秒为单位）。 类型：无符号整数 (uint) 默认值：1 最小值：1 rbd_qos_read_bps_limit每秒读取字节数的期望限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_read_bps_burst读取字节的期望突发限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_read_bps_burst_seconds读取字节的期望突发持续时间（以秒为单位）。 类型：无符号整数 (uint) 默认值：1 最小值：1 rbd_qos_write_bps_limit每秒写入字节数的期望限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_write_bps_burst写入字节的期望突发限制。 类型：无符号整数 (uint) 默认值：0 rbd_qos_write_bps_burst_seconds写入字节的期望突发持续时间（以秒为单位）。 类型：无符号整数 (uint) 默认值：1 最小值：1 rbd_qos_schedule_tick_min此设置决定了 I&#x2F;O 在限制被触发时可以解锁的最小时间（以毫秒为单位）。就令牌桶算法而言，这是添加令牌到桶中的最小间隔。 类型：无符号整数 (uint) 默认值：50 最小值：1 rbd_qos_exclude_ops可选地从 QoS 排除操作。此设置接受整数位掩码值或逗号分隔的操作名称字符串。此设置始终作为整数位掩码值内部存储。操作位掩码值与操作名称的映射如下：+1 -&gt; 读取，+2 -&gt; 写入，+4 -&gt; 丢弃，+8 -&gt; write_same，+16 -&gt; compare_and_write 类型：字符串 (str) RBD 重放RBD 重放是一组用于捕获和重放 RADOS 块设备（RBD）工作负载的工具。要捕获 RBD 工作负载，客户端必须安装 lttng-tools，并且客户端上的 librbd 必须是 v0.87（Giant）版本或更高版本。要重放 RBD 工作负载，客户端上的 librbd 必须是 Giant 版本或更高版本。捕获和重放的步骤如下： 捕获跟踪数据。确保捕获 pthread_id 上下文： 1234567mkdir -p traceslttng create -o traces librbdlttng enable-event -u &#x27;librbd:*&#x27;lttng add-context -u -t pthread_idlttng start# 在这里运行 RBD 工作负载lttng stop 处理跟踪数据，使用 rbd-replay-prep： 1rbd-replay-prep traces/ust/uid/*/* replay.bin 重放跟踪数据，使用 rbd-replay。在确认它做了你想要的操作之前，使用只读模式： 1rbd-replay --read-only replay.bin 重要提示 rbd-replay 默认情况下会销毁数据。除非使用 --read-only 选项，否则不要对希望保留的镜像使用。 重放的工作负载不必与捕获的工作负载使用相同的 RBD 镜像或相同的集群。为了处理差异，可能需要使用 --pool 和 --map-image 选项。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_crush算法实现","slug":"Storage/Ceph/Ceph-crush算法实现","date":"2021-08-02T14:26:17.000Z","updated":"2024-07-28T09:23:25.673Z","comments":true,"path":"Storage/Ceph/Ceph-crush算法实现/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph-crush%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"分布式存储系统的数据分布算法要解决数据如何分布到集群中的各个节点和磁盘上，其面临： 数据分布和负载均衡、灵活应对集群伸缩、大规模集群计算速率三方面的挑战。 数据分布和负载均衡：数据分布均衡，使数据能均匀地分布在各个节点和磁盘上，使数据访问的负载在各个节点和磁盘上。 灵活应对集群伸缩：系统可以方便地增加或者删除存储设备，当增加或删除存储设备后，能自动实现数据的均衡，并且迁移的数据尽可能减少。 大规模集群算法计算速率：要求数据分布算法维护的元数据相对较小，并且计算量不能太大。 在分布式存储系统中，数据分布算法由两种基本实现方法，一种是基于集中式的元数据查询的方式，如HDFS的实现：文件的分布信息是通过访问集中元数据服务器获得；另一种是基于哈希算法计算的方式。例如一致性哈希算法(DHT)。Ceph的数据分布算法CRUSH属于后者。CRUSH(Controlled Replication Under Scalable Hashing)，是一种基于哈希的数据分布算法。与另一种基于集中式的元数据查询的存储方式(文件的分布信息需要先通过访问集中元数据服务器获得)不同。CRUSH算法以数据唯一标识符、当前存储集群的拓扑结构以及数据分布策略作为CRUSH的输入，经过计算获得数据分布位置，直接与OSD进行通信，从而避免集中式查询操作，实现去中心化和高度并发。 Ceph 作为分布式存储系统，采用多节点多副本的数据存放方式，必然要解决数据如何分布到集群中各个节点和磁盘上。Ceph使用CRUSH数据分布算法。例如一个Ceph集群三副本，就存在着如何映射3个OSD存储这3个副本的数据，Ceph写数据时，即写object时，首先需要计算出object属于哪个PG，然后根据PG id 计算出存放的OSD位置。过程分两步：PG id的计算 ；OSD位置的计算。结合rbd的代码介绍这两个过程： 数据分片rbd的写接口（src&#x2F;linrbd&#x2F;librbd.cc）接口传入的参数是起始写位置（ofs）以及写数据大小（len）和要写入的数据（bl），调用io_work_queue-&gt;write()，生成Object写入请求对象，发送到ImageRequestWQ任务队列中，等待工作线程处理。现在看看ImageRequest的数据类型 因为Image的ImageWriteRequest继承AbstractImageWriteRequest类，重点关注AbstractImageWriteRequest类 发送写请求时调用void AbstractImageWriteRequest::send_request()函数，在这个函数进行切分数据，分成大小同等（可设定，一般为4M）的object(最后一块object可能大小小于块大小)。 file_to_extents就是将数据段切分各个object，具体怎么分割就不深入看源码了。然后调用send_object_requests()将分片各个object分别构造写请求 Op请求处理此后会构造objecter的Op请求，发送出去；转到src&#x2F;librados&#x2F;IoCtxImpl.cc，深入了解Op请求的处理。类IoCtxImpl是pool相关的上下文信息，一个pool对应一个IoCtxImpl对象，可以在该pool里创建、删除对象，完成对象数据读写等各种操作，包括同步和异步的实现。类IoCtxImpl把请求封装成ObjectOperation类。然后再添加pool的地址信息，封装成Obejcter::Op对象。Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。类IoCtxImpl的write&#x2F;read等同步操作函数通过调用operate()来调用op_submit()，类IoCtxImpl的aio_write&#x2F;aio_read&#x2F;aio_operate等异步函数直接调用了op_submit(），说明op_submit(）是object读写操作的入口。调用函数objeter-&gt;op_submit发送给相应的OSD，如果是同步操作，就等待操作完成。如果是异步操作，就不用等待，直接返回，当操作完成后，调用相应的回调函数通知。 Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。 发送数据op_submit在op_submit()调用_op_submit_with_budget()处理Throttle相关流量的限制在_op_submit_with_budget()中，如果osd_timeout大于0，就是设置定时器，当操作超时，就调用定时器回调函数op_ cancel取消操作，然后通过调用_op_submit(op, sul, ptid)。 _op_submit函数完成了关键的地址寻址和发送工作，比如_calc_target()、_get_session()、_send_op()等，调用函数_calc_target()计算对象的目标OSD；调用函数_get_session()获取目标OSD的链接，如果返回值为-EAGAIN，就升级为写锁，重新获取。检查当前的状态标志，如果当前是CEPH_OSDMAP_PAUSEWR或者OSD空间满，就暂时不发送，否则调用函数_prepare_osd_op准备请求的信息，调用函数_send_op发送出去。 对象寻址_calc_target重点详细分析下_calc_target函数：首先调用函数osdmap-&gt;get_pg_pool()根据t-&gt;base_oloc.pool获取pool信息，获取pg_pool_t对象；检查pi-&gt;last_force_op_resend是否强制重发，如果强制重发，force_resend设置为true；检查cache tier，如果是读操作，并且有读缓存，就设置t-&gt;target_oloc.pool为该pool的read_tier值；如果是写操作，并且有写缓存，就设置t-&gt;target_oloc.pool为该pool的write_tier值；调用函数osdmap-&gt;object_locator_to_pg()获取目标对象所在的PG；调用函数osdmap-&gt;pg_to_up_acting_osds()通过CRUSH算法，获取该PG对应的OSD列表，即pg_to_up_acting_osds()通过CRUSH算法计算OSD；判断读写操作：读操作，如果设置了CEPH_OSD_FLAG_BALANCE_READS标志，调用rand() 取余随机选择一个副本读取；读操作，如果设置了CEPH_OSD_FLAG_LOCALIZE_READS标志，尽可能从本地副本读取；写操作，target的OSD就设置为主OSD。 首先获取pool信息，判断是否有效： const pg_pool_t *pi = osdmap-&gt;get_pg_pool(t-&gt;base_oloc.pool);然后根据获取pgid，注意pgid是一个结构体pg_tpg_t 的结构如下： m_pool 是pool id， m_seed是函数根据object id算出来的哈希值，m_preferred赋值-1。接下来就是调用osdmap-&gt;pg_to_up_acting_osds()，获取该PG对应的OSD列表，即选择OSD： pg_to_up_acting_osds()函数在src\\osd\\OSDMap.cc中，函数功能是选出up osds以及 acting osds, 两个都是数组类型，大小为副本数 继续跟踪这个函数： 进入_pg_to_raw_osds： 上面函数crush-&gt;do_rule()就是真正调用crush算法计算出相应的osd列表。这里重点解释下参数pps：对象到PG的映射：任何程序通过客户端访问集群时，首先由客户端生成一个字符串形式的对象名，然后基于对象名和命名空间计算得出一个32位哈希值。针对此哈希值，对该存储池的PG总数量pg_num取模(掩码计算)，得到该对象所在的PG的id号。ps_t pps = pool.raw_pg_to_pps(pg); // placement ps 可以看出pps这是一个哈希值，这个哈希值根据pool id，函数中pg.ps()就是我们object哈希算出的m_seed： 调用CRUSH算法下面就是进入do_rule 进行CRUSH算法的处理了：src&#x2F;crush&#x2F;CrushWrapper.h调用crush_do_rule()函数 继续调用crush_do_rule()算法，执行CEUSH算法CRUSH算法：针对指定输入x(要计算PG的pg_id)，CRUSH将输出一个包含n个不同目标存储对象(例如磁盘)的集合(OSD列表)。CRUSH的计算过程使用x、cluster map、placement rule作为哈希函数输入。因此如果cluster map不发生变化(一般placement rule不会轻易变化)，那么结果就是确定的。算法输入需要3个输入参数： 输入x 即PG id的哈希值 crush_map即集群的拓扑结构，集群的层级化描述，形如”数据中心-&gt;机架-&gt;主机-&gt;磁盘”这样的层级拓扑。用树来表示，每个叶子节点都是真实的最小物理存储设备，称为devices；所有中间节点统称为bucket，每个bucket可以是一些devices的集合，也可以是低一级的buckets集合；根节点称为root，是整个集群的入口。 ruleno 即选择策略，就rule规则，这里用编号表示；它决定一个PG的对象副本如何选择(从定义的cluster map的拓扑结构中)的规则，以此完成数据映射。palcement rule可以包含多个操作，这些操作共有3种类型：take(root)、select(replicas, type)、emit(void) crush 算法输入需要3个输入参数： 输入x 即PG id的哈希值 crush_map即集群的拓扑结构 ruleno 即选择策略，就rule规则，这里用编号表示 可以通过集群输出crush_map: vim crush_map如下： 显示的结构和代码中的结构还是有着映射的关系： 其中crush_bucket:对应： crush_rule:对应于： 逐一对比分析其数据结构。这里分析下其选择OSD的过程： 12345int *a = scratch;int *b = scratch + result_max;int *c = scratch + result_max*2;w = a;o= b; a, b, c 分别指向 scratch向量的0, 1, 2的位置.w &#x3D; a; o &#x3D; b; w被用作一个先入先出队列来在CRUSH map中进行横向优先搜索(BFS traversal). o存储crush_choose_firstn选择的结果. c存储最终的OSD选择结果. crush_do_rule函数里面最重要的是函数里面的for循环，这个循环就是筛选osd的过程， for循环中： 首先从rule规则中当前执行的步骤，首次就执行第一条步骤： struct crush_rule_step *curstep = &amp;rule-&gt;steps[step]; 然后根据当前执行步骤的操作类型，选择不同的分支操作，首先一般是take操作，而且是take fault。即crush map树根节点。这个过程就是根据step 逐步选择bucket 知道知道叶子节点，即OSD。 这个过程中，crush_choose_firstn 函数, 递归的选择特定bucket或者设备,并且可以处理冲突,失败的情况. 如果当前是choose过程,通过调用crush_bucket_choose来直接选择. 如果当前是chooseleaf选择叶子节点的过程,该函数将递归直到得到叶子节点. 在for循环中的crush_choose_firstn()计算后如果结果不是OSD类型, o 交给w。以便于 w成为下次crush_choose_firstn的输入参数。在crush_choose_firstn()中，for(){}：副本选择循环判断条件rep是否等于副本数numrep，rep叠加。do{}while (retry_descent)：选择OSD冲突或故障域失效时循环，随机因子r改变。do{}while (retry_bucket)：进行bucket层级选择，当前item type不是OSD时循环，当前进行选择的bucket，即in改变。 在crush_choose_firstn()函数中有crush_bucket_choose函数，这个函数根据bucket类型选择不同的权重计算方法刷选出bucket。如果采用straw2，就会采用bucket_straw2_choose接口进行筛选。 bucket_straw2_choose()功能是通过调用伪随机算法计算伪随机数，以伪随机数最高的作为选择出的节点 generate_exponential_distribution()产生随机数的思想是：采用逆变换采样的思想，先调用crush_hash32_3()计算哈希值，然后取随机数的低16位。计算指数随机变量。作为参考，请参阅指数分布示例：https://en.wikipedia.org/wiki/Inverse_transform_sampling#Examples。 由于某种原因，略小于 0x10000 会产生更准确的分布……可能是舍入效果。 自然对数查找表映射 [0,0xffff]（对应实数 [1&#x2F;0x10000, 1] 到 [0, 0xffffffffffff]（对应实数 [-11.090355,0]）。除以 16.16 定点权重。 请注意，ln 值为负数，因此较大的权重意味着较大的（较小的负数）draw值。 CRUSH算法的一些缺陷： CRUSH算法提供了uniform、list和tree等bucket类型作为straw bucket类型的替代方案，但这些算法在添加或删除服务器时需要进行不必要的重排，这使它们不适合用于大规模存储系统。 CRUSH算法的查找函数需要进行O(log n)的二分查找，以找到与给定对象ID最接近的虚拟ID。这个计算对于系统中的每个对象都需要进行，因此在系统中有大量对象时，计算成本会很高。 CRUSH算法在重建过程中可能会出现瓶颈，因为它需要在placement groups中进行数据放置，这可能会导致数据重建速度变慢。 CRUSH算法的计算复杂度较高，需要进行大量的计算，这可能会影响系统的性能。 综上所述，CRUSH算法虽然是一种灵活的对象放置算法，但它也存在一些缺陷，需要进一步改进和优化。 由于CRUSH算法的计算复杂度较高，需要进行大量的计算，因此使用多线程来加速计算是一种可行的方法。具体来说，可以将CRUSH算法的计算任务分配给多个线程，每个线程负责计算一部分任务，然后将结果合并起来。这样可以充分利用多核处理器的计算能力，提高计算效率。但是，需要注意的是，多线程计算也会带来一些额外的开销，如线程间的同步和通信开销，因此需要进行合理的线程调度和优化，以达到最佳的性能提升效果。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_Bufferlist的设计与使用","slug":"Storage/Ceph/Ceph_Bufferlist的设计与使用","date":"2021-07-14T05:19:06.000Z","updated":"2024-07-27T14:37:12.169Z","comments":true,"path":"Storage/Ceph/Ceph_Bufferlist的设计与使用/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph_Bufferlist%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Ceph Bufferlist的设计与使用做为主要和磁盘、网络打交道的分布式存储系统，序列化是最基础的功能之一。当一个结构通过网络发送或写入磁盘时，它被编码为一串字节。可序列化结构具encode 和 decode方法，将结构体序列化后存入bufferlist和从bufferlist读出字节串反序列化出结构体。bufferlist是ceph的底层组件，用于存储二进制数据，其存储的数据可以直接写入磁盘，在代码中有很广泛的使用。 为什么要用bufferlist？ 为了免拷贝。发送数据时，传统的socket接口通常需要读取一段连续的内存。但是我们要发的数据内存不连续，所以以前的做法是申请一块大的内存，然后将不连续的内存内的数据拷贝到大内存块中，然后将大内存块地址给发送接口。但是找一块连续的大内存并不容易，系统可能会为此做各种腾挪操作，而将数据拷贝的大内存中，又是一个拷贝操作。RDMA的发送支持聚散表，不需要读取连续的内存。有bufferlist之后，我们可以通过bufferlist，将不连续的物理内存管理起来，形成一段“连续”的虚拟内存，然后将bufferlist的内存指针传递给聚散表，再把聚散表交给RDMA 发送接口即可。整个过程免去了内存拷贝操作。大大降低了CPU的消耗。 在ceph中经常需要将一个bufferlist编码(encode)到另一个bufferlist中，例如在msg发送消息的时候，通常msg拿到的osd等逻辑层传递给它的bufferlist，然后msg还需要给这个bufferlist加上消息头和消息尾，而消息头和消息尾也是用bufferlist表示的。这时候，msg通常会构造一个空的bufferlist，然后将消息头、消息尾、内容都encode到这个空的bufferlist。而bufferlist之间的encode实际只需要做ptr的copy，而不涉及到系统内存的申请和copy，效率较高。 补充： 传统内存访问需要通过CPU进行数据copy来移动数据，通过CPU将内存中的Buffer1移动到Buffer2中。 DMA(直接内存访问)是一种能力，允许在计算机主板上的设备直接把数据发送到内存中去，数据搬运不需要CPU的参与。 DMA模式：可以同DMA Engine之间通过硬件将数据从Buffer1移动到Buffer2，而不需要操作系统CPU的参与，大大降低了CPU Copy的开销。 RDMA是一种概念，在两个或者多个计算机进行通讯的时候使用DMA， 从一个主机的内存直接访问另一个主机的内存。RDMA是一种新的直接内存访问技术，RDMA让计算机可以直接存取其他计算机的内存，而不需要经过处理器的处理。RDMA将数据从一个系统快速移动到远程系统的内存中，而不对操作系统造成任何影响。 bufferlist的设计Bufferlist负责管理Ceph中所有的内存。整个Ceph中所有涉及到内存的操作，无论是msg分配内存接收消息，还是OSD构造各类数据结构的持久化表示（encode&#x2F;decode），再到实际磁盘操作，都将bufferlist作为基础。bufferlist对应的类为buffer::list(using bufferlist &#x3D; buffer::list;)，而buffer::list又基于buffer::ptr和buffer::raw实现，探讨buffer::list的实现，不能跳过它们。 123456789101112namespace ceph &#123; namespace buffer &#123; inline namespace v14_2_0 &#123; class ptr; class list; &#125; class hash; &#125; using bufferptr = buffer::ptr; using bufferlist = buffer::list; using bufferhash = buffer::hash;&#125; ceph::buffer是ceph非常底层的实现，负责管理ceph的内存。ceph::buffer的设计较为复杂，但本身没有任何内容，主要包含buffer::list、 buffer::ptr、 buffer::raw、 buffer::hash。这三个类都定义在src&#x2F;include&#x2F;buffer.h和src&#x2F;common&#x2F;buffer.cc中。 buffer::raw：负责维护物理内存的引用计数nref和释放操作。 buffer::ptr：指向buffer::raw的指针。 buffer::list：表示一个ptr的列表（std::list），相当于将N个ptr构成一个更大的虚拟的连续内存。 buffer::hash：一个或多个bufferlist的有效哈希。 buffer这三个类的相互关系可以用下面这个图来表示：图中蓝色的表示bufferlist，橙色表示bufferptr，绿色表示bufferraw。 在这个图中，实际占用的系统内存一共就三段，分别是raw0，raw1和raw2代表的三段内存。 raw0被ptr0，ptr1，ptr2使用 raw1被ptr3，ptr4，ptr6使用 raw2被ptr5，ptr7使用 而list0是由ptr0-5组成的，list1是由ptr6和ptr7组成的。 从这张图上我们就可以看出bufferlist的设计思路： 对于bufferlist来说，仅关心一个个ptr。bufferlist将ptr连在一起，当做是一段连续的内存使用。因此，可以通过bufferlist::iterator一个字节一个字节的迭代整个bufferlist中的所有内容，而不需要关心到底有几个ptr，更不用关心这些ptr到底和系统内存是怎么对应的；也可以通过bufferlist::write_file方法直接将bufferlist中的内容出到一个文件中；或者通过bufferlist::write_fd方法将bufferlist中的内容写入到某个fd中。 bufferraw负责管理系统内存的，bufferraw只关心一件事：维护其所管理的系统内存的引用计数，并且在引用计数减为0时——即没有ptr再使用这块内存时，释放这块内存。 bufferptr负责连接bufferlist和bufferraw。bufferptr关心的是如何使用内存。每一个bufferptr一定有一个bufferraw为其提供系统内存，然后ptr决定使用这块内存的哪一部分。bufferlist只用通过ptr才能对应到系统内存中，而bufferptr而可以独立存在，只是大部分ptr还是为bufferlist服务的，独立的ptr使用的场景并不是很多。通过引入ptr这样一个中间层次，bufferlist使用内存的方式可以非常灵活。 快速encode&#x2F;decode。在Ceph中经常需要将一个bufferlist编码（encode）到另一个bufferlist中，例如在msg发送消息的时候，通常msg拿到的osd等逻辑层传递给它的bufferlist，然后msg还需要给这个bufferlist加上消息头和消息尾，而消息头和消息尾也是用bufferlist表示的。这时候，msg通常会构造一个空的bufferlist，然后将消息头、消息尾、内容都encode到这个空的bufferlist。而bufferlist之间的encode实际只需要做ptr的copy，而不涉及到系统内存的申请和Copy，效率较高。 一次分配，多次使用。调用malloc之类的函数申请内存是非常重量级的操作。利用ptr这个中间层可以缓解这个问题，可以一次性申请一块较大的内存，也就是一个较大的bufferraw，然后每次需要内存的时候，构造一个bufferptr，指向这个bufferraw的不同部分。这样就不再需要向系统申请内存了。最后将这些ptr都加入到一个bufferlist中，就可以形成一个虚拟的连续内存。 减少内存分配次数和碎片。利用bufferptr这个中间层进行内存的多次使用，多个bufferptr可以引用同一段bufferraw的不同区域，这个bufferraw可以预先一次性申请较大一段连续内存，从而避免了多次申请内存以及内存碎片的产生。 buffer::rawraw的数据成员部分代码如下： 1234567891011class buffer::raw&#123;public: char *data; //数据指针 unsigned len; //数据长度 std::atomic&lt;unsigned&gt; nref&#123;0&#125;; //引用计数 int mempool; mutable ceph::spinlock crc_spinlock; //读写锁 map&lt;pair&lt;size_t, size_t&gt;, pair&lt;uint32_t, uint32_t&gt;&gt; crc_map; //crc校验信息 ......&#125;; 最基本的成员：data是指向具体数据的指针，len是数据的长度，nref是引用计数。而mempool是其对应的内存池的index，这个和data空间的分配有关，暂时不去管它。 data指向的数据有很多来源，直接通过malloc从内存分配只是最基础的一种，可能还来自mmap内存映射的空间，甚至可以通过pipe管道＋splice实现零拷贝获取空间。有些时候，分配的空间时，会提出对齐的要求，比如按页对齐等等。对于每一种数据来源，需要不同逻辑的数据分配和释放函数，所以raw对应了很多子类，分别表示不同的数据。 下列类都继承了buffer::raw，实现了对data对应内存空间的申请 类raw_malloc实现了用malloc函数分配内存空间的功能 类class buffer::raw_mmap_pages实现了通过mmap来把内存匿名映射到进程的地址空间 类class buffer::raw_posix_aligned调用了函数posix_memalign来申请内存地址对齐的内存空间。 类class buffer::raw_hack_aligned是在系统不支持内存对齐申请的情况下自己实现了内存地址的对齐 类class buffer::raw_pipe实现了pipe做为Buffer的内存空间 类class buffer::raw_char使用了C++的new操作符来申请空间 这是因为这些来源不同，要求不同，buffer::raw也就有了一些变体，举个例子，对应于malloc的raw子类为buffer::raw_malloc，构造和析构函数中实现了使用malloc进行数据分配和释放的逻辑： 123456789101112131415161718192021222324252627282930313233343536class buffer::raw_malloc : public buffer::raw&#123;public: MEMPOOL_CLASS_HELPERS(); explicit raw_malloc(unsigned l) : raw(l) &#123; if (len) &#123; data = (char *)malloc(len); if (!data) throw bad_alloc(); &#125; else &#123; data = 0; &#125; inc_total_alloc(len); inc_history_alloc(len); bdout &lt;&lt; &quot;raw_malloc &quot; &lt;&lt; this &lt;&lt; &quot; alloc &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; l &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; raw_malloc(unsigned l, char *b) : raw(b, l) &#123; inc_total_alloc(len); bdout &lt;&lt; &quot;raw_malloc &quot; &lt;&lt; this &lt;&lt; &quot; alloc &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; l &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; ~raw_malloc() override &#123; free(data); dec_total_alloc(len); bdout &lt;&lt; &quot;raw_malloc &quot; &lt;&lt; this &lt;&lt; &quot; free &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; raw *clone_empty() override &#123; return new raw_malloc(len); &#125;&#125;; 对应于malloc的raw子类为buffer::raw_mmap_pages，顾名思义，也能够猜到，这个数据的来源是通过mmap分配的匿名内存映射。因此析构的时候，毫不意外，掉用munmap解除映射，归还空间给系统： 12345678910111213141516171819class buffer::raw_mmap_pages : public buffer::raw &#123;public: explicit raw_mmap_pages(unsigned l) : raw(l) &#123; data = (char*)::mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANON, -1, 0); if (!data) throw bad_alloc(); inc_total_alloc(len); inc_history_alloc(len); bdout &lt;&lt; &quot;raw_mmap &quot; &lt;&lt; this &lt;&lt; &quot; alloc &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; l &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; ~raw_mmap_pages() &#123; ::munmap(data, len); dec_total_alloc(len); bdout &lt;&lt; &quot;raw_mmap &quot; &lt;&lt; this &lt;&lt; &quot; free &quot; &lt;&lt; (void *)data &lt;&lt; &quot; &quot; &lt;&lt; buffer::get_total_alloc() &lt;&lt; bendl; &#125; raw* clone_empty() &#123; return new raw_mmap_pages(len); &#125;&#125;; buffer::ptrbuffer::ptr是在buffer::raw系列的基础上，这个类也别名bufferptr， 这个类是raw这个类的包装升级版本，它的_raw就是指向buffer::raw类型的变量。成员部分如下（include&#x2F;buffer.h）： 123456class CEPH_BUFFER_API ptr&#123; raw *_raw; unsigned _off, _len; ......&#125;; 类buffer::ptr就是对于buffer::raw的一部分数据段，ptr是raw里的一个任意的数据段，_off是在_raw里的偏移量，_len是在ptr的长度。raw是真正存储数据的地方，而ptr只是指向某个raw中的一段的指针。其数据成员 _raw为指向raw的指针，_off表示数据起始偏移，_len表示数据长度。这边还有提一下ptr的append函数，直观上ptr不应该提供append函数，事实上ptr的append确实很局限，只有当ptr对应的raw区域后方有空闲空间的时候，才能append成功，至于空间不够的情况，应该是交给list等高层类来处理。代码如下： 123456789unsigned buffer::ptr::append(const char *p, unsigned l)&#123; assert(_raw); assert(l &lt;= unused_tail_length()); char *c = _raw-&gt;data + _off + _len; maybe_inline_memcpy(c, p, l, 32); _len += l; return _len + _off;&#125; buffer::ptr其他常见操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364buffer::ptr&amp; buffer::ptr::operator= (const ptr&amp; p)&#123; if (p._raw) &#123; p._raw-&gt;nref.inc(); bdout &lt;&lt; &quot;ptr &quot; &lt;&lt; this &lt;&lt; &quot; get &quot; &lt;&lt; _raw &lt;&lt; bendl; &#125; buffer::raw *raw = p._raw; release(); if (raw) &#123; _raw = raw; _off = p._off; _len = p._len; &#125; else &#123; _off = _len = 0; &#125; return *this;&#125;buffer::raw *buffer::ptr::clone()&#123; return _raw-&gt;clone();&#125;void buffer::ptr::swap(ptr&amp; other)&#123; raw *r = _raw; unsigned o = _off; unsigned l = _len; _raw = other._raw; _off = other._off; _len = other._len; other._raw = r; other._off = o; other._len = l;&#125;const char&amp; buffer::ptr::operator[](unsigned n) const&#123; assert(_raw); assert(n &lt; _len); return _raw-&gt;get_data()[_off + n];&#125;char&amp; buffer::ptr::operator[](unsigned n)&#123; assert(_raw); assert(n &lt; _len); return _raw-&gt;get_data()[_off + n];&#125;int buffer::ptr::cmp(const ptr&amp; o) const&#123; int l = _len &lt; o._len ? _len : o._len; if (l) &#123; int r = memcmp(c_str(), o.c_str(), l); if (r) return r; &#125; if (_len &lt; o._len) return -1; if (_len &gt; o._len) return 1; return 0;&#125; buffer::list类buffer::list是一个使用广泛的类，它是多个buffer::ptr的列表，也就是多个内存数据段的列表。多个bufferptr形成一个list，这就是bufferlist。简单来说，list就是一个ptr组成的链表：（include&#x2F;buffer.h） 12345678910class CEPH_BUFFER_API list&#123;// my private bitsstd::list&lt;ptr&gt; _buffers; //所有的ptrunsigned _len; //所有的ptr的数据总长度unsigned _memcopy_count; //当调用函数rebuild用来内存对齐时，需要内存拷贝的数据量ptr append_buffer; // 当有小的数据就添加到这个buffer里 mutable iterator last_p; //访问list的迭代器......&#125;; buffers是一个ptr的链表，_len是整个_buffers中所有的ptr的数据的总长度，_memcopy_count用于统计memcopy的字节数，append_buffer是用于优化append操作的缓冲区，可以看出bufferlist将数据以不连续链表的方式存储。 bufferlist的迭代器迭代器中提供的seek(unsigned o)和advance(int o)等函数中的o都是指bufferlist的偏移，而不是单个ptr内的偏移。 123456789101112template &lt;bool is_const&gt;class CEPH_BUFFER_API iterator_impl : public std::iterator&lt;std::forward_iterator_tag, char&gt;&#123;protected: bl_t *bl; list_t *ls; // meh.. just here to avoid an extra pointer dereference.. unsigned off; // in bl list_iter_t p; unsigned p_off; // in *p ......&#125;; 其数据成员的含义如下： bl：指针，指向bufferlist ls：指针，指向bufferlist的成员 _buffers p: 类型是std::list::iterator，用来迭代遍历bufferlist中的bufferptr p_off：当前位置在对应的bufferptr中的偏移量 off：当前位置在整个bufferlist中的偏移量 bufferlist常用函数librados只给出bufferlist API clear() 清空bufferlist中的内容 push_front(raw* &#x2F; ptr &amp;)push_back(raw* &#x2F; ptr &amp;) 在_buffers的前面或后面增加新的ptr rebuild()rebuild(ptr &amp;nb) 将bufferlist中buffers链表中所有的ptr中的数据存到一个ptr中，并将_buffers原有数据clear，然后将新的单个ptr push到_buffers中。 带参数时使用参数传入的ptr作为目标ptr，不带参数时自己创建一个ptr。 claim(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT); 将bl的数据拿过来，替换原有的数据。调用后bl数据被清空。 claim_append(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT);claim_prepend(list &amp;bl, unsigned int flags &#x3D; CLAIM_DEFAULT); 将bl的数据拿过来，splice到_buffers的尾部&#x2F;头部。 append(…) 将数据追加到_buffers尾部，已有ptr空间不够时，会自动分配新的ptr。 splice(unsigned off, unsigned len, list *claim_by &#x3D; 0) bl.splice(10,10,&amp;bl2); 将_buffers中总偏移off处长度为len的数据，move到claim_by对应的bufferlist的尾部。注意是move不是copy。 write(int off, int len, std::ostream &amp;out) 将_buffers中总偏移量off处长度为len的数据，写入到ostream。注意是copy，不是move。 push_front(ptr&amp; pb) 添加一个ptr到list头部 push_front(raw *r)添加一个raw到list头部中，先构造一个ptr，后添加list中 is_aligned(align)判断内存是否以参数align对齐，每一个ptr都必须以align对齐 read_fd()&#x2F;write_fd()把数据写入文件描述符或者从文件描述符读取数据 read_file()&#x2F;write_file()把数据写入文件或从文件读取数据的功能 write_stream() 内存对齐：有些情况下，需要内存地址对齐，例如当以directIO方式写入数据至磁盘时，需要内存地址按照内存页面大小（page）对齐，也即buffer::list的内存地址都需按照page对齐。函数rebuild用来完成对齐的功能。其实现的方法也比较简单，检查没有对齐的ptr，申请一块新对齐的内存，把数据拷贝过去，释放内存空间就可以了。 相关链接： http://bean-li.github.io/bufferlist-in-ceph/ https://www.jianshu.com/p/01e1f4e398df","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph序列化","slug":"Storage/Ceph/Ceph数据序列化","date":"2021-07-10T04:43:01.000Z","updated":"2024-07-27T14:36:56.441Z","comments":true,"path":"Storage/Ceph/Ceph数据序列化/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96/","excerpt":"","text":"Ceph 数据序列化Ceph 作为主要处理磁盘和网络的分布式存储系统，数据序列化是其最基本的功能之一。当一个结构通过网络发送或写入磁盘时，它会被编码为一串字节。可序列化的结构体具有 encode 和 decode 方法，用于将结构体序列化后存入 bufferlist，或从 bufferlist 读取字节串并反序列化为结构体。 在 Ceph 中，经常需要将一个 bufferlist 编码（encode）到另一个 bufferlist 中。例如，在 msg 发送消息时，msg 通常会接收到由 OSD 等逻辑层传递给它的 bufferlist，然后 msg 需要给这个 bufferlist 添加消息头和消息尾，而消息头和消息尾也是用 bufferlist 表示的。在这种情况下，msg 通常会构造一个空的 bufferlist，然后将消息头、消息尾和内容都编码到这个空的 bufferlist 中。 在 bufferlist 之间进行编码实际上只需要进行指针的复制，而不涉及系统内存的申请和复制，因此效率较高。encode 和 decode 方法的主要作用是方便 Ceph 不同模块之间的参数传输。 在Ceph代码中有很多例子，这里有一个例子。 12345678910111213141516171819class AcmeClass&#123; int member1; std::string member2; void encode(bufferlist &amp;bl) &#123; ENCODE_START(1, 1, bl); ::encode(member1, bl); ::encode(member2, bl); ENCODE_FINISH(bl); &#125; void decode(bufferlist::iterator &amp;bl) &#123; DECODE_START(1, bl); ::decode(member1, bl); ::decode(member2, bl); DECODE_FINISH(bl); &#125;&#125;; ENCODE_START宏写入标头 说明version和 compat_version（初值均为 1）。每当对encode进行更改时，version就会增加。仅当更改会影响decode时compat_version才会增加 - 比如新结构体只在尾部添加字段，不会影响旧结构体的解析，因此在结构末尾添加字段的更改不需要增加 compat_version。DECODE_START宏采用一个参数，指定encode代码可以处理的最新消息版本。这与消息中编码的 compat_version 进行比较，如果消息太新，则会抛出异常。因为对 compat_verison 的更改很少，所以在添加字段时通常不需要担心。 Ceph序列化的方式序列化（在 Ceph 中称为 encode）的目的是将数据结构表示为二进制流，以便通过网络传输或保存在磁盘等存储介质上。其逆过程称为反序列化（在 Ceph 中称为 decode）。例如，对于字符串“abc”，其序列化结果为7个字节（bytes）：03 00 00 00 61 62 63，其中前四个字节（03 00 00 00）表示字符串的长度为3个字符，后三个字节（61 62 63）分别是字符“abc”的 ASCII 码的十六进制表示。Ceph 采用 little-endian 的序列化方式，即低地址存放最低有效字节，因此32位整数0x12345678的序列化结果为78 56 34 12。 由于序列化在整个 Ceph 系统中是非常基础且常用的功能，Ceph 将其序列化方式设计为统一的结构，即任何支持序列化的数据结构都必须提供一对定义在全局命名空间中的序列化&#x2F;反序列化（encode&#x2F;decode）函数。例如，如果我们定义了一个结构体 inode，就必须在全局命名空间中定义以下两个方法： encode(struct inode, bufferlist bl); decode(struct inode, bufferlist::iterator bl); 在此基础上，序列化的使用变得非常简单。对于任意可序列化的类型 T 的实例 instance_T，可以通过如下语句将 instance_T 序列化并保存到 bufferlist 类的实例 instance_bufferlist 中。 bufferlist类（定义于include&#x2F;buffer.h）是ceph核心的缓存类，用于保存序列化结果、数据缓存、网络通信等，能够将bufferlist理解为一个可变长度的char数组。 如下代码演示了将一个时间戳以及一个inode序列化到一个bufferlist中。 12345utime_t timestamp;inode_t inode;bufferlist bl;::encode(timetamp, bl)::encode(inode, bl); 序列化后的数据能够经过反序列化方法读取，例如如下代码片断从一个bufferlist中反序列化一个时间戳和一个inode（前提是该bl中已经被序列化了一个utime_t和一个inode，不然会报错）。 123bufferlist::iterator bl;::decode(timetamp, bl)::decode(inode, bl); 各种数据类型的序列化Ceph为其全部用到数据类型提供了序列化方法或反序列化方法，这些数据类型包括了绝大部分基础数据类型（int、bool等）、结构体类型的序列化（ceph_mds_request_head等）、集合类型（vector、list、set、map等）、以及自定义的复杂数据类型（例如表示inode的inode_t等），如下分别介绍不一样数据类型的序列化实现方式。 1、基本数据类型的序列化基本数据类型的序列化结果基本就是该类型在内存中的表示形式。基本数据类型的序列化方法使用手工编写，定义在include&#x2F;encoding.h中，包括如下类型： 12345__u8, __s8, char, boolceph_le64, ceph_le32, ceph_le16,float, double,uint64_t, int64_t, uint32_t, int32_t, uint16_t, int16_t,string, char* 在手工编写encode方法过程当中，为了不重复代码，借助了WRITE_RAW_ENCODER和WRITE_INTTYPE_ENCODER两个宏。 2、结构体类型的序列化结构体类型的序列化方法与基本数据类型的序列化方法一致，即便用结构体的内存布局做为序列化的形式。在结构体定义完成后，经过调用WRITE_RAW_ENCODER宏函数生成结构体的全局encode方法，例如结构体ceph_mds_request_head相关结构实现以下。 1234567891011struct ceph_mds_request_head &#123; __le64 oldest_client_tid; __le32 mdsmap_epoch; __le32 flags; __u8 num_retry, num_fwd; __le16 num_releases; __le32 op; __le32 caller_uid, caller_gid; __le64 ino;&#125; __attribute__ ((packed));WRITE_RAW_ENCODER(ceph_mds_request_head) 其中： ceph_mds_request_head结构体定义在include&#x2F;ceph_fs.h . WRITE_RAW_ENCODER(ceph_mds_request_head)语句位于include&#x2F;types.h WRITE_RAW_ENCODER宏函数定义在include&#x2F;encoding.h WRITE_RAW_ENCODER宏函数其实是经过调用encode_raw实现的，而encode_raw调用bufferlist的append的方法，经过内存拷贝，将数据结构放入到bufferlist中。相关代码为： 12345678910template&lt;class T&gt;inline void encode_raw(const T&amp; t, bufferlist&amp; bl)&#123; bl.append((char*)&amp;t, sizeof(t));&#125;template&lt;class T&gt;inline void decode_raw(T&amp; t, bufferlist::iterator &amp;p)&#123; p.copy(sizeof(t), (char*)&amp;t);&#125; 3、集合数据类型的序列化集合数据类型序列化的基本思路包括两步： 序列化集合大小， 序列化集合内的全部元素 例如vector&amp; v的序列化方法：其中元素的序列化经过调用该元素的encode方法实现。 12345678template&lt;class T&gt;inline void encode(const std::vector&lt;T&gt;&amp; v, bufferlist&amp; bl)&#123; __u32 n = v.size(); encode(n, bl); for (typename std::vector&lt;T&gt;::const_iterator p = v.begin(); p != v.end(); ++p) encode(*p, bl);&#125; 经常使用集合数据类型的序列化已经由Ceph实现，位于include&#x2F;encoding.h中，包括如下集合类型：pair, triple, list, set, vector, map, multimap, hash_map, hash_set, deque。集合类型的序列化方法皆为基于泛型（模板类）的实现方式，适用于全部泛型派生类。 4、复杂数据类型的序列化除以上两种业务无关的数据类型外，其它数据类型的序列化实现包括两部分： 在类型内部现实encode方法，将类型内部的encode方法重定义为全局方法。如下以utime_t类为例：utime_t内部实现了encode和decode两个方法，WRITE_CLASS_ENCODER宏函数将这两个方法转化为全局方法。 1234567891011121314class utime_t &#123; struct &#123; __u32 tv_sec, tv_nsec; &#125; tv; void encode(bufferlist &amp;bl) const &#123; ::encode(tv.tv_sec, bl); ::encode(tv.tv_nsec, bl); &#125; void decode(bufferlist::iterator &amp;p) &#123; ::decode(tv.tv_sec, p); ::decode(tv.tv_nsec, p); &#125;&#125;;WRITE_CLASS_ENCODER(utime_t) 复杂数据结构内部的encode方法的实现方式一般是调用其内部主要数据结构的encode方法，例如utime_t类的encode方法其实是序列化内部的tv.tv_sec和tv.tv_nsec两个成员。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph相关数据结构","slug":"Storage/Ceph/Ceph相关数据结构","date":"2021-07-02T15:33:02.000Z","updated":"2024-07-27T14:36:43.411Z","comments":true,"path":"Storage/Ceph/Ceph相关数据结构/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"Ceph 相关数据结构要想深入到Ceph的源码底层，就必须对代码通用库里的一些关键，常见的数据结构进行学习，这样才能更好的理解源代码。从最高的逻辑层次为Pool的概念，然后是PG的概念。其次是OSDＭap记录了集群的所有的配置信息。数据结构OSDOp是一个操作上下文的封装。结构object_info_t保存了一个元数据信息和访问信息。对象ObjectState是在object_info_t基础上添加了一些内存的状态信息。SnapSetContext和ObjectContext分别保存了快照和对象上下文相关的信息。Session保存了一个端到端的链接相关的上下文。 PoolPool是整个集群层面定义的一个逻辑的存储池。对一个Pool可以设置相应的数据冗余类型，目前有副本和纠删码两种实现。数据结构pg_pool_t用于保存Pool的相关信息。Pool的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150struct pg_pool_t &#123; static const char *APPLICATION_NAME_CEPHFS; static const char *APPLICATION_NAME_RBD; static const char *APPLICATION_NAME_RGW; enum &#123; TYPE_REPLICATED = 1, // replication 副本 //TYPE_RAID4 = 2, // raid4 (never implemented) 从来没实现的raid4 TYPE_ERASURE = 3, // erasure-coded 纠删码 &#125;; enum &#123; FLAG_HASHPSPOOL = 1&lt;&lt;0, // hash pg seed and pool together (instead of adding) FLAG_FULL = 1&lt;&lt;1, // pool is full FLAG_EC_OVERWRITES = 1&lt;&lt;2, // enables overwrites, once enabled, cannot be disabled FLAG_INCOMPLETE_CLONES = 1&lt;&lt;3, // may have incomplete clones (bc we are/were an overlay) FLAG_NODELETE = 1&lt;&lt;4, // pool can&#x27;t be deleted FLAG_NOPGCHANGE = 1&lt;&lt;5, // pool&#x27;s pg and pgp num can&#x27;t be changed FLAG_NOSIZECHANGE = 1&lt;&lt;6, // pool&#x27;s size and min size can&#x27;t be changed FLAG_WRITE_FADVISE_DONTNEED = 1&lt;&lt;7, // write mode with LIBRADOS_OP_FLAG_FADVISE_DONTNEED FLAG_NOSCRUB = 1&lt;&lt;8, // block periodic scrub FLAG_NODEEP_SCRUB = 1&lt;&lt;9, // block periodic deep-scrub FLAG_FULL_QUOTA = 1&lt;&lt;10, // pool is currently running out of quota, will set FLAG_FULL too FLAG_NEARFULL = 1&lt;&lt;11, // pool is nearfull FLAG_BACKFILLFULL = 1&lt;&lt;12, // pool is backfillfull FLAG_SELFMANAGED_SNAPS = 1&lt;&lt;13, // pool uses selfmanaged snaps FLAG_POOL_SNAPS = 1&lt;&lt;14, // pool has pool snaps FLAG_CREATING = 1&lt;&lt;15, // initial pool PGs are being created &#125;; utime_t create_time; //Pool创建时间 uint64_t flags; ///&lt; FLAG_* Pool的相关标志 __u8 type; ///&lt; TYPE_* 类型 __u8 size, min_size; ///&lt;Pool的size和min_size，即副本数和至少保证的副本数 __u8 crush_rule; ///&lt; crush placement rule rule的编号 __u8 object_hash; ///&lt; hash mapping object name to ps 对象映射的hash函数 __u8 pg_autoscale_mode; ///&lt; PG_AUTOSCALE_MODE_ PG数自动增减模式private: __u32 pg_num = 0, pgp_num = 0; ///&lt; pg、pgp的数量 __u32 pg_num_pending = 0; ///&lt; pg_num we are about to merge down to __u32 pg_num_target = 0; ///&lt; pg_num we should converge toward __u32 pgp_num_target = 0; ///&lt; pgp_num we should converge towardpublic: map&lt;string,string&gt; properties; ///&lt; OBSOLETE string erasure_code_profile; ///&lt; name of the erasure code profile in OSDMap epoch_t last_change; ///&lt; most recent epoch changed, exclusing snapshot changes /// last epoch that forced clients to resend epoch_t last_force_op_resend = 0; /// last epoch that forced clients to resend (pre-nautilus clients only) epoch_t last_force_op_resend_prenautilus = 0; /// last epoch that forced clients to resend (pre-luminous clients only) epoch_t last_force_op_resend_preluminous = 0; /// metadata for the most recent PG merge pg_merge_meta_t last_pg_merge_meta; snapid_t snap_seq; ///&lt; seq for per-pool snapshot epoch_t snap_epoch; ///&lt; osdmap epoch of last snap uint64_t auid; ///&lt; who owns the pg uint64_t quota_max_bytes; ///&lt; maximum number of bytes for this pool uint64_t quota_max_objects; ///&lt; maximum number of objects for this pool /* * Pool snaps (global to this pool). These define a SnapContext for * the pool, unless the client manually specifies an alternate * context. */ map&lt;snapid_t, pool_snap_info_t&gt; snaps; /* * Alternatively, if we are defining non-pool snaps (e.g. via the * Ceph MDS), we must track @removed_snaps (since @snaps is not * used). Snaps and removed_snaps are to be used exclusive of each * other! */ interval_set&lt;snapid_t&gt; removed_snaps; unsigned pg_num_mask, pgp_num_mask; // Tier cache : Base Storage = N : 1 // ceph osd tier add &#123;data_pool&#125; &#123;cache pool&#125; set&lt;uint64_t&gt; tiers; ///&lt; pools that are tiers of us int64_t tier_of; ///&lt; pool for which we are a tier // Note that write wins for read+write ops // WriteBack mode, read_tier is same as write_tier. Both are cache pool. // Diret mode. cache pool is read_tier, not write_tier. // ceph osd tier set-overlay &#123;data_pool&#125; &#123;cache_pool&#125; int64_t read_tier; ///&lt; pool/tier for objecter to direct reads to int64_t write_tier; ///&lt; pool/tier for objecter to direct writes to // Set cache mode // ceph osd tier cache-mode &#123;cache-pool&#125; &#123;cache-mode&#125; cache_mode_t cache_mode; ///&lt; cache pool mode uint64_t target_max_bytes; ///&lt; tiering: target max pool size uint64_t target_max_objects; ///&lt; tiering: target max pool size // 目标脏数据率：当脏数据比例达到这个值，后台 agent 开始 flush 数据 uint32_t cache_target_dirty_ratio_micro; ///&lt; cache: fraction of target to leave dirty // 高目标脏数据率：当脏数据比例达到这个值，后台 agent 开始高速 flush 数据 uint32_t cache_target_dirty_high_ratio_micro; ///&lt; cache: fraction of target to flush with high speed // 数据满的比率：当数据达到这个比例时，认为数据已满，需要进行缓存淘汰 uint32_t cache_target_full_ratio_micro; ///&lt; cache: fraction of target to fill before we evict in earnest // 对象在 cache 中被刷入到 storage 层的最小时间 uint32_t cache_min_flush_age; ///&lt; minimum age (seconds) before we can flush // 对象在 cache 中被淘汰的最小时间 uint32_t cache_min_evict_age; ///&lt; minimum age (seconds) before we can evict // HitSet 相关参数 HitSet::Params hit_set_params; ///&lt; The HitSet params to use on this pool // 每间隔 hit_set_period 一段时间，系统重新产生一个新的 hit_set 对象来记录对象的h缓存统计信息 uint32_t hit_set_period; ///&lt; periodicity of HitSet segments (seconds) // 记录系统保存最近的多少个 hit_set 记录 uint32_t hit_set_count; ///&lt; number of periods to retain // hitset archive 对象的命名规则 bool use_gmt_hitset; ///&lt; use gmt to name the hitset archive object uint32_t min_read_recency_for_promote; ///&lt; minimum number of HitSet to check before promote on read uint32_t min_write_recency_for_promote; ///&lt; minimum number of HitSet to check before promote on write uint32_t hit_set_grade_decay_rate; ///&lt; current hit_set has highest priority on objects ///&lt; temperature count,the follow hit_set&#x27;s priority decay ///&lt; by this params than pre hit_set //当前hit_set在对象温度计数上具有最高优先级，后续hit_set的优先级比预hit_set衰减此参数 uint32_t hit_set_search_last_n; ///&lt; accumulate atmost N hit_sets for temperature 为温度累积最多N次hit_sets uint32_t stripe_width; ///&lt; erasure coded stripe size in bytes uint64_t expected_num_objects; ///&lt; expected number of objects on this pool, a value of 0 indicates ///&lt; user does not specify any expected value bool fast_read; ///&lt; whether turn on fast read on the pool or not pool_opts_t opts; ///&lt; options /// application -&gt; key/value metadata map&lt;string, std::map&lt;string, string&gt;&gt; application_metadata;private: vector&lt;uint32_t&gt; grade_table;public: uint32_t get_grade(unsigned i) const &#123; if (grade_table.size() &lt;= i) return 0; return grade_table[i]; &#125; void calc_grade_table() &#123; unsigned v = 1000000; grade_table.resize(hit_set_count); // hit_set_count记录系统保存最近的多少个 hit_set 记录 for (unsigned i = 0; i &lt; hit_set_count; i++) &#123; v = v * (1 - (hit_set_grade_decay_rate / 100.0)); grade_table[i] = v; &#125; &#125;&#125;; 数据结构pg_pool_t的成员变量和方法较多，不一一介绍了。 PGPG可以认为是一组对象的集合，该集合里的对象有共同特征：副本都分布在相同的OSD列表中。结构体pg_t只是一个PG的静态描述信息（只有三个成员变量），类PG及其子类ReplicatedPG都是和PG相关的处理。pg_t的数据结构如下：（src&#x2F;osd&#x2F;osd_types.h） 12345678struct pg_t &#123; uint64_t m_pool; //pg所在的pool uint32_t m_seed; //pg的序号 static const uint8_t calc_name_buf_size = 36; // max length for max values len(&quot;18446744073709551615.ffffffff&quot;) + future suffix len(&quot;_head&quot;) + &#x27;\\0&#x27; hobject_t get_hobj_start() const; hobject_t get_hobj_end(unsigned pg_num) const; static void generate_test_instances(list&lt;pg_t*&gt;&amp; o);&#125;; OSDMapOSDMap类定义了Ceph整个集群的全局信息。它由Monitor实现管理，并以全量或者增量的方式向整个集群扩散。每一个epoch对应的OSDMap都需要持久化保存在meta下对应对象的omap属性中。内部类Incremental以增量的形式保存了OSDMap新增的信息。OSDMap包含了四类信息：首先是集群的信息，其次是pool的信息，然后是临时PG相关信息，最后就是所有OSD的状态信息。OSDMap类的数据结构如下：（src&#x2F;osd&#x2F;OSDMap.h） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889class OSDMap &#123;public: MEMPOOL_CLASS_HELPERS(); typedef interval_set&lt; snapid_t, mempool::osdmap::flat_map&lt;snapid_t,snapid_t&gt;&gt; snap_interval_set_t; class Incremental &#123; public: MEMPOOL_CLASS_HELPERS(); //系统相关的信息 /// feature bits we were encoded with. the subsequent OSDMap /// encoding should match. uint64_t encode_features; uuid_d fsid; //当前集群的fsid值 epoch_t epoch; //当前集群的epoch值 new epoch; we are a diff from epoch-1 to epoch utime_t modified; //创建修改的时间戳 int64_t new_pool_max; //incremented by the OSDMonitor on each pool create int32_t new_flags; int8_t new_require_osd_release = -1; // full (rare) bufferlist fullmap; // in lieu of below. bufferlist crush;......private: //集群相关的信息 uuid_d fsid; //当前集群的fsid值 epoch_t epoch; //当前集群的epoch值 what epoch of the osd cluster descriptor is this utime_t created, modified; //创建、修改的时间戳 epoch start time int32_t pool_max; //最大的pool数量 the largest pool num, ever uint32_t flags; //一些标志信息 //OSD相关的信息 int num_osd; //OSD的总数量 not saved; see calc_num_osds int num_up_osd; //处于up状态的OSD的数量 not saved; see calc_num_osds int num_in_osd; //处于in状态的OSD的数量 not saved; see calc_num_osds int32_t max_osd; //OSD的最大数目 vector&lt;uint32_t&gt; osd_state; //OSD的状态 mempool::osdmap::map&lt;int32_t,uint32_t&gt; crush_node_flags; // crush node -&gt; CEPH_OSD_* flags mempool::osdmap::map&lt;int32_t,uint32_t&gt; device_class_flags; // device class -&gt; CEPH_OSD_* flags utime_t last_up_change, last_in_change; // These features affect OSDMap[::Incremental] encoding, or the // encoding of some type embedded therein (CrushWrapper, something // from osd_types, etc.). static constexpr uint64_t SIGNIFICANT_FEATURES = CEPH_FEATUREMASK_PGID64 | CEPH_FEATUREMASK_PGPOOL3 | CEPH_FEATUREMASK_OSDENC | CEPH_FEATUREMASK_OSDMAP_ENC | CEPH_FEATUREMASK_OSD_POOLRESEND | CEPH_FEATUREMASK_NEW_OSDOP_ENCODING | CEPH_FEATUREMASK_MSG_ADDR2 | CEPH_FEATUREMASK_CRUSH_TUNABLES5 | CEPH_FEATUREMASK_CRUSH_CHOOSE_ARGS | CEPH_FEATUREMASK_SERVER_LUMINOUS | CEPH_FEATUREMASK_SERVER_MIMIC | CEPH_FEATUREMASK_SERVER_NAUTILUS; struct addrs_s &#123; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; client_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; cluster_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; hb_back_addrs; mempool::osdmap::vector&lt;std::shared_ptr&lt;entity_addrvec_t&gt; &gt; hb_front_addrs; &#125;; std::shared_ptr&lt;addrs_s&gt; osd_addrs; //OSD的地址 entity_addrvec_t _blank_addrvec; mempool::osdmap::vector&lt;__u32&gt; osd_weight; //OSD的权重 16.16 fixed point, 0x10000 = &quot;in&quot;, 0 = &quot;out&quot; mempool::osdmap::vector&lt;osd_info_t&gt; osd_info; //OSD 的基本信息 std::shared_ptr&lt; mempool::osdmap::vector&lt;uuid_d&gt; &gt; osd_uuid; //OSD对应的uuid mempool::osdmap::vector&lt;osd_xinfo_t&gt; osd_xinfo; //OSD一些扩展信息 //PG相关的信息 std::shared_ptr&lt;PGTempMap&gt; pg_temp; // temp pg mapping (e.g. while we rebuild) std::shared_ptr&lt; mempool::osdmap::map&lt;pg_t,int32_t &gt; &gt; primary_temp; // temp primary mapping (e.g. while we rebuild) std::shared_ptr&lt; mempool::osdmap::vector&lt;__u32&gt; &gt; osd_primary_affinity; ///&lt; 16.16 fixed point, 0x10000 = baseline // remap (post-CRUSH, pre-up) mempool::osdmap::map&lt;pg_t,mempool::osdmap::vector&lt;int32_t&gt;&gt; pg_upmap; ///&lt; remap pg mempool::osdmap::map&lt;pg_t,mempool::osdmap::vector&lt;pair&lt;int32_t,int32_t&gt;&gt;&gt; pg_upmap_items; ///&lt; remap osds in up set //pool的相关信息 mempool::osdmap::map&lt;int64_t,pg_pool_t&gt; pools; //pool的id到pg_pool_t的映射 mempool::osdmap::map&lt;int64_t,string&gt; pool_name; //pool的id到pool的名字的映射 mempool::osdmap::map&lt;string,map&lt;string,string&gt; &gt; erasure_code_profiles; //pool的EC相关信息 mempool::osdmap::map&lt;string,int64_t&gt; name_pool; //pool的名字到pool的id的映射 Op结构体Op封装了完成一个操作的相关上下文信息，包括target地址信息(op_target_t)、链接信息(session)等 123456789101112131415161718192021222324252627282930313233343536//Op封装了完成一个操作的相关的上下文信息，包括target地址信息、链接信息等。 struct Op : public RefCountedObject &#123; OSDSession *session; //OSD相关的Session信息 int incarnation; //引用次数 op_target_t target; //地址信息 ConnectionRef con; // for rx buffer only uint64_t features; // explicitly specified op features vector&lt;OSDOp&gt; ops; // 对应多个操作的封装 snapid_t snapid; //快照的ID SnapContext snapc; //pool层级的快照信息 ceph::real_time mtime; bufferlist *outbl; //输出的bufferlist vector&lt;bufferlist*&gt; out_bl; //每个操作对应的bufferlist vector&lt;Context*&gt; out_handler; //每个操作对应的回调函数 vector&lt;int*&gt; out_rval; //每个操作对应的输出结果 int priority; Context *onfinish; uint64_t ontimeout; ceph_tid_t tid; int attempts; version_t *objver; epoch_t *reply_epoch; ceph::coarse_mono_time stamp; epoch_t map_dne_bound; int budget; /// true if we should resend this message on failure bool should_resend; /// true if the throttle budget is get/put on a series of OPs, /// instead of per OP basis, when this flag is set, the budget is /// acquired before sending the very first OP of the series and /// released upon receiving the last OP reply. bool ctx_budgeted; int *data_offset; osd_reqid_t reqid; // explicitly setting reqid ZTracer::Trace trace; op_target_t数据结构op_target_t封装了对象所在的PG，以及PG对应的OSD列表等地址信息。 12345678910111213141516171819202122232425262728293031//封装了对象所在的PG，以及PG对应的OSD列表等地址信息 struct op_target_t &#123; int flags = 0; //标志 epoch_t epoch = 0; ///&lt; latest epoch we calculated the mapping object_t base_oid; //读取的对象 object_locator_t base_oloc; //对象的pool信息 object_t target_oid; //最终读取的目标对象 object_locator_t target_oloc; //最终目标对象的pool信息 ///&lt; true if we are directed at base_pgid, not base_oid bool precalc_pgid = false; ///&lt; true if we have ever mapped to a valid pool bool pool_ever_existed = false; ///&lt; explcit pg target, if any pg_t base_pgid; pg_t pgid; ///&lt; last (raw) pg we mapped to spg_t actual_pgid; ///&lt; last (actual) spg_t we mapped to unsigned pg_num = 0; ///&lt; last pg_num we mapped to unsigned pg_num_mask = 0; ///&lt; last pg_num_mask we mapped to unsigned pg_num_pending = 0; ///&lt; last pg_num we mapped to vector&lt;int&gt; up; ///&lt; set of up osds for last pg we mapped to vector&lt;int&gt; acting; ///&lt; set of acting osds for last pg we mapped to int up_primary = -1; ///&lt; last up_primary we mapped to int acting_primary = -1; ///&lt; last acting_primary we mapped to int size = -1; ///&lt; the size of the pool when were were last mapped int min_size = -1; ///&lt; the min size of the pool when were were last mapped bool sort_bitwise = false; ///&lt; whether the hobject_t sort order is bitwise bool recovery_deletes = false; ///&lt; whether the deletes are performed during recovery instead of peering bool used_replica = false; bool paused = false; int osd = -1; ///&lt; the final target osd, or -1 epoch_t last_force_resend = 0; CRUSH Map123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146struct crush_rule_step &#123; __u32 op; //操作类型 __s32 arg1; //操作数1 __s32 arg2; //操作数2&#125;;enum crush_opcodes &#123; CRUSH_RULE_NOOP = 0, CRUSH_RULE_TAKE = 1, /* arg1 = value to start with */ CRUSH_RULE_CHOOSE_FIRSTN = 2, /* arg1 = num items to pick */ /* arg2 = type */ CRUSH_RULE_CHOOSE_INDEP = 3, /* same */ CRUSH_RULE_EMIT = 4, /* no args */ CRUSH_RULE_CHOOSELEAF_FIRSTN = 6, CRUSH_RULE_CHOOSELEAF_INDEP = 7, CRUSH_RULE_SET_CHOOSE_TRIES = 8, /* override choose_total_tries */ CRUSH_RULE_SET_CHOOSELEAF_TRIES = 9, /* override chooseleaf_descend_once */ CRUSH_RULE_SET_CHOOSE_LOCAL_TRIES = 10, CRUSH_RULE_SET_CHOOSE_LOCAL_FALLBACK_TRIES = 11, CRUSH_RULE_SET_CHOOSELEAF_VARY_R = 12, CRUSH_RULE_SET_CHOOSELEAF_STABLE = 13&#125;;/* * 用于指定相对于传递给 do_rule 的 max 参数的选择 num (arg1) */#define CRUSH_CHOOSE_N 0#define CRUSH_CHOOSE_N_MINUS(x) (-(x))/* * 规则掩码用于描述规则的用途。 * 给定规则集和输出集的大小，我们在规则列表中搜索匹配的 rule_mask。 */struct crush_rule_mask &#123; __u8 ruleset; //ruleId __u8 type; //多副本还是纠删码 __u8 min_size; //副本数大于等于时适用 __u8 max_size; //副本数小于等于时适用&#125;;struct crush_rule &#123; __u32 len; //steps数组的长度 struct crush_rule_mask mask; //releset相关的配置参数 struct crush_rule_step steps[0]; //step集合&#125;;#define crush_rule_size(len) (sizeof(struct crush_rule) + \\ (len)*sizeof(struct crush_rule_step))/* * A bucket is a named container of other items (either devices or * other buckets). * 桶是其他item（设备或其他存储桶）的命名容器 *//** * 使用三种算法中的一种来选择的，这些算法代表了性能和重组效率之间的权衡。 * 如果您不确定要使用哪种存储桶类型，我们建议您使用 ::CRUSH_BUCKET_STRAW2。 * 该表总结了在添加或删除item时每个选项的速度如何与映射稳定性相比较。 * Bucket Alg Speed Additions Removals * ------------------------------------------------ * uniform O(1) poor poor * list O(n) optimal poor * straw2 O(n) optimal optimal */enum crush_algorithm &#123; CRUSH_BUCKET_UNIFORM = 1, CRUSH_BUCKET_LIST = 2, CRUSH_BUCKET_TREE = 3, CRUSH_BUCKET_STRAW = 4, CRUSH_BUCKET_STRAW2 = 5,&#125;;extern const char *crush_bucket_alg_name(int alg);#define CRUSH_LEGACY_ALLOWED_BUCKET_ALGS ( \\ (1 &lt;&lt; CRUSH_BUCKET_UNIFORM) | \\ (1 &lt;&lt; CRUSH_BUCKET_LIST) | \\ (1 &lt;&lt; CRUSH_BUCKET_STRAW))struct crush_bucket &#123; __s32 id; //bucket的编号。小于0 /*!&lt; bucket identifier, &lt; 0 and unique within a crush_map */ __u16 type; //bucket的类型/*!&lt; &gt; 0 bucket type, defined by the caller */ __u8 alg; //使用的crush算法/*!&lt; the item selection ::crush_algorithm */ __u8 hash; //使用的hash算法/* which hash function to use, CRUSH_HASH_* */ __u32 weight; //权重 /*!&lt; 16.16 fixed point cumulated children weight */ __u32 size; //items的数量/*!&lt; size of the __items__ array */ __s32 *items; //子bucket/*!&lt; array of children: &lt; 0 are buckets, &gt;= 0 items */&#125;;struct crush_weight_set &#123; __u32 *weights; /*!&lt; 16.16 fixed point weights in the same order as items */ __u32 size; /*!&lt; size of the __weights__ array */&#125;;struct crush_choose_arg &#123; __s32 *ids; /*!&lt; values to use instead of items */ __u32 ids_size; /*!&lt; size of the __ids__ array */ struct crush_weight_set *weight_set; /*!&lt; weight replacements for a given position */ __u32 weight_set_positions; /*!&lt; size of the __weight_set__ array */&#125;;struct crush_choose_arg_map &#123; struct crush_choose_arg *args; /*!&lt; replacement for each bucket in the crushmap */ __u32 size; /*!&lt; size of the __args__ array */&#125;;struct crush_bucket_uniform &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __u32 item_weight; /*!&lt; 16.16 fixed point weight for each item */&#125;;struct crush_bucket_list &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __u32 *item_weights; /*!&lt; 16.16 fixed point weight for each item */ __u32 *sum_weights; /*!&lt; 16.16 fixed point sum of the weights */&#125;;struct crush_bucket_tree &#123; struct crush_bucket h; /* note: h.size is _tree_ size, not number of actual items */ __u8 num_nodes; __u32 *node_weights;&#125;;struct crush_bucket_straw &#123; struct crush_bucket h; __u32 *item_weights; /* 16-bit fixed point */ __u32 *straws; /* 16-bit fixed point */&#125;;struct crush_bucket_straw2 &#123; struct crush_bucket h; /*!&lt; generic bucket information */ __. /*!&lt; 16.16 fixed point weight for each item */&#125;;struct crush_map &#123; struct crush_bucket **buckets; **类型，所有的bucket都存在这里 /*! 一个大小为__max_rules__ 的crush_rule 指针数组。 * 如果规则被删除，数组的一个元素可能为NULL（没有API 可以这样做，但将来可能会有一个）。 * 规则必须使用crunch_add_rule() 添加。 */ struct crush_rule **rules; //**类型，多层嵌套的rules __s32 max_buckets; /*!&lt; the size of __buckets__ */ // bucket的总数 __u32 max_rules; /*!&lt; the size of __rules__ */ // rule的总数 __s32 max_devices; // osd的总数 __u32 choose_local_tries; //选择的总次数 __u32 choose_local_fallback_tries; __u32 choose_total_tries; __u32 chooseleaf_descend_once; __u8 chooseleaf_vary_r; __u8 chooseleaf_stable; /* 该值是在构建器解码或构建后计算的。 它在此处公开（而不是具有“构建 CRUSH 工作空间”功能），以便调用者可以保留静态缓冲区、在堆栈上分配空间，或者在需要时避免调用堆分配器。 工作空间的大小取决于映射，而传递给映射器的临时向量的大小取决于所需结果集的大小。尽管如此，没有什么能阻止调用者在一个膨胀 foop 中分配两个点并传递两个点。 */ size_t working_size;","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph源码编译调试","slug":"Storage/Ceph/Ceph源码编译调试","date":"2021-06-20T07:24:13.000Z","updated":"2024-07-27T14:38:39.225Z","comments":true,"path":"Storage/Ceph/Ceph源码编译调试/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95/","excerpt":"","text":"对于一个ceph开发人员来说编译源码以及打rpm是其必备技能。无论是fix bug还是向社区提交pull request都离不开编译源码。 编译环境环境介绍 ceph version: N版 14.2.16 硬件环境：Centos7虚拟机 网络环境与源加速 12345678910111213141516171819202122232425262728293031323334353637383940414243# 额外软件源、生成新的缓存yum -y install centos-release-sclyum -y install epel-release yum clean all &amp;&amp; yum makecacheyum listyum update# 更换pip源，创建 .pip 目录mkdir ~/.pip cd ~/.pip vi pip.conf# 写入以下配置[global]index-url = https://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com#配置yum源vim /etc/yum.repos.d/ceph.repo[norch]name=norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[x86_64]name=x86 64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0[SRPMS]name=SRPMSbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS/enabled=1gpgcheck=0[aarch64]name=aarch64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/aarch64/enabled=1gpgcheck=0 安装编译环境及依赖包 123456789101112131415yum -y install rdma-core-devel systemd-devel keyutils-libs-devel openldap-devel leveldb-devel snappy-devel lz4-devel curl-devel nss-develyum -y install libzstd zstd gcc cmake make git wgetyum -y install devtoolset-7-gcc devtoolset-7-gcc-c++ devtoolset-7-binutils # 安装gcc 7.2scl enable devtoolset-7 bash #临时生效source /opt/rh/devtoolset-7/enableecho &quot;source /opt/rh/devtoolset-7/enable&quot; &gt;&gt;/etc/profile #长期生效gcc -v #查看环境gcc版本wget https://github.com/Kitware/CMake/releases/download/v3.18.2/cmake-3.18.2.tar.gz #安装cmake3tar -zxvf cmake-3.18.2.tar.gzcd cmake-3.18.2 yum -y install ncurses-devel openssl-devel./bootstrapgmake &amp;&amp; gmake installln -s /usr/local/share/cmake /usr/bin/cmake -version 安装 ccache 加速编译 1234567891011121314151617181920# 下载安装包并解压mkdir /home/ccache cd /home/ccachewget https://github.com/ccache/ccache/releases/download/v4.0/ccache-4.0.tar.gztar -zxvf ccache-4.0.tar.gzcd ccache-4.0# 编译安装mkdir build cd buildcmake -DCMAKE_BUILD_TYPE=Release -DZSTD_FROM_INTERNET=ON ..make -j12make install# 修改配置mkdir -p /root/.config/ccache/ vi /root/.config/ccache/ccache.confmax_size = 16Gsloppiness = time_macrosrun_second_cpp = true 编译ceph代码1234567891011121314151617181920212223242526272829303132## 下载Ceph源码一mkdir /home/cephcd /home/cephgit clone git://github.com/ceph/ceph.git #(git clone https://github.com/ceph/ceph.git)cd cephgit checkout nautilus #切换分支，这里以 N 版本为例git submodule update --init --recursive #进入ceph目录，下载ceph代码依赖 ## 下载Ceph源码二wget https://mirrors.aliyun.com/ceph/debian-nautilus/pool/main/c/ceph/ceph_14.2.22.orig.tar.gztar -zxvf ceph_14.2.22.orig.tar.gzcd ceph_14.2.2./install-deps.sh #执行依赖安装脚本，ceph 自带的解决依赖的脚本## 修改cmake参数，因为后面需要使用gdb debug客户端程序，客户端程序会依赖librados库，所以我们必须以debug的模式去编译ceph，否则编译器会优化掉很多参数，导致很多信息缺失，需要修改一下ceph cmake的参数。如图所示vim do_cmake.sh $&#123;CMAKE&#125; -DCMAKE_C_FLAGS=&quot;-O0 -g3 -gdwarf-4&quot; -DCMAKE_CXX_FLAGS=&quot;-O0 -g3 -gdwarf-4&quot; -DBOOST_J=$(nproc) $ARGS &quot;$@&quot; ..# 可以看到这里修改了cmake的参数，增加了两个配置项，稍微解释一下# CMAKE_C_FLAGS=“-O0 -g3 -gdwarf-4” ： c 语言编译配置# CMAKE_CXX_FLAGS=“-O0 -g3 -gdwarf-4” ：c++ 编译配置# -O0 : 关闭编译器的优化，如果没有，使用GDB追踪程序时，大多数变量被优化,无法显示, 生产环境必须关掉# -g3 : 意味着会产生大量的调试信息# -gdwarf-4 : dwarf 是一种调试格式，dwarf-4 版本为4 ./do_cmake.sh -DWITH_MANPAGE=OFF -DWITH_BABELTRACE=OFF -DWITH_MGR_DASHBOARD_FRONTEND=OFF -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_CCACHE=ON --DWITH_PYTHON3=ON --DMGR_PYTHON_VERSION=3# 执行 cmake，解释一下，DWITH_MGR_DASHBOARD_FRONTEND=OFF 主要是因为 ceph dashboard 用到了一些国外的 nodejs源，国内无法下载，会导致编译失败超时。-DWITH_CCACHE=ON 如果你没有安装 步骤 2-2 的 ccache 的话，可以去掉这个参数。 cd buildmake -j20 #（线程数等于cpu core的2倍，可以提高编译的速度，20核CPU、32G内存的服务器） 修改do_cmake.sh编译进度 自此已经编译完ceph源代码！ 运行测试集群发行版的 ceph 安装包安装的集群默认是没有办法debug调试。这里推荐 ceph 内置的debug调试——vstart，非常方便模仿特殊场景进行debug调试。 12345678cd /home/watson/ceph/build # 进入build目录make vstart # 编译模拟启动环境（make help 查看有哪些target可以单独编译）MDS=0 RGW=1 ../src/vstart.sh -d -l -n --bluestore # (模拟启动，指令前半部分的MDS=0 RGW=1之类的就是设定你想要模拟的集群结构（集群的配置文件在ceph/build/ceph.conf）)# 启动完成后，可以在模拟集群环境下执行各种 ceph 指令(模拟集群所有的指令都在 build/bin 目录)bin/ceph -s # 查看 ceph 集群状态bin/radosgw-admin user list # 查看用户../src/stop.sh # 关闭测试集群 编译vstasrt环境启动vstart环境查看 ceph 集群状态查看Ceph用户 运行单元测试用例更改了代码准备提交到公司内部repo或者社区repo都需要先执行一下最小测试集，看看自己修改的代码有没有影响到别的模块(社区也会进行同样的测试)。 1234567cd buildmake #修改代码后先编译，可以模块编译man ctest #查看ctest的功能ctest -j20 #运行所有测试（使用所有处理器并行）ctest -R [regex matching test name(s)] #运行部分模块测试，使用 -R（正则表达式匹配）ctest -V -R [regex matching test name(s)] #使用 -V（详细）标志运行ctest -j20 -V -R [regex matching test name(s)] #运行正则表达式匹配的模块测试，显示详细信息，并发进行 注意：许多从 src&#x2F;test 构建的目标不是使用ctest运行的。以 “unittest” 开头的目标在其中运行make check，因此可以使用运行ctest。以 “ceph_test” 开头的目标不能，应该手动运行。发生故障时，请在 build&#x2F;Testing&#x2F;Temporary 中查找日志。 开发编译测试过程 1231. 编写保存源代码2. make -j20 unittest_crush #模块编译3. ctest -j20 -V -R unittest_crush #模块测试 通过librados客户端调试CRUSH算法编写客户端代码调用librados 库写入数据 运行librados代码 12yum install librados2-devel libradospp libradosstriper-devel -y #安装相关开发包（C/C++开发包）gcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib #编译客户端程序 rados_write.c 这里解释一下gcc 几个参数，首先需要理解的是c程序在编译时依赖的库和运行时依赖库是分开指定的，也就是说，编译的时候使用的库，不一定就是运行时使用的库 g : 允许gdb调试 lrados : -l 指定依赖库的名字为rados L : 指定编译时依赖库的的路径， 如果不指定将在系统目录下寻找 o : 编译的二进制文件名 Wl : 指定编译时参数 rpath : 指定运行时依赖库的路径， 如果不指定将在系统目录下寻找 运行客户端程序 12./rados_writebin/rados ls -p default.rgw.meta #在集群中确认一下是否写入数据 运行rados_write程序确认写入数据 ceph的开发者模式是测试ceph功能和调试代码非常方便的途径，因为集群默认开启了debug模式，所有的日志都会详细的输出，并且为了调试的方便，在正式环境中的多线程多队列，在这都会简化。 使用GDB调试分析Object至OSD映射1234567yum install -y gdb #安装gdbgcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib #编译客户端程序 rados_write.cgdb ./rados_write #使用gdb 调试 rados_write 程序#启动程序后，需要设置断点，这里选择的是 crush_do_rule 函数，因为这个函数是 object–&gt;到PG 流程的终点b crush_do_rule #在crush_do_rule 函数设置断点bt #查看当前的函数堆栈 gdb调试raodos_wirte程序设置调试断点查看当前函数栈 得到的函数流程如下 12345678910111213#0 crush_do_rule at /home/watson/ceph/src/crush/mapper.c:904#1 do_rule at /home/watson/ceph/src/crush/CrushWrapper.h:1570#2 OSDMap::_pg_to_raw_osds at /home/watson/ceph/src/osd/OSDMap.cc:2340#3 OSDMap::_pg_to_up_acting_osds at /home/watson/ceph/src/osd/OSDMap.cc:2586#4 pg_to_up_acting_osds at /home/watson/ceph/src/osd/OSDMap.h:1209#5 Objecter::_calc_target at /home/watson/ceph/src/osdc/Objecter.cc:2846#6 Objecter::_op_submit at /home/watson/ceph/src/osdc/Objecter.cc:2367#7 Objecter::_op_submit_with_budget at /home/watson/ceph/src/osdc/Objecter.cc:2284#8 Objecter::op_submit at /home/watson/ceph/src/osdc/Objecter.cc:2251#9 librados::IoCtxImpl::operate at /home/watson/ceph/src/librados/IoCtxImpl.cc:690#10 librados::IoCtxImpl::write at /home/watson/ceph/src/librados/IoCtxImpl.cc:623#11 rados_write at /home/watson/ceph/src/librados/librados_c.cc:1133#12 main at rados_write.c:73 不关心librados是如何封装请求，只关心object到pg的计算过程，所以这里决定从 Objecter::_calc_target 函数开始debug 整个过程，重新开始，然后再次设置断点。重新开始，计算 object的hash值 ps 1b Objecter::_calc_target #断点 卡住在断点处，现在我们打开tui模式跟踪代码， crtl + x + a 可以切换到tui界面这里按 n 逐行debug代码， 这里我想显示打印 pg_pool_t *p 和 op_target_t *t 的信息其中 pg_pool_t 是pool的结构体，包含pool相关的所有信息 1p *pi #查看pi的数据结构 而 op_target_t 则是整个写入操作封装的结构信息，包含对象的名字，写入pool的id继续 n 单步调试，这里我们会进去 osdmap-&gt;object_locator_to_pg 函数。然后一步一步调试……object到PG的函数流程图PG映射到OSD函数流程图crush_choose_firstn选择的过程 使用VScode远程调试Ceph以ceph osd部分为例，为您演示通过第三方社区提供的vscode 编辑软件，对ceph osd进行进行图形化单步调试以及配置操作。vscode是微软公司一个开源的编译器具备轻量的特点，通过插件安装方式提供了丰富的调试功能。通常 Linux环境的c&#x2F;c++软件开发使用GDB进行命令行调试，命令行操方式极其不方便。使用vscode 的图形化界面可替代gdb 命令行 ，整个开发调试过程更加便捷。Ceph源码路径在&#x2F;home&#x2F;watson&#x2F;ceph目录下，其编译运行文件在&#x2F;home&#x2F;watson&#x2F;ceph&#x2F;build&#x2F;bin当中。启动调试前需要停止本地的osd运行服务。下载安装windows的vscode和ssh在以下地址下载vscode: https://code.visualstudio.com/安装openssh (一般情况不用自己手动安装)如果需要远程开发，Windows机器也需要支持openssh，如果本机没有，会报错。可以到微软官网上下载ssh。在vscode安装Remote Development和Remote-SSH在安装完成之后，点击左侧的Remote-SSH选项卡，再将鼠标移向CONNECTIONS栏，点击出现的configure：填写linux服务器的ssh端口和用户名（如果是默认的22端口可不用填写）按下ctrl + s 保存 然后连接（&#x2F;home&#x2F;watson&#x2F;ceph&#x2F;）输入密码，总共有多次输入密码的流程留意窗口变化打开远程服务器的文件夹 远程连接遇到的问题以及技巧 因为ceph工程文件数量众多会出现无法在这个大型工作区中监视文件更改。请按照说明链接来解决此问题的问题。原因：工作区很大并且文件很多，导致VS Code文件观察程序的句柄达到上限。解决方法：编辑linux服务器中的 &#x2F;etc&#x2F;sysctl.conf；将以下一行添加到文件末尾，可以将限制增加到最大值 fs.inotify.max_user_watches=524288 保存之后终端窗口 输入sysctl -p可解决。远程调试首先前提Linux服务器已经安装了GDB，否则会提示出错。在ceph工程目录下添加launch.json文件。在最左上栏运行(R) -&gt; 添加配置 ，注意一定要在ceph当前工程目录。修改配置launch.json中的program、args选项。 12345678910111213141516171819202122232425262728launch.json&#123; // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;ceph-debug&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;/build/bin/unittest_crush&quot;, &quot;args&quot;: [&quot;-d&quot;, &quot;--cluster&quot;, &quot;ceph&quot;,&quot;--id&quot;, &quot;0&quot;, &quot;--setuser&quot;, &quot;root&quot;, &quot;--setgroup&quot;, &quot;root&quot;], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ &#123; &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true &#125; ] &#125; ]&#125; 按照下图点击就可以开始调试之路 报错记录报错1 1234RPC failed; result=35, HTTP code = 0 fatal: The remote end hung up unexpectedly无法克隆 &#x27;https://github.com/xxxx/xxxxxxxx.git&#x27; 到子模组路径 &#x27;xxxxxxxxx&#x27;解决： 通过设置Git的http缓存大小，解决了这个问题，在当前工程目录下运行如下命令： git config --global http.postBuffer 20M (如果20M不行就50M) 报错2 12编译出现了一个问题，卡在5%Built target rocksdb_ext这里 原因：国外网络太慢，下载boost_1_72_0.tar.bz2太慢了，换网络或者在先用本地下载再传到服务器上（ceph/build/boost/src目录下） 报错3 12No Package found for python-scipyvim ceph.spec.in 报错4 1234&quot;Error: Package: golang-github-prometheus-2.26.1-2.el7.x86_64 (epel) Requires: /usr/bin/systemd-sysusers&quot;, 去掉该需求vim ~/ceph-14.2.16/ceph.spec.in# 内容#BuildRequires: golang-github-prometheus","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_librados_api使用","slug":"Storage/Ceph/Ceph_librados_api使用","date":"2021-06-18T06:28:31.000Z","updated":"2024-07-27T14:37:33.694Z","comments":true,"path":"Storage/Ceph/Ceph_librados_api使用/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph_librados_api%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Librados API概述Ceph存储集群提供基本的存储服务，Ceph以独特的方式将对象、块和文件存储集成到一个存储系统中。基于RADOS，可以不限于RESTful或POSIX接口，使用librados API能够创建自定义的Ceph存储集群接口（除了块存储、对象存储和文件系统存储外）。librados API能够与Ceph存储集群中的两种类型的守护进程进行交互： Ceph Mon守护进程，维护集群映射的主副本 Ceph OSD守护进程，它将数据作为对象存储在存储节点上要使用 API，您需要一个正在运行的 Ceph 存储集群。（本教程教程使用ceph编译的vstart启动的开发编程环境）编译模拟启动环境1234make vstart #模拟启动MDS=0 RGW=1 ../src/vstart.sh -d -l -n --bluestore #模拟集群所有的指令都在 build/bin 目录bin/ceph -s #查看 ceph 集群状态../src/stop.sh #停止模拟集群 第 1 步：获取libradosCeph客户端应用必须绑定librados才能连接Ceph存储集群。在写使用librados的ceph客户端应用前，要安装librados及其依赖包。librados API本身是用C++实现，也有C、Python、Java和PHP的API。（本教程仅限于librados C&#x2F;C++API）获取C&#x2F;C++的librados 要在 Debian&#x2F;Ubuntu 发行版上安装C&#x2F;C++ 的librados开发支持文件，执行以下命令： sudo apt-get install librados-dev 要在 RHEL&#x2F;CentOS 发行版上安装C&#x2F;C++ 的librados开发支持文件，执行以下命令： sudo yum install librados2-devel 安装librados 后，可以在&#x2F;usr&#x2F;include&#x2F;rados 下找到 C&#x2F;C++所需的头文件 ls /usr/include/rados 第 2 步：配置集群句柄一个Ceph客户端，通过librados直接与OSD交互，来存储和取出数据。为了与OSD交互，客户端应用必须直接调用libradosAPI连接一个Ceph Monitor。一旦连接好以后，librados会从Monitor处取回一个Cluster map。当客户端的应用想读或者取数据的时候，它会创建一个I&#x2F;O上下文并且与一个pool绑定。通过这个I&#x2F;O上下文，客户端将Object的名字提供给librados，然后librados会根据Object的名字和Cluster map计算出相应的PG和OSD的位置。然后客户端就可以读或者写数据。客户端的应用无需知道这个集群的拓扑结构。Ceph存储集群手柄封装客户端配置，包括： 基于用户ID的rados_create() 或者基于用户名的rados_create2()(首选) cephx认证密钥 Mon ID和IP地址 日志记录级别 调试级别 因此，Ceph客户端应用程序使用Ceph群集的步骤： 创建一个集群句柄，客户端应用将使用该句柄连接到存储集群中； 使用该手柄进行连接。要连接到集群的客户端应用必须提供Mon地址，用户名和认证密钥（默认启用cephx）。提示：与不同的 Ceph 存储集群或与具有不同用户的同一个集群通信需要不同的集群句柄。RADOS 提供了多种设置所需值的方法。对于Mon和加密密钥设置，处理它们的一种简单方法是确保 Ceph 配置文件包含密钥环文件的密钥环路径和至少一个Mon地址（例如mon host）。例如:123[global]mon host = 192.168.1.1keyring = /etc/ceph/ceph.client.admin.keyring 创建句柄后，读取 Ceph 配置文件来配置句柄。可以将参数传递给客户端应用程序并使用解析命令行参数的函数（例如rados_conf_parse_argv()）或解析 Ceph 环境变量（例如rados_conf_parse_env()）来解析它们。 连接后，客户端应用程序可以调用仅使用集群句柄影响整个集群的函数。例如，一旦有了集群句柄，就可以： • 获取集群统计信息 • 使用池操作（存在、创建、列出、删除） • 获取和设置配置 Ceph 的强大功能之一是能够绑定到不同的池。每个池可能有不同数量的归置组、对象副本和复制策略。例如，可以将池设置为使用 SSD 存储常用对象的“热”池或使用纠删码的“冷”池。各种语言的librados 绑定的主要区别在于 C 与C++、Java 和 Python 的面向对象绑定之间。面向对象的绑定使用对象来表示集群句柄、IO 上下文、迭代器、异常等。 C调用librados 示例对于 C，使用管理员用户创建一个简单的集群句柄，配置它并连接到集群如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#include &lt;rados/librados.h&gt;int main(int argc,const char* argv[])&#123; rados_t cluster; char cluster_name[] = &quot;ceph&quot;; char user_name[] = &quot;client.admin&quot;; char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; uint64_t flags; int err; err = rados_create2(&amp;cluster,cluster_name,user_name,flags); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: Couldn&#x27;t create the cluster handle!%s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Create a cluster handle!!!\\n&quot;); &#125; err = rados_conf_read_file(cluster,conf_flie); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: cannot read config file: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Read the config flie\\n&quot;); &#125; err = rados_conf_parse_argv(cluster,argc,argv); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: cannot parse command line arguments: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Read the command line arguments\\n&quot;); &#125; err = rados_connect(cluster); if(err &lt; 0) &#123; fprintf(stderr,&quot;%s: cannot connect to cluster: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Connected to the cluster\\n&quot;); &#125; return 0;&#125; 使用-lrados编译客户端应用代码并链接到librados，如下： 1gcc ceph-client.c -lrados -o ceph-client ceph源码开发vstart环境下的编译，如下： 1gcc -g rados_write.c -lrados -L/home/watson/ceph/build/lib -o rados_write -Wl,-rpath,/home/watson/ceph/build/lib C++调用librados示例Ceph项目在ceph&#x2F;examples&#x2F;librados目录中提供了一个 C++ 示例。对于 C++，使用管理员用户的简单集群句柄需要初始化librados::Rados集群句柄对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;rados/librados.hpp&gt;/* *通过librados::Rados句柄处理整个RADOS系统层面以及pool层面的管理。*/int main(int argc,const char* argv[])&#123; int ret = 0; librados::Rados cluster; //定义一个操控集群的句柄对象 char cluster_name[] = &quot;ceph&quot;; //集群名字 char user_name[] = &quot;client.admin&quot;; //集群用户名 char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; //集群配置文件 uint64_t flags; ret = cluster.init2(user_name,cluster_name,flags); //初始化句柄对象 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t initialize the cluster handle! error: &quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Create a cluster handle.&quot;&lt;&lt;std::endl; &#125; ret = cluster.conf_read_file(conf_flie); //读配置文件获取Mon的信息 if(ret &lt; 0 ) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read the ceph configuration file! error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Read the ceph configuration file.&quot;&lt;&lt;std::endl; &#125; ret = cluster.conf_parse_argv(argc,argv); //解析命令行输入的参数 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t parsed command line options!error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Parsed command line options.&quot;&lt;&lt;std::endl; &#125; ret = cluster.connect(); //连接集群 if(ret &lt; 0 ) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t connect to cluster! error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Connected to the cluster.&quot;&lt;&lt;std::endl; &#125; cluster.pool_create(&quot;testpool&quot;); //创建存储池 std::list&lt;std::string&gt; poolList; cluster.pool_list(poolList); //获取存储池列表 for(auto iter : poolList)&#123; std::cout&lt;&lt;iter&lt;&lt;std::endl; &#125; return 0;&#125; 编译源码，然后，使用-lrados链接librados，如下： 12g++ -g -c ceph-client.cc -o ceph-client.o g++ -g ceph-client.o -lrados -o ceph-client ceph源码开发vstart环境下的编译，如下： 1g++ -g librados_rados.cpp -lrados -L/home/watson/ceph/build/lib -o librados_rados -Wl,-rpath,/home/watson/ceph/build/lib 第 3 步：创建 I&#x2F;O 上下文一旦客户端应用程序拥有集群句柄并连接到 Ceph 存储集群，就可以创建 I&#x2F;O 上下文并开始读取和写入数据。I&#x2F;O 上下文将连接绑定到特定池。用户必须具有适当的CAPS权限才能访问指定的池。例如，具有读取权限但没有写入权限的用户将只能读取数据。I&#x2F;O 上下文功能包括： 写入&#x2F;读取数据和扩展属性 列出并迭代对象和扩展属性 快照池、列表快照等RADOS 使客户端应用程序能够进行同步和异步交互。一旦应用程序具有 I&#x2F;O 上下文，读&#x2F;写操作只需要知道对象&#x2F;xattr 名称。librados中封装的 CRUSH 算法使用Cluster map来选择合适的 OSD。OSD 守护进程自动处理副本。librados库将对象映射到归置组。以下示例使用默认数据池。但是，也可以使用 API 列出池、确保它们存在或创建和删除池。对于写操作，示例说明了如何使用同步模式。对于读取操作，示例说明了如何使用异步模式。(提示：使用此 API 删除池时要小心。如果删除池，则该池和池中的所有数据都将丢失。)C创建Ceph IO上下文示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;#include &lt;rados/librados.h&gt;int main(int argc,const char* argv[])&#123; rados_t cluster; //集群句柄 rados_ioctx_t io; //io上下文 char cluster_name[] = &quot;ceph&quot;; char user_name[] = &quot;client.admin&quot;; char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; char poolname[] = &quot;testpool&quot;; uint64_t flags; int err; /* 为了使示例代码更可观性，不对返回值判错，实际应用中需要进行判错，请养成良好习惯！ */ err = rados_create2(&amp;cluster,cluster_name,user_name,flags); err = rados_conf_read_file(cluster,conf_flie); err = rados_conf_parse_argv(cluster,argc,argv); err = rados_connect(cluster); if(err &lt; 0) //检查是否连接到集群上 &#123; fprintf(stderr,&quot;%s: Cannot connect to cluster: %s\\n&quot;,argv[0],strerror(-err)); exit(0); &#125;else&#123; printf(&quot;Connected to the cluster......\\n&quot;); &#125; //err = rados_pool_delete(cluster,poolname); int poolID = rados_pool_lookup(cluster,poolname); //通过poolname获取pool的ID，若池不存在返回-ENOENT if(poolID == -ENOENT) &#123; printf(&quot;this pool does not exist,and create the pool...... \\n&quot;); rados_pool_create(cluster,poolname); &#125; err = rados_ioctx_create(cluster,poolname,&amp;io); //初始化io上下文 char obj_name[] = &quot;obj&quot;; char obj_content[] = &quot;Hello librados&quot;; err = rados_write(io,obj_name,obj_content,strlen(obj_content),0); //往集群写入对象 if(err == 0) &#123; printf(&quot;rados_write success......\\n&quot;); &#125; char xattr[] = &quot;en_US&quot;; err = rados_setxattr(io,obj_name,&quot;lang&quot;,xattr,5); //给对象设置属性 if(err == 0) &#123; printf(&quot;Set object xattr success......\\n&quot;); &#125; rados_completion_t comp; err = rados_aio_create_completion(NULL,NULL,NULL,&amp;comp); //异步读 char read_ret[1024]; err = rados_aio_read(io,obj_name,comp,read_ret,sizeof(read_ret),0); rados_aio_wait_for_complete(comp); if( err == 0) &#123; printf(&quot;%s\\&#x27;s content is %s\\n&quot;,obj_name,read_ret); &#125;else&#123; printf(&quot;read_aio_read: err\\n&quot;); &#125; rados_aio_release(comp); err = rados_read(io,obj_name,read_ret,sizeof(read_ret),0); //同步读 if( err &gt; 0) &#123; printf(&quot;%s\\&#x27;s content is %s\\n&quot;,obj_name,read_ret); &#125;else&#123; printf(&quot;read_read: err\\n&quot;); &#125; char xattr_ret[100]; err = rados_getxattr(io,obj_name,&quot;lang&quot;,xattr_ret,6); //获取对象属性 if( err &gt; 0) &#123; printf(&quot;Read %s\\&#x27;s xattr \\&quot;lang\\&quot; is %s\\n&quot;,obj_name,xattr_ret); &#125;else&#123; printf(&quot;rados_getxattr: err\\n&quot;); &#125; err = rados_rmxattr(io,obj_name,&quot;lang&quot;); //删除对象属性 err = rados_remove(io,obj_name); //删除对象 rados_ioctx_destroy(io); //释放io上下文 rados_shutdown(cluster); //关闭集群句柄 return 0;&#125; C++创建Ceph IO上下文示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;rados/librados.hpp&gt;int main(int argc,const char* argv[])&#123; librados::Rados cluster; librados::IoCtx io_ctx; char cluster_name[] = &quot;ceph&quot;; char user_name[] = &quot;client.admin&quot;; char conf_flie[] = &quot;/home/watson/ceph/build/ceph.conf&quot;; char poolname[] = &quot;testpool&quot;; uint64_t flags; int ret; /* 为了使示例代码更可观性，不对返回值判错，实际应用中需要进行判错，请养成良好习惯！ */ ret = cluster.init2(user_name,cluster_name,flags); ret = cluster.conf_read_file(conf_flie); ret = cluster.conf_parse_argv(argc,argv); ret = cluster.connect(); if(ret &lt; 0 ) //测试集群连接情况 &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t connect to cluster! error&quot;&lt;&lt;ret&lt;&lt;std::endl; return 1; &#125;else&#123; std::cout&lt;&lt;&quot;Connected to the cluster.&quot;&lt;&lt;std::endl; &#125; int poolID = cluster.pool_lookup(poolname); //通过pool名检测是否存在pool if(poolID == -ENOENT) &#123; printf(&quot;this pool does not exist,and create the pool...... \\n&quot;); cluster.pool_create(poolname); &#125;else&#123; std::cout&lt;&lt;&quot;pool &quot;&lt;&lt;poolID&lt;&lt;&quot; is using......&quot;&lt;&lt;std::endl; &#125; ret = cluster.ioctx_create(poolname,io_ctx); //初始化io_ctx char obj_name[] = &quot;obj&quot;; librados::bufferlist bl; bl.append(&quot;Hello Librados!&quot;); ret = io_ctx.write_full(obj_name,bl); //往集群写入数据 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t write object! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;&quot;Write success......&quot;&lt;&lt;std::endl; &#125; librados::bufferlist lang_bl; lang_bl.append(&quot;en_US&quot;); ret = io_ctx.setxattr(obj_name,&quot;lang&quot;,lang_bl); //给对象设置属性 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t write object xattr! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;&quot;Set xattr success......&quot;&lt;&lt;std::endl; &#125; librados::bufferlist read_bl; //异步读 int read_len = 1024; librados::AioCompletion *read_completion = librados::Rados::aio_create_completion(); ret = io_ctx.aio_read(obj_name,read_completion,&amp;read_bl,read_len,0); if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read object! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125; read_completion-&gt;wait_for_complete(); //等待异步完成 ret = read_completion-&gt;get_return_value(); //获取返回值 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read object! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;read_bl.c_str()&lt;&lt;std::endl; &#125; librados::bufferlist lang_res; ret = io_ctx.getxattr(obj_name,&quot;lang&quot;,lang_res); //获取属性 if(ret &lt; 0) &#123; std::cerr&lt;&lt;&quot;Couldn&#x27;t read object xattr! error&quot;&lt;&lt;ret&lt;&lt;std::endl; exit(EXIT_FAILURE); &#125;else&#123; std::cout&lt;&lt;lang_res.c_str()&lt;&lt;std::endl; &#125; ret = io_ctx.rmxattr(obj_name,&quot;lang&quot;); //删除对象属性 ret = io_ctx.remove(obj_name); //删除对象 io_ctx.close(); //关闭io cluster.shutdown(); //关闭集群句柄 return 0;&#125; 第 4 步：结束会话一旦客户端应用程序完成了 I&#x2F;O 上下文和集群句柄，应用程序应该关闭连接并关闭句柄。对于异步 I&#x2F;O，应用程序还应确保挂起的异步操作已完成。C结束会话示例 12rados_ioctx_destroy(io); rados_shutdown(cluster); C++结束会话示例 12io_ctx.close(); cluster.shutdown(); 补充：查看pool下的object对象 –all 显示所有namespace的object 1rados ls -p pool --all LIBRADOS常用接口 集群配置：提供了获取和设置配置值的方法，读取Ceph配置文件，并解析参数。 Rados.conf_get(option) Rados.conf_set(option, val) Rados.conf_read_file(path) Rados.conf_parse_argv(args) Rados.version() 连接管理：连接到集群、检查集群、检索集群的统计数据，并从集群断开连接。也可以断言集群句柄处于一个特定的状态（例如，”配置”，”连接”等等）。 Rados.connect(timeout) Rados.shutdown() Rados.get_fsid() Rados.get_cluster_stats() 池操作：列出可用的池，创建一个池，检查一个池是否存在，并删除一个池。 Rados.list_pools() Rados.create_pool(pool_name, crush_rule, auid) Rados.pool_exists(pool_name) Rados.delete_pool(pool_name) CLI 命令：Ceph CLI命令在内部使用以下librados Python绑定方法。 Rados.mon_command(cmd, inbuf, timeout, target) Rados.osd_command(osdid, cmd, inbuf, timeout) Rados.mgr_command(cmd, inbuf, timeout, target) Rados.pg_command(pgid, cmd, inbuf, timeout) I&#x2F;O上下文：为了将数据写入Ceph对象存储和从Ceph对象存储读取数据，必须创建一个输入&#x2F;输出上下文（ioctx）。Rados类提供了open_ioctx()和open_ioctx2()方法。其余的操作涉及调用Ioctx和其他类的方法。 Rados.open_ioctx(ioctx_name) Ioctx.require_ioctx_open() Ioctx.get_stats() Ioctx.get_last_version() Ioctx.close() 对象操作：同步或异步地读和写对象。一个对象有一个名称（或键）和数据。 Ioctx.aio_write(object_name, to_write, offset, oncomplete, onsafe) Ioctx.aio_write_full(object_name, to_write, oncomplete, onsafe) Ioctx.aio_append(object_name, to_append, oncomplete, onsafe) Ioctx.write(key, data, offset) Ioctx.write_full(key, data) Ioctx.aio_flush() Ioctx.set_locator_key(loc_key) Ioctx.aio_read(object_name, length, offset, oncomplete) Ioctx.read(key, length, offset) Ioctx.stat(key) Ioctx.trunc(key, size) Ioctx.remove_object(key) 对象扩展属性：在一个对象上设置扩展属性(XATTRs)。 Ioctx.set_xattr(key, xattr_name, xattr_value) Ioctx.get_xattrs(oid) XattrIterator.next() Ioctx.get_xattr(key, xattr_name) Ioctx.rm_xattr(key, xattr_name) 对象接口：从一个池中检索一个对象的列表，并对它们进行迭代。提供的对象接口使每个对象看起来像一个文件，可以对对象进行同步操作。对于异步操作，应该使用I&#x2F;O上下文的方法。 Ioctx.list_objects() ObjectIterator.next() Object.read(length&#x3D;1024 * 1024) Object.write(string_to_write) Object.get_xattrs() Object.get_xattr(xattr_name) Object.set_xattr(xattr_name, xattr_value) Object.rm_xattr(xattr_name) Object.stat() Object.remove()","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph_librados介绍","slug":"Storage/Ceph/Ceph_librados介绍","date":"2021-06-05T06:15:14.000Z","updated":"2024-07-27T14:37:27.947Z","comments":true,"path":"Storage/Ceph/Ceph_librados介绍/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph_librados%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"Ceph Librados介绍Ceph Librados 概述一个Ceph客户端，通过librados直接与OSD交互，来存储和取出数据。为了与OSD交互，客户端应用必须直接调用librados，连接一个Ceph Monitor。一旦连接好以后，librados会从Monitor处取回一个Cluster map。当客户端的应用想读或者取数据的时候，它要创建一个I&#x2F;O上下文并且与一个pool绑定。通过这个I&#x2F;O上下文，客户端将Object的名字提供给librados，然后librados会根据Object的名字和Cluster map计算出相应的PG和OSD的位置。然后客户端就可以读或者写数据。客户端的应用无需知道这个集群的拓扑结构。 Ceph客户端主要是实现了接口，对外提供了访问的功能。上层可以通过接口访问Ceph存储。Ceph的客户端通过一套名为librados的接口进行集群的访问，这里的访问包括对集群的整体访问和对象的访问两类接口。这套接口（API）包括C、C++和Python常见语言的实现，接口通过网络实现对Ceph集群的访问。在用户层面，可以在自己的程序中调用该接口，从而集成Ceph集群的存储功能，或者在监控程序中实现对Ceph集群状态的监控。所谓集群的整体访问包括连接集群、创建存储池、删除存储池和获取集群状态等等。所谓对象访问是之对存储池中对象的访问，包括创建删除对象、向对象写数据或者追加数据和读对象数据等接口。 客户端基本架构概述librados客户端基本架构如下图所示，主要包括4层，分别是API层、IO处理层、对象处理层和消息收发层。 API层是一个抽象层，为上层提供统一的接口。API层提供的原生接口包括C和C++两种语言的实现外，还有Python的实现。 IO处理层用于实现IO的简单封装，其通过一个名为ObjectOperation类实现，该类主要包括的是读写操作的数据信息。之后在IO处理层在IoCtxImpl::operate函数中将ObjectOperation转换为Objecter::Op类的对象，并将该对象提交到对象处理层进行进一步的处理。 对象处理层包括了Ceph对象处理所需要的信息，包括通信管道、OSDMap和MonMap等内容。因此，在这里，根据对象的信息可以计算出对象存储的具体位置，最终找到客户端与OSD的连接信息（Session）。 消息收发层的接口会被对象处理层调用，此时消息会传递到本层，并且通过本层的线程池发送到具体的OSD。这里需要注意的是，消息收发层与服务端的消息收发公用Messager的代码。 核心流程图先根据配置文件调用librados创建Rados，接下来为这个Rados创建一个RadosClient，RadosClient包含3个主要模块(finisher、Messenger、Objecter)。再根据pool创建对应的ioctx，在ioctx中能够找到RadosClient。再调用OSDC生成对应的OSD请求，与OSD进行通信响应请求。这从大体上叙述了librados与osdc在整个Ceph中的作用。 具体细节可以按照该流程读对应源代码理解。在这个流程中需要注意的是_op_submit函数会调用_calc_target和_get_session两个函数，两个函数的作用分别是获取目的OSD和对应的Session（连接），这个是后面发送数据的基础。 Librados与OSDC的关系Librados与OSDC位于ceph客户端中比较底层的位置。 Librados模块是RADOS对象存储系统访问的接口，它提供了pool的创建、删除、对象的创建、删除、读写等基本操作接口。类RadosClient是librados模块的核心管理类，处理整个RADOS系统层面以及pool层面的管理。类ioctxlmpl实现单个pool层的对象读写等操作。 OSDC模块实现了请求的封装和通过网络模块发送请求的逻辑，其核心类Object完成对象的地址计算、消息的发送和处理超时等工作。librados模块包含两个部分，分别是RadosClient 模块和IoctxImpl。RadosClient处于最上层，是librados的核心管理类，管理着整个RADOS系统层面以及pool层面的管理。 Librados模块类RadosClientRadosClient处于最上层，是librados的核心管理类，管理着整个RADOS系统层面以及pool层面的管理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103class librados::RadosClient : public Dispatcher //继承Dispatcher(消息分发类)&#123; //unique_ptr智能指针 std::unique_ptr&lt;CephContext,std::function&lt;void(CephContext*)&gt; &gt; cct_deleter;public: using Dispatcher::cct; const ConfigProxy&amp; conf; //配置文件private: enum &#123; DISCONNECTED, CONNECTING, CONNECTED, &#125; state; //Monitor的网络连接状态 MonClient monclient; //Monitor客户端 MgrClient mgrclient; //MGR客户端 Messenger *messenger; //网络消息接口 uint64_t instance_id; //rados客户端实例的ID //相关消息分发 Dispatcher类的函数重写 bool _dispatch(Message *m); bool ms_dispatch(Message *m) override; bool ms_get_authorizer(int dest_type, AuthAuthorizer **authorizer) override; void ms_handle_connect(Connection *con) override; bool ms_handle_reset(Connection *con) override; void ms_handle_remote_reset(Connection *con) override; bool ms_handle_refused(Connection *con) override; Objecter *objecter; //OSDC模块中用于发送封装好的OP消息 Mutex lock; Cond cond; SafeTimer timer; //定时器 int refcnt; //引用计算 version_t log_last_version; rados_log_callback_t log_cb; rados_log_callback2_t log_cb2; void *log_cb_arg; string log_watch; bool service_daemon = false; string daemon_name, service_name; map&lt;string,string&gt; daemon_metadata; int wait_for_osdmap(); Finisher finisher; //用于执行回调函数的finisher类 explicit RadosClient(CephContext *cct_); ~RadosClient() override; int ping_monitor(string mon_id, string *result); int connect(); //RadosClient的初始化函数、 连接 void shutdown(); int watch_flush(); int async_watch_flush(AioCompletionImpl *c); uint64_t get_instance_id(); int get_min_compatible_osd(int8_t* require_osd_release); int get_min_compatible_client(int8_t* min_compat_client,int8_t* require_min_compat_client); int wait_for_latest_osdmap(); //创建一个pool相关的上下文信息IoCtxImpl对象（根据pool名字或Id创建ioctx） int create_ioctx(const char *name, IoCtxImpl **io); int create_ioctx(int64_t, IoCtxImpl **io); int get_fsid(std::string *s); //用于查找pool int64_t lookup_pool(const char *name); bool pool_requires_alignment(int64_t pool_id); int pool_requires_alignment2(int64_t pool_id, bool *requires); uint64_t pool_required_alignment(int64_t pool_id); int pool_required_alignment2(int64_t pool_id, uint64_t *alignment); int pool_get_name(uint64_t pool_id, std::string *name, bool wait_latest_map = false); //用于列出所有的pool int pool_list(std::list&lt;std::pair&lt;int64_t, string&gt; &gt;&amp; ls); //用于获取pool的统计信息 int get_pool_stats(std::list&lt;string&gt;&amp; ls, map&lt;string,::pool_stat_t&gt; *result,bool *per_pool); //用于获取系统的统计信息 int get_fs_stats(ceph_statfs&amp; result); bool get_pool_is_selfmanaged_snaps_mode(const std::string&amp; pool); //pool的同步创建 int pool_create(string&amp; name, int16_t crush_rule=-1); //pool的异步创建 int pool_create_async(string&amp; name, PoolAsyncCompletionImpl *c,int16_t crush_rule=-1); int pool_get_base_tier(int64_t pool_id, int64_t* base_tier); //同步删除pool int pool_delete(const char *name); //异步删除pool int pool_delete_async(const char *name, PoolAsyncCompletionImpl *c); int blacklist_add(const string&amp; client_address, uint32_t expire_seconds); //处理Mon相关命令,调用monclient.start_mon_command 把命令发送给Mon处理 int mon_command(const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl,bufferlist *outbl, string *outs); void mon_command_async(………); int mon_command(int rank,const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl,bufferlist *outbl, string *outs); int mon_command(string name,const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl,bufferlist *outbl, string *outs); int mgr_command(const vector&lt;string&gt;&amp; cmd, const bufferlist &amp;inbl, bufferlist *outbl, string *outs); //处理OSD相关命令 int osd_command(int osd, vector&lt;string&gt;&amp; cmd, const bufferlist&amp; inbl,bufferlist *poutbl, string *prs); //处理PG相关命令 int pg_command(pg_t pgid, vector&lt;string&gt;&amp; cmd, const bufferlist&amp; inbl,bufferlist *poutbl, string *prs); void handle_log(MLog *m); int monitor_log(const string&amp; level, rados_log_callback_t cb,rados_log_callback2_t cb2, void *arg); void get(); bool put(); void blacklist_self(bool set); std::string get_addrs() const; int service_daemon_register( const std::string&amp; service, ///&lt; service name (e.g., &#x27;rgw&#x27;) const std::string&amp; name, ///&lt; daemon name (e.g., &#x27;gwfoo&#x27;) const std::map&lt;std::string,std::string&gt;&amp; metadata); ///&lt; static metadata about daemon int service_daemon_update_status(std::map&lt;std::string,std::string&gt;&amp;&amp; status); mon_feature_t get_required_monitor_features() const; int get_inconsistent_pgs(int64_t pool_id, std::vector&lt;std::string&gt;* pgs);&#125;; 类IoctxImpl类IoctxImpl是对于其中的某一个pool进行管理，如对 对象的读写等操作的控制。该类是pool的上下文信息，一个pool对应一个IoctxImpl对象。librados中所有关于io操作的API都设计在librados::IoCtx中，接口的真正实现在ioCtxImpl中，它的处理过程如下： 把请求封装成ObjectOperation类(osdc类中) 把相关的pool信息添加到里面，封装成Object::Op对象 调用响应的函数object-&gt;op_submit发送给相应的OSD 操作完成后，调用相应的回调函数。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138struct librados::IoCtxImpl &#123; std::atomic&lt;uint64_t&gt; ref_cnt = &#123; 0 &#125;; RadosClient *client; int64_t poolid; snapid_t snap_seq; ::SnapContext snapc; uint64_t assert_ver; version_t last_objver; uint32_t notify_timeout; object_locator_t oloc; Mutex aio_write_list_lock; ceph_tid_t aio_write_seq; Cond aio_write_cond; xlist&lt;AioCompletionImpl*&gt; aio_write_list; map&lt;ceph_tid_t, std::list&lt;AioCompletionImpl*&gt; &gt; aio_write_waiters; Objecter *objecter; IoCtxImpl(); IoCtxImpl(RadosClient *c, Objecter *objecter,int64_t poolid, snapid_t s); void dup(const IoCtxImpl&amp; rhs); void set_snap_read(snapid_t s); int set_snap_write_context(snapid_t seq, vector&lt;snapid_t&gt;&amp; snaps); void get(); void put(); void queue_aio_write(struct AioCompletionImpl *c); void complete_aio_write(struct AioCompletionImpl *c); void flush_aio_writes_async(AioCompletionImpl *c); void flush_aio_writes(); int64_t get_id(); string get_cached_pool_name(); int get_object_hash_position(const std::string&amp; oid, uint32_t *hash_position); int get_object_pg_hash_position(const std::string&amp; oid, uint32_t *pg_hash_position); ::ObjectOperation *prepare_assert_ops(::ObjectOperation *op); // snaps int snap_list(vector&lt;uint64_t&gt; *snaps); int snap_lookup(const char *name, uint64_t *snapid); int snap_get_name(uint64_t snapid, std::string *s); int snap_get_stamp(uint64_t snapid, time_t *t); int snap_create(const char* snapname); int selfmanaged_snap_create(uint64_t *snapid); void aio_selfmanaged_snap_create(uint64_t *snapid, AioCompletionImpl *c); int snap_remove(const char* snapname); int rollback(const object_t&amp; oid, const char *snapName); int selfmanaged_snap_remove(uint64_t snapid); void aio_selfmanaged_snap_remove(uint64_t snapid, AioCompletionImpl *c); int selfmanaged_snap_rollback_object(const object_t&amp; oid,::SnapContext&amp; snapc, uint64_t snapid); // io int nlist(Objecter::NListContext *context, int max_entries); uint32_t nlist_seek(Objecter::NListContext *context, uint32_t pos); uint32_t nlist_seek(Objecter::NListContext *context, const rados_object_list_cursor&amp; cursor); rados_object_list_cursor nlist_get_cursor(Objecter::NListContext *context); void object_list_slice(……); int create(const object_t&amp; oid, bool exclusive); int write(const object_t&amp; oid, bufferlist&amp; bl, size_t len, uint64_t off); int append(const object_t&amp; oid, bufferlist&amp; bl, size_t len); int write_full(const object_t&amp; oid, bufferlist&amp; bl); int writesame(const object_t&amp; oid, bufferlist&amp; bl,size_t write_len, uint64_t offset); int read(const object_t&amp; oid, bufferlist&amp; bl, size_t len, uint64_t off); int mapext(const object_t&amp; oid, uint64_t off, size_t len,std::map&lt;uint64_t,uint64_t&gt;&amp; m); int sparse_read(const object_t&amp; oid, std::map&lt;uint64_t,uint64_t&gt;&amp; m,bufferlist&amp; bl, size_t len, uint64_t off); int checksum(……); int remove(const object_t&amp; oid); int remove(const object_t&amp; oid, int flags); int stat(const object_t&amp; oid, uint64_t *psize, time_t *pmtime); int stat2(const object_t&amp; oid, uint64_t *psize, struct timespec *pts); int trunc(const object_t&amp; oid, uint64_t size); int cmpext(const object_t&amp; oid, uint64_t off, bufferlist&amp; cmp_bl); int tmap_update(const object_t&amp; oid, bufferlist&amp; cmdbl); int exec(const object_t&amp; oid, const char *cls, const char *method, bufferlist&amp; inbl, bufferlist&amp; outbl); int getxattr(const object_t&amp; oid, const char *name, bufferlist&amp; bl); int setxattr(const object_t&amp; oid, const char *name, bufferlist&amp; bl); int getxattrs(const object_t&amp; oid, map&lt;string, bufferlist&gt;&amp; attrset); int rmxattr(const object_t&amp; oid, const char *name); int operate(const object_t&amp; oid, ::ObjectOperation *o, ceph::real_time *pmtime, int flags=0); int operate_read(const object_t&amp; oid, ::ObjectOperation *o, bufferlist *pbl, int flags=0); int aio_operate(……); int aio_operate_read(…………); struct C_aio_stat_Ack : public Context &#123;…… &#125;; struct C_aio_stat2_Ack : public Context &#123;…… &#125;; struct C_aio_Complete : public Context &#123;…… &#125;; int aio_read(……); int aio_read(………); int aio_sparse_read(………); int aio_cmpext(const object_t&amp; oid, AioCompletionImpl *c, uint64_t off,bufferlist&amp; cmp_bl); int aio_cmpext(………); int aio_write(………); int aio_append(const object_t &amp;oid, AioCompletionImpl *c,const bufferlist&amp; bl, size_t len); int aio_write_full(const object_t &amp;oid, AioCompletionImpl *c,const bufferlist&amp; bl); int aio_writesame(const object_t &amp;oid, AioCompletionImpl *c,const bufferlist&amp; bl, size_t write_len, uint64_t off); int aio_remove(const object_t &amp;oid, AioCompletionImpl *c, int flags=0); int aio_exec(………); int aio_exec(………); int aio_stat(const object_t&amp; oid, AioCompletionImpl *c, uint64_t *psize, time_t *pmtime); int aio_stat2(const object_t&amp; oid, AioCompletionImpl *c, uint64_t *psize, struct timespec *pts); int aio_getxattr(const object_t&amp; oid, AioCompletionImpl *c,const char *name, bufferlist&amp; bl); int aio_setxattr(const object_t&amp; oid, AioCompletionImpl *c, const char *name, bufferlist&amp; bl); int aio_getxattrs(const object_t&amp; oid, AioCompletionImpl *c,map&lt;string, bufferlist&gt;&amp; attrset); int aio_rmxattr(const object_t&amp; oid, AioCompletionImpl *c,const char *name); int aio_cancel(AioCompletionImpl *c); int hit_set_list(uint32_t hash, AioCompletionImpl *c,std::list&lt; std::pair&lt;time_t, time_t&gt; &gt; *pls); int hit_set_get(uint32_t hash, AioCompletionImpl *c, time_t stamp,bufferlist *pbl); int get_inconsistent_objects(………); int get_inconsistent_snapsets(………); void set_sync_op_version(version_t ver); int watch(………); int watch(………); int aio_watch(……); int aio_watch(……); int watch_check(uint64_t cookie); int unwatch(uint64_t cookie); int aio_unwatch(uint64_t cookie, AioCompletionImpl *c); int notify(……); int notify_ack(const object_t&amp; oid, uint64_t notify_id, uint64_t cookie,bufferlist&amp; bl); int aio_notify(………); int set_alloc_hint(……); version_t last_version(); void set_assert_version(uint64_t ver); void set_notify_timeout(uint32_t timeout); int cache_pin(const object_t&amp; oid); int cache_unpin(const object_t&amp; oid); int application_enable(const std::string&amp; app_name, bool force); void application_enable_async(const std::string&amp; app_name, bool force,PoolAsyncCompletionImpl *c); int application_list(std::set&lt;std::string&gt; *app_names); int application_metadata_get(const std::string&amp; app_name,const std::string &amp;key,std::string* value); int application_metadata_set(const std::string&amp; app_name,const std::string &amp;key,const std::string&amp; value); int application_metadata_remove(const std::string&amp; app_name,const std::string &amp;key); int application_metadata_list(const std::string&amp; app_name, std::map&lt;std::string, std::string&gt; *values);&#125;; librados主要接口 集群句柄创建librados::Rados对象是用来操纵ceph集群的句柄，使用init来创建RadosClient，之后读取指定的ceph配置文件，获取monitor的ip和端口号。RadosClient里面有与monitor通信的MonClient和用于与OSD通信的Messenger。 集群连接初始化集群句柄之后，就可以使用这个句柄来连接集群了RadosClient::connect完成了连接操作： a. 调用monclient.build_inital_monmap，从配置文件种检查是否有初始化的monitor的地址信息 b. 创建网络通信模块messenger，并设置相关的Policy信息 c. 创建Objecter对象并初始化 d. 调用monclient.init()函数初始化monclient e. Timer定时器初始化，Finisher对象初始化 IO上下文环境初始化使用句柄创建好存储池后，还需要创建与存储池相关的IO上下文句柄rados.ioctx_create(pool_name, io_ctx) 对象读写创建对象并写入数据：io_ctx.create_full(object_name,bl)读取对象中的数据到bufferlist中，对象读取有同步读取和异步读取两种接口：io_ctx.read和io_ctx.aio_read a. 同步读取：io_ctx.read(object_name,read_bl,read_len,0) b. 异步读取：需要指定完成读取数据后的回调，用于检查读取是否完成 librados::AioCompletion *read_completion &#x3D; librados::Rados::aio_create_completion(); io_ctx.aio_read(object_name,read_completion,&amp;read_buff,read_len,0) read_completion-&gt;wait_for_complete() 同时还要获取返回值，得到读取对象的字节数 IO上下文关闭io_ctx.close() 集群句柄关闭rados.shutdown()上述功能通过Rados和IoCtx两个类实现，两个类的主要函数如下图所示（这里仅是示例，实际接口数量要多很多，具体参考源代码）。 Ceph官方的示例代码为了了解如何使用这些API，这里给出一些代码片段。具体完整的代码大家可以参考Ceph官方的示例代码。 12345678910111213141516171819202122librados::IoCtx io_ctx;const char *pool_name = &quot;test&quot;;cluster.ioctx_create(pool_name, io_ctx); /* 创建进行IO处理的上下文，其实就是用于访问Ceph的对象 *//* 同步写对象 */librados::bufferlist bl;bl.append(&quot;Hello World!&quot;); /* 对象的内容 */ret = io_ctx.write_full(&quot;itworld123&quot;, bl); /*写入对象itworld123*//* 向对象添加属性，这里的属性与文件系统中文件的扩展属性类似。 */librados::bufferlist attr_bl;attr_bl.append(&quot;en_US&quot;);io_ctx.setxattr(&quot;itworld123&quot;, &quot;test_attr&quot;, attr_bl);/* 异步读取对象内容 */librados::bufferlist read_buf;int read_len = 1024;librados::AioCompletion *read_completion = librados::Rados::aio_create_completion(); /* 创建一个异步完成类对象 */io_ctx.aio_read(&quot;itworld123&quot;, read_completion, &amp;read_buf, read_len, 0); /* 发送读请求 */read_completion-&gt;wait_for_complete(); /* 等待请求完成 */read_completion-&gt;get_return_value(); librados::bufferlist attr_res;io_ctx.getxattr(&quot;itworld123&quot;, &quot;test_attr&quot;, attr_res); /* 读取对象属性 */io_ctx.rmxattr(&quot;itworld123&quot;, &quot;test_attr&quot;); /* 删除对象的属性 */io_ctx.remove(&quot;itworld123&quot;); /* 删除对象 */","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph数据读写过程","slug":"Storage/Ceph/Ceph数据读写过程","date":"2021-05-19T09:06:37.000Z","updated":"2024-07-27T14:31:10.525Z","comments":true,"path":"Storage/Ceph/Ceph数据读写过程/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/","excerpt":"","text":"Ceph数据映射过程在一个大规模分布式存储系统中，需要解决两个核心问题：“我应该把数据写到哪里？”和“我之前把数据存储在了哪里？”。这就引出了数据寻址的问题。Ceph 的寻址流程可以如下描述。 File：此处的File就是用户需要存储或访问的文件。对于一个基于Ceph开发的对象存储应用而言，这个File也就对应于应用中的“对象” ，也就是用户直接操作的“对象” Object： 在 RADOS（Reliable Autonomic Distributed Object Store）中，”对象” 是指系统中存储的基本单位。与 File 的不同之处在于，Object 的最大尺寸受到 RADOS 的限制，通常为 2MB 或 4MB。这一限制是为了优化底层存储的管理和组织。因此，当上层应用向 RADOS 存储一个较大的 File 时，需要将其拆分成多个统一大小的 Object（最后一个 Object 的大小可能不同）进行存储。 PG（Placement Group）： 顾名思义，PG 用于组织对象的存储并映射其位置。具体来说，一个 PG 负责管理多个对象，而每个对象只能映射到一个 PG 中，即 PG 和对象之间是“一对多”的映射关系。同时，一个 PG 会被映射到多个 OSD（Object Storage Device）上，通常 n 至少为 2，而在生产环境中，n 通常至少为 3。每个 OSD 上会承载大量的 PG，可能达到数百个。PG 的数量设置直接影响数据的分布均匀性，因此在实际配置中需要谨慎考虑。 OSD（Object Storage Device）： OSD 是 Ceph 中用于存储数据的对象存储设备。OSD 的数量对系统的数据分布均匀性有直接影响，因此不宜过少。为了充分发挥 Ceph 系统的优势，通常需要配置至少数百个 OSD。 File → Object映射 这个映射过程的目的是将用户操作的 File 转换为 RADOS 能够处理的 Object。这个过程相对简单，本质上就是按照 Object 的最大尺寸对 File 进行切分，类似于磁盘阵列中的条带化（striping）过程。这种切分有两个主要好处： 将大小不定的 File 转换为具有一致最大尺寸的 Object，使得 RADOS 能够更高效地管理这些数据。 将对单一 File 的串行处理转变为对多个 Object 的并行处理，从而提高处理效率。 每一个切分后的 Object 将获得一个唯一的 Object ID (oid)，其生成方式非常简单，是一种线性映射。具体来说，ino 表示待操作 File 的元数据，可以简单理解为该 File 的唯一 ID；ono 则是由该 File 切分产生的某个 Object 的序号。而 oid 就是将这个序号简单地附加在该 File 的 ID 之后得到的。举个例子，如果一个 ID 为 filename 的 File 被切分成了 3 个 Object，那么其 Object 的序号依次为 0、1 和 2，最终得到的 oid 就依次为 filename0、filename1 和 filename2。 这里有一个隐含的问题，即 ino 的唯一性必须得到保证，否则后续的映射将无法正确进行。 Object → PG 映射 当一个 File 被映射为一个或多个 Object 后，需要将每个 Object 独立地映射到一个 PG 中。这个过程相对简单，具体计算过程如下： 1Hash(oid) &amp; mask -&gt; pgid 这个计算过程分为两步： 使用 Ceph 系统指定的静态哈希算法计算 oid 的哈希值，将 oid 转换为一个近似均匀分布的伪随机值。 将这个伪随机值与 mask 进行按位与操作，得到最终的 PG 序号 (pgid)。 根据 RADOS 的设计，PG 的总数为 m（m 应该为 2 的整数幂），则 mask 的值为 m - 1。哈希值计算和按位与操作的结果就是从所有 m 个 PG 中近似均匀地随机选择一个。这种机制保证了在大量 Object 和大量 PG 存在的情况下，Object 和 PG 之间的映射近似均匀。由于 Object 是由 File 切分而来，大部分 Object 的尺寸相同，因此这一映射最终保证了各个 PG 中存储的 Object 的总数据量的近似均匀性。 这里强调“大量”是因为，只有在 Object 和 PG 数量较多时，这种伪随机关系的近似均匀性才有效，Ceph 的数据存储均匀性才能得到保障。为了确保这一点，一方面，Object 的最大尺寸应该被合理配置，以使得相同数量的 File 能被切分成更多的 Object；另一方面，Ceph 建议 PG 的总数应为 OSD 总数的数百倍，以确保有足够数量的 PG 供映射使用。 PG → OSD 映射 第三次映射是将作为对象逻辑组织单元的 PG 映射到实际存储单元 OSD 上。RADOS 使用了一种称为 CRUSH（Controlled Replication Under Scalable Hashing）的算法，将 pgid 代入其中，然后得到一组包含 n 个 OSD。这 n 个 OSD 共同负责存储和维护一个 PG 中的所有对象。通常，n 的值根据实际应用中的可靠性需求而定，在生产环境下通常为 3。具体到每个 OSD，由其上运行的 OSD Daemon 负责执行映射到本地的对象在本地文件系统中的存储、访问、元数据维护等操作。 与对象到 PG 的映射中采用的哈希算法不同，CRUSH 算法的结果并非绝对不变，而会受到其他因素的影响，主要有两个： 当前系统状态：即集群运行图。当系统中的 OSD 状态或数量发生变化时，集群运行图可能会改变，这将影响 PG 与 OSD 之间的映射关系。 存储策略配置：这与数据的安全性相关。系统管理员可以通过策略配置指定承载同一个 PG 的 3 个 OSD 分别位于数据中心的不同服务器或机架上，从而提高存储的可靠性。 因此，只有在系统状态和存储策略都不发生变化时，PG 和 OSD 之间的映射关系才是固定的。在实际使用中，策略配置通常一经设定就不会改变。而系统状态的变化可能是由于设备损坏或存储集群规模的扩大。好在 Ceph 提供了对这些变化的自动化支持，因此，即便 PG 与 OSD 之间的映射关系发生变化，也不会对应用产生影响。实际上，Ceph 利用 CRUSH 算法的动态特性，可以根据需要将一个 PG 动态迁移到不同的 OSD 组合上，从而自动实现高可靠性和数据分布再平衡等特性。 选择 CRUSH 算法而非其他哈希算法的原因有两点： 可配置性：CRUSH 算法具有可配置特性，可以根据管理员的配置参数决定 OSD 的物理位置映射策略。 稳定性：CRUSH 算法具有特殊的“稳定性”，即当系统中加入新的 OSD 导致系统规模增大时，大部分 PG 与 OSD 之间的映射关系不会改变，只有少部分 PG 的映射关系会发生变化并引发数据迁移。这种特性使得系统在扩展时能够保持相对稳定，避免了普通哈希算法可能带来的大规模数据迁移问题。 至此为止，Ceph通过3次映射，完成了从File到Object、Object到PG、PG再到OSD的整个映射过程。从整个过程可以看到，这里没有任何的全局性查表操作需求。至于唯一的全局性数据结构：集群运行图。它的维护和操作都是轻量级的，不会对系统的可扩展性、性能等因素造成影响。 接下来的一个问题是:为什么需要引人PG并在Object与OSD之间增加一层映射呢？可以想象一下，如果没有 PG 这一层的映射，会是什么情况？在这种情况下，需要采用某种算法将 Object 直接映射到一组 OSD 上。如果这种算法是某种固定映射的哈希算法，这就意味着一个 Object 将被固定映射在一组 OSD 上。当其中一个或多个 OSD 损坏时，Object 无法自动迁移到其他 OSD 上（因为映射函数不允许），而当系统为了扩容新增 OSD 时，Object 也无法被再平衡到新的 OSD 上（同样因为映射函数不允许）。这些限制违背了 Ceph 系统高可靠性和高自动化的设计初衷。 即便使用一个动态算法（如 CRUSH 算法）来完成这一映射，似乎可以避免静态映射带来的问题。但这样会导致各个 OSD 处理的本地元数据量大幅增加，计算复杂度和维护工作量也会大幅上升。 例如，在 Ceph 的现有机制中，一个 OSD 通常需要与其他承载同一个 PG 的 OSD 交换信息，以确定各自是否工作正常或是否需要进行维护。由于每个 OSD 承载约数百个 PG，而每个 PG 通常有 3 个 OSD，因此，在一定时间内，一个 OSD 大约需要进行数百次至数千次的信息交换。 然而，如果没有 PG 存在，一个 OSD 需要与其他承载同一个 Object 的 OSD 交换信息。由于每个 OSD 可能承载高达数百万个 Object，在同样时间内，一个 OSD 大约需要进行数百万次甚至数千万次的信息交换。这种状态维护成本显然过高。 综上所述，引入 PG 有至少两方面的好处：一方面，实现了 Object 和 OSD 之间的动态映射，为 Ceph 的可靠性和自动化等特性的实现提供了可能；另一方面，有效简化了数据的存储组织，大大降低了系统的维护和管理成本。 Ceph数据读写过程Ceph的读&#x2F;写操作采用Primary-Replica模型，客户端只向Object所对应OSD set的Primary OSD发起读&#x2F;写请求，这保证了数据的强一致性。当Primary OSD收到Object的写请求时，它负责把数据发送给其他副本，只有这个数据被保存在所有的OSD上时，Primary OSD才应答Object的写请求，这保证了副本的一致性。 写入数据这里以Object写入为例，假定一个PG被映射到3个OSD上。Object写入流程如图所示。 当某个客户端需要向Ceph集群写入一个File时，首先需要在本地完成前面所述的寻址流程，将File变为一个Object，然后找出存储该Object的一组共3个OSD，这3个OSD具有各自不同的序号，序号最靠前的那个OSD就是这一组中的Primary OSD，而后两个则依次Secondary OSD和Tertiary OSD。找出3个OSD后，客户端将直接和Primary OSD进行通信，发起写入操作(步骤1)。 Primary OSD收到请求后，分别向Secondary OSD和Tertiary OSD发起写人操作(步骤2和步骤3)。当Secondary OSD和Tertiary OSD各自完成写入操作后，将分别向Primary OSD发送确认信息(步骤4和步骤5)。当Primary OSD确认其他两个OSD的写入完成后，则自己也完成数据写入，并向客户端确认Object写入操作完成(步骤6)。之所以采用这样的写入流程，本质上是为了保证写入过程中的可靠性，尽可能避免出现数据丢失的情况。同时，由于客户端只需要向Primary OSD发送数据，因此在互联网使用场景下的外网带宽和整体访问延迟又得到了一定程度的优化。当然，这种可靠性机制必然导致较长的延迟，特别是，如果等到所有的OSD都将数据写入磁盘后再向客户端发送确认信号，则整体延迟可能难以忍受。因此， Ceph可以分两次向客户端进行确认。当各个OSD都将数据写入内存缓冲区后，就先向客户端发送一次确认，此时客户端即可以向下执行。待各个OSD都将数据写入磁盘后，会向客户端发送一个最终确认信号，此时客户端可以根据需要删除本地数据。分析上述流程可以看出，在正常情况下，客户端可以独立完成OSD寻址操作，而不必依赖于其他系统模块。因此，大量的客户端可以同时和大量的OSD进行并行操作。同时，如果一个File被切分成多个Object，这多个Object也可被并行发送至多个OSD上。从OSD的角度来看，由于同一个OSD在不同的PG中的角色不同，因此，其工作压力也可以被尽可能均匀地分担，从而避免单个OSD变成性能瓶颈。 读取数据如果需要读取数据，客户端只需完成同样的寻址过程，并直接和Primary OSD联系。在目前的Ceph设计中，被读取的数据默认由Primary OSD提供，但也可以设置允许从其他OSD中获取，以分散读取压力从而提高性能。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph相关配置","slug":"Storage/Ceph/Ceph相关配置","date":"2021-05-12T05:36:31.000Z","updated":"2024-08-18T12:38:42.554Z","comments":true,"path":"Storage/Ceph/Ceph相关配置/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/","excerpt":"","text":"当 Ceph 服务启动时，初始化过程会激活一组在后台运行的守护进程。Ceph 存储集群的运行时间 Ceph相关守护进程： Ceph监视器 (ceph-mon) Ceph管理器 (ceph-mgr) Ceph OSD守护进程 (ceph-osd) Ceph元数据服务器 （ceph-mds） Ceph RADOS网关守护进程 (radosgw) 每个守护进程都有多个配置选项，每个选项都有一个默认值。通过更改这些配置选项可以调整系统的行为。在覆盖默认值之前，请确保了解其后果，因为不当配置可能会显著降低集群的性能和稳定性。默认值有时会在不同版本之间发生变化。因此，最好查看适用于您 Ceph 版本的文档版本。 每个 Ceph 守护进程、进程和库从以下一个或多个来源获取其配置。列表中后出现的来源会覆盖前面出现的来源（当两者都存在时）。 编译时默认值 监视器集群的集中配置数据库 存储在本地主机上的配置文件 环境变量 命令行参数 管理员设置的运行时覆盖Ceph 进程启动时的第一件事之一是解析通过命令行、环境和本地配置文件提供的配置选项。接下来，进程会联系MON集群以获取集中存储的整个集群配置。在获得完整的配置视图后，守护进程或进程的启动将开始。 网络和通信配置参考 mon_host：每个MON守护程序都配置为绑定到特定的IP地址。这些地址通常由部署工具配置。其他组件在Ceph集群中，通过配置发现MON，通常在文件的部分中指定mon_host。 public_network_interface：指定用于绑定到public_network的网络接口名称；必须同时指定public_network。 public_network：公共网络的IP地址和子网掩码（例如，192.168.0.0&#x2F;24）。在[global]中设置。可以指定用逗号分隔的子网列表，格式为 &#123;ip地址&#125;/&#123;子网掩码&#125; [, &#123;ip地址&#125;/&#123;子网掩码&#125;]。 public_addr：每个守护进程在公共网络中的IP地址。 cluster_network_interface：指定用于绑定到cluster_network的网络接口名称；必须同时指定cluster_network。 cluster_network：集群网络的IP地址和子网掩码（例如，10.0.0.0&#x2F;24）。在[global]中设置。可以指定用逗号分隔的子网列表，格式为 &#123;ip地址&#125;/&#123;子网掩码&#125; [, &#123;ip地址&#125;/&#123;子网掩码&#125;]。 cluster_addr：每个守护进程在集群网络中的IP地址。 ms_bind_port_min：OSD或MDS守护进程绑定的最小端口号。 ms_bind_port_max：OSD或MDS守护进程绑定的最大端口号。 ms_bind_ipv4：启用Ceph守护进程绑定到IPv4地址。 ms_bind_ipv6：启用Ceph守护进程绑定到IPv6地址。 public_bind_addr：在某些动态部署中，Ceph MON守护进程可能会在本地绑定到一个与在网络中广告给其他节点的public_addr不同的IP地址。如果设置了public_bind_addr，Ceph监视器守护进程将本地绑定到此地址，并在monmaps中使用public_addr来向其他节点广告其地址。此行为仅限于监视器守护进程。 ms_tcp_nodelay：Ceph启用了ms_tcp_nodelay，以便每个请求立即发送（无缓冲）。禁用Nagle算法会增加网络流量，可能引入延迟。如果遇到大量的小数据包，您可以尝试禁用ms_tcp_nodelay。 ms_tcp_rcvbuf：网络连接接收端的套接字缓冲区大小。默认情况下禁用。 ms_type：Async Messenger使用的传输类型。可以是async+posix、async+dpdk或async+rdma。Posix使用标准的TCP&#x2F;IP网络，是默认选项。其他传输类型可能是实验性的，支持可能有限。 ms_async_op_threads：每个Async Messenger实例使用的初始工作线程数。应至少等于最高副本数，但如果CPU核心数量较少和&#x2F;或在单台服务器上托管了大量OSD，可以减少此值。 ms_initial_backoff：在发生故障后重新连接之前的初始等待时间。 ms_max_backoff：在发生故障后重新连接之前的最长等待时间。 ms_dispatch_throttle_bytes：限制等待调度的消息总大小。 ms_cluster_mode：用于Ceph守护进程之间的集群内通信的连接模式（或允许的模式）。如果列出了多个模式，则优先使用列出的模式。 ms_service_mode：客户端连接到集群时允许使用的模式列表。 ms_client_mode：客户端在与Ceph集群通信时使用（或允许）的连接模式列表，按优先顺序排列。 ms_mon_cluster_mode：监视器之间使用的连接模式（或允许的模式）。 ms_mon_service_mode：客户端或其他Ceph守护进程连接到监视器时允许使用的模式列表。 ms_mon_client_mode：客户端或非监视器守护进程连接到监视器时使用的连接模式列表，按优先顺序排列。 ms_compress_secure：将加密与压缩结合使用会降低消息之间安全性的水平。如果启用了加密和压缩，则会忽略压缩设置，消息将不会被压缩。可以通过此设置覆盖此行为。 ms_osd_compress_mode：在Messenger中与OSD通信时使用的压缩策略。 ms_osd_compress_min_size：符合在线压缩条件的最小消息大小。 ms_osd_compression_algorithm：与OSD连接时优先顺序中的压缩算法。默认值为snappy，也可以接受列表（如snappy zlib zstd等）。 CEPHX 配置参考 auth_cluster_required：如果启用了此配置设置，则Ceph存储集群守护进程（即ceph-mon、ceph-osd、ceph-mds和ceph-mgr）必须相互认证。有效的设置为cephx或none。 auth_service_required：如果启用了此配置设置，则Ceph客户端只能在通过认证后访问Ceph服务。有效的设置为cephx或none。 auth_client_required：如果启用了此配置设置，则Ceph客户端与Ceph存储集群之间的通信只能在Ceph存储集群对Ceph客户端进行认证后建立。有效的设置为cephx或none。 cephx_require_signatures：如果此配置设置为true，Ceph要求在Ceph客户端和Ceph存储集群之间以及集群内守护进程之间的所有消息通信上使用签名。 cephx_cluster_require_signatures：如果此配置设置为true，Ceph要求在Ceph存储集群内部守护进程之间的所有消息通信上使用签名。 cephx_service_require_signatures：如果此配置设置为true，Ceph要求在Ceph客户端和Ceph存储集群之间的所有消息通信上使用签名。 cephx_sign_messages：如果此配置设置为true，并且Ceph版本支持消息签名，则Ceph将对所有消息进行签名，以提高其防伪性。 auth_service_ticket_ttl：当Ceph存储集群向Ceph客户端发送认证票据时，Ceph存储集群为该票据分配一个生存时间（TTL）。 MON配置参考 mon_force_quorum_join：强制监视器加入仲裁，即使它之前已从映射中移除。 fsid：集群ID。每个集群一个。如果未指定，可以由部署工具生成。 mon_initial_members：启动期间集群中的初始监视器ID。如果指定，Ceph要求形成初始仲裁的监视器数为奇数（例如3个）。 mon_data_size_warn：当监视器的数据存储增长到大于此大小时，触发HEALTH_WARN状态，默认15GB。 mon_data_avail_warn：当存放监视器数据存储的文件系统报告其可用容量小于或等于此百分比时，触发HEALTH_WARN状态。 mon_data_avail_crit：当存放监视器数据存储的文件系统报告其可用容量小于或等于此百分比时，触发HEALTH_ERR状态。 mon_warn_on_crush_straw_calc_version_zero：当CRUSH straw_calc_version为0时，触发HEALTH_WARN。 mon_warn_on_legacy_crush_tunables：当CRUSH tunables过旧（早于mon_min_crush_required_version）时，触发HEALTH_WARN。 mon_crush_min_required_version：集群要求的最低可调配置文件。 mon_warn_on_osd_down_out_interval_zero：当mon_osd_down_out_interval为0时，触发HEALTH_WARN。在领导者上将此选项设置为0的效果类似于noout标志。没有设置noout标志的集群很难诊断问题，因此在这种情况下报告警告。 mon_warn_on_slow_ping_ratio：当OSD之间的任何心跳超过osd_heartbeat_grace的mon_warn_on_slow_ping_ratio时，触发HEALTH_WARN。 mon_warn_on_slow_ping_time：用具体值覆盖mon_warn_on_slow_ping_ratio。如果OSD之间的任何心跳超过mon_warn_on_slow_ping_time毫秒，触发HEALTH_WARN。默认值为0（禁用）。 mon_warn_on_pool_no_redundancy：如果任何池配置为没有副本，触发HEALTH_WARN。 mon_cache_target_full_warn_ratio：在池的cache_target_full和target_max_object之间的位置开始警告。 mon_health_to_clog：启用定期将健康摘要发送到集群日志。 mon_health_to_clog_tick_interval：监视器将健康摘要发送到集群日志的频率（以秒为单位）（非正数禁用）。如果当前健康摘要为空或与上次相同，监视器将不会将其发送到集群日志。 mon_health_to_clog_interval：监视器将健康摘要发送到集群日志的频率（以秒为单位）（非正数禁用）。无论当前健康摘要是否与上次不同，监视器总是会将摘要发送到集群日志。 mon_osd_full_ratio：OSD被视为已满的设备空间利用率的阈值百分比，默认值为0.95。 mon_osd_backfillfull_ratio：设备空间利用率的阈值，当OSD被认为太满以至于无法回填时。默认值为0.90。 mon_osd_nearfull_ratio：设备空间利用率的阈值，当OSD被认为接近满时。默认值为0.85。 mon_sync_timeout：监视器在放弃并重新启动之前等待其同步提供程序的下一条更新消息的秒数。默认值为1分钟。 mon_sync_max_payload_size：同步负载的最大大小（以字节为单位）。默认值为1MiB。 paxos_max_join_drift：在必须首先同步监视器数据存储之前，Paxos迭代的最大次数。当一个监视器发现其对等体远远领先时，它将首先与数据存储同步，然后再继续。默认值为10。 paxos_stash_full_interval：保存PaxosService状态的完整副本的频率（以提交次数计）。目前此设置仅影响mds、mon、auth和mgr的PaxosServices。默认值为25。 paxos_propose_interval：在提议地图更新之前收集更新的时间间隔。 paxos_min：保留的最小Paxos状态数。 paxos_min_wait：在一段不活动时间后收集更新的最短时间。 paxos_trim_min：在修剪前允许的额外提议次数。 paxos_trim_max：一次修剪时允许修剪的最大额外提议次数。 paxos_service_trim_min：触发修剪的最小版本数量（0表示禁用）。 paxos_service_trim_max：在单个提议期间修剪的最大版本数量（0表示禁用）。 paxos_service_trim_max_multiplier：当修剪大小较大时，paxos_service_trim_max将被乘以的因子，以获得新的上限（0表示禁用）。 mon_mds_force_trim_to：强制监视器修剪到但不包括此FSMap纪元。值为0时禁用（默认值）。此命令可能有危险，请谨慎使用。 mon_osd_force_trim_to：osdmaps缓存的大小，不依赖于底层存储的缓存。 mon_election_timeout：在选举提议者上，所有ACK的最长等待时间（以秒为单位）。 mon_lease：监视器版本的租约长度（以秒为单位）。 mon_lease_renew_interval_factor：mon_lease * mon_lease_renew_interval_factor将是Leader更新其他监视器租约的时间间隔。因子应小于1.0。 mon_lease_ack_timeout_factor：Leader将等待mon_lease * mon_lease_ack_timeout_factor的时间，以便提供者确认租约扩展。 mon_accept_timeout_factor：Leader将等待mon_lease * mon_accept_timeout_factor的时间，以便请求者接受Paxos更新。在Paxos恢复阶段也会用于类似目的。 mon_min_osdmap_epochs：始终保留的最小OSD地图纪元数。 mon_max_log_epochs：监视器应保留的最大日志纪元数。 mon_tick_interval：监视器的tick间隔时间（以秒为单位）。 mon_clock_drift_allowed：在发出健康警告之前允许的监视器之间的时钟漂移（以秒为单位）。 mon_clock_drift_warn_backoff：在集群日志中记录时钟漂移警告的指数退避因子。 mon_timecheck_interval：Leader的时间检查间隔（时钟漂移检查）（以秒为单位）。 mon_timecheck_skew_interval：在存在偏差时，Leader的时间检查间隔（时钟漂移检查）（以秒为单位）。 mon_client_hunt_interval：客户端每N秒尝试连接新的监视器，直到建立连接为止。 mon_client_ping_interval：客户端每N秒ping一次监视器。 mon_client_max_log_entries_per_message：监视器每个客户端消息生成的最大日志条目数。 mon_client_bytes：允许在内存中保留的客户端消息数据量（以字节为单位）。 mon_allow_pool_delete：监视器是否应允许删除池，而不考虑池标志的设置？ osd_pool_default_ec_fast_read：是否启用池的快速读取功能。如果在创建时未指定fast_read，则它将用作新创建的纠删码池的默认设置。 osd_pool_default_flag_hashpspool：在新池上设置hashpspool（更好的哈希方案）标志。 osd_pool_default_flag_nodelete：在新池上设置nodelete标志，防止删除池。 osd_pool_default_flag_nopgchange：在新池上设置nopgchange标志。不允许更改PG数量。 osd_pool_default_flag_nosizechange：在新池上设置nosizechange标志。不允许更改大小。 mon_max_osd：集群中允许的最大OSD数量。 mon_globalid_prealloc：为集群中的客户端和守护程序预分配的全局ID数量。 mon_subscribe_interval：订阅刷新间隔（以秒为单位）。订阅机制允许获取集群地图和日志信息。 mon_stat_smooth_intervals：Ceph将在过去N个PG地图上平滑统计数据。 mon_probe_timeout：监视器在引导之前等待找到对等体的秒数。 mon_daemon_bytes：元数据服务器和OSD消息的消息内存上限（以字节为单位）。 mon_max_log_entries_per_event：每个事件的最大日志条目数。 mon_osd_prime_pg_temp：在外部OSD重新加入集群时，启用或禁用使用先前的OSD来预热PGMap。如果设置为true，客户端将继续使用先前的OSD，直到PG的新加入OSD完成对等。 mon_osd_prime_pg_temp_max_time：在外部OSD重新加入集群时，监视器应花费多少时间（以秒为单位）尝试预热PGMap。 mon_osd_prime_pg_temp_max_estimate：在并行预热所有PG之前，我们在每个PG上花费的最大估计时间。 mon_mds_skip_sanity：跳过FSMap的安全断言（在我们希望继续的bug情况下）。如果FSMap完整性检查失败，监视器将终止，但我们可以通过启用此选项来禁用它。 mon_max_mdsmap_epochs：在单个提议期间修剪的最大mdsmap纪元数。 mon_config_key_max_entry_size：配置密钥条目的最大大小（以字节为单位）。 mon_scrub_interval：监视器通过比较存储的校验和与所有存储密钥的计算校验和来清理其存储的频率（0表示禁用，危险，请谨慎使用）。 mon_scrub_max_keys：每次清理的最大密钥数量。 mon_compact_on_start：在ceph-mon启动时压缩Ceph监视器存储使用的数据库。手动压缩有助于缩小监视器数据库并提高其性能，如果常规压缩无法正常工作。 mon_compact_on_bootstrap：在引导时压缩Ceph监视器存储使用的数据库。监视器在引导后相互探测以建立仲裁。如果监视器在加入仲裁之前超时，它将重新启动并再次引导。 mon_compact_on_trim：修剪旧状态时压缩某个前缀（包括paxos）。 mon_cpu_threads：执行监视器上CPU密集型工作的线程数。 mon_osd_mapping_pgs_per_chunk：我们按块计算从放置组到OSD的映射。此选项指定每块的放置组数量。 mon_session_timeout：监视器将在超过此时间限制的闲置会话期间终止非活动会话。 mon_osd_cache_size_min：为osd监视器缓存保持映射在内存中的最小字节数。 mon_memory_target：与启用缓存自动调整相关的OSD监视器缓存和KV缓存保持映射在内存中的字节数。 mon_memory_autotune：自动调整OSD监视器和KV数据库使用的缓存内存。 mon_dns_srv_name：用于查询监视器主机&#x2F;地址的DNS服务名称。 MON&#x2F;OSD交互配置参考 mon_osd_min_up_ratio：在Ceph将Ceph OSD守护进程标记为“已宕机”之前，Ceph OSD守护进程的最小正常比例。 mon_osd_min_in_ratio：在Ceph将Ceph OSD守护进程标记为“已剔除”之前，Ceph OSD守护进程的最小在场比例。 mon_osd_laggy_halflife：滞后估计的衰减时间（以秒为单位）。 mon_osd_laggy_weight：在滞后估计衰减中用于新样本的权重。 mon_osd_laggy_max_interval：滞后估计中滞后间隔的最大值（以秒为单位）。监视器使用自适应方法来评估特定OSD的滞后间隔。此值将用于计算该OSD的宽限时间。 mon_osd_adjust_heartbeat_grace：如果设置为true，Ceph将根据滞后估计进行缩放。 mon_osd_adjust_down_out_interval：如果设置为true，Ceph将根据滞后估计进行缩放。 mon_osd_auto_mark_in：Ceph将自动标记任何启动中的Ceph OSD守护进程为集群中的。 mon_osd_auto_mark_auto_out_in：Ceph将自动标记启动中的Ceph OSD守护进程为集群中的，而不是标记为已剔除。 mon_osd_auto_mark_new_in：Ceph将自动标记启动中的新Ceph OSD守护进程为集群中的。 mon_osd_down_out_interval：Ceph在标记一个Ceph OSD守护进程为“已宕机”或“已剔除”之前等待的时间（以秒为单位）。 mon_osd_down_out_subtree_limit：Ceph不会自动标记的最小CRUSH单元类型。例如，如果设置为host，并且一个主机上的所有OSD都宕机，Ceph将不会自动标记这些OSD。 mon_osd_report_timeout：在声明未响应的Ceph OSD守护进程为“已宕机”之前的宽限时间（以秒为单位）。 mon_osd_min_down_reporte：报告一个“已宕机”Ceph OSD守护进程所需的最小Ceph OSD守护进程数量。 mon_osd_reporter_subtree_level：报告者计数的父桶层级。OSD在发现某个对等体未响应时，会将故障报告发送给监视器。监视器在宽限期后将报告的OSD标记为“已剔除”，然后标记为“已宕机”。 osd_heartbeat_interval：Ceph OSD守护进程向其对等体发送心跳的频率（以秒为单位）。 osd_heartbeat_grace：Ceph OSD守护进程未显示心跳时，被Ceph存储集群视为“已宕机”的时间。这一设置必须在[mon]和[osd]或[global]部分中设置，以便监视器和OSD守护进程都能读取。 osd_mon_heartbeat_interval：如果Ceph OSD守护进程没有Ceph OSD对等体，它向Ceph监视器发送心跳的频率。 osd_mon_heartbeat_stat_stale：停止报告未更新心跳时间的统计信息的秒数。设置为零以禁用此操作。 osd_mon_report_interval：Ceph OSD守护进程从启动或其他可报告事件开始，等待的秒数，之后向Ceph监视器报告。 OSD 配置参考 osd_uuid: Ceph OSD守护进程的全局唯一标识符（UUID）。 osd_data: OSD数据的路径。部署Ceph时必须创建此目录。建议在此挂载点上挂载用于OSD数据的驱动器，不推荐更改默认值。 osd_max_write_size: 写入的最大大小（以MB为单位）。 osd_max_object_size: RADOS对象的最大大小（以字节为单位）。 osd_client_message_size_cap: 允许在内存中存储的最大客户端数据消息大小。 osd_class_dir: RADOS类插件的类路径。 osd_mkfs_options {fs-type}: 创建新Ceph Filestore OSD时使用的选项，类型为{fs-type}。 osd_mount_options {fs-type}: 挂载Ceph Filestore OSD时使用的选项，类型为{fs-type}。 osd_journal: OSD日志的路径。可以是文件或块设备（如SSD的分区）。如果是文件，则必须创建包含该文件的目录。建议在osd_data驱动器是HDD时使用单独的快速设备。 osd_journal_size: 日志的大小（以MB为单位）。 osd_max_scrubs: 允许同时进行的最大扫描操作数。 osd_scrub_begin_hour: 限制扫描操作的开始小时。与osd_scrub_end_hour一起定义了扫描的时间窗口。设置为0和0允许全天扫描。 osd_scrub_end_hour: 限制扫描操作的结束小时。与osd_scrub_begin_hour一起定义了扫描的时间窗口。设置为0和0允许全天扫描。 osd_scrub_begin_week_day: 限制扫描操作的开始周天。0 &#x3D; 星期天，1 &#x3D; 星期一，依此类推。与osd_scrub_end_week_day一起定义了扫描的时间窗口。设置为0和0允许全天扫描。 osd_scrub_end_week_day: 限制扫描操作的结束周天。0 &#x3D; 星期天，1 &#x3D; 星期一，依此类推。与osd_scrub_begin_week_day一起定义了扫描的时间窗口。设置为0和0允许全天扫描。 osd_scrub_during_recovery: 允许在恢复过程中进行扫描。设置为false将禁用在活动恢复时调度新的扫描（和深度扫描），已运行的扫描将继续进行。 osd_scrub_load_threshold: 归一化的最大负载。系统负载（按getloadavg() &#x2F; 在线CPU数量定义）高于此值时，Ceph将不会进行扫描。默认值为0.5。 osd_scrub_min_interval: 当Ceph存储集群负载较低时，OSD扫描的最小间隔（以秒为单位）。 osd_scrub_max_interval: 无论集群负载如何，OSD扫描的最大间隔（以秒为单位）。 osd_scrub_chunk_min: 每次操作扫描的最小对象存储块数量。扫描期间Ceph会阻塞对单个块的写入。 osd_scrub_chunk_max: 每次操作扫描的最大对象存储块数量。 osd_scrub_sleep: 在扫描下一组块之前的等待时间。增加此值将减慢整体扫描速度，以减少对客户端操作的影响。 osd_deep_scrub_interval: “深度”扫描的间隔（完全读取所有数据）。osd_scrub_load_threshold不影响此设置。 osd_scrub_interval_randomize_ratio: 在为PG调度下一个扫描作业时，添加一个随机延迟。延迟是小于osd_scrub_min_interval * osd_scrub_interval_randomized_ratio的随机值。默认设置将扫描分散到[1, 1.5] * osd_scrub_min_interval的允许时间窗口中。 osd_deep_scrub_stride: 执行深度扫描时的读取大小。 osd_scrub_auto_repair: 如果设置为true，扫描或深度扫描发现错误时将启用自动PG修复。如果发现的错误超过osd_scrub_auto_repair_num_errors，则不执行修复。 osd_scrub_auto_repair_num_errors: 如果发现的错误超过此值，则不会进行自动修复。 osd_op_num_shards: 为给定OSD分配的分片数量。每个分片有自己的处理队列。OSD上的PG均匀分配到各个分片。此设置会覆盖_ssd和_hdd设置（如果非零）。 osd_op_num_shards_hdd: 为给定OSD（针对旋转介质）分配的分片数量。 osd_op_num_shards_ssd: 为给定OSD（针对固态介质）分配的分片数量。 osd_op_queue: 设置用于在每个OSD内优先处理操作的队列类型。两种队列都有一个严格的子队列，严格子队列在正常队列之前出队。正常队列在不同的实现中有所不同。WeightedPriorityQueue (wpq) 根据操作的优先级出队，以防止队列的饥饿。mClockQueue (mclock_scheduler) 根据操作所属的类别（恢复、扫描、snaptrim、客户端操作、osd子操作）优先处理操作。此设置需要重新启动。 osd_op_queue_cut_off: 选择将哪些优先级操作发送到严格队列与正常队列。低设置将所有复制操作及更高优先级操作发送到严格队列，高设置仅将复制确认操作及更高优先级操作发送到严格队列。设置为高有助于在一些OSD非常繁忙时，尤其是结合wpq设置时，减少客户端流量的饥饿现象。此设置需要重新启动。 osd_client_op_priority: 客户端操作的优先级。相对于下面的osd_recovery_op_priority值。默认值强烈偏向客户端操作而非恢复。 osd_recovery_op_priority: 恢复操作与客户端操作的优先级（如果池的recovery_op_priority未指定）。默认值优先处理客户端操作（见上文）而非恢复操作。您可以通过降低此值以增加客户端操作的优先级，或者提高此值以偏向恢复。 osd_scrub_priority: 池未指定scrub_priority值时，默认的工作队列优先级。可以提升到osd_client_op_priority值，当扫描阻塞客户端操作时使用。 osd_requested_scrub_priority: 用户请求的扫描在工作队列中的优先级。如果此值小于osd_client_op_priority，则可以提升到osd_client_op_priority值，当扫描阻塞客户端操作时使用。 osd_snap_trim_priority: 快照修剪工作队列的优先级。 osd_snap_trim_sleep: 下一个快照修剪操作之前的等待时间（以秒为单位）。增加此值将减慢快照修剪过程，此选项覆盖特定后端的变体。 osd_snap_trim_sleep_hdd: HDD上下一个快照修剪操作之前的等待时间（以秒为单位）。 osd_snap_trim_sleep_ssd: SSD（包括NVMe）上下一个快照修剪操作之前的等待时间（以秒为单位）。 osd_snap_trim_sleep_hybrid: 当OSD数据在HDD上，而OSD日志或WAL+DB在SSD上时，下一次快照修剪操作之前的等待时间（以秒为单位）。 osd_op_thread_timeout: Ceph OSD守护进程操作线程超时时间（以秒为单位）。 osd_op_complaint_time: 操作在指定的秒数后变得需要投诉。 osd_op_history_size: 跟踪的最大已完成操作数。 osd_op_history_duration: 跟踪的最旧已完成操作。 osd_op_log_threshold: 一次显示的操作日志数量。 osd_async_recovery_min_cost: 当前日志条目差异和历史丢失对象的混合度量值，超过此值时适当切换到异步恢复。 osd_push_per_object_cost: 提供推送操作的开销。 osd_mclock_scheduler_client_res: 为每个客户端保留的IO比例（默认值）。 osd_mclock_scheduler_client_wgt: 每个客户端的IO份额（默认值），超过保留比例。 osd_mclock_scheduler_client_lim: 每个客户端的IO限制（默认值），超过保留比例。 osd_mclock_scheduler_background_recovery_res:为后台恢复保留的IO比例（默认值）。 osd_mclock_scheduler_background_recovery_wgt: 背景恢复的IO份额（默认值），超过保留比例。 osd_mclock_scheduler_background_recovery_lim: 背景恢复的IO限制（默认值），超过保留比例。 osd_mclock_scheduler_background_best_effort_res: 为后台最佳努力保留的IO比例（默认值）。 osd_mclock_scheduler_background_best_effort_wgt: 每个后台最佳努力的IO份额（默认值），超过保留比例。 osd_mclock_scheduler_background_best_effort_lim: 背景最佳努力的IO限制（默认值），超过保留比例。 osd_max_backfills: 允许到单个OSD的最大回填数量。注意，此设置对读取和写入操作分别应用。 osd_backfill_scan_min: 每个回填扫描的最小对象数量。 osd_backfill_scan_max: 每个回填扫描的最大对象数量。 osd_backfill_retry_interval: 在重试回填请求之前等待的秒数。 osd_map_dedup: 启用OSD映射中的重复项删除。 osd_map_cache_size: 保留的OSD映射数量。 osd_map_message_max: 每个MOSDMap消息允许的最大映射条目数。 osd_recovery_delay_start: 在peering完成后，Ceph将在开始恢复RADOS对象之前延迟指定的秒数。 osd_recovery_max_active: 每次最多允许的活动恢复请求数量。更多请求将加速恢复，但会增加集群负载。 osd_recovery_max_active_hdd: 如果主设备为旋转设备，则每个OSD允许的活动恢复请求数量。 osd_recovery_max_active_ssd: 如果主设备为非旋转设备（如SSD），则每个OSD允许的活动恢复请求数量。 osd_recovery_max_chunk: 恢复操作可以承载的数据块的最大总大小。 osd_recovery_max_single_start: 当OSD正在恢复时，每个OSD允许新启动的恢复操作的最大数量。 osd_recover_clone_overlap: 在恢复过程中保留克隆重叠。应始终设置为true。 osd_recovery_sleep: 下一次恢复或回填操作之前的等待时间（以秒为单位）。增加此值将减慢恢复操作的速度，同时减少对客户端操作的影响。 osd_recovery_sleep_hdd: 下一次恢复或回填操作之前的等待时间（以秒为单位），适用于HDD。 osd_recovery_sleep_ssd: 下一次恢复或回填操作之前的等待时间（以秒为单位），适用于SSD。 osd_recovery_sleep_hybrid: 当OSD数据在HDD上，而OSD日志或WAL+DB在SSD上时，下一次恢复或回填操作之前的等待时间（以秒为单位）。 osd_recovery_priority: 恢复工作队列的默认优先级。不与池的recovery_priority相关。 osd_agent_max_ops: 高速模式下每个分层代理的最大同时刷新操作数。 osd_agent_max_low_ops: 低速模式下每个分层代理的最大同时刷新操作数。 osd_default_notify_timeout: OSD默认通知超时时间（以秒为单位）。 osd_check_for_log_corruption: 检查日志文件是否损坏。可能会计算开销很大。 osd_delete_sleep: 下一次删除事务之前的等待时间（以秒为单位）。此设置会限制PG删除过程的速度。 osd_delete_sleep_hdd: 下一次删除事务之前的等待时间（以秒为单位），适用于HDD。 osd_delete_sleep_ssd: 下一次删除事务之前的等待时间（以秒为单位），适用于SSD。 osd_delete_sleep_hybrid: 当OSD数据在HDD上，而OSD日志或WAL+DB在SSD上时，下一次删除事务之前的等待时间（以秒为单位）。 osd_command_max_records: 限制返回的丢失对象数量。 osd_fast_fail_on_connection_refused: 如果启用此选项，崩溃的OSD会被连接的对等体和MON立即标记为不可用（假设崩溃的OSD主机仍然存在）。禁用它将恢复旧的行为，但可能会在OSD在I&#x2F;O操作中崩溃时造成长时间的I&#x2F;O延迟。 MCLOCK 配置参考 osd_mclock_profile：设置用于根据不同类别操作（如背景恢复、擦洗、快照修剪、客户端操作、OSD 子操作）提供服务质量（QoS）的 mClock 配置类型。一旦启用了内置配置文件，低级 mClock 资源控制参数（[预留、权重、限制]）和一些 Ceph 配置参数将自动设置。请注意，这不适用于自定义配置文件。 osd_mclock_max_capacity_iops_hdd：每个 OSD 的最大随机写入 IOPS 容量（以 4 KiB 块大小为单位），适用于旋转介质（HDD）。 osd_mclock_max_capacity_iops_ssd：每个 OSD 的最大随机写入 IOPS 容量（以 4 KiB 块大小为单位），适用于固态介质（SSD）。 osd_mclock_max_sequential_bandwidth_hdd：每个 OSD 的最大顺序带宽（以字节&#x2F;秒为单位），适用于旋转介质（HDD）。 osd_mclock_max_sequential_bandwidth_ssd：每个 OSD 的最大顺序带宽（以字节&#x2F;秒为单位），适用于固态介质（SSD）。 osd_mclock_force_run_benchmark_on_init：强制在 OSD 初始化或启动时运行 OSD 基准测试。 osd_mclock_override_recovery_settings：启用此选项将允许 mClock 调度器覆盖由 osd_recovery_max_active_hdd、osd_recovery_max_active_ssd 和 osd_max_backfills 选项定义的恢复&#x2F;回填限制。 osd_mclock_iops_capacity_threshold_hdd：超过此阈值的 IOPS 容量（以 4 KiB 块大小为单位），将忽略 OSD 基准测试结果，适用于旋转介质（HDD）。 osd_mclock_iops_capacity_threshold_ssd：超过此阈值的 IOPS 容量（以 4 KiB 块大小为单位），将忽略 OSD 基准测试结果，适用于固态介质（SSD）。 bluestore配置参考 bluestore_cache_autotune：自动调整分配给各种 BlueStore 缓存的空间比例，同时尊重最小值。 osd_memory_target：当 TCMalloc 可用且启用了缓存自动调优时，尝试保持此数量的字节映射在内存中。注意：这可能与进程的 RSS 内存使用情况不完全匹配。尽管进程映射的堆内存总量通常应接近此目标，但内核是否实际回收已取消映射的内存没有保证。在初期开发中发现，一些内核导致 OSD 的 RSS 内存超出映射内存最多 20%。不过，假设内核在内存压力较大时通常可能会更积极地回收未映射的内存。实际情况可能有所不同。 bluestore_cache_autotune_interval：启用缓存自动调优时，重新平衡之间的等待秒数。bluestore_cache_autotune_interval 设置 Ceph 重新计算各种缓存分配比例的速度。注意：将此间隔设置得过小可能导致高 CPU 使用率和性能下降。 osd_memory_base：当启用 TCMalloc 和缓存自动调优时，估算 OSD 需要的最小内存量（以字节为单位）。这用于帮助自动调优器估算缓存的预期总内存消耗。 osd_memory_expected_fragmentation：当启用 TCMalloc 和缓存自动调优时，估算内存碎片的百分比。这用于帮助自动调优器估算缓存的预期总内存消耗。 osd_memory_cache_min：当启用 TCMalloc 和缓存自动调优时，设置用于缓存的最小内存量。注意：将此值设置得过低可能导致显著的缓存抖动。 osd_memory_cache_resize_interval：当启用 TCMalloc 和缓存自动调优时，在调整缓存大小之间等待的秒数。此设置改变 BlueStore 可用于缓存的总内存量。注意：将此间隔设置得过小可能导致内存分配器抖动和性能下降。 bluestore_cache_size：BlueStore 将用于其缓存的内存量。如果为零，则使用 bluestore_cache_size_hdd 或 bluestore_cache_size_ssd。 bluestore_cache_size_hdd：当 BlueStore 由 HDD 支持时，默认用于缓存的内存量。 bluestore_cache_size_ssd：当 BlueStore 由 SSD 支持时，默认用于缓存的内存量。 bluestore_cache_meta_ratio：分配给元数据的 bluestore 缓存比例。 bluestore_cache_kv_ratio：分配给键值数据库（RocksDB）的 bluestore 缓存比例。 bluestore_csum_type：使用的默认校验和算法。有效选择：none、crc32c、crc32c_16、crc32c_8、xxhash32、xxhash64。 bluestore_compression_algorithm：如果没有设置每池属性 compression_algorithm，则使用的默认压缩器。注意，由于压缩少量数据时 CPU 开销较高，zstd 不推荐用于 BlueStore。有效选择：snappy、zlib、zstd、lz4。 bluestore_compression_mode：如果没有设置每池属性 compression_mode，则使用的默认压缩策略。none 表示从不使用压缩。passive 表示在客户端提示数据可以压缩时使用压缩。aggressive 表示除非客户端提示数据不可压缩，否则使用压缩。force 表示在所有情况下使用压缩，即使客户端提示数据不可压缩。有效选择：none、passive、aggressive、force。 bluestore_compression_required_ratio：压缩后数据块大小与原始大小的比例必须至少小于此值才能存储压缩版本。 bluestore_compression_min_blob_size：小于此值的块从不压缩。每池属性 compression_min_blob_size 将覆盖此设置。 bluestore_compression_min_blob_size_hdd：旋转介质（HDD）的默认 BlueStore 压缩最小 blob 大小值。 bluestore_compression_min_blob_size_ssd：非旋转介质（固态介质）的默认 BlueStore 压缩最小 blob 大小值。 bluestore_compression_max_blob_size：大于此值的块在压缩前被拆分成最多 bluestore_compression_max_blob_size 字节的小 blob。每池属性 compression_max_blob_size 将覆盖此设置。 bluestore_compression_max_blob_size_hdd：旋转介质（HDD）的默认 BlueStore 压缩最大 blob 大小值。 bluestore_compression_max_blob_size_ssd：非旋转介质（SSD、NVMe）的默认 BlueStore 压缩最大 blob 大小值。 bluestore_rocksdb_cf：启用 BlueStore 的 RocksDB 分片。设置为 true 时，使用 bluestore_rocksdb_cfs。仅在 OSD 执行 --mkfs 时应用。 bluestore_rocksdb_cfs：BlueStore RocksDB 分片的定义。最佳值取决于多个因素，不建议修改。此设置仅在 OSD 执行 --mkfs 时使用。OSD 的下一次运行将从磁盘检索分片。 bluestore_throttle_bytes：在限制 IO 提交之前的最大飞行字节数。 bluestore_throttle_deferred_bytes：在限制 IO 提交之前的最大延迟写入字节数。 bluestore_throttle_cost_per_io：每次 IO 交易增加的开销（以字节为单位）。 bluestore_throttle_cost_per_io_hdd：旋转介质（HDD）的默认 bluestore_throttle_cost_per_io。 bluestore_throttle_cost_per_io_ssd：非旋转介质（固态介质）的默认 bluestore_throttle_cost_per_io。 bluestore_min_alloc_size：较小的分配大小通常意味着在触发写时复制操作（例如，当写入最近快照的内容时）时，读取和重写的数据更少。类似地，在执行覆盖写入之前，日志中记录的数据也更少（小于 min_alloc_size 的写入必须首先通过 BlueStore 日志）。较大的 min_alloc_size 减少了描述磁盘上布局所需的元数据量，并减少了总体碎片化。 bluestore_min_alloc_size_hdd：旋转介质（HDD）的默认 min_alloc_size 值。 bluestore_min_alloc_size_ssd：非旋转介质（固态介质）的默认 min_alloc_size 值。 bluestore_use_optimal_io_size_for_min_alloc_size：发现介质的最佳 IO 大小并用于 min_alloc_size。 日志配置参考 journal_dio：启用直接 I&#x2F;O（Direct I&#x2F;O）到日志。这要求 journal_block_align 设置为 true。 journal_aio：启用使用 libaio 进行异步写入日志。这要求 journal_dio 设置为 true。版本 0.61 及更高版本为 true，版本 0.60 及更早版本为 false。 journal_block_align：将写操作对齐到块。dio 和 aio 都需要此设置。 journal_max_write_bytes：日志一次最多写入的字节数。 journal_max_write_entries：日志一次最多写入的条目数。 journal_align_min_size：对大于指定最小值的数据有效负载进行对齐。 journal_zero_on_create：在 mkfs 期间将整个日志用 0 覆盖。 POOL&#x2F;PG&#x2F;CRUSH配置参考 mon_max_pool_pg_num：每个池的最大 placement group 数量。 mon_pg_stuck_threshold：PG 被视为卡住的秒数阈值。 mon_pg_warn_min_per_osd：如果每个 OSD 平均 PG 数量低于此值，则触发 HEALTH_WARN。非正值将禁用此警告。 mon_pg_warn_min_objects：如果集群中 RADOS 对象总数低于此值，则不发出警告。 mon_pg_warn_min_pool_objects：如果池中 RADOS 对象数量低于此值，则不发出警告。 mon_pg_check_down_all_threshold：下线 OSD 的百分比阈值，超过此阈值时我们检查所有 PG 是否过时。 mon_pg_warn_max_object_skew：如果任何池的每个 PG 的平均 RADOS 对象数大于所有池的每个 PG 平均 RADOS 对象数的 mon_pg_warn_max_object_skew 倍，则触发 HEALTH_WARN。零或非正值将禁用此警告。注意此选项适用于 ceph-mgr 守护进程。 mon_delta_reset_interval：在重置 PG delta 为 0 之前的非活动秒数。我们跟踪每个池的使用空间 delta，例如，这有助于理解恢复进度或缓存层性能。如果某个池没有活动报告，我们将重置该池的 delta 历史记录。 osd_crush_chooseleaf_type：CRUSH 规则中 chooseleaf 使用的桶类型。使用序号而非名称。 osd_crush_initial_weight：新添加的 OSD 的初始 CRUSH 权重。此选项的默认值为新添加的 OSD 的大小（以 TB 为单位）。默认情况下，新添加的 OSD 的初始 CRUSH 权重设置为其设备大小（以 TB 为单位）。有关详细信息，请参见权重桶项。 osd_pool_default_crush_rule：创建复制池时使用的默认 CRUSH 规则。默认值 -1 表示“选择具有最低数值 ID 的规则并使用该规则”。这是为了在没有规则 0 的情况下使池创建正常工作。 osd_pool_erasure_code_stripe_unit：设置默认的擦除编码池对象条带的大小（以字节为单位）。每个大小为 S 的对象将存储为 N 条带，每条数据块接收条带单位字节。每个条带 N * 条带单位字节将单独编码&#x2F;解码。此选项可以被擦除编码配置文件中的 stripe_unit 设置覆盖。 osd_pool_default_size：设置池中对象的副本数量。默认值与 ceph osd pool set &#123;pool-name&#125; size &#123;size&#125; 相同。 osd_pool_default_min_size：设置池中对象写入副本的最小数量，以便确认 I&#x2F;O 操作。如果未达到最低数量，Ceph 将不会确认 I&#x2F;O 操作，这可能导致数据丢失。此设置确保在降级模式下具有最小副本数量。默认值为 0，表示没有特定的最小值。如果为 0，最小值为 size - (size / 2)。 osd_pool_default_pg_num：池的默认 placement group 数量。默认值与 pg_num 和 mkpool 相同。 osd_pool_default_pgp_num：池的 placement group 数量的默认值。默认值与 pgp_num 和 mkpool 相同。PG 和 PGP 应该相等（目前）。注意：除非禁用自动扩展，否则不应设置此值。 osd_pool_default_pg_autoscale_mode：默认值为启用时，自动扩展器以 1 个 PG 启动新池，除非用户指定了 pg_num。 osd_pool_default_flags：新池的默认标志。 osd_max_pgls：最大 placement group 列表数。请求大量列表的客户端可能会占用 Ceph OSD 守护进程。 osd_min_pg_log_entries：修剪日志文件时要维护的最小 placement group 日志条目数。 osd_max_pg_log_entries：修剪日志文件时要维护的最大 placement group 日志条目数。 osd_default_data_pool_replay_window：OSD 等待客户端重放请求的时间（以秒为单位）。 osd_max_pg_per_osd_hard_ratio：集群允许的每个 OSD 的 PG 数量比率，超过此比率时 OSD 将拒绝创建新的 PG。如果 OSD 服务的 PG 数量超过 osd_max_pg_per_osd_hard_ratio * mon_max_pg_per_osd，则 OSD 停止创建新的 PG。 常规配置参考 admin_socket：用于执行守护进程的管理命令的套接字，无论 Ceph Monitor 是否已建立法定人数。 pid_file：mon、osd 或 mds 将其 PID 写入的文件。例如，/var/run/$cluster/$type.$id.pid 将为运行在 ceph 集群中的 ID 为 a 的 mon 创建 /var/run/ceph/mon.a.pid。当守护进程正常停止时，PID 文件会被删除。如果进程没有被守护进程化（即使用 -f 或 -d 选项运行），则不会创建 PID 文件。 chdir：Ceph 守护进程启动并运行后切换到的目录。推荐使用默认值 &#x2F; 目录。 atal_signal_handlers：如果设置，将安装 SEGV、ABRT、BUS、ILL、FPE、XCPU、XFSZ、SYS 信号的信号处理程序，以生成有用的日志消息。 max_open_files：如果设置，当 Ceph 存储集群启动时，Ceph 会在操作系统级别设置最大打开文件描述符数（即最大文件描述符数）。适当大的值可以防止 Ceph 守护进程用尽文件描述符。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph硬件要求","slug":"Storage/Ceph/Ceph硬件要求","date":"2021-05-09T08:37:13.000Z","updated":"2024-08-18T12:38:47.171Z","comments":true,"path":"Storage/Ceph/Ceph硬件要求/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E7%A1%AC%E4%BB%B6%E8%A6%81%E6%B1%82/","excerpt":"","text":"Ceph 设计用于在普通硬件上运行，这使得构建和维护PB级数据集群变得灵活且经济可行。在规划集群的硬件时，仍需要平衡多个考虑因素，包括故障域、成本和性能。硬件规划应包括将 Ceph 守护进程和使用 Ceph 的其他进程分布在多个主机上。通常，建议在配置为特定类型守护进程的主机上运行该类型的 Ceph 守护进程。每个 Ceph 集群的需求各不相同，但以下是一些通用指南。 CPU MON节点和MGR节点对 CPU 的要求不高，只需要适度的处理器。如果主机除了运行 Ceph 守护进程之外还要运行 CPU 密集型进程，请确保有足够的处理能力来同时运行这些 CPU 密集型进程和 Ceph 守护进程。建议将非 Ceph 的 CPU 密集型进程运行在独立的主机上（即不在MON节点和MGR节点上），以避免资源竞争。如果集群部署了 Ceph 对象网关，RGW 守护进程可以与MON和MGR服务共存在于资源充足的节点上。 OSD节点需要足够的处理能力来运行 RADOS 服务，计算 CRUSH 数据放置，复制数据，并维护自身的集群映射副本。在早期的 Ceph 版本中，我们会根据每个 OSD 的核心数量来制定硬件建议，但这种每 OSD 核心数的指标现在不如每 IOP 的周期数和每 OSD 的 IOPS 数量有用。例如，使用 NVMe OSD 驱动器时，Ceph 可以轻松利用真实集群中的五到六个核心，并在单个 OSD 中达到大约十四个核心。因此，每 OSD 的核心数不再像以前那样成为主要问题。选择硬件时，应选择每核心的 IOPS 数量。 MDS节点对 CPU 要求很高。它们是单线程的，具有高时钟速率 (GHz) 的 CPU 表现最佳。除非 MDS 服务器还托管其他服务（例如用于 CephFS 元数据池的 SSD OSD），否则它们不需要大量的 CPU 内核。 内存 MON节点和MGR节点：一般来说，更多的 RAM 更好。对于一个适中的集群，可能用 64GB 就足够了；对于一个拥有数百个 OSD 的大型集群，建议使用 128GB。MON和MGR守护进程的内存使用量会随着集群规模的扩大而增加。请注意，在启动时、拓扑更改和恢复期间，这些守护进程需要的内存会比在稳定运行状态下更多，因此需要计划应对峰值使用量。对于非常小的集群，32GB 的内存就足够了。对于最多 300 个 OSD 的集群，建议使用 64GB。对于构建中或将要扩展到更多 OSD 的集群，应配备 128GB 的内存。您还可能需要考虑调整以下设置：mon_osd_cache_size和rocksdb_cache_size。 OSD节点：Bluestore 使用其自己的内存来缓存数据，而不是依赖操作系统的页面缓存。在 Bluestore 中，可以通过更改 osd_memory_target 配置选项来调整 OSD 尝试消耗的内存量，默认值为 4GB。不建议将 osd_memory_target 设置低于 2GB。Ceph 可能无法将内存消耗控制在 2GB 以下，可能会导致极其缓慢的性能。将内存目标设置在 2GB 到 4GB 之间通常能正常工作，但可能会导致性能下降：如果活动数据集相对较大，元数据可能需要从磁盘读取。4GB 是当前 osd_memory_target 的默认值。这一默认值适用于典型用例，旨在平衡 RAM 成本和 OSD 性能。将 osd_memory_target 设置高于 4GB 可以在处理大量（小）对象或大型（256GB&#x2F;OSD 或更多）数据集时提高性能。特别是在使用快速 NVMe OSD 时，这种效果尤为显著。 OSD 内存管理是“尽力而为”的。虽然 OSD 可能会解除内存映射以允许内核回收内存，但不能保证内核会在特定时间框架内实际回收释放的内存。这一点在旧版本的 Ceph 中尤其明显，因为透明大页面可能会阻止内核回收从碎片化的大页面中释放的内存。现代版本的 Ceph 在应用级别禁用了透明大页面，以避免这种情况，但这并不能保证内核会立即回收未映射的内存。OSD 仍然有可能在某些时候超出其内存目标。因此，我们建议在系统上预留至少 20% 的额外内存，以防止 OSD 在临时内存峰值或由于内核回收释放页面的延迟而发生 OOM（内存不足）。具体所需的 20% 值可能会因系统的确切配置而有所不同。 MDS节点：CephFS 元数据守护进程的内存利用率取决于其缓存的配置大小。对于大多数系统，建议至少配置 1 GB 的内存。相关设置为 mds_cache_memory_limit。 不建议通过为操作系统配置交换空间来为守护进程提供额外的虚拟内存。这样做可能会导致性能下降。 磁盘 在规划数据存储时需要考虑显著的成本和性能权衡。操作系统的并发操作和多个守护进程对单个驱动器进行读写操作的请求可能会影响性能。OSD 需要大量的存储驱动器空间来存储 RADOS 数据。我们建议至少使用 1 TB 的驱动器。小于 1 TB 的 OSD 驱动器会将相当大一部分容量用于元数据，小于 100 GB 的驱动器效果更差，几乎不具备有效性。强烈建议为 Ceph MON和 Ceph Mgr主机、CephFS MDS元数据池和 Ceph RGW索引池配置企业级 SSD。 硬盘驱动器: 仔细考虑更大磁盘的每 GB 成本优势。我们建议将磁盘驱动器的价格除以其容量来得出每 GB 成本，因为较大的驱动器可能对每 GB 成本产生显著影响。例如，一个价格为 75 美元的 1 TB 硬盘，其每 GB 成本为 0.07 美元（即 75 美元 &#x2F; 1024 GB &#x3D; 0.0732 美元&#x2F;GB）。相比之下，一个价格为 150 美元的 3 TB 硬盘，其每 GB 成本为 0.05 美元（即 150 美元 &#x2F; 3072 GB &#x3D; 0.0488 美元&#x2F;GB）。在上述示例中，使用 1 TB 磁盘通常会使每 GB 成本增加 40%，从而使您的集群在成本效率上显著降低。不建议在单个 SAS&#x2F;SATA HDD 上托管多个 OSD。最好只托管一个 OSD 并直接访问磁盘（即 OSD 在非 LVM 上），因为 LVM 和其他抽象层可能会显着降低性能。 固态硬盘 (SSD): 使用固态硬盘 (SSD) 时，Ceph 的性能得到显著提升。SSD 减少了随机访问时间和延迟，同时增加了吞吐量。虽然 SSD 的每 GB 成本高于 HDD，但 SSD 通常提供至少比 HDD 快 100 倍的访问时间。SSD 可以避免热点问题和繁忙集群中的瓶颈问题，并且在综合评估总拥有成本 (TCO) 时，它们可能提供更好的经济效益。尤其是在给定 IOPS 数量的情况下，SSD 的摊销驱动器成本远低于 HDD。SSD 不会遭受旋转或寻道延迟，除了改进客户端性能外，它们还大幅提高了集群变化（包括 OSD 或监视器的增加、移除或故障）时的速度和对客户端的影响。由于 SSD 没有移动机械部件，因此它们不受 HDD 的许多限制。但 SSD 也有一些重要的限制。在评估 SSD 时，必须考虑顺序和随机读写的性能。 网络 Ceph 是一个高吞吐量、低延迟的分布式存储系统，对网络配置有较高的要求。集群网络的吞吐量和延迟对 Ceph 集群的性能有直接影响。因此，在配置 Ceph 网络时，应尽量选择高性能、低延迟的网络设备和网络拓扑。 对于 Ceph 集群的网络配置，建议使用 10GbE 或更高速率的网络设备，并配置两个网络，一个用于 Ceph 集群的内部通信（cluster network），一个用于 Ceph 客户端与 Ceph 集群之间的通信（public network）。这种网络配置可以有效地隔离 Ceph 集群的内部通信和外部通信，从而提高网络的性能和安全性。 在 1 Gb&#x2F;s 网络上复制 1 TB 数据需要 3 个小时，在 1 Gb&#x2F;s 网络上复制 10 TB 数据需要 30 个小时。但在 10 Gb&#x2F;s 网络上复制 1 TB 数据仅需 20 分钟，在 10 Gb&#x2F;s 网络上复制 10 TB 数据仅需 1 个小时。 最低硬件建议Ceph 可以运行在廉价的通用硬件上。小型生产集群和开发集群可以在配置适中的硬件上成功运行。正如我们之前提到的，当我们讨论 CPU 核心时，在启用超线程 (HT) 的情况下，我们指的是逻辑线程。每个现代物理 x64 CPU 核心通常提供两个逻辑 CPU 线程；其他 CPU 架构可能有所不同。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"Ceph体系架构","slug":"Storage/Ceph/Ceph体系架构","date":"2021-05-07T02:19:42.000Z","updated":"2024-07-27T14:30:54.244Z","comments":true,"path":"Storage/Ceph/Ceph体系架构/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/","excerpt":"","text":"Ceph 官方定义Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.(Ceph 是一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式的存储系统。) Ceph 设计思路 充分发挥存储设备自身的计算能力。 采用具有计算能力的设备作为存储系统的存储节点。 去除所有的中心点。 解决单点故障点和当系统规模扩大时出现的规模和性能瓶颈问题。 Ceph的设计哲学 每个组件必须可扩展 不存在单点故障 解决方案必须是基于软件的 可摆脱专属硬件的束缚即可运行在常规硬件上 推崇自我管理 Ceph体系结构首先作为一个存储系统，Ceph在物理上必然包含一个存储集群，以及这个存储集群的应用或客户端。Ceph客户端又需要一定的协议与Ceph存储集群进行交互，Ceph的逻辑层次演化如图所示。OSD：主要功能包括存储数据，处理数据的复制、恢复、回补、平衡数据分布，并将一些相关数据提供给Ceph Monitor。一个Ceph的存储集群，至少需要两个Ceph OSD来实现active+clean健康状态和有效的保存数据的双副本。一旦应用程序向ceph集群发出写操作，数据就以对象的形式存储在OSD中，OSD是Ceph集群中存储实际用户数据的唯一组件。通常，一个OSD守护进程绑定到集群中的一个物理磁盘。因此，通常来说，Ceph集群中物理磁盘的总数与在每个物理磁盘上存储用户数据的OSD守护进程的总数相同。 MON：Ceph的监控器，主要功能是维护整个集群健康状态，提供一致性的决策。 MDS：主要保存的是Ceph文件系统的元数据。（Ceph的块存储和对象存储都不需要Ceph MDS） RADOS：Ceph基于可靠的、自动化的、分布式的对象存储(Reliabl,Autonomous,Distributed Object Storage, RADOS )提供了一个可无限扩展的存储集群，RADOS是Ceph最为关键的技术，它是一个支持海量存储对象的分布式对象存储系统。RADOS层本身就是一个完整的对象存储系统，事实上，所有存储在Ceph系统中的用户数据最终都是由这一层来存储。RADOS层确保数据始终保持一致，他执行数据复制、故障检测和恢复，以及跨集群节点的数据迁移和再平衡。 RADOS集群主要由两种节点组成：为数众多的OSD，负责完成数据存储和维护；若干个Monitor，负责完成系统状态检测和维护。OSD和Monion之间互相传递节点的状态信息，共同得出系统的总体运行状态，并保存在一个全局数据结构中，即所谓的集群运行图(Cluster Map )里。集群运行图与RADOS提供的特定算法相配合，便实现了Ceph的许多优秀特性。 Librados：Librados库实际上是对RADOS进行抽象和封装，并向上层提供API，支持PHP、Ruby、Java、Python、C和C++编程语言。它为Ceph存储集群（RADOS）提供了本机接口，并为其他服务提供基础，如RBD、RGW和CephFS，这些服务构建在Librados之上，Librados还支持从应用程序直接访问RADOS，没有HTTP开销。 RBD：RBD提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建存储卷，Red Hat已经将RBD驱动集成在QEMU&#x2F;KVM中，以提高虚拟机的访问性能。 RADOS GW：Ceph对象网关RADOS GW提供对象存储服务，是一个构建在Librados库之上的对象存储接口，为应用访问Ceph集群提供了一个与Amazon S3和OpenStack Swift兼容的RESTful风格的 网关。 Ceph FS：Ceph文件系统提供了一个符合posix标准的文件系统，它使用Ceph存储集群在文件系统上存储用户数据。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"ansible搭建Ceph集群","slug":"Storage/Ceph/Ceph集群搭建","date":"2021-05-05T14:08:55.000Z","updated":"2024-07-29T14:45:54.603Z","comments":true,"path":"Storage/Ceph/Ceph集群搭建/","permalink":"https://watsonlu6.github.io/Storage/Ceph/Ceph%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","excerpt":"","text":"环境准备共计20台服务器，单台前置热插拔硬盘位10T * 10（业务盘）+2T * 2（系统盘）或10T * 10（业务盘）+4T * 2（系统盘），其中两块系统盘配置为RAID 1，安装CentOS 7系统，分布如下图。 Ceph01 - Ceph20依次对应IP地址：172.25.7.201 - 172.25.7.220其中，Ceph01、Ceph02、Ceph03、Ceph11、Ceph12为Monitor节点，其余为OSD节点。业务盘不配置RAID，每块业务盘作为一个OSD，资源池数据使用单副本。 环境配置hosts配置 （所有节点执行）hostnamectl –static set-hostname 节点对应主机名echo -e “127.0.0.1\\tlocalhost localhost.localdomain localhost4 localhost4.localdomain4::1\\tlocalhost localhost.localdomain localhost6 localhost6.localdomain6172.25.7.201\\tceph-node1-mon1172.25.7.202\\tceph-node2-mon2172.25.7.203\\tceph-node3-mon3172.25.7.204\\tceph-node4172.25.7.205\\tceph-node5172.25.7.206\\tceph-node6172.25.7.207\\tceph-node7172.25.7.208\\tceph-node8172.25.7.209\\tceph-node9172.25.7.210\\tceph-node10172.25.7.211\\tceph-node11-mon4172.25.7.212\\tceph-node12-mon5172.25.7.213\\tceph-node13172.25.7.214\\tceph-node14172.25.7.215\\tceph-node15172.25.7.216\\tceph-node16172.25.7.217\\tceph-node17172.25.7.218\\tceph-node18172.25.7.219\\tceph-node19172.25.7.220\\tceph-node20” &gt; &#x2F;etc&#x2F;hosts 配置SSH免密登陆（在ceph-ansible节点上执行）ssh-keygen -t rsassh-copy-id root@ceph-node1-mon1ssh-copy-id root@ceph-node2-mon2ssh-copy-id root@ceph-node3-mon3ssh-copy-id root@ceph-node4ssh-copy-id root@ceph-node5ssh-copy-id root@ceph-node6ssh-copy-id root@ceph-node7ssh-copy-id root@ceph-node8ssh-copy-id root@ceph-node9ssh-copy-id root@ceph-node10ssh-copy-id root@ceph-node11-mon4ssh-copy-id root@ceph-node12-mon5ssh-copy-id root@ceph-node13ssh-copy-id root@ceph-node14ssh-copy-id root@ceph-node15ssh-copy-id root@ceph-node16ssh-copy-id root@ceph-node17ssh-copy-id root@ceph-node18ssh-copy-id root@ceph-node19ssh-copy-id root@ceph-node20验证各节点ssh是否能免密登陆 关闭SELINUX和防火墙（所有节点执行）sed -i “s&#x2F;SELINUX&#x3D;enforcing&#x2F;SELINUX&#x3D;disabled&#x2F;g” &#x2F;etc&#x2F;selinux&#x2F;configsystemctl stop firewalldsystemctl disable firewalldsystemctl status firewalldreboot 配置时间同步所有节点执行yum -y install ntp ntpdatecd &#x2F;etc &amp;&amp; mv ntp.conf ntp.conf.bak 在ceph-ansible节点执行编辑ntpd配置文件vi &#x2F;etc&#x2F;ntp.conf 12345restrict 127.0.0.1restrict ::1restrict 172.25.7.0 mask 255.255.255.0server 127.127.1.0fudge 127.127.1.0 stratum 8 启动ntpd 12systemctl start ntpdsystemctl enable ntpd 其余节点执行编辑ntp服务vi &#x2F;etc&#x2F;ntp.conf 1server 172.25.7.201 启动ntp，同步时间 123456ntpdate ceph-node1-mon1hwclock -wcrontab -e#键入*/10 * * * * /usr/sbin/ntpdate 172.25.7.201 配置Ceph源(所有节点)编辑ceph源 1234567891011121314151617181920212223242526272829vi /etc/yum.repos.d/ceph.repo#键入[Ceph]name=Ceph packages for $basearchbaseurl=http://download.ceph.com/rpm-nautilus/el7/$basearchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://download.ceph.com/rpm-nautilus/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=http://download.ceph.com/rpm-nautilus/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1 更新ceph源 12yum -y install epel-releaseyum clean all &amp;&amp; yum makecache Ceph-ansible配置（仅在ceph-ansible节点安装）安装ansible安装ansible，并修改&#x2F;etc&#x2F;ansible&#x2F;hostsyum -y install ansible注意对应的版本号参考官网文档：https://docs.ceph.com/projects/ceph-ansible/en/latest/ 检测是否成功安装ansibleansible --version 修改&#x2F;etc&#x2F;ansible&#x2F;hosts 1234567891011121314151617181920212223242526272829303132333435363738394041424344vi /etc/ansible/hosts#添加以下内容[mons]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node11-mon4ceph-node12-mon5[mgrs]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node11-mon4ceph-node12-mon5[osds]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node4ceph-node5ceph-node6ceph-node7ceph-node8ceph-node9ceph-node10ceph-node11-mon4ceph-node12-mon5ceph-node13ceph-node14ceph-node15ceph-node16ceph-node17ceph-node18ceph-node19ceph-node20[grafana-server]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node11-mon4ceph-node12-mon5 测试ansible是否能正常运行：ansible all -m ping 安装ceph-absible确保环境上安装了git，可通过一下方式安装 yum -y install git 配置“http.sslVerify”参数为“false”，跳过系统证书。 git config --global http.sslVerify false 下载Ceph-ansible，注意ceph N版的版本号是stable-4.0 git clone -b stable-4.0 https://github.com/ceph/ceph-ansible.git --recursive 安装 Ceph-ansible 依赖 1234yum install -y python-pip #安装python-pippip install --upgrade pip # 将pip更新到最新版本cd /root/ceph-ansible/ #进入ceph-ansible目录pip install -r requirements.txt #检查并安装需要的软件版本 在ceph-ansible目录内新建hosts文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344vi /root/ceph-ansible/hosts[mons]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node11-mon4ceph-node12-mon5[mgrs]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node11-mon4ceph-node12-mon5[osds]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node4ceph-node5ceph-node6ceph-node7ceph-node8ceph-node9ceph-node10ceph-node11-mon4ceph-node12-mon5ceph-node13ceph-node14ceph-node15ceph-node16ceph-node17ceph-node18ceph-node19ceph-node20[grafana-server]ceph-node1-mon1ceph-node2-mon2ceph-node3-mon3ceph-node11-mon4ceph-node12-mon5 使用Ceph-ansible提供的ansible变量用来设置ceph集群的配置。所有选项及默认配置放在group_vars目录下，每种ceph进程对应相关的配置文件。 1234567cp mons.yml.sample mons.ymlcp mgrs.yml.sample mgrs.ymlcp mdss.yml.sample mdss.ymlcp rgws.yml.sample rgws.ymlcp osds.yml.sample osds.ymlcp clients.yml.sample clients.ymlcp all.yml.sample all.yml 修改group_vars&#x2F;all.yml文件（注意网络接口） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465vi group_vars/all.ymlceph_origin: repositoryceph_repository: communityceph_mirror: http://download.ceph.comceph_stable_release: nautilusceph_stable_repo: &quot;&#123;&#123; ceph_mirror &#125;&#125;/rpm-&#123;&#123; ceph_stable_release &#125;&#125;&quot;ceph_stable_redhat_distro: el7journal_size: 5120monitor_interface: p1p1public_network: &quot;172.25.7.0/24&quot;cluster_network: &quot;172.25.7.0/24&quot;mon_host: 172.25.7.201, 172.25.7.202, 172.25.7.203, 172.25.7.211, 172.25.7.212osd_objectstore: bluestoredashboard_enabled: Truedashboard_protocol: httpdashboard_port: 8443dashboard_admin_user: admindashboard_admin_password: admingrafana_admin_user: admingrafana_admin_password: admingrafana_uid: 472grafana_datasource: Dashboardgrafana_dashboard_version: nautilusgrafana_port: 3000grafana_allow_embedding: Truegrafana_crt: &#x27;&#x27;grafana_key: &#x27;&#x27;grafana_container_image: &quot;grafana/grafana:5.2.4&quot;grafana_container_cpu_period: 100000grafana_container_cpu_cores: 2grafana_container_memory: 4grafana_dashboards_path: &quot;/etc/grafana/dashboards/ceph-dashboard&quot;grafana_dashboard_files: - ceph-cluster.json - cephfs-overview.json - host-details.json - hosts-overview.json - osd-device-details.json - osds-overview.json - pool-detail.json - pool-overview.json - radosgw-detail.json - radosgw-overview.json - rbd-overview.jsongrafana_plugins: - vonage-status-panel - grafana-piechart-panelprometheus_container_image: &quot;prom/prometheus:v2.7.2&quot;prometheus_container_cpu_period: 100000prometheus_container_cpu_cores: 2prometheus_container_memory: 4prometheus_data_dir: /var/lib/prometheusprometheus_conf_dir: /etc/prometheusprometheus_user_id: &#x27;65534&#x27; prometheus_port: 9092ceph_conf_overrides: global:osd_pool_default_pg_num: 64 osd_pool_default_pgp_num: 64 osd_pool_default_size: 2 mon: mon_allow_pool_create: true 修改group_vars&#x2F;osds.yml文件在osds.yml添加以下内容 12345678910111213vi group_vars/osds.ymldevices: - /dev/sda - /dev/sdb - /dev/sdc - /dev/sdd - /dev/sde - /dev/sdf - /dev/sdg - /dev/sdh - /dev/sdi - /dev/sdj 修改site.yml文件 12cp site.yml.sample site.ymlvi site.yml Ceph 集群部署执行命令：ansible-playbook -i hosts site.yml执行结束，在执行页面会有相关的提示，如图所示，所有节点显示failed&#x3D;0，则处于部署过程中。 如果是过程出错，先清空集群，在进行部署 12cp infrastructure-playbooks/purge-cluster.yml purge-cluster.yml # 必须copy到项目根目录下ansible-playbook -i hosts purge-cluster.yml Rados性能测试工具创建pool ceph osd pool create testbench 100 100清除缓存 echo 3 &gt; /proc/sys/vm/drop_caches 4M写入测试 rados bench -p testbench 180 write -t 32 --no-cleanup 4k写入测试 rados bench -p testbench 180 write -t 32 -b 4096 --no-cleanup 4K顺序读 rados bench -p testbench 180 seq -t 32 --no-cleanup 4K随机读 rados bench -p testbench 180 rand -t 32 --no-cleanup 清除数据 rados -p testbench cleanup 参数说明格式：rados bench -p -b -t –no-cleanup pool-name：测试存储池名称 seconds：测试时间，单位秒 mode：操作模式，write：写，seq：顺序读；rand：随机读 -b：block size，块大小，默认为 4M,单位字节，只有在写的时候有效。 -t：读&#x2F;写并行数，默认为 16 –no-cleanup 表示测试完成后不删除测试用数据。注意：在测试之前要执行一次命令加–no-cleanup产生数据 部署过程的问题 执行完ansible-playbook -i hosts site.yml 命令后，前面无报错但某些节点不正常解决方法：属于正常现象，再执行ansible-playbook -i hosts site.yml命令可显示正常状态。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"}]},{"title":"块存储_文件系统存储_对象存储的区别","slug":"Storage/块存储-文件存储-对象存储的区别","date":"2021-04-23T08:29:15.000Z","updated":"2024-07-27T14:30:41.200Z","comments":true,"path":"Storage/块存储-文件存储-对象存储的区别/","permalink":"https://watsonlu6.github.io/Storage/%E5%9D%97%E5%AD%98%E5%82%A8-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"定义角度 块存储指以扇区为基础，一个或者连续的扇区组成一个块，也叫物理块。它是在文件系统与块设备(例如：磁盘驱动器)之间。利用多个物理硬盘的并发能力。关注的是写入偏移位置。 文件系统存储文件系统存储也称为文件级存储或基于文件的存储，数据会以单条信息的形式存储在文件夹中。当需要访问该数据时，计算机需要知道相应的查找路径，存储在文件中的数据会根据数量有限的元数据来进行整理和检索，这些元数据会告诉计算机文件所在的确切位置。它就像是数据文件的库卡目录。 对象存储对象存储，也称为基于对象的存储，是一种扁平结构，其中的文件被拆分成多个部分并散布在多个硬件间。在对象存储中，数据会被分解为称为“对象”的离散单元，并保存在单个存储库中，而不是作为文件夹中的文件或服务器上的块来保存。 使用角度 块存储生活中常见的块存储设备（也叫“块设备”）比如，插在你本地电脑上的U盘、硬盘，你电脑连接的iSCSI等。从使用上来说，块级的存储如果是第一次使用，那么必须需要进行一次格式化的操作，创建出一个文件系统，然后才可以使用。例如新买的U盘、硬盘、或者新发现的iSCSI设备等，首次使用的时候都需要进行一次格式化操作，创建出一个文件系统，然后才可以将你的文件拷贝到U盘、硬盘、或者新发现的iSCSI设备中。 文件系统存储文件系统存储是最常见的一种文件内系统，我们日常对操作系使用中，基本上能够直接接触到的都就是这种，能够直接访问的C、D、E盘，电脑里的一个目录，网上邻居的空间都是文件级的存储。块级的存储设备经过格式化以及挂载（win 会自动挂载）之后，你就将一个块级的存储变成了文件级的存储。 对象存储对象存储一般来说并不是给我们人直接去使用的，从使用者角度来说，它更适合用用于给程序使用。平时最常见的一般就是百度网盘，其后端对接的就是对象存储。还有就网页上的图片、视频，其本身也是存储在对象存储的文件系统中的。如果要直接使用对象级的存储，你会发现对象级的存储本身是非常的简单的（但是对人来说不方便），它只有简单的几种命令如上传、下载、删除，并且你只需要知道某个文件的编号（如：”d5t35e6tdud725dgs6u2hdsh27dh27d7” 这不是名字）就可以直接对它进行上传、下载、删除等操作，不需要像文件级那样，直到文件的具体的路径（如:D:\\photo\\1.jpg），并且他也只有这几种操作，如果你想编辑文件，那只能将文件下载下来编辑好之后在进行上传（这也是它对人来说不方便的原因之一） 技术角度块级、文件级、对象级技术上的区别，首先要明白两个概念第一，无论是那个级别的存储系统，其数据都是会存储在物理的存储设备上的，这些存储设备现在常见的基本上就两种机械硬盘、固态硬盘。第二，任何数据都是由两部”数据“分组成的，一部分是”数据本身”(下文中“数据”指”数据本身“)，另一部分就是这些“数据”的”元数据“。所谓的”元数据”就是用来描述”数据”的”数据”。包括数据所在的位置，文件的长度（大小），文件的访问权限、文件的时间戳（创建时间、修改时间….），元数据本身也是数据。 块存储对于块级来说，如果要通过块设备来访问一段数据的话，你自己需要知道这些数据具体是存在于那个存储设备上的位置上，例如如果你要从块设备上读取一张照片，你就要高速存储设备：我要从第2块硬盘中的从A位置开始到B位置的数据，硬盘的驱动就会将这个数据给你。读取照片的过程中照片的具体位置就是元数据，也就是说块级的存储中要求程序自己保存元数据。 文件系统存储如果需要自己保存元数据的话就太麻烦了，上文也说了，元数据本身也是数据，实际上元数据也是存储在硬盘上的，那么如何访问元数据这个数据呢其实，文件级的元数据是存储在固定位置的，存储的位置和方式是大家事先约定好的，这个约定就叫做文件系统，例如EXT4、FAT32、XFS、NTFS等。借助于这些约定，我们就不用自己去维护一个表去记录每一份数据的具体存储位置了。我们只需要直到我们存储的文件的路径和名字就好了，例如我们想要 D:\\1.jpg 这个文件，那么你只需要告诉文件系统 D:\\1.jpg 这个位置就可以了，去硬盘的哪里找D:\\1.jpg 数据的真身，就是文件系统的工作了 对象存储对象级存储，文件级的元数据实际上是和数据放在一起的，就像一本书每本书都有一个目录，这个目录描述的是这本书上内容的索引，目录就是书内容的“元数据”，而对象存储，会有一本书只放目录（元数据），其他更多的书只有内容，并且内容都是被拆分好的一段一段的，就是说你会看每本书上面的内容完全是混在在一起的，这一页的前两行是书A的某句话，后面就跟的是书D的某句话，如果只放目录（元数据）那本书，你根本不知道这里写的是啥。对象及存储将一切的文件都视作对象，并且将对象按照固定的”形式”组合或拆分的存储在存储设备中，并且将数据的元数据部分完全的独立出来，进行单独的管理。 对比 从距离（io路径） 上来说（相对于传统的存储），块存储的使用者距离最底层实际存储数据的存储设备是最近的，对象级是最远的。 从使用上来说，块存储需要使用者自己直到数据的真是位置，需要自己管理记录这些数据，所以使用上是最复杂的，而对象存储的接口最简单，基本上只有上传、下载、删除，并且不需要自己保存元数据，也不需要直到文件的索引路径，所以使用上是最简单的。但是从方便角度来讲还是文件存储最方便。 从性能上来说，综合的来讲（在特定的应用场合）性能最好的是块存储，它主要用在数据库、对延时要求非常高的场景中，对象存储多用于互联网，因为扩展性好，容量可以做的非常的大。对于人类来说，如果不借助特定的客户端、APP，使用文件存储是最友好最简单的。 应用场景 块存储： 要求高性能的应用，如数据库需要高IO，用块存储比较合适。 文件系统存储： 需局域网共享的应用，如文件共享，视频处理，动画渲染&#x2F;高性能计算。 对象存储： 互联网领域的存储，如点播&#x2F;视频监控的视频存储、图片存储、网盘存储、静态网页存储等，以及异地备份存储&#x2F;归档等。 为什么块级的存储性能最好&emsp;&emsp;首先要明确一点，要明确，每次在发生数据读取访问的时候，实际上对应系统的底层是发生了多次IO的（主要是要对元数据进行访问），例如，你要打开文件1.txt ，操作系统回去进行文件是否存在的查询，以及读写权限的查询等操作，这些操作实际上都是对于元数据的访问。&emsp;&emsp;然后，相对于其它的存储方式，块存储的元数据是有操作系统自己管理的，也就是说整个文件系统（元数据）是存在在操做系统的内存中的，这样操作系统在进行元数据管理的时候可以直和自己的内存打交道。而文件系统存储和对象存储，它的文件系统是存在于另一台服务器上的，这样在进行元数据访问时就需要从网络进行访问，这样要比从内存访问慢得多。&emsp;&emsp;总结来讲，就是块级存储的元数据在系统本机中，在进行元数据访问（每次读写文件实际都会在操作系统底层发生），会更快，因为其它的级别的存储元数据都要通过网络访问。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"云存储概述","slug":"Storage/云存储概述","date":"2021-04-18T15:09:26.000Z","updated":"2024-07-27T14:30:28.553Z","comments":true,"path":"Storage/云存储概述/","permalink":"https://watsonlu6.github.io/Storage/%E4%BA%91%E5%AD%98%E5%82%A8%E6%A6%82%E8%BF%B0/","excerpt":"","text":"云存储概述 云存储的概述云存储是指通过网络，将分布在不同地方的多种存储设备，通过应用软件集合起来共同对外提供数据存储和业务访问功能的一个系统。云存储是云计算系统中的一种新型网络存储技术。云存储对外提供的存储能力以统一、简单的数据服务接口来提供和展现。用户不用关心数据具体存放在哪个设备、哪个区域，甚至不知道数据到底是怎么保存的，他们只需要关心需要多少存储空间、什么时间能够拿到数据、数据存放的安全性和可用性如何即可。 云存储的实现模式云存储的实现模式有多种，云存储的架构可以由传统的存储架构延伸而来，也可以采用全新的云计算架构。云存储的实现可以是软件的，也可以是硬件的。云存储的实现模式可以是块存储、文件存储和对象存储。 块存储 块存储：最接近底层的存储，可以对数据进行任意格式化操作，可以在上面运行数据库等性能要求高的应用。可以给虚拟机使用。 文件存储 文件存储：对数据以文件的形式进行存储，支持复杂的文件操作，适合共享文件和协作工作。典型应用场景：NAS。 对象存储 对象存储：将数据以对象形式存储，通过唯一的对象ID进行访问，适合存储大规模非结构化数据，支持RESTful API接口。典型应用场景：云存储服务，视频、图片存储。 为什么需要多元化存储？由于不同的应用场景对存储的需求不同，单一的存储类型无法满足所有需求。大规模存储系统需要支持多种存储类型和多种存储协议，比如NFS、iSCSI、HDFS、S3等。多元化存储可以更好地适应各种应用场景，提高存储系统的灵活性和适应性。 文件如何通过分布式存储在许多服务器中分布式存储系统将数据分散存储在多个物理设备上。文件被切分成多个小块，存储在不同的服务器上。通过分布式哈希表（DHT）等算法确定数据块的位置，实现数据的快速定位和访问。通过数据复制和纠删码技术提高数据的可靠性和可用性。 文件被读取时如何快速找到数据块，确保大目标的组合数据不会丢失？元数据服务器（MDS）存储文件系统的元数据，包括文件名、文件大小、数据块位置等信息。客户端请求文件时，首先查询MDS获取元数据，然后根据元数据访问对应的数据块。分布式文件系统中常用的元数据管理技术包括分布式哈希表（DHT）、目录树、名称节点（NameNode）等。 如果文件丢失怎么办？由于分布式存储系统的特性，单一数据副本的丢失不会导致数据不可恢复。分布式存储系统采用数据冗余和副本机制，常见的冗余技术包括数据复制和纠删码。数据复制是将同一份数据存储在多个节点上，副本数通常为3个或更多。纠删码是一种冗余编码技术，通过增加校验数据，在数据块丢失的情况下，可以通过校验数据恢复原始数据。分布式存储系统在后台自动检测数据块的健康状态，发现数据丢失或损坏时，自动启动数据恢复机制，确保数据的完整性和可用性。 存储系统的数据可靠性（就像RAID）以及可用性（如高可用性）是如何解决的？存储系统的数据可靠性和可用性通过多种技术手段来保证。RAID技术通过数据条带化、镜像、奇偶校验等方法，提高单一存储设备的数据可靠性。分布式存储系统通过数据复制和纠删码技术，在多个节点上存储数据副本，提高数据的可靠性和可用性。高可用性通过冗余设计实现，常见的高可用架构包括双机热备、集群等。通过负载均衡技术，将用户请求分散到多个节点上，提高系统的可用性。 写入的数据是如何被保护的？数据写入时，采用多副本机制，确保数据的一致性和可靠性。写时复制（Copy-On-Write，COW）是一种常见的技术，通过在写入数据前复制一份旧数据，确保数据写入过程中的一致性。在分布式存储系统中，数据写入时，通常会先写入多个副本，只有所有副本写入成功后，才算写入成功。数据写入过程中的故障检测和处理机制，确保数据的可靠性和一致性。 多人多设备协作时，如何保证远程协作时数据的一致性？分布式存储系统通过分布式一致性协议（如Paxos、Raft）确保数据的一致性。在多个节点之间进行数据写入时，一致性协议保证数据的一致性和正确性。冲突检测和处理机制，在多人协作时，检测并解决数据冲突。分布式锁和事务机制，确保数据的一致性和完整性。 节省存储空间存储系统采用数据压缩和数据去重技术，减少存储空间占用。数据压缩通过减少数据的冗余，提高存储空间的利用率。数据去重通过检测和删除重复数据，节省存储空间。在大规模存储系统中，数据压缩和去重技术可以显著降低存储成本，提高存储效率。 避免存储固定的文件存储系统采用分级存储和冷热数据分离策略，提高存储资源的利用率。根据数据的访问频率和重要性，将数据存储在不同的存储介质上。频繁访问的数据存储在高速存储设备上，减少访问延迟。较少访问的数据存储在低成本存储设备上，降低存储成本。通过冷热数据分离，优化存储资源的使用，提高存储系统的性能和效率。 IO速度要有保证存储系统通过多种技术手段保证IO速度。使用高速缓存技术，将热点数据缓存到内存或SSD中，减少数据访问延迟。采用预取技术，在数据请求到达前提前加载数据，提高数据访问速度。使用QoS（Quality of Service）技术，为不同的应用场景和用户提供不同的IO优先级和带宽保障。负载均衡技术，将IO请求分散到多个存储节点上，避免单点瓶颈，提高IO性能。 版本控制存储系统提供版本控制功能，允许用户对数据进行版本管理。在数据修改前，保存一份旧版本的数据，用户可以根据需要回滚到旧版本。版本控制功能确保数据的可追溯性和可恢复性，防止数据丢失和误操作。在分布式存储系统中，版本控制功能通过元数据管理和数据快照技术实现。元数据管理记录数据的版本信息和变更历史，数据快照技术保存数据的不同版本。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"Linux存储栈","slug":"Storage/Linux存储栈","date":"2021-04-06T12:24:56.000Z","updated":"2024-07-27T03:55:23.469Z","comments":true,"path":"Storage/Linux存储栈/","permalink":"https://watsonlu6.github.io/Storage/Linux%E5%AD%98%E5%82%A8%E6%A0%88/","excerpt":"","text":"Linux存储栈Linux存储系统包括用户接口和存储设备接口两个部分，前者以流形式处理数据，后者以块形式处理数据，文件系统在中间起到承上启下的作用。应用程序通过系统调用发出写请求，文件系统定位请求位置并转换成块设备所需的块，然后发送到设备上。内存在此过程中作为磁盘缓冲，将上下两部分隔离成异步运行的两个过程，避免频繁的磁盘同步。当数据需要从页面缓存同步到磁盘时，请求被包装成包含多个bio的request，每个bio包含需要同步的数据页。磁盘在执行写操作时，需要通过IO请求调度合理安排顺序，减少磁头的频繁移动，提高磁盘性能。 用户视角的数据流接口 应用程序通过系统调用（如write、read等）与操作系统交互。这些调用使得数据以流的形式被处理。 存储设备的块接口 数据在底层存储设备（如硬盘、SSD等）中以块（通常是512字节或4096字节）为单位进行读写操作。 文件系统的中间角色 位置定位：文件系统负责将用户的读写请求定位到存储设备的具体块位置。 数据转换：将数据流转换为存储设备所需的块结构，并将这些块组织成bio（block I&#x2F;O）请求。 内存作为缓冲 页面缓存：内存中的页面缓存（Page Cache）用于暂时存储数据，以减少频繁的磁盘I&#x2F;O操作。 异步运行：将用户操作与底层存储设备的实际写操作异步化，提升系统效率。对于用户态程序来说，数据尽量保留在内存中，这样可以减少频繁的数据同步。 I&#x2F;O请求调度 请求封装：从页面缓存同步到磁盘的请求被封装成request，每个request包含多个bio，而每个bio又包含具体的数据页。 调度策略：操作系统会对I&#x2F;O请求进行调度，优化执行顺序，尽量减少磁盘磁头的来回移动，提高磁盘的读写效率。 Linux数据写入流程 应用程序发出写请求：比如，应用程序通过write系统调用写入数据。 文件系统处理：文件系统接收请求，找到对应的文件位置，将数据写入页面缓存。 内存缓冲处理：数据暂存在内存的页面缓存中，以等待后续的写入操作。 请求调度与封装：页面缓存的数据需要同步到磁盘时，被封装成bio和request。 I&#x2F;O调度执行：调度器优化I&#x2F;O请求的执行顺序，减少磁头移动，提高写入效率。 数据写入磁盘：最终，数据从页面缓存同步到磁盘的指定位置，完成写操作。通过以上流程，Linux存储系统在保证数据一致性的同时，最大限度地提高了性能和效率。 系统调用&emsp;&emsp;用户应用程序访问并使用内核所提供的各种服务的途径即是系统调用。在内核和应用程序交叉的地方，内核提供了一组系统调用接口，通过这组接口，应用程序可以访问系统硬件和各种操作系统资源。用户可以通过文件系统相关的调用请求系统打开问价、关闭文件和读写文件。&emsp;&emsp;内核提供的这组系统调用称为系统调用接口层。系统调用接口把应用程序的请求传达给内核，待内核处理完请求后再将处理结果返回给应用程序。&emsp;&emsp;32位Linux，CPU能访问4GB的虚拟空间，其中低3GB的地址是应用层的地址空间，高地址的1GB是留给内核使用的。内核中所有线程共享这1GB的地址空间，而每个进程可以有自己的独立的3GB的虚拟空间，互不干扰。&emsp;&emsp;当一个进程运行的时候，其用到文件的代码段，数据段等都是映射到内存地址区域的，这个功能是通过系统调用mmap()来完成的。mmap()将文件（由文件句柄fd所指定）从偏移offset的位置开始的长度为length的一个块映射到内存区域中，从而把文件的某一段映射到进程的地址空间，这样程序就可以通过访问内存的方式访问文件了。与read()&#x2F;write()相比，使用mmap的方式对文件进行访问，带来的一个显著好处就是可以减少一次用户空间到内核空间的复制，在某些场景下，如音频、视频等大文件，可以带来性能的提升。 文件系统&emsp;&emsp;Linux文件系统的体系结构是一个对复杂系统进行抽象化，通过使用一组通用的API函数，Linux就可以在多种存储设备上支持多种文件系统，使得它拥有了与其他操作系统和谐共存的能力。&emsp;&emsp;Linux中文件的概念并不局限于普通的磁盘文件，而是由字节序列构成的信息载体，I&#x2F;O设备、socket等也被包括在内。因为有了文件的存在，所以需要衍生文件系统去进行组织和管理文件，而为了支持各种各样的文件系统，所以有了虚拟文件系统的出现。文件系统是一种对存储设备上的文件、数据进行存储和组织的机制。&emsp;&emsp;虚拟文件系统通过在各种具体的文件系统上建立了一个抽象层，屏蔽了不同文件系统间的差异，通过虚拟文件系统分层架构，在对文件进行操作时，便不需要去关心相关文件所在的具体文件系统细节。通过系统调用层，可以在不同文件系统之间复制和移动数据，正是虚拟文件系统使得这种跨越不同存储设备和不同文件系统的操作成为了可能。 虚拟文件系统象类型 超级块（Super Block）超级块对象代表了一个已经安装的文件系统，用于存储该文件系统的相关信息，如文件系统的类型、大小、状态等。对基于磁盘的文件系统， 这类对象通常存放在磁盘特定的扇区上。对于并非基于磁盘的文件系统，它们会现场创建超级块对象并将其保存在内存中。 索引节点（Inode）索引节点对象代表存储设备上的一个实际物理文件，用于存储该文件的有关信息。Linux将文件的相关信息（如访问权限、大小、创建时间等）与文件本身区分开。文件的相关信息又被称为文件的元数据。 目录项（Dentry) 目录项对象描述了文件系统的层次结构，一个路径的各个组成部分，不管是目录（虚拟文件系统将目录当作文件来处理）还是普通文件，都是一个目录项对象。 文件 文件对象代表已经被进程打开的文件，主要用于建立进程和文件之间的对应关系。它由open()系统调用创建，由close()系统调用销毁，当且仅当进程访问文件期间存在于内存中，同一个物理文件可能存在多个对应的文件对象，但其对应的索引节点对象是唯一的。 Page Cache&emsp;&emsp;Page Cache，通常也称为文件缓存，使用内存Page Cache文件的逻辑内容，从而提高对磁盘文件的访问速度。Page Cache是以物理页为单位对磁盘文件进行缓存的。&emsp;&emsp;应用程序尝试读取某块数据的时候，会首先查找Page Cache，如果这块数据已经存放在Page Cache中，那么就可以立即返回给应用程序，而不需要再进行实际的物理磁盘操作。如果不能在Page Cache中发现要读取的数据，那么就需要先将数据从磁盘读取到Page Cache中，同样，对于写操作来说，应用程序也会将数据写到Page Cache中，再根据所采用的写操作机制，判断数据是否应该立即被写到磁盘上 Direct I&#x2F;O和Buffered I&#x2F;O进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条 标准I&#x2F;O： 也称为Buffered I&#x2F;O；Linux会将I&#x2F;O的数据缓存在Page Cache中，也就是说，数据会先被复制到内核的缓冲区，再从内核的缓冲区复制到应用程序的用户地址空间。在Buffered I&#x2F;O机制中，在没有CPU干预的情况下，可以通过DMA操作在磁盘和Page Cache之间直接进行数据的传输，在一定程度上分离了应用程序和物理设备，但是没有方法能直接在应用程序的地址空间和磁盘之间进行数据传输，数据在传输过程中需要在用户空间和Page Cache之间进行多次数据复制操作，这将带来较大的CPU开销。 Direct I&#x2F;O： 可以省略使用Buffered I&#x2F;O中的内核缓冲区，数据可以直接在用户空间和磁盘之间进行传输，从而使得缓存应用程序可以避开复杂系统级别的缓存结构，执行自定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。如果在块设备中执行Direct I&#x2F;O，那么进程必须在打开文件的时候将对文件的访问模式设置为O_DIRECT，这样就等于告诉Linux进程在接下来将使用Direct I&#x2F;O方式去读写文件，且传输的数据不经过内核中的Page Cache。Direct I&#x2F;O最主要的优点就是通过减少内核缓冲区和用户空间的数据复制次数，降低文件读写时所带来的CPU负载能力及内存带宽的占用率。如果传输的数据量很大，使用Direct IO的方式将会大大提高性能。然而，不经过内湖缓冲区直接进行磁盘的读写，必然会引起阻塞，因此通常Direct IO和AIO（异步IO）一起使用。 块层（Block Layer）&emsp;&emsp;块设备访问时，需要在介质的不同区间前后移动，对于内核来说，管理块设备要比管理字符设备复杂得多。&emsp;&emsp;系统调用read()触发相应的虚拟文件系统函数，虚拟文件系统判断请求是否已经在内核缓冲区里，如果不在，则判断如何执行读操作。如果内核必须从块设备上读取数据，就必须要确定数据在物理设备上的位置。这由映射层，即磁盘文件系统来完成。文件系统将文件访问映射为设备访问。在通用块层中，使用bio结构体来描述一个I&#x2F;O请求在上层文件系统与底层物理磁盘之间的关系。而到了Linux驱动，则是使用request结构体来描述向块设备发出的I&#x2F;O请求的。对于慢速的磁盘而言，请求的处理速度很慢，这是内核就提供一种队列的机制把这些I&#x2F;O请求添加到队列中，使用request_queue结构体来描述。&emsp;&emsp;bio和request是块层最核心的两个数据结构，其中，bio描述了磁盘里要真实操作的位置和Page Cache中的映射关系。作为Linux I&#x2F;O请求的基本单元，bio结构贯穿块层对I&#x2F;O请求处理的始终，每个bio对应磁盘里面一块连续的位置，bio结构中的bio_vec是一个bio的数据容器，专门用来保存bio的数据，包含一块数据所在页，以及页内的偏移及长度信息，通过这些信息就可以很清晰地描述数据具体什么位置。request用来描述单次I&#x2F;O请求，request_queue用来描述与设备相关的请求队列，每个块设备在块层都有一个request_queue与之对应，所有对该块设备的I&#x2F;O请求最后都会流经request_queue。块层正是借助bio、bio_vec、request、request_queue这几个结构将I&#x2F;O请求在内核I&#x2F;O子系统各个层次的处理过程联系起来。 I&#x2F;O调度算法： noop算法（不调度算法）、deadline算法（改良的电梯算法）、CFQ算法（完全公平调度算法，对于通用的服务器来说，CFQ是较好的选择，从Linux2.6.18版本开始，CFQ成为了默认的IO调度算法）。 I&#x2F;O合并： 将符合条件的多个IO请求合并成单个IO请求进行一并处理，从而提升IO请求的效率。进程产生的IO路径主要有Direct I&#x2F;O和Buffered I&#x2F;O两条，无论哪一条路径，在bio结构转换为request结构进行IO调度前都需要进入Plug队列进行蓄流（部分Direct IO产生的请求不需要经过蓄流），所以对IO请求来说，能够进行合并的位置主要有Page Cache、Plug List、IO调度器3个。而在块层中，Plug将块层的IO请求聚集起来，使得零散的请求有机会进行合并和排序，最终达到高效利用存储设备的目的。每个进程都有一个私有的Plug队列，进程在向通用块层派发IO派发请求之前如果开始蓄流的功能，那么IO请求在被发送给IO调度器之前都被保存在Plug队列中，直到泄流的时候才被批量交给调度器。蓄流主要是为了增加请求合并的机会，bio在进入Plug队列之前会尝试与Plug队列保存的request进行合并。当应用需要发多个bio请求的时候，比较好的办法是先蓄流，而不是一个个单独发给最终的硬盘。 LVM&emsp;&emsp;LVM，即Logical Volume Manager，逻辑卷管理器，是一种硬盘的虚拟化技术，可以允许用户的硬盘资源进行灵活的调整和动态管理。&emsp;&emsp;LVM是Linux系统对于硬盘分区管理的一种机制，诞生是为了解决硬盘设备在创建分区后不易修改分区大小的缺陷。尽管对硬盘的强制性扩容和缩容理论上是可行的，但是却可能造成数据丢失。LVM技术是通过在硬盘分区和文件系统之间增加一个逻辑层，提供了一个抽象的卷组，就可以把多块硬盘设备、硬盘分区，甚至RAID整体进行卷则合并。并可以根据情况进行逻辑上的虚拟分割，这样一来，用户不用关心物理硬盘设备的底层架构和布局，就可以实现对硬盘分区设备的动态调整。&emsp;&emsp;LVM通过在操作系统与物理存储资源之间引入逻辑卷（Logical Volume）的抽象，来解决传统磁盘分区管理工具的问题。LVM将众多不同的物理存储器资源（物理卷，Physical Volume）组成卷组（Volume Group），该卷组可以理解为普通系统的物理磁盘，但是卷组上不能创建或者安装文件系统，而是需要LVM先在卷组中创建一个逻辑卷，然后将ext3等文件系统安装在这个逻辑卷上，可以在不重新引导系统的前提下通过在卷组划分额外的空间，来为这个逻辑卷动态扩容。LVM的架构体系中，有三个很重要的概念： PV，物理卷，即实际存在的硬盘、分区或者RAID VG，卷组，是由多个物理卷组合形成的大的整体的卷组 LV，逻辑卷，是从卷组上分割出来的，可以使用使用的逻辑存储设备 条带化&emsp;&emsp;大多数磁盘系统都对访问次数（每秒的 I&#x2F;O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）有限制。当达到这些限制时，后面需要访问磁盘的进程就需要等待，这时就是所谓的磁盘冲突。避免磁盘冲突是优化 I&#x2F;O 性能的一个重要目标，而 I&#x2F;O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别 ,I&#x2F;O 优化最有效的手段是将 I&#x2F;O 最大限度的进行平衡。条带化技术就是一种自动的将 I&#x2F;O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I&#x2F;O 并行能力，从而获得非常好的性能。很多操作系统、磁盘设备供应商、各种第三方软件都能做到条带化。","categories":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"}]},{"title":"个人感悟","slug":"个人感悟","date":"2021-04-06T08:18:06.000Z","updated":"2024-08-18T12:36:04.796Z","comments":true,"path":"个人感悟/","permalink":"https://watsonlu6.github.io/%E4%B8%AA%E4%BA%BA%E6%84%9F%E6%82%9F/","excerpt":"","text":"论学习 多花点时间在自己专业上，不要分心旁骛太多。 先做好本份工作，行有余力，再自学 。 及时总结心得经验。记录，总结做过的项目，能总结多少就多少。如果有扎实的理论基础，深刻的理解能力，坚强的毅力，无论什么新技术新东西都能很快学会，但最宝贵的东西——经验，解决问题的钥匙。是无法学会的，只能慢慢体会,慢慢总结。 技术不求新，涉及哪方面的技术&#x2F;知识，就去学习，想办法精通。先有深度，再求广度。 论生活 生活应该简单，但不应该单调。电影，音乐，都是不错的消遣。适当放松，有不足才有期待。 跑步能让我很开心，多去跑步。 别被一些俗事打乱了生活的节奏，要懂得把握生活的节奏。 每星期总结一周的活动。 维持7.5小时睡眠。 抓住该努力的时间去努力，该松弛的时候去松弛。 运动能使人变得开朗，有特长能使人变得自信。 论做人 自视高谓之骄，怀激愤谓之躁。做人应该尽量避免骄傲，浮躁。 要在3年达到别人10年才能达到的高度，就意味着3年内要承受10年的苦。请你衡量。 不要拿社会标准来衡量自己的价值，而应该用心灵的意义去裁决。 实验室很浮躁，越是浮躁的地方，越要扎实下来，厚积薄发。 调整自己的心态，不要太在乎一得一失。你目前所努力的，其实不是为了成功，而是为了当成功的机会来临时，你能把握住而已。为了成功和为了成功把握机会，两者的区别很重要。 做人应该谦虚，而且不要有自虐狂的心理，并不是每个人都会针对你。 高手是别人认为的。不是自封的，不要自大。 坚持理想，理想不难，难的是坚持 论心理 烦恼和焦虑：烦恼和焦虑表示你为一些事情担心着，而你目前并没有付出能够解决这些问题的努力。 宽容面对自己的缺陷和不足。 把自己的消极想法都记下，逐点去分析，攻破。分析问题所在，制定方法去解决（生活与心理都是） 知足常乐 论工作 对待补贴：不管导师发不发补贴，只要你还在这个实验室一天，就请努力专心的做事。 对待每一件事：努力认真去做好每一件事。只要你想做，总有法子可以做到的。 做好本分工作。技术人员有时候对PM分配自己一些较没技术含量的工作会很不满。记住，都是做事而已。无论怎样，分配到你的工作，请你做好。孔子尚且养过马，薛仁贵尚且当过伙头军。我认为，做好一个茶叶蛋，比做砸了原子弹更有意义。不以技术难度定优劣，都是做事而已。 团队的交流,配合开发： a. 别人的失误要及时指出，当然了，语气要婉转。这样才能调整进度，别发现了错误隐瞒不报。 b. 对于自己不懂的环节，勇敢承认自己的缺点，大胆去估计进度，并认真学习。 c. 如果可能，每天汇报进度，也许只是几分钟，让PM看看你的构思，你的代码，你的成果。PM是最熟悉业务的，他能指出你的程序流是否正确，页面是否恰当。根据这些，你和他才能估计进度，这样，整个项目的进度才算可控。 d. 学会承担更多的责任。把困难的任务交给你，通常意味着只有你才能完成。请你好好享受这种”唯一”的乐趣。 出了问题，努力想办法去解决。别人或者不在意你的能力，但很在意你的态度。 最佳印象 早到。早到就显得你很重视这份工作。 不要过于固执。工作时时在扩展，不要老是以“这不是我份内的工作”为由来逃避责任。当前额外的工作指派到你头上时，不妨视之为考验。 苦中求乐。不管你接受的工作多么艰巨，鞠躬尽瘁也要做好，千万别表现出你做不来或不知从何入手的样子。 立刻动手。接到工作要立刻动手，迅速准确及时完成，反应敏捷给人的印象是金钱买不到的。 谨言。职务上的机密必须守口如瓶。 听从上司的临时指派。上司的时间比你的时间宝贵，不管他临时指派了什么工作给你，都比你手头上的工作来得重要。 荣耀归于上司。即让上司在人前人后永远光鲜。 保持冷静。面对任何状况都能处之泰然的人，一开始就取得了优势。老板、客户不仅钦佩那些面对危机声色不变的人，更欣赏能妥善解决问题的人。 别存在太多的希望。千万别期盼所有的事情都会照你的计划而行。相反，你得时时为可能产生的错误做准备。 敢于做出果断决定。遇事犹豫不决或过度依赖他人意见的人，是一辈子注定要被打入冷宫的。 广收资讯。要想成为一个成功的人，光是从影音媒体取得资讯是不够的，多看报章杂志才是最直接的知识来源。","categories":[{"name":"个人生活","slug":"个人生活","permalink":"https://watsonlu6.github.io/categories/%E4%B8%AA%E4%BA%BA%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"个人生活","slug":"个人生活","permalink":"https://watsonlu6.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%94%9F%E6%B4%BB/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-04-01T15:09:26.000Z","updated":"2024-07-27T03:58:36.598Z","comments":true,"path":"hello-world/","permalink":"https://watsonlu6.github.io/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"云计算/libvirt","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/libvirt/"},{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"云存储/Ceph","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/Ceph/"},{"name":"存储基础","slug":"云存储/存储基础","permalink":"https://watsonlu6.github.io/categories/%E4%BA%91%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"},{"name":"个人生活","slug":"个人生活","permalink":"https://watsonlu6.github.io/categories/%E4%B8%AA%E4%BA%BA%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"libvirt","slug":"libvirt","permalink":"https://watsonlu6.github.io/tags/libvirt/"},{"name":"云存储","slug":"云存储","permalink":"https://watsonlu6.github.io/tags/%E4%BA%91%E5%AD%98%E5%82%A8/"},{"name":"Ceph","slug":"Ceph","permalink":"https://watsonlu6.github.io/tags/Ceph/"},{"name":"存储基础","slug":"存储基础","permalink":"https://watsonlu6.github.io/tags/%E5%AD%98%E5%82%A8%E5%9F%BA%E7%A1%80/"},{"name":"个人生活","slug":"个人生活","permalink":"https://watsonlu6.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%94%9F%E6%B4%BB/"}]}